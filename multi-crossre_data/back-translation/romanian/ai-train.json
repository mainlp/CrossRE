{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 19, "task"], [24, 26, "task"], [29, 30, "field"], [31, 35, "researcher"], [37, 39, "researcher"], [41, 42, "researcher"], [44, 45, "researcher"], [47, 49, "researcher"], [51, 52, "researcher"], [54, 55, "researcher"], [57, 58, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 19, "part-of", "", false, false], [3, 7, 19, 19, "usage", "", false, false], [3, 7, 29, 30, "part-of", "", false, false], [3, 7, 29, 30, "usage", "", false, false], [24, 26, 19, 19, "part-of", "", false, false], [24, 26, 19, 19, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "opinion", "-", "based", "recommender", "systems", "use", "various", "techniques", ",", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X", ".", "Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to opinion-based recommender systems use various techniques, including text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis) and deep learning X. Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 55], [56, 59], [60, 67], [68, 78], [78, 79], [80, 89], [90, 94], [95, 101], [101, 102], [103, 114], [115, 124], [124, 125], [126, 135], [136, 144], [145, 146], [146, 149], [150, 154], [155, 165], [166, 175], [176, 184], [184, 185], [186, 189], [190, 194], [195, 203], [204, 205], [205, 206], [207, 208], [208, 209], [210, 214], [214, 215], [216, 217], [217, 218], [219, 224], [224, 225], [226, 230], [231, 234], [234, 235], [236, 240], [241, 246], [246, 247], [248, 249], [249, 250], [251, 254], [254, 255], [256, 260], [261, 266], [266, 267], [268, 272], [273, 277], [277, 278], [279, 281], [282, 284], [284, 285], [286, 287], [287, 291], [291, 292], [292, 293], [293, 294], [295, 297], [298, 299], [299, 300], [300, 301], [301, 302], [303, 309], [309, 310]]}
{"doc_key": "ai-train-2", "ner": [[8, 8, "university"], [14, 15, "researcher"], [17, 18, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[14, 15, 8, 8, "physical", "", false, false], [14, 15, 8, 8, "role", "", false, false], [17, 18, 8, 8, "physical", "", false, false], [17, 18, 8, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Proponents", "of", "procedural", "representations", "were", "mainly", "concentrated", "at", "MIT", ",", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Proponents of procedural representations were mainly concentrated at MIT, under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 10], [11, 13], [14, 24], [25, 40], [41, 45], [46, 52], [53, 65], [66, 68], [69, 72], [72, 73], [74, 79], [80, 83], [84, 94], [95, 97], [98, 104], [105, 111], [112, 115], [116, 123], [124, 130], [130, 131]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "computer", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the computer interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 39], [40, 49], [50, 53], [54, 61], [62, 64], [65, 69], [69, 70]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [25, 25, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 25, 25, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "solve", "linear", "and", "nonlinear", "problems", "numerically", ",", "as", "well", "as", "perform", "other", "numerical", "experiments", ",", "using", "a", "program", "that", "is", "largely", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps solve linear and nonlinear problems numerically, as well as perform other numerical experiments, using a program that is largely compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 18], [19, 25], [26, 29], [30, 39], [40, 48], [49, 60], [60, 61], [62, 64], [65, 69], [70, 72], [73, 80], [81, 86], [87, 96], [97, 108], [108, 109], [110, 115], [116, 117], [118, 125], [126, 130], [131, 133], [134, 141], [142, 152], [153, 157], [158, 164], [164, 165]]}
{"doc_key": "ai-train-5", "ner": [[3, 6, "algorithm"], [11, 12, "misc"], [14, 15, "researcher"], [20, 22, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 6, 14, 15, "origin", "", false, false], [11, 12, 14, 15, "origin", "", false, false], [14, 15, 20, 22, "physical", "", false, false], [14, 15, 20, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", ",", "as", "well", "as", "unsupervised", "methods", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", ",", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", "{", "{", "read", "review"], "sentence-detokenized": "Variants of the back-propagation algorithm, as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto, can be used to train deep, highly nonlinear neural architectures {{read review", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [42, 43], [44, 46], [47, 51], [52, 54], [55, 67], [68, 75], [76, 78], [79, 84], [85, 91], [92, 95], [96, 106], [107, 109], [110, 113], [114, 124], [125, 127], [128, 135], [135, 136], [137, 140], [141, 143], [144, 148], [149, 151], [152, 157], [158, 162], [162, 163], [164, 170], [171, 180], [181, 187], [188, 201], [202, 203], [203, 204], [204, 208], [209, 215]]}
{"doc_key": "ai-train-6", "ner": [[5, 6, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", ",", "equivalently", ",", "using", "DCG", "notation", ":"], "sentence-detokenized": "or, equivalently, using DCG notation:", "token2charspan": [[0, 2], [2, 3], [4, 16], [16, 17], [18, 23], [24, 27], [28, 36], [36, 37]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 12, "algorithm"], [14, 17, "algorithm"], [20, 22, "algorithm"], [25, 25, "algorithm"], [27, 28, "algorithm"], [40, 44, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 12, "type-of", "", false, false], [0, 3, 14, 17, "usage", "part-of?", true, false], [14, 17, 20, 22, "compare", "", false, false], [25, 25, 20, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "apply", "competitive", "learning", ",", "as", "opposed", "to", "error", "-correction", "learning", "such", "as", "backpropagation", "with", "gradient", "descent", ",", "and", "in", "that", "they", "use", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps differ from other artificial neural networks in that they apply competitive learning, as opposed to error-correction learning such as backpropagation with gradient descent, and in that they use a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 84], [85, 96], [97, 105], [105, 106], [107, 109], [110, 117], [118, 120], [121, 126], [126, 137], [138, 146], [147, 151], [152, 154], [155, 170], [171, 175], [176, 184], [185, 192], [192, 193], [194, 197], [198, 200], [201, 205], [206, 210], [211, 214], [215, 216], [217, 229], [230, 238], [239, 241], [242, 250], [251, 254], [255, 266], [267, 277], [278, 280], [281, 284], [285, 290], [291, 296], [296, 297]]}
{"doc_key": "ai-train-8", "ner": [[10, 14, "organisation"], [27, 28, "misc"], [36, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "several", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", ",", "which", "is", "then", "filtered", "in", "the", "noise", "threshold", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "muting", "circuits", "."], "sentence-detokenized": "Since the early 1990s, several authorities, including the Audio Engineering Society, have recommended that dynamic range measurements be made in the presence of an audio signal, which is then filtered in the noise threshold measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or muting circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 30], [31, 42], [42, 43], [44, 53], [54, 57], [58, 63], [64, 75], [76, 83], [83, 84], [85, 89], [90, 101], [102, 106], [107, 114], [115, 120], [121, 133], [134, 136], [137, 141], [142, 144], [145, 148], [149, 157], [158, 160], [161, 163], [164, 169], [170, 176], [176, 177], [178, 183], [184, 186], [187, 191], [192, 200], [201, 203], [204, 207], [208, 213], [214, 223], [224, 235], [236, 240], [241, 243], [244, 253], [254, 261], [262, 267], [267, 268], [269, 273], [274, 280], [281, 293], [294, 306], [307, 312], [313, 315], [316, 319], [320, 323], [324, 326], [327, 332], [333, 338], [339, 341], [342, 348], [349, 357], [357, 358]]}
{"doc_key": "ai-train-9", "ner": [[5, 5, "misc"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [29, 29, "task"], [30, 34, "task"], [36, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[5, 5, 17, 18, "part-of", "concept_used_in", true, false], [5, 5, 20, 21, "part-of", "concept_used_in", false, false], [5, 5, 23, 24, "part-of", "concept_used_in", false, false], [5, 5, 26, 27, "part-of", "concept_used_in", false, false], [5, 5, 29, 29, "part-of", "concept_used_in", false, false], [5, 5, 30, 34, "part-of", "concept_used_in", false, false], [5, 5, 36, 38, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "techniques", "used", "in", "creating", "own", "faces", "and", "using", "them", "for", "recognition", "are", "also", "used", "outside", "of", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "The techniques used in creating own faces and using them for recognition are also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation and medical image analysis.", "token2charspan": [[0, 3], [4, 14], [15, 19], [20, 22], [23, 31], [32, 35], [36, 41], [42, 45], [46, 51], [52, 56], [57, 60], [61, 72], [73, 76], [77, 81], [82, 86], [87, 94], [95, 97], [98, 102], [103, 114], [114, 115], [116, 127], [128, 139], [139, 140], [141, 144], [145, 152], [152, 153], [154, 159], [160, 171], [171, 172], [173, 177], [178, 186], [186, 187], [187, 191], [192, 199], [200, 214], [215, 218], [219, 226], [227, 232], [233, 241], [241, 242]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [10, 14, "organisation"], [16, 16, "organisation"], [20, 23, "organisation"], [26, 33, "organisation"], [34, 37, "organisation"], [40, 44, "organisation"], [46, 46, "organisation"], [51, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 14, 0, 3, "part-of", "", false, false], [16, 16, 10, 14, "named", "", false, false], [20, 23, 0, 3, "part-of", "", false, false], [26, 33, 0, 3, "part-of", "", false, false], [34, 37, 0, 3, "part-of", "", false, false], [40, 44, 0, 3, "part-of", "", false, false], [46, 46, 40, 44, "named", "", false, false], [51, 54, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "has", "been", "an", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "U.S.", "Department", "of", "Energy", ",", "the", "U.S.", "Department", "of", "Commerce", ",", "NIST", ",", "the", "U.S.", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", ",", "and", "the", "Office", "of", "Naval", "Research", "coordinated", "studies", "to", "inform", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation has been an umbrella for the National Aeronautics and Space Administration (NASA), the U.S. Department of Energy, the U.S. Department of Commerce, NIST, the U.S. Department of Defense, the Defense Advanced Research Projects Agency (DARPA), and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 40], [41, 43], [44, 52], [53, 56], [57, 60], [61, 69], [70, 81], [82, 85], [86, 91], [92, 106], [107, 108], [108, 112], [112, 113], [113, 114], [115, 118], [119, 123], [124, 134], [135, 137], [138, 144], [144, 145], [146, 149], [150, 154], [155, 165], [166, 168], [169, 177], [177, 178], [179, 183], [183, 184], [185, 188], [189, 193], [194, 204], [205, 207], [208, 215], [215, 216], [217, 220], [221, 228], [229, 237], [238, 246], [247, 255], [256, 262], [263, 264], [264, 269], [269, 270], [270, 271], [272, 275], [276, 279], [280, 286], [287, 289], [290, 295], [296, 304], [305, 316], [317, 324], [325, 327], [328, 334], [335, 344], [345, 353], [354, 356], [357, 362], [363, 376], [376, 377]]}
{"doc_key": "ai-train-11", "ner": [[5, 9, "metrics"], [10, 11, "algorithm"], [15, 16, "researcher"], [21, 21, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 9, 10, 11, "part-of", "", false, false], [15, 16, 21, 21, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "rapid", "method", "for", "calculating", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "as", "an", "appendix", "to", "Bliss", "'s", "paper", "in", "1935", "."], "sentence-detokenized": "A rapid method for calculating maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an appendix to Bliss's paper in 1935.", "token2charspan": [[0, 1], [2, 7], [8, 14], [15, 18], [19, 30], [31, 38], [39, 49], [50, 59], [60, 63], [64, 67], [68, 74], [75, 80], [81, 84], [85, 93], [94, 96], [97, 103], [104, 110], [111, 113], [114, 116], [117, 125], [126, 128], [129, 134], [134, 136], [137, 142], [143, 145], [146, 150], [150, 151]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [13, 15, "product"], [19, 19, "organisation"], [21, 21, "product"], [24, 26, "organisation"], [26, 26, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[21, 21, 13, 15, "usage", "uses_software", false, false], [21, 21, 19, 19, "artifact", "", false, false], [21, 21, 26, 26, "named", "", false, false], [26, 26, 24, 26, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Some", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", ",", "which", "powers", "AltaVista", "'s", "BabelFish", "(", "now", "Yahoo", "'s", "Babelfish", ",", "as", "of", "9", "May", "2008", ")", "."], "sentence-detokenized": "Some of these programs are available online, such as Google Translate and the SYSTRAN system, which powers AltaVista's BabelFish (now Yahoo's Babelfish, as of 9 May 2008).", "token2charspan": [[0, 4], [5, 7], [8, 13], [14, 22], [23, 26], [27, 36], [37, 43], [43, 44], [45, 49], [50, 52], [53, 59], [60, 69], [70, 73], [74, 77], [78, 85], [86, 92], [92, 93], [94, 99], [100, 106], [107, 116], [116, 118], [119, 128], [129, 130], [130, 133], [134, 139], [139, 141], [142, 151], [151, 152], [153, 155], [156, 158], [159, 160], [161, 164], [165, 169], [169, 170], [170, 171]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 24, "field"], [26, 27, "misc"], [31, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 24, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 31, 32, "related-to", "", true, false], [7, 8, 20, 24, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 31, 32, "related-to", "", true, false], [10, 11, 20, 24, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 31, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealized", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealized intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 21, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 21, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measure", "."], "sentence-detokenized": "The most common way is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 25], [26, 29], [30, 33], [34, 36], [36, 37], [37, 43], [44, 49], [50, 51], [51, 57], [57, 58], [58, 66], [67, 77], [78, 81], [82, 89], [90, 100], [100, 101], [102, 109], [109, 110]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [15, 16, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [0, 0, 15, 16, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "schemas", ",", "models", "and", "learning", "algorithms", "and", "can", "be", "extended", "with", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides schemas, models and learning algorithms and can be extended with R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 27], [27, 28], [29, 35], [36, 39], [40, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 84], [85, 86], [87, 90], [91, 97], [98, 105], [105, 106], [107, 112], [113, 119], [119, 120], [121, 126], [127, 135], [135, 136], [137, 139], [140, 148], [149, 153], [153, 154]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [13, 14, "task"], [18, 20, "misc"], [34, 36, "programlang"], [39, 43, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 13, 14, "related-to", "", false, false], [0, 0, 39, 43, "related-to", "", true, false], [18, 20, 0, 0, "part-of", "", false, false], [39, 43, 34, 36, "general-affiliation", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["tity", "contains", "a", "collection", "of", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modelling", ",", "together", "with", "graphical", "user", "interfaces", "for", "easy", "access", "to", "these", "functions", ".", "but", "the", "more", "recent", ",", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "whose", "development", "began", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "especially", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "tity contains a collection of visualisation tools and algorithms for data analysis and predictive modelling, together with graphical user interfaces for easy access to these functions. but the more recent, fully Java-based version (Weka 3), whose development began in 1997, is now used in many different application areas, especially for educational and research purposes.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 107], [107, 108], [109, 117], [118, 122], [123, 132], [133, 137], [138, 148], [149, 152], [153, 157], [158, 164], [165, 167], [168, 173], [174, 183], [183, 184], [185, 188], [189, 192], [193, 197], [198, 204], [204, 205], [206, 211], [212, 216], [216, 217], [217, 222], [223, 230], [231, 232], [232, 236], [237, 238], [238, 239], [239, 240], [241, 246], [247, 258], [259, 264], [265, 267], [268, 272], [272, 273], [274, 276], [277, 280], [281, 285], [286, 288], [289, 293], [294, 303], [304, 315], [316, 321], [321, 322], [323, 333], [334, 337], [338, 349], [350, 353], [354, 362], [363, 371], [371, 372]]}
{"doc_key": "ai-train-17", "ner": [[0, 1, "product"], [15, 22, "misc"], [24, 27, "misc"], [30, 37, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[15, 22, 0, 1, "topic", "", false, false], [15, 22, 24, 27, "win-defeat", "", false, false], [24, 27, 30, 37, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "has", "made", "many", "interesting", "discoveries", "and", "has", "enjoyed", "significant", "acclaim", ",", "with", "his", "paper", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "winning", "the", "best", "paper", "award", "at", "the", "1982", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "."], "sentence-detokenized": "Eurisko has made many interesting discoveries and has enjoyed significant acclaim, with his paper Heuretics: Theoretical and Study of Heuristic Rules winning the best paper award at the 1982 Association for the Advancement of Artificial Intelligence.", "token2charspan": [[0, 7], [8, 11], [12, 16], [17, 21], [22, 33], [34, 45], [46, 49], [50, 53], [54, 61], [62, 73], [74, 81], [81, 82], [83, 87], [88, 91], [92, 97], [98, 107], [107, 108], [109, 120], [121, 124], [125, 130], [131, 133], [134, 143], [144, 149], [150, 157], [158, 161], [162, 166], [167, 172], [173, 178], [179, 181], [182, 185], [186, 190], [191, 202], [203, 206], [207, 210], [211, 222], [223, 225], [226, 236], [237, 249], [249, 250]]}
{"doc_key": "ai-train-18", "ner": [[8, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "allow", "for", "multiple", "entities", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To allow for multiple entities, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 8], [9, 12], [13, 21], [22, 30], [30, 31], [32, 33], [34, 42], [43, 48], [49, 53], [54, 56], [57, 67], [68, 71], [72, 76], [77, 84], [84, 85]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 13, "product"], [15, 16, "product"], [18, 19, "product"], [21, 23, "product"], [25, 26, "product"], [35, 39, "product"], [42, 43, "product"], [45, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 25, 26, "type-of", "", false, false], [12, 13, 25, 26, "type-of", "", false, false], [15, 16, 25, 26, "type-of", "", false, false], [18, 19, 25, 26, "type-of", "", false, false], [21, 23, 25, 26, "type-of", "", false, false], [42, 43, 35, 39, "type-of", "", false, false], [45, 46, 35, 39, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "conversational", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "Far", "Field", "voice", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of conversational assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, voice portals can now be accessed via mobile devices and Far Field voice smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 33], [34, 44], [45, 49], [50, 52], [53, 58], [58, 60], [61, 65], [65, 66], [67, 73], [74, 79], [79, 80], [81, 87], [88, 97], [97, 98], [99, 108], [109, 116], [117, 120], [121, 128], [128, 130], [131, 136], [136, 137], [138, 143], [144, 151], [152, 155], [156, 159], [160, 162], [163, 171], [172, 175], [176, 182], [183, 190], [191, 194], [195, 198], [199, 204], [205, 210], [211, 216], [217, 225], [226, 230], [231, 233], [234, 240], [241, 245], [246, 249], [250, 256], [257, 261], [261, 262]]}
{"doc_key": "ai-train-20", "ner": [[2, 5, "field"], [6, 8, "algorithm"], [10, 12, "algorithm"], [14, 15, "algorithm"], [18, 18, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 5, "type-of", "", false, false], [10, 12, 2, 5, "type-of", "", false, false], [14, 15, 2, 5, "type-of", "", false, false], [18, 18, 2, 5, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "the", "Naive", "Bayes", "classifier", ",", "support", "vector", "machine", ",", "Gaussian", "mixtures", "and", "the", "network", "."], "sentence-detokenized": "Examples of supervised learning are the Naive Bayes classifier, support vector machine, Gaussian mixtures and the network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 39], [40, 45], [46, 51], [52, 62], [62, 63], [64, 71], [72, 78], [79, 86], [86, 87], [88, 96], [97, 105], [106, 109], [110, 113], [114, 121], [121, 122]]}
{"doc_key": "ai-train-21", "ner": [[3, 6, "algorithm"], [29, 31, "algorithm"], [28, 28, "task"], [35, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 6, 29, 31, "part-of", "", true, false], [35, 36, 28, 28, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "derive", "the", "math", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "math", "regret", "bounds", "for", "the", "online", "version", "of", "the", "classification", "support", "vector", "machine", ",", "which", "uses", "hinge", "loss", "math", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to derive the math O(\\ sqrt {T}) / math regret bounds for the online version of the classification support vector machine, which uses hinge loss math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)} / math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 39], [40, 43], [44, 48], [49, 50], [50, 51], [51, 52], [53, 57], [58, 59], [59, 60], [60, 61], [61, 62], [63, 64], [65, 69], [70, 76], [77, 83], [84, 87], [88, 91], [92, 98], [99, 106], [107, 109], [110, 113], [114, 128], [129, 136], [137, 143], [144, 151], [151, 152], [153, 158], [159, 163], [164, 169], [170, 174], [175, 179], [180, 181], [182, 184], [185, 186], [186, 187], [187, 188], [189, 191], [192, 195], [195, 196], [197, 198], [198, 199], [199, 200], [201, 202], [203, 204], [205, 206], [207, 209], [210, 211], [211, 212], [212, 213], [214, 218], [219, 220], [221, 223], [223, 224], [224, 225], [226, 227], [228, 232]]}
{"doc_key": "ai-train-22", "ner": [[2, 2, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"], [26, 27, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", "and", "match", "movement", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, individual wildlife identification and match movement.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 102], [102, 103], [104, 111], [112, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 151], [152, 160], [161, 175], [176, 179], [180, 185], [186, 194], [194, 195]]}
{"doc_key": "ai-train-23", "ner": [[8, 9, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [28, 32, "university"], [34, 36, "university"], [38, 40, "university"], [42, 43, "university"], [45, 50, "university"], [52, 52, "university"], [55, 59, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[8, 9, 14, 15, "related-to", "", true, false], [8, 9, 17, 19, "related-to", "", true, false], [8, 9, 21, 22, "related-to", "", true, false], [8, 9, 24, 25, "related-to", "", true, false], [8, 9, 28, 32, "related-to", "", true, false], [8, 9, 34, 36, "related-to", "", true, false], [8, 9, 38, 40, "related-to", "", true, false], [8, 9, 42, 43, "related-to", "", true, false], [8, 9, 45, 50, "related-to", "", true, false], [8, 9, 52, 52, "related-to", "", true, false], [8, 9, 55, 59, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "researching", "position", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are researching position estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 48], [49, 57], [58, 68], [68, 69], [70, 79], [80, 86], [87, 89], [90, 95], [96, 106], [106, 107], [108, 116], [117, 123], [124, 134], [134, 135], [136, 139], [140, 152], [152, 153], [154, 162], [163, 173], [173, 174], [175, 185], [186, 188], [189, 199], [199, 200], [201, 204], [205, 210], [210, 211], [212, 222], [223, 225], [226, 233], [233, 234], [235, 240], [241, 249], [250, 255], [255, 256], [257, 260], [261, 267], [267, 268], [269, 277], [278, 288], [289, 291], [292, 300], [301, 304], [305, 315], [316, 317], [317, 321], [321, 322], [323, 326], [327, 337], [338, 340], [341, 351], [351, 352], [353, 359], [359, 360]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "Sigmoid", "Cross", "entropy", "loss", "function", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "0.1", "math", "/", "math", "."], "sentence-detokenized": "The Sigmoid Cross entropy loss function is used to predict K independent probability values in 0.1 math/math.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 25], [26, 30], [31, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 98], [99, 103], [103, 104], [104, 108], [108, 109]]}
{"doc_key": "ai-train-25", "ner": [[10, 11, "misc"], [14, 14, "field"], [16, 17, "field"], [20, 22, "university"], [25, 25, "country"], [28, 30, "misc"], [33, 36, "university"], [38, 38, "country"], [5, 5, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 11, 14, 14, "topic", "", false, false], [10, 11, 16, 17, "topic", "", false, false], [10, 11, 20, 22, "physical", "", true, false], [20, 22, 25, 25, "physical", "", false, false], [28, 30, 33, 36, "physical", "", true, false], [33, 36, 38, 38, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Before", "becoming", "a", "professor", "at", "Cambridge", ",", "he", "held", "the", "Johann", "Bernoulli", "Chair", "of", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "."], "sentence-detokenized": "Before becoming a professor at Cambridge, he held the Johann Bernoulli Chair of Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan.", "token2charspan": [[0, 6], [7, 15], [16, 17], [18, 27], [28, 30], [31, 40], [40, 41], [42, 44], [45, 49], [50, 53], [54, 60], [61, 70], [71, 76], [77, 79], [80, 91], [92, 95], [96, 104], [105, 112], [113, 115], [116, 119], [120, 130], [131, 133], [134, 143], [144, 146], [147, 150], [151, 162], [163, 166], [167, 170], [171, 178], [179, 186], [187, 192], [193, 195], [196, 199], [200, 205], [206, 215], [216, 218], [219, 229], [230, 232], [233, 238], [238, 239]]}
{"doc_key": "ai-train-26", "ner": [[6, 7, "algorithm"], [13, 16, "algorithm"], [11, 11, "algorithm"], [24, 25, "researcher"], [27, 28, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 7, 13, 16, "usage", "", true, false], [13, 16, 24, 25, "origin", "", false, false], [13, 16, 27, 28, "origin", "", false, false], [11, 11, 13, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "used", "in", "particular", "for", "recurrent", "neural", "networks", "is", "the", "LSTM", "(", "Long", "Short", "Term", "Memory", ")", "network", "from", "1997", ",", "developed", "by", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "."], "sentence-detokenized": "Another technique used in particular for recurrent neural networks is the LSTM (Long Short Term Memory) network from 1997, developed by Sepp Hochreiter and J\u00fcrgen Schmidhuber.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 25], [26, 36], [37, 40], [41, 50], [51, 57], [58, 66], [67, 69], [70, 73], [74, 78], [79, 80], [80, 84], [85, 90], [91, 95], [96, 102], [102, 103], [104, 111], [112, 116], [117, 121], [121, 122], [123, 132], [133, 135], [136, 140], [141, 151], [152, 155], [156, 162], [163, 174], [174, 175]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 9, "product"], [15, 15, "product"], [46, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 4, 5, "general-affiliation", "", false, false], [8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", "onwards", ")", "makes", "this", "package", "very", "versatile", "as", "it", "can", "be", "used", "interactively", ",", "scripted", "and", "compiled", ",", "in", "a", "similar", "way", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT up to version 5.34, Cling from version 6 onwards) makes this package very versatile as it can be used interactively, scripted and compiled, in a similar way to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 43], [44, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [82, 89], [89, 90], [91, 96], [97, 101], [102, 109], [110, 114], [115, 124], [125, 127], [128, 130], [131, 134], [135, 137], [138, 142], [143, 156], [156, 157], [158, 166], [167, 170], [171, 179], [179, 180], [181, 183], [184, 185], [186, 193], [194, 197], [198, 200], [201, 211], [212, 220], [221, 225], [226, 228], [229, 235], [235, 236]]}
{"doc_key": "ai-train-28", "ner": [[0, 2, "product"], [21, 23, "field"], [27, 28, "task"], [30, 32, "task"], [34, 35, "task"], [38, 39, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 2, 21, 23, "related-to", "", false, false], [27, 28, 21, 23, "part-of", "", false, false], [30, 32, 21, 23, "part-of", "", false, false], [34, 35, 21, 23, "part-of", "", false, false], [38, 39, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Speech", "user", "interfaces", "that", "interpret", "and", "manage", "conversational", "state", "pose", "a", "design", "challenge", "because", "of", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "resolving", "correlations", ",", "recognizing", "named", "entities", ",", "retrieving", "information", ",", "and", "managing", "dialogue", "."], "sentence-detokenized": "Speech user interfaces that interpret and manage conversational state pose a design challenge because of the inherent difficulty of integrating complex natural language processing tasks such as resolving correlations, recognizing named entities, retrieving information, and managing dialogue.", "token2charspan": [[0, 6], [7, 11], [12, 22], [23, 27], [28, 37], [38, 41], [42, 48], [49, 63], [64, 69], [70, 74], [75, 76], [77, 83], [84, 93], [94, 101], [102, 104], [105, 108], [109, 117], [118, 128], [129, 131], [132, 143], [144, 151], [152, 159], [160, 168], [169, 179], [180, 185], [186, 190], [191, 193], [194, 203], [204, 216], [216, 217], [218, 229], [230, 235], [236, 244], [244, 245], [246, 256], [257, 268], [268, 269], [270, 273], [274, 282], [283, 291], [291, 292]]}
{"doc_key": "ai-train-29", "ner": [[5, 6, "algorithm"], [9, 10, "algorithm"], [14, 15, "researcher"], [21, 25, "organisation"], [34, 35, "field"], [37, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 14, 15, "origin", "", false, false], [5, 6, 34, 35, "part-of", "", false, false], [5, 6, 37, 38, "part-of", "", false, false], [9, 10, 14, 15, "origin", "", false, false], [9, 10, 34, 35, "part-of", "", false, false], [9, 10, 37, 38, "part-of", "", false, false], [14, 15, 21, 25, "physical", "", false, false], [14, 15, 21, 25, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "advanced", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "artificial", "intelligence", "laboratory", "IDSIA", "won", "eight", "international", "competitions", "in", "the", "field", "of", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and advanced neural networks developed in J\u00fcrgen Schmidhuber's research group at the Swiss artificial intelligence laboratory IDSIA won eight international competitions in the field of pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 61], [62, 68], [69, 77], [78, 87], [88, 90], [91, 97], [98, 109], [109, 111], [112, 120], [121, 126], [127, 129], [130, 133], [134, 139], [140, 150], [151, 163], [164, 174], [175, 180], [181, 184], [185, 190], [191, 204], [205, 217], [218, 220], [221, 224], [225, 230], [231, 233], [234, 241], [242, 253], [254, 257], [258, 265], [266, 274], [274, 275]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 12, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 12, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 12, "misc"], [14, 14, "field"], [17, 19, "university"], [26, 29, "field"], [31, 34, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 12, 14, 14, "topic", "topic_of_award", false, false], [7, 12, 17, 19, "origin", "", true, false], [26, 29, 31, 34, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "an", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "PhD", "in", "industrial", "design", "and", "engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, an S. V. della laurea ad honorem in psychology from the University of Padua in 1995 and a PhD in industrial design and engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 36], [37, 39], [40, 42], [43, 48], [49, 55], [56, 58], [59, 66], [67, 69], [70, 80], [81, 85], [86, 89], [90, 100], [101, 103], [104, 109], [110, 112], [113, 117], [118, 121], [122, 123], [124, 127], [128, 130], [131, 141], [142, 148], [149, 152], [153, 164], [165, 169], [170, 175], [176, 186], [187, 189], [190, 200], [200, 201]]}
{"doc_key": "ai-train-32", "ner": [[7, 10, "researcher"], [14, 17, "organisation"], [19, 19, "location"], [21, 21, "researcher"], [32, 48, "misc"], [65, 66, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 5, 6], "relations": [[7, 10, 14, 17, "physical", "", false, false], [7, 10, 14, 17, "role", "", false, false], [14, 17, 19, 19, "physical", "", false, false], [21, 21, 32, 48, "related-to", "works_with", true, false], [21, 21, 65, 66, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "with", "impaired", "multiplication", "but", "preserved", "shrinkage", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobe", ")", "and", "other", "patients", "with", "impaired", "shrinkage", "but", "preserved", "multiplication", "(", "associated", "with", "lesions", "in", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Together with his long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication but preserved shrinkage (associated with lesions of the inferior parietal lobe) and other patients with impaired shrinkage but preserved multiplication (associated with lesions in the intraparietal sulcus).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [41, 48], [49, 54], [54, 55], [56, 57], [58, 69], [70, 72], [73, 76], [77, 82], [82, 83], [83, 94], [95, 103], [104, 106], [107, 112], [112, 113], [114, 121], [122, 126], [127, 137], [138, 146], [147, 151], [152, 159], [160, 162], [163, 172], [173, 180], [181, 183], [184, 187], [188, 196], [197, 201], [202, 206], [207, 215], [216, 230], [231, 234], [235, 244], [245, 254], [255, 256], [256, 266], [267, 271], [272, 279], [280, 282], [283, 286], [287, 295], [296, 304], [305, 309], [309, 310], [311, 314], [315, 320], [321, 329], [330, 334], [335, 343], [344, 353], [354, 357], [358, 367], [368, 382], [383, 384], [384, 394], [395, 399], [400, 407], [408, 410], [411, 414], [415, 428], [429, 435], [435, 436], [436, 437]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [13, 16, "misc"], [18, 19, "misc"], [26, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 8, "topic", "", false, false], [18, 19, 6, 8, "topic", "", false, false], [26, 26, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "depictions", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", "and", "the", "2016", "TV", "adaptation", "of", "Westworld", "have", "drawn", "public", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional depictions of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have drawn public sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 35], [36, 38], [39, 51], [52, 63], [64, 70], [71, 73], [74, 79], [80, 84], [85, 87], [88, 91], [91, 92], [93, 103], [104, 116], [117, 120], [121, 123], [124, 131], [132, 135], [136, 139], [140, 144], [145, 147], [148, 158], [159, 161], [162, 171], [172, 176], [177, 182], [183, 189], [190, 198], [199, 202], [203, 206], [207, 213], [214, 224], [224, 225]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [10, 12, "algorithm"], [14, 15, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 12, 7, 8, "part-of", "", false, false], [14, 15, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [21, 22, "misc"], [27, 28, "misc"], [30, 32, "person"], [37, 38, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[21, 22, 0, 3, "artifact", "", false, false], [27, 28, 0, 3, "artifact", "", false, false], [27, 28, 30, 32, "role", "director_of", false, false], [27, 28, 37, 38, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "make", "more", "use", "of", "3D", "films", "in", "special", "spaces", "to", "impress", "audiences", ",", "with", "Magic", "Travels", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "being", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to make more use of 3D films in special spaces to impress audiences, with Magic Travels (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) being notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 42], [43, 47], [48, 51], [52, 54], [55, 57], [58, 63], [64, 66], [67, 74], [75, 81], [82, 84], [85, 92], [93, 102], [102, 103], [104, 108], [109, 114], [115, 122], [123, 124], [124, 128], [128, 129], [130, 133], [134, 141], [142, 144], [145, 146], [146, 153], [154, 158], [159, 166], [166, 167], [168, 172], [172, 173], [174, 182], [183, 190], [191, 198], [198, 199], [200, 205], [206, 213], [214, 222], [222, 223]]}
{"doc_key": "ai-train-36", "ner": [[9, 11, "field"], [16, 21, "task"], [23, 24, "task"], [26, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[16, 21, 9, 11, "part-of", "", false, false], [23, 24, 9, 11, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "analysis", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in natural language processing for tasks such as part-of-speech tagging and syntactic analysis (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 61], [62, 70], [71, 81], [82, 85], [86, 91], [92, 96], [97, 99], [100, 104], [104, 105], [105, 107], [107, 108], [108, 114], [115, 122], [123, 126], [127, 136], [137, 145], [146, 147], [147, 154], [154, 155], [156, 160], [160, 161], [161, 162]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [10, 14, "organisation"], [16, 17, "organisation"], [19, 19, "country"], [23, 26, "product"], [30, 31, "researcher"], [41, 41, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[10, 14, 2, 3, "role", "introduces_to_market", true, false], [16, 17, 2, 3, "role", "introduces_to_market", true, false], [16, 17, 19, 19, "physical", "", false, false], [23, 26, 41, 41, "related-to", "sold_to", true, false], [30, 31, 23, 26, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletizing", "robot", "was", "introduced", "in", "1963", "by", "the", "Fuji", "Yusoki", "Kogyo", "Company", ".", "by", "KUKA", "robotics", "of", "Germany", ",", "and", "the", "universal", "programmable", "assembly", "machine", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", ",", "and", "the", "project", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. by KUKA robotics of Germany, and the universal programmable assembly machine was invented by Victor Scheinman in 1976, and the project was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 57], [58, 62], [63, 69], [70, 75], [76, 83], [83, 84], [85, 87], [88, 92], [93, 101], [102, 104], [105, 112], [112, 113], [114, 117], [118, 121], [122, 131], [132, 144], [145, 153], [154, 161], [162, 165], [166, 174], [175, 177], [178, 184], [185, 194], [195, 197], [198, 202], [202, 203], [204, 207], [208, 211], [212, 219], [220, 223], [224, 228], [229, 231], [232, 241], [241, 242]]}
{"doc_key": "ai-train-38", "ner": [[8, 8, "conference"], [10, 10, "researcher"], [38, 39, "researcher"], [46, 50, "researcher"], [19, 62, "field"]], "ner_mapping_to_source": [0, 1, 3, 4, 5], "relations": [[10, 10, 8, 8, "role", "president_of", false, false], [10, 10, 38, 39, "role", "colleagues", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "the", "mid-1990s", ",", "while", "president", "of", "the", "AAAI", ",", "Hayes", "began", "a", "series", "of", "attacks", "on", "critics", "of", "artificial", "intelligence", ",", "mostly", "couched", "in", "a", "tongue", "-", "in", "-", "cheek", "light", ",", "and", "(", "with", "his", "colleague", "Kenneth", "Ford", ")", "invented", "an", "award", "named", "after", "Simon", "Newcomb", ",", "to", "be", "given", "for", "the", "most", "ridiculous", "argument", "disproving", "the", "possibility", "of", "artificial", "intelligence", "."], "sentence-detokenized": "In the mid-1990s, while president of the AAAI, Hayes began a series of attacks on critics of artificial intelligence, mostly couched in a tongue-in-cheek light, and (with his colleague Kenneth Ford) invented an award named after Simon Newcomb, to be given for the most ridiculous argument disproving the possibility of artificial intelligence.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 33], [34, 36], [37, 40], [41, 45], [45, 46], [47, 52], [53, 58], [59, 60], [61, 67], [68, 70], [71, 78], [79, 81], [82, 89], [90, 92], [93, 103], [104, 116], [116, 117], [118, 124], [125, 132], [133, 135], [136, 137], [138, 144], [144, 145], [145, 147], [147, 148], [148, 153], [154, 159], [159, 160], [161, 164], [165, 166], [166, 170], [171, 174], [175, 184], [185, 192], [193, 197], [197, 198], [199, 207], [208, 210], [211, 216], [217, 222], [223, 228], [229, 234], [235, 242], [242, 243], [244, 246], [247, 249], [250, 255], [256, 259], [260, 263], [264, 268], [269, 279], [280, 288], [289, 299], [300, 303], [304, 315], [316, 318], [319, 329], [330, 342], [342, 343]]}
{"doc_key": "ai-train-39", "ner": [[15, 17, "algorithm"], [41, 41, "algorithm"], [57, 57, "algorithm"], [62, 64, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[15, 17, 41, 41, "named", "same", false, false], [57, 57, 15, 17, "type-of", "", false, false], [62, 64, 15, 17, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "optimal", "value", "for", "math0", "\\", "alpha", "/", "math", "can", "be", "found", "by", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math0", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "usually", "using", "a", "line", "search", "in", "the", "range", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "line", "search", "with", "backtracking", ",", "such", "as", "the", "Armijo", "line", "search", "."], "sentence-detokenized": "An optimal value for math0\\ alpha/math can be found by using a line search algorithm, i.e. the size of math0\\ alpha/math is determined by finding the value that minimizes S, usually using a line search in the range math0\\ alpha 1/math or a line search with backtracking, such as the Armijo line search.", "token2charspan": [[0, 2], [3, 10], [11, 16], [17, 20], [21, 26], [26, 27], [28, 33], [33, 34], [34, 38], [39, 42], [43, 45], [46, 51], [52, 54], [55, 60], [61, 62], [63, 67], [68, 74], [75, 84], [84, 85], [86, 90], [91, 94], [95, 99], [100, 102], [103, 108], [108, 109], [110, 115], [115, 116], [116, 120], [121, 123], [124, 134], [135, 137], [138, 145], [146, 149], [150, 155], [156, 160], [161, 170], [171, 172], [172, 173], [174, 181], [182, 187], [188, 189], [190, 194], [195, 201], [202, 204], [205, 208], [209, 214], [215, 220], [220, 221], [222, 227], [228, 229], [229, 230], [230, 234], [235, 237], [238, 239], [240, 244], [245, 251], [252, 256], [257, 269], [269, 270], [271, 275], [276, 278], [279, 282], [283, 289], [290, 294], [295, 301], [301, 302]]}
{"doc_key": "ai-train-40", "ner": [[2, 9, "algorithm"], [6, 8, "algorithm"], [19, 19, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "Breadth", "-", "first", "and", "Depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "embody", "a", "lot", "of", "technical", "knowledge", ",", "but", "do", "n't", "shed", "much", "light", "on", "the", "mental", "processes", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses Breadth-first and Depth-first search techniques, but ultimately concludes that the results represent expert systems that embody a lot of technical knowledge, but don't shed much light on the mental processes people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 30], [31, 36], [36, 37], [37, 42], [43, 49], [50, 60], [60, 61], [62, 65], [66, 76], [77, 86], [87, 91], [92, 95], [96, 103], [104, 113], [114, 120], [121, 128], [129, 133], [134, 140], [141, 142], [143, 146], [147, 149], [150, 159], [160, 169], [169, 170], [171, 174], [175, 177], [177, 180], [181, 185], [186, 190], [191, 196], [197, 199], [200, 203], [204, 210], [211, 220], [221, 227], [228, 231], [232, 234], [235, 240], [241, 245], [246, 253], [253, 254]]}
{"doc_key": "ai-train-41", "ner": [[1, 1, "task"], [0, 3, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "synthesis", "refers", "to", "the", "way", "spoken", "language", "can", "be", "understood", "or", "created", "with", "the", "help", "of", "computers", "."], "sentence-detokenized": "Speech recognition and synthesis refers to the way spoken language can be understood or created with the help of computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 32], [33, 39], [40, 42], [43, 46], [47, 50], [51, 57], [58, 66], [67, 70], [71, 73], [74, 84], [85, 87], [88, 95], [96, 100], [101, 104], [105, 109], [110, 112], [113, 122], [122, 123]]}
{"doc_key": "ai-train-42", "ner": [[11, 11, "algorithm"], [10, 27, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^{*}/math", "is", "normally", "estimated", "using", "a", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^{", "*}", "=\\", "theta", "^{", "ML}", "/math", ")", "or", "Maximum", "A", "Posteriori", "(", "math\\", "theta", "^{*}", "=\\", "theta", "^{", "MAP}/math", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta^{*}/math is normally estimated using a Maximum Likelihood (math\\ theta^{*} =\\ theta^{ML}/math) or Maximum A Posteriori (math\\ theta^{*} =\\ theta^{MAP}/math) procedure.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [16, 25], [26, 28], [29, 37], [38, 47], [48, 53], [54, 55], [56, 63], [64, 74], [75, 76], [76, 80], [80, 81], [82, 87], [87, 89], [89, 91], [92, 94], [95, 100], [100, 102], [102, 105], [105, 110], [110, 111], [112, 114], [115, 122], [123, 124], [125, 135], [136, 137], [137, 142], [143, 148], [148, 152], [153, 155], [156, 161], [161, 163], [163, 172], [172, 173], [174, 183], [183, 184]]}
{"doc_key": "ai-train-43", "ner": [[6, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "lesser", "-", "spoken", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesizer", "for", "speech", ",", "producing", "a", "strange", ",", "robotic", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some lesser-spoken languages use the open-source eSpeak synthesizer for speech, producing a strange, robotic voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 11], [11, 12], [12, 18], [19, 28], [29, 32], [33, 36], [37, 41], [41, 42], [42, 48], [49, 55], [56, 67], [68, 71], [72, 78], [78, 79], [80, 89], [90, 91], [92, 99], [99, 100], [101, 108], [109, 114], [115, 119], [120, 123], [124, 126], [127, 136], [137, 139], [140, 150], [150, 151]]}
{"doc_key": "ai-train-44", "ner": [[18, 18, "programlang"], [33, 34, "programlang"], [36, 36, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[18, 18, 33, 34, "compare", "", false, false], [18, 18, 36, 36, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "used", "primarily", "by", "statisticians", "and", "other", "practitioners", "who", "need", "a", "statistical", "computing", "and", "software", "development", "environment", ",", "R", "can", "also", "function", "as", "a", "general", "matrix", "computing", "toolkit", "-", "with", "performance", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although used primarily by statisticians and other practitioners who need a statistical computing and software development environment, R can also function as a general matrix computing toolkit - with performance comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 13], [14, 23], [24, 26], [27, 40], [41, 44], [45, 50], [51, 64], [65, 68], [69, 73], [74, 75], [76, 87], [88, 97], [98, 101], [102, 110], [111, 122], [123, 134], [134, 135], [136, 137], [138, 141], [142, 146], [147, 155], [156, 158], [159, 160], [161, 168], [169, 175], [176, 185], [186, 193], [194, 195], [196, 200], [201, 212], [213, 223], [224, 226], [227, 230], [231, 237], [238, 240], [241, 247], [247, 248]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 4, "part-of", "", false, false], [0, 0, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "engineer", "-", "inventor", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "combining", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian engineer-inventor Reginald Fessenden that creates new frequencies by combining two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [75, 76], [76, 84], [85, 93], [94, 103], [104, 108], [109, 116], [117, 120], [121, 132], [133, 135], [136, 145], [146, 149], [150, 161], [161, 162]]}
{"doc_key": "ai-train-46", "ner": [[15, 16, "person"], [18, 18, "misc"], [22, 24, "organisation"], [27, 27, "organisation"], [28, 31, "misc"], [33, 34, "person"], [37, 37, "organisation"], [38, 41, "misc"], [43, 44, "person"], [46, 47, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[15, 16, 18, 18, "role", "actor_in", false, false], [18, 18, 22, 24, "artifact", "", false, false], [28, 31, 27, 27, "artifact", "", false, false], [33, 34, 28, 31, "role", "actor_in", false, false], [38, 41, 37, 37, "artifact", "", false, false], [43, 44, 38, 41, "role", "actor_in", false, false], [46, 47, 38, 41, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["A", "few", "other", "films", "that", "helped", "put", "3D", "back", "on", "the", "map", "that", "month", "were", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "starring", "Rita", "Hayworth", ",", "and", "Paramount", "'s", "Money", "From", "Home", "starring", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "A few other films that helped put 3D back on the map that month were John Wayne's Hondo (distributed by Warner Bros. ), Columbia's Miss Sadie Thompson starring Rita Hayworth, and Paramount's Money From Home starring Dean Martin and Jerry Lewis.", "token2charspan": [[0, 1], [2, 5], [6, 11], [12, 17], [18, 22], [23, 29], [30, 33], [34, 36], [37, 41], [42, 44], [45, 48], [49, 52], [53, 57], [58, 63], [64, 68], [69, 73], [74, 79], [79, 81], [82, 87], [88, 89], [89, 100], [101, 103], [104, 110], [111, 115], [115, 116], [117, 118], [118, 119], [120, 128], [128, 130], [131, 135], [136, 141], [142, 150], [151, 159], [160, 164], [165, 173], [173, 174], [175, 178], [179, 188], [188, 190], [191, 196], [197, 201], [202, 206], [207, 215], [216, 220], [221, 227], [228, 231], [232, 237], [238, 243], [243, 244]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 4, "field"], [5, 6, "task"], [11, 11, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 6, "general-affiliation", "", false, false], [0, 0, 11, 11, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "Facebook", "research", "group", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a Facebook research group.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 84], [85, 90], [90, 91]]}
{"doc_key": "ai-train-48", "ner": [[8, 8, "conference"], [15, 16, "field"], [0, 27, "conference"]], "ner_mapping_to_source": [1, 2, 3], "relations": [], "relations_mapping_to_source": [], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "topic", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the leading academic conference on computer graphics, and the main topic of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 91], [92, 94], [95, 103], [104, 112], [112, 113], [114, 117], [118, 121], [122, 126], [127, 132], [133, 135], [136, 139], [140, 146], [147, 155], [156, 166], [167, 176], [176, 177]]}
{"doc_key": "ai-train-49", "ner": [[1, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [36, 36, "misc"], [43, 45, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 15, 36, 36, "general-affiliation", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 36, 36, "general-affiliation", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 36, 36, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "by", "k", "-", "NN", "on", "feature", "vectors", "in", "low-dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k -NN on feature vectors in low-dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [158, 160], [161, 170], [171, 182], [183, 191], [192, 193], [193, 196], [196, 197], [198, 208], [209, 211], [212, 213], [214, 228], [229, 233], [233, 234], [235, 243], [244, 246], [247, 257], [258, 260], [261, 262], [263, 264], [264, 266], [267, 269], [270, 277], [278, 285], [286, 288], [289, 304], [305, 310], [310, 311]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "at", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel at machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [39, 40, "researcher"], [42, 44, "researcher"], [36, 52, "misc"], [54, 63, "conference"], [65, 65, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [36, 52, 39, 40, "artifact", "", false, false], [36, 52, 42, 44, "artifact", "", false, false], [36, 52, 54, 63, "temporal", "", false, false], [65, 65, 54, 63, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histogram", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as histogram oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 143], [144, 150], [151, 154], [155, 160], [161, 169], [170, 174], [175, 177], [178, 187], [188, 196], [197, 206], [207, 209], [210, 215], [215, 216], [217, 218], [218, 219], [220, 226], [226, 227], [228, 238], [239, 241], [242, 250], [251, 260], [261, 264], [265, 270], [271, 280], [280, 281], [282, 286], [287, 295], [296, 303], [304, 314], [315, 317], [318, 326], [327, 333], [334, 337], [338, 345], [346, 357], [358, 359], [359, 363], [363, 364], [364, 365], [366, 371], [372, 373], [373, 374], [375, 382], [382, 383], [384, 388], [389, 400], [400, 401]]}
{"doc_key": "ai-train-52", "ner": [[1, 1, "algorithm"], [6, 9, "algorithm"], [12, 12, "task"], [16, 17, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 1, 6, 9, "type-of", "", false, false], [12, 12, 1, 1, "usage", "", true, false], [12, 12, 16, 17, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "Feature", "Learning", "in", "an", "unsupervised", "learning", "fashion", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network used to learn Feature Learning in an unsupervised learning fashion.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 61], [62, 67], [68, 75], [76, 84], [85, 87], [88, 90], [91, 103], [104, 112], [113, 120], [120, 121]]}
{"doc_key": "ai-train-53", "ner": [[0, 0, "researcher"], [6, 6, "organisation"], [11, 12, "field"], [21, 25, "organisation"], [24, 34, "field"], [14, 37, "field"], [27, 41, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 4, 6, 7, 8], "relations": [[0, 0, 6, 6, "role", "fellow_of", false, false], [0, 0, 11, 12, "related-to", "contributes_to", false, false], [0, 0, 21, 25, "role", "fellow_of", false, false], [0, 0, 24, 34, "related-to", "contributes_to", false, false], [0, 0, 14, 37, "related-to", "contributes_to", false, false], [27, 41, 21, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 3, 4, 5, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "the", "IEEE", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "pattern", "recognition", ",", "image", "processing", "and", "service", "to", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of the IEEE for his contributions to computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions to pattern recognition, image processing and service to IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 40], [41, 54], [55, 57], [58, 66], [67, 73], [74, 77], [78, 83], [84, 94], [95, 98], [99, 100], [101, 107], [108, 110], [111, 114], [115, 128], [129, 140], [141, 144], [145, 152], [153, 164], [165, 166], [166, 170], [170, 171], [172, 175], [176, 179], [180, 193], [194, 196], [197, 204], [205, 216], [216, 217], [218, 223], [224, 234], [235, 238], [239, 246], [247, 249], [250, 254], [254, 255]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [16, 18, "algorithm"], [20, 20, "algorithm"], [25, 26, "researcher"], [28, 29, "organisation"], [31, 32, "researcher"], [35, 37, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 9, 16, 18, "usage", "", false, false], [16, 18, 25, 26, "origin", "", true, false], [16, 18, 31, 32, "origin", "", true, false], [20, 20, 16, 18, "named", "", false, false], [25, 26, 28, 29, "physical", "", false, false], [25, 26, 28, 29, "role", "", false, false], [31, 32, 35, 37, "physical", "", false, false], [31, 32, 35, 37, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "that", "of", "systems", "based", "on", "connectionist", "temporal", "classification", "(", "CTC", ")", ",", "introduced", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was that of systems based on connectionist temporal classification (CTC), introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 44], [45, 47], [48, 55], [56, 61], [62, 64], [65, 78], [79, 87], [88, 102], [103, 104], [104, 107], [107, 108], [108, 109], [110, 120], [121, 123], [124, 128], [129, 135], [136, 138], [139, 145], [146, 154], [155, 158], [159, 166], [167, 173], [174, 176], [177, 180], [181, 191], [192, 194], [195, 202], [203, 205], [206, 210], [210, 211]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear-", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-fractional programming (LFP) is a generalisation of linear programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 1, "researcher"], [8, 13, "misc"], [16, 22, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 8, 13, "win-defeat", "", false, false], [8, 13, 16, 22, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "2011", "and", "2012", "International", "Machine", "Learning", "Conference", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the 2011 and 2012 International Machine Learning Conference,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 84], [85, 88], [89, 93], [94, 107], [108, 115], [116, 124], [125, 135], [135, 136]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [24, 25, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "implement", "the", "neural", "network", "developed", "in", "these", "frameworks", "as", "legacy", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to implement the neural network developed in these frameworks as legacy components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 131], [132, 135], [136, 142], [143, 150], [151, 160], [161, 163], [164, 169], [170, 180], [181, 183], [184, 190], [191, 201], [201, 202]]}
{"doc_key": "ai-train-58", "ner": [[2, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "illustrations", ")", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see illustrations) between two sentences, the candidate translation string and the reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 73], [74, 79], [80, 87], [88, 90], [91, 100], [101, 102], [102, 105], [106, 119], [119, 120], [121, 128], [129, 132], [133, 142], [142, 143], [144, 147], [148, 157], [158, 169], [170, 176], [177, 180], [181, 184], [185, 194], [195, 206], [207, 213], [213, 214]]}
{"doc_key": "ai-train-59", "ner": [[7, 11, "conference"], [21, 21, "task"], [23, 24, "task"], [3, 29, "metrics"], [31, 37, "metrics"], [42, 45, "conference"], [47, 47, "conference"], [50, 50, "location"], [52, 52, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[7, 11, 21, 21, "related-to", "subject_at", false, false], [7, 11, 23, 24, "related-to", "subject_at", false, false], [3, 29, 7, 11, "temporal", "", false, false], [31, 37, 3, 29, "named", "", true, false], [47, 47, 42, 45, "named", "", false, false], [50, 50, 52, 52, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "the", "annual", "NIST", "Document", "Understanding", "Conferences", ",", "where", "research", "groups", "present", "their", "systems", "for", "both", "summarization", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used at the annual NIST Document Understanding Conferences, where research groups present their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 30], [31, 37], [38, 42], [43, 51], [52, 65], [66, 77], [77, 78], [79, 84], [85, 93], [94, 100], [101, 108], [109, 114], [115, 122], [123, 126], [127, 131], [132, 145], [146, 149], [150, 161], [162, 167], [167, 168], [169, 171], [172, 175], [176, 181], [182, 188], [189, 190], [190, 196], [196, 197], [197, 205], [206, 216], [217, 220], [221, 228], [229, 239], [239, 240], [241, 243], [244, 252], [253, 255], [256, 262], [263, 274], [275, 285], [286, 293], [294, 295], [295, 299], [299, 300], [300, 301], [302, 310], [310, 311], [312, 318], [318, 319], [320, 328], [329, 330], [331, 335], [335, 336]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [10, 11, "programlang"], [15, 15, "product"], [21, 21, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 10, 11, "type-of", "", false, false], [6, 6, 21, 21, "named", "", false, false], [8, 8, 10, 11, "part-of", "", false, false], [8, 8, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "to", "run", "in", "Java", "with", "JShell", "(", "Java", "9", "minimum", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, to run in Java with JShell (Java 9 minimum): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 23], [24, 27], [28, 30], [31, 35], [36, 40], [41, 47], [48, 49], [49, 53], [54, 55], [56, 63], [63, 64], [64, 65], [66, 76], [77, 87], [88, 89], [90, 109], [110, 114], [115, 116], [117, 121]]}
{"doc_key": "ai-train-61", "ner": [[1, 6, "metrics"], [0, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[1, 6, 0, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLUE", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLUE metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "additional", "constraint", "that", "the", "edges", "are", "restricted", "to", "be", "part", "of", "a", "limited", "set", "of", "possible", "relations", ",", "to", "facilitate", "algebras", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called Knowledge Graphs, which are semantic networks, but with the additional constraint that the edges are restricted to be part of a limited set of possible relations, to facilitate algebras on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 116], [117, 118], [119, 126], [127, 133], [134, 143], [144, 150], [150, 151], [152, 157], [158, 161], [162, 170], [171, 179], [179, 180], [181, 184], [185, 189], [190, 193], [194, 204], [205, 215], [216, 220], [221, 224], [225, 230], [231, 234], [235, 245], [246, 248], [249, 251], [252, 256], [257, 259], [260, 261], [262, 269], [270, 273], [274, 276], [277, 285], [286, 295], [295, 296], [297, 299], [300, 310], [311, 319], [320, 322], [323, 326], [327, 332], [332, 333]]}
{"doc_key": "ai-train-63", "ner": [[0, 3, "product"], [17, 18, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 3, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "word", "processor", ",", "but", "they", "are", "also", "available", "as", "stand", "-", "alone", "applications", "that", "can", "be", "activated", "from", "within", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but they are also available as stand-alone applications that can be activated from within programs that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [76, 77], [78, 82], [83, 85], [86, 87], [88, 92], [93, 102], [102, 103], [104, 107], [108, 112], [113, 116], [117, 121], [122, 131], [132, 134], [135, 140], [140, 141], [141, 146], [147, 159], [160, 164], [165, 168], [169, 171], [172, 181], [182, 186], [187, 193], [194, 202], [203, 207], [208, 212], [213, 217], [218, 226], [227, 231], [231, 232]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [10, 21, "conference"], [25, 27, "organisation"], [33, 34, "conference"], [37, 38, "conference"], [36, 43, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "fellow", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a fellow of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence, and the Cognitive Science Society, and editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [137, 138], [139, 142], [143, 146], [147, 156], [157, 164], [165, 172], [172, 173], [174, 177], [178, 184], [185, 187], [188, 190], [191, 200], [201, 210], [210, 211], [212, 214], [215, 223], [224, 232], [232, 233], [234, 237], [238, 240], [241, 248], [249, 257], [257, 258]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "task"], [18, 20, "researcher"], [21, 22, "university"], [24, 25, "researcher"], [27, 30, "organisation"], [32, 32, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 18, 20, "origin", "", false, false], [0, 2, 24, 25, "origin", "", false, false], [4, 4, 0, 2, "named", "", false, false], [18, 20, 21, 22, "physical", "", false, false], [18, 20, 21, 22, "role", "", false, false], [24, 25, 27, 30, "role", "", false, false], [32, 32, 27, 30, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "be", "developed", "by", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear Predictive Coding (LPC), a form of speech coding, began to be developed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 68], [69, 78], [79, 81], [82, 90], [91, 98], [99, 101], [102, 108], [109, 119], [120, 123], [124, 129], [130, 135], [136, 138], [139, 145], [146, 155], [156, 159], [160, 169], [170, 171], [171, 174], [174, 175], [176, 178], [179, 183], [183, 184]]}
{"doc_key": "ai-train-66", "ner": [[58, 61, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "still", "ergodic", ",", "all", "sampling", "paths", "exhibit", "the", "same", "mean", "over", "time", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "root", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is still ergodic, all sampling paths exhibit the same mean over time and thus mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the sense of root mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 22], [23, 30], [30, 31], [32, 35], [36, 44], [45, 50], [51, 58], [59, 62], [63, 67], [68, 72], [73, 77], [78, 82], [83, 86], [87, 91], [92, 97], [98, 99], [100, 101], [102, 103], [104, 105], [105, 106], [107, 108], [109, 110], [111, 112], [113, 114], [114, 115], [116, 117], [117, 118], [119, 122], [122, 123], [124, 126], [127, 134], [135, 136], [136, 137], [137, 138], [139, 140], [141, 142], [143, 144], [145, 146], [146, 147], [148, 149], [150, 151], [152, 153], [154, 155], [155, 156], [157, 158], [158, 159], [160, 163], [163, 164], [165, 166], [167, 171], [172, 174], [175, 178], [179, 184], [185, 187], [188, 192], [193, 197], [198, 204], [205, 210], [210, 211]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [34, 36, "algorithm"], [38, 38, "algorithm"], [11, 44, "misc"], [50, 52, "algorithm"], [54, 55, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[13, 15, 11, 44, "related-to", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 11, 44, "related-to", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 11, 44, "related-to", "", false, false], [31, 31, 27, 29, "named", "", false, false], [34, 36, 11, 44, "related-to", "", false, false], [38, 38, 34, 36, "named", "", false, false], [50, 52, 54, 55, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorization", "(", "NMF", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "by", "K", "-", "NN", "on", "feature", "vectors", "in", "low-", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorization (NMF) techniques as a pre-processing step, followed by clustering by K-NN on feature vectors in low-dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [157, 158], [159, 168], [169, 180], [181, 189], [190, 191], [191, 194], [194, 195], [196, 198], [199, 211], [212, 218], [219, 232], [233, 234], [234, 237], [237, 238], [239, 249], [250, 252], [253, 254], [255, 269], [270, 274], [274, 275], [276, 284], [285, 287], [288, 298], [299, 301], [302, 303], [303, 304], [304, 306], [307, 309], [310, 317], [318, 325], [326, 328], [329, 333], [333, 344], [345, 350], [350, 351]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[6, 8, "task"], [10, 15, "task"], [30, 31, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[6, 8, 10, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognizing", "named", "entities", "in", "the", "text", "is", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognizing named entities in the text is Named Entity Recognition, while the task of determining the identity of named entities mentioned in the text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 59], [60, 66], [67, 78], [78, 79], [80, 85], [86, 89], [90, 94], [95, 97], [98, 109], [110, 113], [114, 122], [123, 125], [126, 131], [132, 140], [141, 150], [151, 153], [154, 157], [158, 162], [163, 165], [166, 172], [173, 179], [180, 187], [187, 188]]}
{"doc_key": "ai-train-70", "ner": [[1, 1, "algorithm"], [28, 28, "programlang"], [27, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 27, 27, "part-of", "", true, false], [27, 27, 28, 28, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "starting", "with", "version", "0.8.0", ",", "they", "were", "published", "in", "a", "separate", "sigmoid", "R", "package", "with", "the", "intention", "of", "allowing", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, starting with version 0.8.0, they were published in a separate sigmoid R package with the intention of allowing more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 107], [108, 112], [113, 120], [121, 126], [126, 127], [128, 132], [133, 137], [138, 147], [148, 150], [151, 152], [153, 161], [162, 169], [170, 171], [172, 179], [180, 184], [185, 188], [189, 198], [199, 201], [202, 210], [211, 215], [216, 223], [224, 227], [227, 228]]}
{"doc_key": "ai-train-71", "ner": [[0, 1, "programlang"], [7, 11, "organisation"], [13, 13, "organisation"], [20, 20, "location"], [22, 22, "location"], [25, 26, "researcher"], [28, 29, "researcher"], [31, 32, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 1, 25, 26, "artifact", "", true, false], [0, 1, 28, 29, "artifact", "", true, false], [0, 1, 31, 32, "artifact", "", true, false], [13, 13, 7, 11, "named", "", false, false], [13, 13, 20, 20, "physical", "", false, false], [20, 20, 22, 22, "physical", "", false, false], [25, 26, 7, 11, "role", "", false, false], [28, 29, 7, 11, "role", "", false, false], [31, 32, 7, 11, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "research", "firm", "in", "Cambridge", ",", "Massachusetts", ",", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The logo was created in 1967 at Bolt, Beranek and Newman (BBN), a research firm in Cambridge, Massachusetts, by Wally Feurzeig, Cynthia Solomon and Seymour Papert.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 36], [36, 37], [38, 45], [46, 49], [50, 56], [57, 58], [58, 61], [61, 62], [62, 63], [64, 65], [66, 74], [75, 79], [80, 82], [83, 92], [92, 93], [94, 107], [107, 108], [109, 111], [112, 117], [118, 126], [126, 127], [128, 135], [136, 143], [144, 147], [148, 155], [156, 162], [162, 163]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [17, 18, "field"], [22, 23, "algorithm"], [26, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 17, 18, "compare", "", false, false], [22, 23, 17, 18, "part-of", "", false, false], [26, 27, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "compared", "to", "conventional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and can be compared to conventional deep learning techniques that use gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 86], [87, 89], [90, 98], [99, 101], [102, 114], [115, 119], [120, 128], [129, 139], [140, 144], [145, 148], [149, 157], [158, 165], [166, 168], [169, 170], [171, 177], [178, 185], [186, 190], [191, 192], [193, 198], [199, 207], [207, 208]]}
{"doc_key": "ai-train-73", "ner": [[3, 5, "algorithm"], [55, 57, "metrics"], [59, 59, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[59, 59, 55, 57, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "least", "squares", "to", "fit", "a", "function", "in", "the", "form", "of", "a", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "under", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "we", "could", "then", "evaluate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use least squares to fit a function in the form of a hyperplane \u0177 = a + \u03b2 supT/sup x to data (x sub i/sub, y sub i/sub) under 1 \u2264 i \u2264n/sub, we could then evaluate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 15], [16, 23], [24, 26], [27, 30], [31, 32], [33, 41], [42, 44], [45, 48], [49, 53], [54, 56], [57, 58], [59, 69], [70, 71], [72, 73], [74, 75], [76, 77], [78, 79], [80, 84], [84, 85], [85, 88], [89, 90], [91, 93], [94, 98], [99, 100], [100, 101], [102, 105], [106, 107], [107, 108], [108, 111], [111, 112], [113, 114], [115, 118], [119, 120], [120, 121], [121, 124], [124, 125], [126, 131], [132, 133], [134, 135], [136, 137], [138, 140], [140, 141], [141, 144], [144, 145], [146, 148], [149, 154], [155, 159], [160, 168], [169, 172], [173, 176], [177, 182], [183, 186], [187, 191], [192, 199], [200, 205], [206, 207], [207, 210], [210, 211], [211, 212]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [48, 49, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 152], [152, 153], [154, 160], [160, 161], [162, 171], [171, 172], [173, 178], [179, 185], [185, 186], [187, 192], [192, 193], [194, 200], [200, 201], [202, 210], [210, 211], [212, 218], [219, 222], [223, 226], [227, 233], [234, 241], [241, 242]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 8, "field"], [13, 13, "organisation"], [16, 20, "university"], [27, 29, "organisation"], [31, 34, "university"], [42, 43, "university"], [45, 45, "university"], [49, 51, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 8, "topic", "", false, false], [3, 3, 13, 13, "origin", "", false, false], [3, 3, 16, 20, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "as", "well", "as", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering (2000) from Inria and the University of Nice Sophia Antipolis and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, as well as visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 90], [91, 93], [94, 98], [99, 105], [106, 115], [116, 119], [120, 123], [124, 128], [129, 138], [139, 148], [149, 151], [152, 159], [160, 169], [170, 180], [180, 181], [182, 187], [188, 191], [192, 197], [198, 207], [207, 208], [209, 211], [212, 216], [217, 219], [220, 228], [229, 238], [239, 241], [242, 249], [250, 260], [260, 261], [262, 266], [267, 277], [278, 281], [282, 285], [286, 296], [297, 299], [300, 307], [307, 308]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [10, 10, "researcher"], [14, 15, "product"], [18, 19, "country"], [22, 22, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 7, 8, "role", "licensing_patent_to", false, false], [10, 10, 18, 19, "physical", "", false, false], [22, 22, 10, 10, "artifact", "", false, false], [22, 22, 14, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["By", "licensing", "the", "original", "patent", "to", "inventor", "George", "Devol", ",", "Engelberger", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "the", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "By licensing the original patent to inventor George Devol, Engelberger developed the first industrial robot in the United States, the Unimate, in the 1950s.", "token2charspan": [[0, 2], [3, 12], [13, 16], [17, 25], [26, 32], [33, 35], [36, 44], [45, 51], [52, 57], [57, 58], [59, 70], [71, 80], [81, 84], [85, 90], [91, 101], [102, 107], [108, 110], [111, 114], [115, 121], [122, 128], [128, 129], [130, 133], [134, 141], [141, 142], [143, 145], [146, 149], [150, 155], [155, 156]]}
{"doc_key": "ai-train-77", "ner": [[4, 6, "task"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[3, 3, "programlang"], [6, 6, "programlang"], [14, 14, "programlang"], [17, 17, "programlang"], [26, 26, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 3, 14, 14, "named", "", false, false], [6, 6, 3, 3, "origin", "descendant_of", false, false], [6, 6, 17, 17, "general-affiliation", "", false, false], [6, 6, 26, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Descendants", "of", "the", "CLIPS", "language", "include", "Jess", "(", "the", "rule", "-", "based", "portion", "of", "CLIPS", "rewritten", "in", "Java", ",", "later", "grown", "in", "another", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by"], "sentence-detokenized": "Descendants of the CLIPS language include Jess (the rule-based portion of CLIPS rewritten in Java, later grown in another direction), JESS was originally inspired by", "token2charspan": [[0, 11], [12, 14], [15, 18], [19, 24], [25, 33], [34, 41], [42, 46], [47, 48], [48, 51], [52, 56], [56, 57], [57, 62], [63, 70], [71, 73], [74, 79], [80, 89], [90, 92], [93, 97], [97, 98], [99, 104], [105, 110], [111, 113], [114, 121], [122, 131], [131, 132], [132, 133], [134, 138], [139, 142], [143, 153], [154, 162], [163, 165]]}
{"doc_key": "ai-train-79", "ner": [[6, 6, "product"], [10, 14, "product"], [16, 17, "organisation"], [21, 22, "product"], [42, 45, "product"], [40, 44, "product"], [61, 62, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[10, 14, 6, 6, "type-of", "", false, false], [16, 17, 10, 14, "usage", "", false, false], [21, 22, 16, 17, "artifact", "", false, false], [42, 45, 16, 17, "origin", "", true, false], [42, 45, 61, 62, "related-to", "", true, false], [40, 44, 16, 17, "origin", "", true, false], [40, 44, 61, 62, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["He", "has", "also", "created", "flexible", "intelligent", "AGV", "applications", ",", "designing", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", ",", "used", "for", "complex", "pick", "and", "place", "operations", ",", "along", "with", "industrial", "robotic", "gate", "and", "arm", "systems", "used", "in", "leading", "automotive", "supply", "plants", "to", "move", "products", "from", "one", "process", "to", "another", "in", "non-linear", "configurations", "."], "sentence-detokenized": "He has also created flexible intelligent AGV applications, designing the Motivity control system used by RMT Robotics to develop the ADAM iAGV (Self-Guided Vehicle), used for complex pick and place operations, along with industrial robotic gate and arm systems used in leading automotive supply plants to move products from one process to another in non-linear configurations.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 19], [20, 28], [29, 40], [41, 44], [45, 57], [57, 58], [59, 68], [69, 72], [73, 81], [82, 89], [90, 96], [97, 101], [102, 104], [105, 108], [109, 117], [118, 120], [121, 128], [129, 132], [133, 137], [138, 142], [143, 144], [144, 148], [148, 149], [149, 155], [156, 163], [163, 164], [164, 165], [166, 170], [171, 174], [175, 182], [183, 187], [188, 191], [192, 197], [198, 208], [208, 209], [210, 215], [216, 220], [221, 231], [232, 239], [240, 244], [245, 248], [249, 252], [253, 260], [261, 265], [266, 268], [269, 276], [277, 287], [288, 294], [295, 301], [302, 304], [305, 309], [310, 318], [319, 323], [324, 327], [328, 335], [336, 338], [339, 346], [347, 349], [350, 360], [361, 375], [375, 376]]}
{"doc_key": "ai-train-80", "ner": [[7, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "\u03b2", "parameters", "are", "typically", "estimated", "by", "maximum", "likelihood", "."], "sentence-detokenized": "The \u03b2 parameters are typically estimated by maximum likelihood.", "token2charspan": [[0, 3], [4, 5], [6, 16], [17, 20], [21, 30], [31, 40], [41, 43], [44, 51], [52, 62], [62, 63]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 0, 1, "part-of", "", false, false], [8, 8, 0, 1, "part-of", "", false, false], [10, 10, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", ",", "such", "as", "precision", "and", "recall", "or", "DCG", ",", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics, such as precision and recall or DCG, are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [29, 30], [31, 35], [36, 38], [39, 48], [49, 52], [53, 59], [60, 62], [63, 66], [66, 67], [68, 71], [72, 78], [79, 82], [83, 92], [93, 96], [97, 104], [105, 107], [108, 109], [110, 124], [125, 131], [131, 132]]}
{"doc_key": "ai-train-82", "ner": [[6, 8, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "contains", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 26], [27, 35], [36, 38], [39, 49], [50, 56], [57, 64], [65, 67], [68, 73], [74, 83], [84, 94], [95, 100], [100, 101], [102, 106], [107, 110], [111, 116], [117, 120], [121, 126], [127, 130], [131, 136], [137, 144], [144, 145]]}
{"doc_key": "ai-train-83", "ner": [[5, 7, "product"], [13, 14, "field"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [30, 31, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[13, 14, 5, 7, "usage", "", false, true], [18, 19, 13, 14, "part-of", "", false, false], [21, 22, 13, 14, "part-of", "", false, false], [24, 25, 13, 14, "part-of", "", false, false], [27, 28, 13, 14, "part-of", "", false, false], [30, 31, 13, 14, "part-of", "", false, false], [33, 34, 13, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", "including", ":", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "enhancement", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications including: image segmentation, feature generation, face extraction, motion detection, region enhancement and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [89, 98], [98, 99], [100, 105], [106, 118], [118, 119], [120, 127], [128, 138], [138, 139], [140, 144], [145, 155], [155, 156], [157, 163], [164, 173], [173, 174], [175, 181], [182, 193], [194, 197], [198, 203], [204, 213], [213, 214]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 16, "field"], [22, 22, "misc"], [26, 32, "conference"], [34, 34, "conference"], [38, 41, "misc"], [44, 48, "conference"], [49, 50, "conference"], [52, 56, "conference"], [58, 58, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 16, 16, "related-to", "contributes_to", false, false], [0, 0, 22, 22, "win-defeat", "", false, false], [0, 0, 38, 41, "win-defeat", "", false, false], [22, 22, 26, 32, "temporal", "", false, false], [34, 34, 26, 32, "named", "", false, false], [38, 41, 44, 48, "temporal", "", false, false], [38, 41, 52, 56, "temporal", "", false, false], [49, 50, 44, 48, "named", "", false, false], [58, 58, 52, 56, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "and", "the", "Best", "Reviewer", "Award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision and won the Best Paper Award at the International Conference on Non-Photorealistic Rendering and Animation (NPAR) 2012 and the Best Reviewer Award at the Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 160], [161, 171], [172, 174], [175, 193], [194, 203], [204, 207], [208, 217], [218, 219], [219, 223], [223, 224], [225, 229], [230, 233], [234, 237], [238, 242], [243, 251], [252, 257], [258, 260], [261, 264], [265, 270], [271, 281], [282, 284], [285, 293], [294, 300], [301, 305], [306, 310], [311, 314], [315, 328], [329, 339], [340, 342], [343, 351], [352, 358], [359, 360], [360, 364], [364, 365], [366, 370], [370, 371]]}
{"doc_key": "ai-train-85", "ner": [[2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 14, "researcher"], [0, 18, "misc"]], "ner_mapping_to_source": [1, 2, 3, 4, 5], "relations": [[0, 18, 13, 14, "origin", "", false, false]], "relations_mapping_to_source": [4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "an", "ontological", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is an ontological language used by Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 70], [71, 79], [80, 84], [85, 87], [88, 92], [93, 98], [98, 100], [101, 104], [105, 115], [116, 123], [123, 124]]}
{"doc_key": "ai-train-86", "ner": [[3, 9, "task"], [8, 15, "metrics"], [16, 19, "metrics"], [21, 28, "metrics"], [37, 40, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 15, 3, 9, "part-of", "", false, false], [16, 19, 8, 15, "named", "", false, false], [21, 28, 8, 15, "named", "", false, false], [37, 40, 8, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", ",", "in", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "value", "of", "the", "squared", "deviations", "of", "predictions", "from", "the", "TRUE", "values", ",", "over", "an", "out", "-", "of", "-", "sample", "test", "space", ",", "generated", "by", "a", "model", "estimated", "over", "a", "given", "sample", "space", "."], "sentence-detokenized": "Also, in regression analysis, the mean squared error, often referred to as the mean squared prediction error or out-of-sample mean squared error, can refer to the average value of the squared deviations of predictions from the TRUE values, over an out-of-sample test space, generated by a model estimated over a given sample space.", "token2charspan": [[0, 4], [4, 5], [6, 8], [9, 19], [20, 28], [28, 29], [30, 33], [34, 38], [39, 46], [47, 52], [52, 53], [54, 59], [60, 68], [69, 71], [72, 74], [75, 78], [79, 83], [84, 91], [92, 102], [103, 108], [109, 111], [112, 115], [115, 116], [116, 118], [118, 119], [119, 125], [126, 130], [131, 138], [139, 144], [144, 145], [146, 149], [150, 155], [156, 158], [159, 162], [163, 170], [171, 176], [177, 179], [180, 183], [184, 191], [192, 202], [203, 205], [206, 217], [218, 222], [223, 226], [227, 231], [232, 238], [238, 239], [240, 244], [245, 247], [248, 251], [251, 252], [252, 254], [254, 255], [255, 261], [262, 266], [267, 272], [272, 273], [274, 283], [284, 286], [287, 288], [289, 294], [295, 304], [305, 309], [310, 311], [312, 317], [318, 324], [325, 330], [330, 331]]}
{"doc_key": "ai-train-87", "ner": [[10, 11, "algorithm"], [5, 22, "algorithm"], [35, 36, "metrics"]], "ner_mapping_to_source": [1, 2, 3], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "terms", "of", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptors", "retaining", "a", "slight", "advantage", "in", "terms", "of", "error", "rate", "detection", "at", "fixed", "positive", "FALSE", "rates", "in", "both", "datasets", "."], "sentence-detokenized": "In terms of results, the C-HOG and R-HOG block descriptors perform comparably, with the C-HOG descriptors retaining a slight advantage in terms of error rate detection at fixed positive FALSE rates in both datasets.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 19], [19, 20], [21, 24], [25, 26], [26, 27], [27, 30], [31, 34], [35, 37], [37, 40], [41, 46], [47, 58], [59, 66], [67, 77], [77, 78], [79, 83], [84, 87], [88, 89], [89, 90], [90, 93], [94, 105], [106, 115], [116, 117], [118, 124], [125, 134], [135, 137], [138, 143], [144, 146], [147, 152], [153, 157], [158, 167], [168, 170], [171, 176], [177, 185], [186, 191], [192, 197], [198, 200], [201, 205], [206, 214], [214, 215]]}
{"doc_key": "ai-train-88", "ner": [[4, 7, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 14, "algorithm"], [17, 19, "algorithm"], [21, 23, "algorithm"], [25, 27, "algorithm"], [29, 32, "misc"], [15, 38, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 7, 8, 8, "usage", "", false, false], [10, 12, 29, 32, "usage", "", false, false], [14, 14, 29, 32, "usage", "", false, false], [17, 19, 29, 32, "usage", "", false, false], [21, 23, 29, 32, "usage", "", false, false], [25, 27, 29, 32, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "the", "Fisherface", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neuron", "-", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using the Fisherface algorithm, hidden Markov model, multilinear subspace learning using tensor representation, and neuron-motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 142], [143, 153], [154, 163], [163, 164], [165, 171], [172, 178], [179, 184], [184, 185], [186, 197], [198, 206], [207, 215], [216, 221], [222, 228], [229, 243], [243, 244], [245, 248], [249, 255], [255, 256], [256, 265], [266, 273], [274, 278], [279, 287], [287, 288]]}
{"doc_key": "ai-train-89", "ner": [[3, 7, "misc"], [17, 19, "location"], [34, 36, "location"], [50, 50, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 19, 3, 7, "temporal", "", false, false], [34, 36, 3, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "may", "be", "restricted", "from", "screening", "at", "the", "Scotiabank", "Theatre", "Toronto", "-", "one", "of", "the", "main", "festival", "venues", "-", "and", "screen", "elsewhere", "(", "such", "as", "TIFF", "Bell", "Lightbox", "and", "other", "local", "theatres", ")", "if", "they", "are", "distributed", "by", "a", "service", "like", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films may be restricted from screening at the Scotiabank Theatre Toronto - one of the main festival venues - and screen elsewhere (such as TIFF Bell Lightbox and other local theatres) if they are distributed by a service like Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 69], [70, 72], [73, 83], [84, 88], [89, 98], [99, 101], [102, 105], [106, 116], [117, 124], [125, 132], [133, 134], [135, 138], [139, 141], [142, 145], [146, 150], [151, 159], [160, 166], [167, 168], [169, 172], [173, 179], [180, 189], [190, 191], [191, 195], [196, 198], [199, 203], [204, 208], [209, 217], [218, 221], [222, 227], [228, 233], [234, 242], [242, 243], [244, 246], [247, 251], [252, 255], [256, 267], [268, 270], [271, 272], [273, 280], [281, 285], [286, 293], [293, 294]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 3, "researcher"], [4, 7, "organisation"], [22, 26, "product"], [35, 35, "researcher"], [44, 46, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 6], "relations": [[0, 0, 4, 7, "related-to", "purchases", false, false], [2, 3, 35, 35, "named", "same", false, false], [4, 7, 2, 3, "origin", "founded_by", false, false], [22, 26, 0, 0, "artifact", "", false, false], [44, 46, 35, 35, "artifact", "", true, false]], "relations_mapping_to_source": [0, 2, 3, 4, 5], "sentence": ["Unimation", "purchased", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", "and", ",", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "producing", "the", "Universal", "Programmable", "Assembly", "Machine", ",", "a", "new", "robotic", "arm", "design", ",", "using", "Scheinman", "'s", "state", "-", "of", "-", "the", "-", "art", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation purchased Victor Scheinman's Vicarm Inc. in 1977 and, with Scheinman's help, the company created and began producing the Universal Programmable Assembly Machine, a new robotic arm design, using Scheinman's state-of-the-art VAL programming language.", "token2charspan": [[0, 9], [10, 19], [20, 26], [27, 36], [36, 38], [39, 45], [46, 50], [51, 53], [54, 58], [59, 62], [62, 63], [64, 68], [69, 78], [78, 80], [81, 85], [85, 86], [87, 90], [91, 98], [99, 106], [107, 110], [111, 116], [117, 126], [127, 130], [131, 140], [141, 153], [154, 162], [163, 170], [170, 171], [172, 173], [174, 177], [178, 185], [186, 189], [190, 196], [196, 197], [198, 203], [204, 213], [213, 215], [216, 221], [221, 222], [222, 224], [224, 225], [225, 228], [228, 229], [229, 232], [233, 236], [237, 248], [249, 257], [257, 258]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [10, 11, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 10, 11, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[0, 4, "metrics"], [14, 15, "product"], [20, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 4, 14, 15, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", ",", "according", "to", "Google", "Scholar", ",", "and", "also", "received", "the", "2016", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", ",", "which", "indicates", "an", "unusually", "high", "-", "impact", "paper", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20,000 times, according to Google Scholar, and also received the 2016 IEEE Signal Processing Society Sustained Impact Award, which indicates an unusually high-impact paper for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 51], [52, 57], [57, 58], [59, 68], [69, 71], [72, 78], [79, 86], [86, 87], [88, 91], [92, 96], [97, 105], [106, 109], [110, 114], [115, 119], [120, 126], [127, 137], [138, 145], [146, 155], [156, 162], [163, 168], [168, 169], [170, 175], [176, 185], [186, 188], [189, 198], [199, 203], [203, 204], [204, 210], [211, 216], [217, 220], [221, 223], [224, 229], [230, 232], [233, 238], [239, 244], [245, 256], [256, 257]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [24, 25, "product"], [34, 36, "product"], [39, 39, "organisation"], [40, 40, "product"], [45, 45, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 39, 39, "artifact", "", false, false], [24, 25, 0, 1, "related-to", "performs", false, false], [24, 25, 34, 36, "part-of", "", false, false], [39, 39, 45, 45, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "getting", "closer", "to", "being", "completely", "indistinguishable", "from", "the", "real", "human", "voice", "with", "the", "2016", "introduction", "of", "voice", "editing", "and", "generation", "software", "Adobe", "Voco", ",", "a", "prototype", "to", "be", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", "."], "sentence-detokenized": "Speech synthesis is getting closer to being completely indistinguishable from the real human voice with the 2016 introduction of voice editing and generation software Adobe Voco, a prototype to be part of the Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 27], [28, 34], [35, 37], [38, 43], [44, 54], [55, 72], [73, 77], [78, 81], [82, 86], [87, 92], [93, 98], [99, 103], [104, 107], [108, 112], [113, 125], [126, 128], [129, 134], [135, 142], [143, 146], [147, 157], [158, 166], [167, 172], [173, 177], [177, 178], [179, 180], [181, 190], [191, 193], [194, 196], [197, 201], [202, 204], [205, 208], [209, 214], [215, 223], [224, 229], [229, 230], [231, 234], [235, 243], [244, 251], [251, 252], [253, 254], [255, 264], [265, 269], [270, 276], [276, 277]]}
{"doc_key": "ai-train-94", "ner": [[0, 4, "researcher"], [7, 9, "organisation"], [15, 25, "organisation"], [26, 26, "conference"], [34, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 4, 7, 9, "role", "", false, false], [0, 4, 15, 25, "role", "", false, false], [0, 4, 26, 26, "role", "", false, false], [0, 4, 34, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "member", "of", "AAAI", ",", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary fellow of the Neuroscience Research Program, a fellow of the American Academy of Arts and Sciences, a founding member of AAAI, and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 123], [124, 132], [133, 139], [140, 142], [143, 147], [147, 148], [149, 152], [153, 154], [155, 163], [164, 170], [171, 173], [174, 177], [178, 186], [187, 196], [197, 200], [201, 206], [207, 215], [215, 216]]}
{"doc_key": "ai-train-95", "ner": [[9, 9, "task"], [8, 11, "task"], [18, 19, "task"], [26, 26, "misc"], [27, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 9, 18, 19, "cause-effect", "", false, false], [8, 11, 18, 19, "cause-effect", "", false, false], [27, 28, 18, 19, "topic", "", false, false], [27, 28, 26, 26, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "encouraged", "by", "successes", "in", "speech", "recognition", "and", "synthesis", ",", "research", "in", "the", "field", "of", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, encouraged by successes in speech recognition and synthesis, research in the field of speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 37], [38, 40], [41, 47], [48, 59], [60, 63], [64, 73], [73, 74], [75, 83], [84, 86], [87, 90], [91, 96], [97, 99], [100, 106], [107, 118], [119, 124], [125, 129], [130, 133], [134, 145], [146, 148], [149, 152], [153, 159], [160, 169], [170, 177], [177, 178]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 15, "algorithm"], [21, 22, "algorithm"], [26, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 15, 3, 4, "origin", "", false, false], [15, 15, 8, 9, "origin", "", false, false], [15, 15, 11, 12, "origin", "", false, false], [15, 15, 26, 27, "part-of", "", false, false], [21, 22, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisors", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "oblivion", "gate", "(", "also", "called", "the", "retention", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisors J\u00fcrgen Schmidhuber and Fred Cummins introduced the oblivion gate (also called the retention gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 87], [88, 96], [97, 101], [102, 103], [103, 107], [108, 114], [115, 118], [119, 128], [129, 133], [133, 134], [135, 139], [140, 143], [144, 148], [149, 161], [161, 162]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 1, 3, "part-of", "", false, false], [9, 11, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sinc", "function", "is", "commonly", "defined", "by"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sinc function is commonly defined by", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 93], [94, 101], [102, 104]]}
{"doc_key": "ai-train-98", "ner": [[2, 2, "field"], [8, 9, "researcher"], [17, 21, "conference"], [23, 27, "organisation"], [29, 29, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 2, 8, 9, "origin", "coined_term", false, false], [8, 9, 17, 21, "role", "", false, false], [8, 9, 23, 27, "role", "", false, false], [29, 29, 23, 27, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "for", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee for Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 38], [39, 44], [45, 51], [52, 54], [55, 60], [61, 65], [65, 66], [67, 68], [69, 77], [78, 84], [85, 87], [88, 92], [93, 96], [97, 108], [109, 112], [113, 126], [127, 138], [139, 142], [143, 146], [147, 160], [161, 170], [171, 174], [175, 188], [189, 200], [201, 202], [202, 206], [206, 207], [207, 208]]}
{"doc_key": "ai-train-99", "ner": [[11, 21, "misc"], [13, 13, "misc"], [39, 41, "metrics"], [43, 43, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[43, 43, 39, 41, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct", "2011", "In", "the", "case", "of", "one", "-dimensional", "DPD", "with", "memory", "(", "or", "without", "memory", ")", "based", "polynomial", ",", "to", "solve", "for", "the", "polynomial", "polynomial", "coefficients", "of", "the", "digital", "predistortion", "and", "to", "minimize", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "captures", "the", "nonlinear", "products", "of", "the", "order", "of", "the", "digital", "predistortion", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct 2011 In the case of one-dimensional DPD with memory (or without memory) based polynomial, to solve for the polynomial polynomial coefficients of the digital predistortion and to minimize the mean squared error (MSE), the distorted output of the nonlinear system must be oversampled at a rate that captures the nonlinear products of the order of the digital predistortion.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 22], [23, 27], [28, 30], [31, 34], [35, 39], [40, 42], [43, 46], [46, 58], [59, 62], [63, 67], [68, 74], [75, 76], [76, 78], [79, 86], [87, 93], [93, 94], [95, 100], [101, 111], [111, 112], [113, 115], [116, 121], [122, 125], [126, 129], [130, 140], [141, 151], [152, 164], [165, 167], [168, 171], [172, 179], [180, 193], [194, 197], [198, 200], [201, 209], [210, 213], [214, 218], [219, 226], [227, 232], [233, 234], [234, 237], [237, 238], [238, 239], [240, 243], [244, 253], [254, 260], [261, 263], [264, 267], [268, 277], [278, 284], [285, 289], [290, 292], [293, 304], [305, 307], [308, 309], [310, 314], [315, 319], [320, 328], [329, 332], [333, 342], [343, 351], [352, 354], [355, 358], [359, 364], [365, 367], [368, 371], [372, 379], [380, 393], [393, 394]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [12, 13, "location"], [15, 16, "country"], [10, 20, "location"], [22, 22, "country"], [37, 44, "organisation"], [46, 49, "organisation"], [51, 51, "location"], [57, 59, "organisation"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 46, 49, "physical", "", false, false], [0, 1, 57, 59, "role", "", false, false], [12, 13, 15, 16, "physical", "", false, false], [37, 44, 46, 49, "part-of", "", false, false], [46, 49, 51, 51, "physical", "", false, false], [57, 59, 37, 44, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "October", "5", ",", "1947", ",", "Chisinau", ",", "Moldovan", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chisinau", ",", "Moldova", ")", ")", "is", "an", "American", "senior", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "and", "head", "of", "the", "lab", "'s", "InfoLab", "Group", "."], "sentence-detokenized": "Boris Katz, (born October 5, 1947, Chisinau, Moldovan SSR, Soviet Union, (now Chisinau, Moldova)) is an American senior research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and head of the lab's InfoLab Group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 25], [26, 27], [27, 28], [29, 33], [33, 34], [35, 43], [43, 44], [45, 53], [54, 57], [57, 58], [59, 65], [66, 71], [71, 72], [73, 74], [74, 77], [78, 86], [86, 87], [88, 95], [95, 96], [96, 97], [98, 100], [101, 103], [104, 112], [113, 119], [120, 128], [129, 138], [139, 140], [140, 148], [149, 158], [158, 159], [160, 162], [163, 166], [167, 170], [171, 179], [180, 187], [188, 191], [192, 202], [203, 215], [216, 226], [227, 229], [230, 233], [234, 247], [248, 257], [258, 260], [261, 271], [272, 274], [275, 284], [285, 288], [289, 293], [294, 296], [297, 300], [301, 304], [304, 306], [307, 314], [315, 320], [320, 321]]}
