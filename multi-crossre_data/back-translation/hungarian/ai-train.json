{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [30, 31, "field"], [32, 34, "researcher"], [36, 38, "researcher"], [40, 41, "researcher"], [43, 44, "researcher"], [46, 48, "researcher"], [50, 51, "researcher"], [53, 54, "researcher"], [56, 57, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 20, "part-of", "", false, false], [3, 7, 19, 20, "usage", "", false, false], [3, 7, 30, 31, "part-of", "", false, false], [3, 7, 30, 31, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "opinion", "-", "based", "recommender", "systems", "use", "various", "techniques", ",", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "Multimodal", "Sentiment", "Analysis", ")", ",", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to opinion-based recommender systems use various techniques, including text mining, information retrieval, sentiment analysis (see also Multimodal Sentiment Analysis), and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 55], [56, 59], [60, 67], [68, 78], [78, 79], [80, 89], [90, 94], [95, 101], [101, 102], [103, 114], [115, 124], [124, 125], [126, 135], [136, 144], [145, 146], [146, 149], [150, 154], [155, 165], [166, 175], [176, 184], [184, 185], [185, 186], [187, 190], [191, 195], [196, 204], [205, 208], [208, 209], [210, 214], [214, 215], [216, 217], [217, 218], [219, 224], [224, 225], [226, 230], [231, 234], [234, 235], [236, 240], [241, 246], [246, 247], [248, 249], [249, 250], [251, 254], [254, 255], [256, 260], [261, 266], [266, 267], [268, 272], [273, 277], [277, 278], [279, 281], [282, 284], [284, 285], [286, 287], [287, 291], [291, 292], [292, 293], [293, 294], [295, 297], [298, 299], [299, 300], [300, 301], [301, 302], [303, 309], [309, 310]]}
{"doc_key": "ai-train-2", "ner": [[8, 8, "university"], [12, 13, "researcher"], [15, 16, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[12, 13, 8, 8, "physical", "", false, false], [12, 13, 8, 8, "role", "", false, false], [15, 16, 8, 8, "physical", "", false, false], [15, 16, 8, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "proponents", "of", "procedural", "representations", "were", "mainly", "at", "MIT", ",", "led", "by", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The proponents of procedural representations were mainly at MIT, led by Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 3], [4, 14], [15, 17], [18, 28], [29, 44], [45, 49], [50, 56], [57, 59], [60, 63], [63, 64], [65, 68], [69, 71], [72, 78], [79, 85], [86, 89], [90, 97], [98, 104], [104, 105]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [26, 26, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 26, 26, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "you", "solve", "linear", "and", "nonlinear", "problems", "numerically", ",", "as", "well", "as", "perform", "other", "numerical", "experiments", ",", "using", "a", "program", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps you solve linear and nonlinear problems numerically, as well as perform other numerical experiments, using a program that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 16], [17, 22], [23, 29], [30, 33], [34, 43], [44, 52], [53, 64], [64, 65], [66, 68], [69, 73], [74, 76], [77, 84], [85, 90], [91, 100], [101, 112], [112, 113], [114, 119], [120, 121], [122, 129], [130, 134], [135, 137], [138, 144], [145, 155], [156, 160], [161, 167], [167, 168]]}
{"doc_key": "ai-train-5", "ner": [[2, 6, "algorithm"], [11, 12, "misc"], [15, 16, "researcher"], [21, 23, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[2, 6, 15, 16, "origin", "", false, false], [11, 12, 15, 16, "origin", "", false, false], [15, 16, 21, 23, "physical", "", false, false], [15, 16, 21, 23, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", ",", "as", "well", "as", "unsupervised", "methods", "used", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", ",", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm, as well as unsupervised methods used by Geoff Hinton and colleagues at the University of Toronto, can be used to train deep, highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [42, 43], [44, 46], [47, 51], [52, 54], [55, 67], [68, 75], [76, 80], [81, 83], [84, 89], [90, 96], [97, 100], [101, 111], [112, 114], [115, 118], [119, 129], [130, 132], [133, 140], [140, 141], [142, 145], [146, 148], [149, 153], [154, 156], [157, 162], [163, 167], [167, 168], [169, 175], [176, 185], [186, 192], [193, 206], [206, 207], [208, 209], [209, 210], [210, 214], [215, 222]]}
{"doc_key": "ai-train-6", "ner": [[1, 1, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "DCG", "marking", "accordingly", ":"], "sentence-detokenized": "or DCG marking accordingly:", "token2charspan": [[0, 2], [3, 6], [7, 14], [15, 26], [26, 27]]}
{"doc_key": "ai-train-7", "ner": [[0, 4, "algorithm"], [6, 13, "algorithm"], [15, 18, "algorithm"], [21, 23, "algorithm"], [27, 27, "algorithm"], [28, 30, "algorithm"], [45, 48, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 4, 6, 13, "type-of", "", false, false], [0, 4, 15, 18, "usage", "part-of?", true, false], [15, 18, 21, 23, "compare", "", false, false], [27, 27, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "are", "different", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "use", "competitive", "learning", "(", "as", "opposed", "to", "error", "-correcting", "learning", ",", "such", "as", "backpropagation", "by", "gradient", "descent", ")", "and", "in", "the", "sense", "that", "they", "use", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps are different from other artificial neural networks in that they use competitive learning (as opposed to error-correcting learning, such as backpropagation by gradient descent) and in the sense that they use a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 24], [25, 34], [35, 39], [40, 45], [46, 56], [57, 63], [64, 72], [73, 75], [76, 80], [81, 85], [86, 89], [90, 101], [102, 110], [111, 112], [112, 114], [115, 122], [123, 125], [126, 131], [131, 142], [143, 151], [151, 152], [153, 157], [158, 160], [161, 176], [177, 179], [180, 188], [189, 196], [196, 197], [198, 201], [202, 204], [205, 208], [209, 214], [215, 219], [220, 224], [225, 228], [229, 230], [231, 243], [244, 252], [253, 255], [256, 264], [265, 268], [269, 280], [281, 291], [292, 294], [295, 298], [299, 304], [305, 310], [310, 311]]}
{"doc_key": "ai-train-8", "ner": [[10, 14, "organisation"], [28, 29, "misc"], [36, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "several", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "should", "be", "made", "in", "the", "presence", "of", "a", "sound", "signal", ",", "which", "is", "then", "filtered", "out", "in", "the", "noise", "level", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "mute", "circuits", "."], "sentence-detokenized": "Since the early 1990s, several authorities, including the Audio Engineering Society, have recommended that dynamic range measurements should be made in the presence of a sound signal, which is then filtered out in the noise level measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or mute circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 30], [31, 42], [42, 43], [44, 53], [54, 57], [58, 63], [64, 75], [76, 83], [83, 84], [85, 89], [90, 101], [102, 106], [107, 114], [115, 120], [121, 133], [134, 140], [141, 143], [144, 148], [149, 151], [152, 155], [156, 164], [165, 167], [168, 169], [170, 175], [176, 182], [182, 183], [184, 189], [190, 192], [193, 197], [198, 206], [207, 210], [211, 213], [214, 217], [218, 223], [224, 229], [230, 241], [242, 246], [247, 249], [250, 259], [260, 267], [268, 273], [273, 274], [275, 279], [280, 286], [287, 299], [300, 312], [313, 318], [319, 321], [322, 325], [326, 329], [330, 332], [333, 338], [339, 344], [345, 347], [348, 352], [353, 361], [361, 362]]}
{"doc_key": "ai-train-9", "ner": [[6, 6, "misc"], [13, 14, "task"], [16, 17, "task"], [19, 21, "task"], [23, 24, "task"], [27, 31, "task"], [33, 35, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[6, 6, 13, 14, "part-of", "concept_used_in", true, false], [6, 6, 16, 17, "part-of", "concept_used_in", false, false], [6, 6, 19, 21, "part-of", "concept_used_in", false, false], [6, 6, 23, 24, "part-of", "concept_used_in", false, false], [6, 6, 27, 31, "part-of", "concept_used_in", false, false], [6, 6, 33, 35, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["Techniques", "used", "to", "create", "and", "recognise", "faces", "are", "also", "used", "in", "addition", "to", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "-", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "Techniques used to create and recognise faces are also used in addition to face recognition: handwriting recognition, lip-reading, voice recognition, sign language/hand gesture interpretation and medical image analysis.", "token2charspan": [[0, 10], [11, 15], [16, 18], [19, 25], [26, 29], [30, 39], [40, 45], [46, 49], [50, 54], [55, 59], [60, 62], [63, 71], [72, 74], [75, 79], [80, 91], [91, 92], [93, 104], [105, 116], [116, 117], [118, 121], [121, 122], [122, 129], [129, 130], [131, 136], [137, 148], [148, 149], [150, 154], [155, 163], [163, 164], [164, 168], [169, 176], [177, 191], [192, 195], [196, 203], [204, 209], [210, 218], [218, 219]]}
{"doc_key": "ai-train-10", "ner": [[1, 3, "organisation"], [10, 13, "organisation"], [15, 15, "organisation"], [19, 22, "organisation"], [25, 30, "organisation"], [34, 37, "organisation"], [41, 44, "organisation"], [46, 46, "organisation"], [50, 53, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 13, 1, 3, "part-of", "", false, false], [15, 15, 10, 13, "named", "", false, false], [19, 22, 1, 3, "part-of", "", false, false], [25, 30, 1, 3, "part-of", "", false, false], [34, 37, 1, 3, "part-of", "", false, false], [41, 44, 1, 3, "part-of", "", false, false], [46, 46, 41, 44, "named", "", false, false], [50, 53, 1, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "has", "coordinated", "studies", "with", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "(", "NIST", ")", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "to", "produce", "strategic", "planners", "'", "deliberations", "."], "sentence-detokenized": "The National Science Foundation has coordinated studies with the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce (NIST), the US Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research to produce strategic planners' deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 47], [48, 55], [56, 60], [61, 64], [65, 73], [74, 85], [86, 89], [90, 95], [96, 110], [111, 112], [112, 116], [116, 117], [117, 118], [119, 122], [123, 125], [126, 136], [137, 139], [140, 146], [146, 147], [148, 151], [152, 154], [155, 165], [166, 168], [169, 177], [178, 179], [179, 183], [183, 184], [184, 185], [186, 189], [190, 192], [193, 203], [204, 206], [207, 214], [214, 215], [216, 219], [220, 227], [228, 236], [237, 245], [246, 254], [255, 261], [262, 263], [263, 268], [268, 269], [270, 273], [274, 277], [278, 284], [285, 287], [288, 293], [294, 302], [303, 305], [306, 313], [314, 323], [324, 332], [332, 333], [334, 347], [347, 348]]}
{"doc_key": "ai-train-11", "ner": [[20, 21, "metrics"], [25, 26, "algorithm"], [3, 4, "researcher"], [10, 11, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[20, 21, 25, 26, "part-of", "", false, false], [3, 4, 10, 11, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "1935", ",", "Ronald", "Fisher", ",", "as", "an", "appendix", "to", "Bliss", "'s", "work", ",", "proposed", "a", "quick", "method", "for", "calculating", "maximum", "likelihood", "estimates", "of", "the", "probit", "model", "."], "sentence-detokenized": "In 1935, Ronald Fisher, as an appendix to Bliss's work, proposed a quick method for calculating maximum likelihood estimates of the probit model.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [16, 22], [22, 23], [24, 26], [27, 29], [30, 38], [39, 41], [42, 47], [47, 49], [50, 54], [54, 55], [56, 64], [65, 66], [67, 72], [73, 79], [80, 83], [84, 95], [96, 103], [104, 114], [115, 124], [125, 127], [128, 131], [132, 138], [139, 144], [144, 145]]}
{"doc_key": "ai-train-12", "ner": [[11, 12, "product"], [15, 16, "product"], [20, 21, "organisation"], [22, 22, "product"], [30, 30, "organisation"], [31, 31, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[22, 22, 15, 16, "usage", "uses_software", false, false], [22, 22, 20, 21, "artifact", "", false, false], [22, 22, 31, 31, "named", "", false, false], [31, 31, 30, 30, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "such", "programs", "are", "available", "on", "the", "Internet", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", ",", "which", "powers", "AltaVista", "'s", "BabelFish", "(", "as", "of", "9", "May", "2008", ",", "Yahoo", "Babelfish", ")", "."], "sentence-detokenized": "Several such programs are available on the Internet, such as Google Translate and the SYSTRAN system, which powers AltaVista's BabelFish (as of 9 May 2008, Yahoo Babelfish).", "token2charspan": [[0, 7], [8, 12], [13, 21], [22, 25], [26, 35], [36, 38], [39, 42], [43, 51], [51, 52], [53, 57], [58, 60], [61, 67], [68, 77], [78, 81], [82, 85], [86, 93], [94, 100], [100, 101], [102, 107], [108, 114], [115, 124], [124, 126], [127, 136], [137, 138], [138, 140], [141, 143], [144, 145], [146, 149], [150, 154], [154, 155], [156, 161], [162, 171], [171, 172], [172, 173]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 22, "field"], [26, 27, "misc"], [23, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 23, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 23, 32, "related-to", "", true, false], [10, 11, 20, 22, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 23, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[10, 10, "metrics"], [12, 18, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[10, 10, 12, 18, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "widely", "used", "method", "is", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measurement", "."], "sentence-detokenized": "The most widely used method is the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measurement.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 20], [21, 27], [28, 30], [31, 34], [35, 37], [37, 38], [38, 44], [45, 50], [51, 52], [52, 58], [58, 59], [59, 67], [68, 78], [79, 82], [83, 90], [91, 101], [101, 102], [103, 114], [114, 115]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [15, 15, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [0, 0, 15, 15, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "learning", "schemas", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "with", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides learning schemas, models and algorithms and can be extended with R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 84], [85, 86], [87, 90], [91, 97], [98, 105], [105, 106], [107, 112], [113, 119], [119, 120], [121, 126], [127, 135], [135, 136], [137, 139], [140, 148], [149, 153], [153, 154]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [9, 11, "field"], [13, 14, "task"], [19, 21, "misc"], [39, 40, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[0, 0, 9, 11, "related-to", "", false, false], [0, 0, 13, 14, "related-to", "", false, false], [0, 0, 39, 40, "related-to", "", true, false], [19, 21, 0, 0, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["tity", "includes", "a", "collection", "of", "visualization", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modeling", ",", "as", "well", "as", "graphical", "user", "interfaces", "for", "easy", "access", "to", "these", "functions", ".", "but", "the", "latest", ",", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "whose", "development", "began", "in", "1997", ",", "is", "now", "used", "in", "a", "wide", "variety", "of", "applications", ",", "particularly", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "tity includes a collection of visualization tools and algorithms for data analysis and predictive modeling, as well as graphical user interfaces for easy access to these functions. but the latest, fully Java-based version (Weka 3), whose development began in 1997, is now used in a wide variety of applications, particularly for educational and research purposes.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 106], [106, 107], [108, 110], [111, 115], [116, 118], [119, 128], [129, 133], [134, 144], [145, 148], [149, 153], [154, 160], [161, 163], [164, 169], [170, 179], [179, 180], [181, 184], [185, 188], [189, 195], [195, 196], [197, 202], [203, 207], [207, 208], [208, 213], [214, 221], [222, 223], [223, 227], [228, 229], [229, 230], [230, 231], [232, 237], [238, 249], [250, 255], [256, 258], [259, 263], [263, 264], [265, 267], [268, 271], [272, 276], [277, 279], [280, 281], [282, 286], [287, 294], [295, 297], [298, 310], [310, 311], [312, 324], [325, 328], [329, 340], [341, 344], [345, 353], [354, 362], [362, 363]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [15, 26, "misc"], [37, 39, "misc"], [29, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[15, 26, 0, 0, "topic", "", false, false], [15, 26, 37, 39, "win-defeat", "", false, false], [37, 39, 29, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "a", "number", "of", "interesting", "discoveries", "and", "received", "considerable", "recognition", "for", "his", "paper", "on", "Heuretics", ":", "his", "work", "on", "The", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "won", "the", "1982", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "Best", "Paper", "Award", "."], "sentence-detokenized": "Eurisko made a number of interesting discoveries and received considerable recognition for his paper on Heuretics: his work on The Theoretical and Study of Heuristic Rules won the 1982 Association for the Advancement of Artificial Intelligence Best Paper Award.", "token2charspan": [[0, 7], [8, 12], [13, 14], [15, 21], [22, 24], [25, 36], [37, 48], [49, 52], [53, 61], [62, 74], [75, 86], [87, 90], [91, 94], [95, 100], [101, 103], [104, 113], [113, 114], [115, 118], [119, 123], [124, 126], [127, 130], [131, 142], [143, 146], [147, 152], [153, 155], [156, 165], [166, 171], [172, 175], [176, 179], [180, 184], [185, 196], [197, 200], [201, 204], [205, 216], [217, 219], [220, 230], [231, 243], [244, 248], [249, 254], [255, 260], [260, 261]]}
{"doc_key": "ai-train-18", "ner": [[9, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "take", "account", "of", "multiple", "units", ",", "a", "separate", "wrist", "loss", "should", "be", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To take account of multiple units, a separate wrist loss should be calculated for each capsule.", "token2charspan": [[0, 2], [3, 7], [8, 15], [16, 18], [19, 27], [28, 33], [33, 34], [35, 36], [37, 45], [46, 51], [52, 56], [57, 63], [64, 66], [67, 77], [78, 81], [82, 86], [87, 94], [94, 95]]}
{"doc_key": "ai-train-19", "ner": [[8, 9, "product"], [11, 12, "product"], [14, 15, "product"], [17, 18, "product"], [20, 21, "product"], [23, 25, "product"], [33, 38, "product"], [41, 42, "product"], [44, 45, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 9, 23, 25, "type-of", "", false, false], [11, 12, 23, 25, "type-of", "", false, false], [14, 15, 23, 25, "type-of", "", false, false], [17, 18, 23, 25, "type-of", "", false, false], [20, 21, 23, 25, "type-of", "", false, false], [41, 42, 33, 38, "type-of", "", false, false], [44, 45, 33, 38, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "emergence", "of", "conversational", "assistants", "such", "as", "Apple", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "Bixby", ",", "voice", "portals", "are", "now", "available", "on", "mobile", "devices", "and", "through", "Far", "Field", "voice", "-enabled", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the emergence of conversational assistants such as Apple Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung Bixby, voice portals are now available on mobile devices and through Far Field voice-enabled smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 36], [37, 47], [48, 52], [53, 55], [56, 61], [62, 66], [66, 67], [68, 74], [75, 80], [80, 81], [82, 88], [89, 98], [98, 99], [100, 109], [110, 117], [118, 121], [122, 129], [130, 135], [135, 136], [137, 142], [143, 150], [151, 154], [155, 158], [159, 168], [169, 171], [172, 178], [179, 186], [187, 190], [191, 198], [199, 202], [203, 208], [209, 214], [214, 222], [223, 228], [229, 237], [238, 242], [243, 245], [246, 252], [253, 257], [258, 261], [262, 268], [269, 273], [273, 274]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 14, "algorithm"], [16, 16, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "type-of", "", false, false], [9, 11, 2, 3, "type-of", "", false, false], [13, 14, 2, 3, "type-of", "", false, false], [16, 16, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "include", "Naive", "Bayes", "classifiers", ",", "support", "vector", "machines", ",", "Gaussian", "mixtures", "and", "networks", "."], "sentence-detokenized": "Examples of supervised learning include Naive Bayes classifiers, support vector machines, Gaussian mixtures and networks.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 39], [40, 45], [46, 51], [52, 63], [63, 64], [65, 72], [73, 79], [80, 88], [88, 89], [90, 98], [99, 107], [108, 111], [112, 120], [120, 121]]}
{"doc_key": "ai-train-21", "ner": [[0, 2, "algorithm"], [25, 27, "algorithm"], [29, 30, "task"], [35, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 25, 27, "part-of", "", true, false], [35, 36, 29, 30, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "OSD", "algorithm", "can", "be", "used", "to", "derive", "mathematical", "O", "(", "\\", "sqrt", "{", "T", "})", "/mathematical", "regret", "bounds", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "used", "for", "classification", ",", "which", "uses", "the", "joint", "loss", "mathematical", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "The OSD algorithm can be used to derive mathematical O(\\ sqrt {T})/mathematical regret bounds for the online version of the support vector machine used for classification, which uses the joint loss mathematical v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 24], [25, 29], [30, 32], [33, 39], [40, 52], [53, 54], [54, 55], [55, 56], [57, 61], [62, 63], [63, 64], [64, 66], [66, 79], [80, 86], [87, 93], [94, 97], [98, 101], [102, 108], [109, 116], [117, 119], [120, 123], [124, 131], [132, 138], [139, 146], [147, 151], [152, 155], [156, 170], [170, 171], [172, 177], [178, 182], [183, 186], [187, 192], [193, 197], [198, 210], [211, 212], [213, 215], [216, 217], [217, 218], [218, 219], [220, 222], [223, 226], [226, 227], [228, 229], [229, 230], [230, 231], [232, 233], [234, 235], [236, 237], [238, 240], [241, 242], [242, 243], [243, 244], [245, 249], [250, 251], [252, 254], [254, 255], [255, 257], [258, 259], [260, 264]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 26, "task"], [28, 29, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "unique", "identification", "of", "wild", "animals", "and", "mobile", "matches", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, unique identification of wild animals and mobile matches.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 102], [102, 103], [104, 111], [112, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 147], [148, 162], [163, 165], [166, 170], [171, 178], [179, 182], [183, 189], [190, 197], [197, 198]]}
{"doc_key": "ai-train-23", "ner": [[7, 8, "task"], [11, 12, "university"], [14, 16, "university"], [18, 19, "university"], [21, 22, "university"], [24, 29, "university"], [31, 33, "university"], [35, 37, "university"], [39, 40, "university"], [45, 47, "university"], [44, 49, "university"], [52, 56, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[7, 8, 11, 12, "related-to", "", true, false], [7, 8, 14, 16, "related-to", "", true, false], [7, 8, 18, 19, "related-to", "", true, false], [7, 8, 21, 22, "related-to", "", true, false], [7, 8, 24, 29, "related-to", "", true, false], [7, 8, 31, 33, "related-to", "", true, false], [7, 8, 35, 37, "related-to", "", true, false], [7, 8, 39, 40, "related-to", "", true, false], [7, 8, 45, 47, "related-to", "", true, false], [7, 8, 44, 49, "related-to", "", true, false], [7, 8, 52, 56, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Several", "groups", "and", "companies", "are", "working", "on", "pose", "estimation", ",", "including", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "Several groups and companies are working on pose estimation, including Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 7], [8, 14], [15, 18], [19, 28], [29, 32], [33, 40], [41, 43], [44, 48], [49, 59], [59, 60], [61, 70], [71, 76], [77, 87], [87, 88], [89, 97], [98, 104], [105, 115], [115, 116], [117, 120], [121, 133], [133, 134], [135, 143], [144, 154], [154, 155], [156, 166], [167, 169], [170, 180], [180, 181], [182, 185], [186, 191], [191, 192], [193, 203], [204, 206], [207, 214], [214, 215], [216, 221], [222, 230], [231, 236], [236, 237], [238, 241], [242, 248], [248, 249], [250, 258], [259, 269], [270, 272], [273, 281], [282, 285], [286, 296], [297, 298], [298, 302], [302, 303], [304, 307], [308, 318], [319, 321], [322, 332], [332, 333], [334, 340], [340, 341]]}
{"doc_key": "ai-train-24", "ner": [[0, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Sigmoid", "function", "Cross", "entropy", "loss", "K", "is", "used", "to", "predict", "independent", "probability", "values", "of", "mathematical", "0.1", "/mathematical", "0.1", "/mathematical", "."], "sentence-detokenized": "Sigmoid function Cross entropy loss K is used to predict independent probability values of mathematical 0.1/mathematical 0.1/mathematical.", "token2charspan": [[0, 7], [8, 16], [17, 22], [23, 30], [31, 35], [36, 37], [38, 40], [41, 45], [46, 48], [49, 56], [57, 68], [69, 80], [81, 87], [88, 90], [91, 103], [104, 107], [107, 120], [121, 124], [124, 137], [137, 138]]}
{"doc_key": "ai-train-25", "ner": [[3, 6, "misc"], [7, 7, "field"], [9, 12, "field"], [13, 15, "university"], [16, 18, "country"], [21, 25, "misc"], [26, 30, "university"], [31, 31, "country"], [37, 37, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 6, 7, 7, "topic", "", false, false], [3, 6, 9, 12, "topic", "", false, false], [3, 6, 13, 15, "physical", "", true, false], [13, 15, 16, 18, "physical", "", false, false], [21, 25, 26, 30, "physical", "", true, false], [26, 30, 31, 31, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Chair", "of", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Foundation", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "a", "professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Chair of Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Foundation Chair at the Tokyo Institute of Technology in Japan before becoming a professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 34], [35, 37], [38, 49], [50, 53], [54, 62], [63, 70], [71, 73], [74, 77], [78, 88], [89, 91], [92, 101], [102, 104], [105, 108], [109, 120], [121, 124], [125, 128], [129, 136], [137, 147], [148, 153], [154, 156], [157, 160], [161, 166], [167, 176], [177, 179], [180, 190], [191, 193], [194, 199], [200, 206], [207, 215], [216, 217], [218, 227], [228, 230], [231, 240], [240, 241]]}
{"doc_key": "ai-train-26", "ner": [[6, 7, "algorithm"], [13, 16, "algorithm"], [12, 18, "algorithm"], [23, 24, "researcher"], [26, 27, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 7, 13, 16, "usage", "", true, false], [13, 16, 23, 24, "origin", "", false, false], [13, 16, 26, 27, "origin", "", false, false], [12, 18, 13, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", ",", "particularly", "used", "in", "recurrent", "neural", "networks", ",", "is", "the", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "network", "developed", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another technique, particularly used in recurrent neural networks, is the long short-term memory (LSTM) network developed by Sepp Hochreiter & J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 17], [17, 18], [19, 31], [32, 36], [37, 39], [40, 49], [50, 56], [57, 65], [65, 66], [67, 69], [70, 73], [74, 78], [79, 84], [84, 85], [85, 89], [90, 96], [97, 98], [98, 102], [102, 103], [104, 111], [112, 121], [122, 124], [125, 129], [130, 140], [141, 142], [143, 149], [150, 161], [162, 164], [165, 169], [169, 170]]}
{"doc_key": "ai-train-27", "ner": [[0, 2, "programlang"], [5, 6, "product"], [12, 12, "product"], [42, 42, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 0, 2, "general-affiliation", "", false, false], [5, 6, 12, 12, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "versatile", ",", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "reverse", "modes", ",", "similar", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The C++ interpreter (CINT up to version 5.34, Cling from version 6) makes this package very versatile, as it can be used in interactive, scripted and reverse modes, similar to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 5], [5, 7], [8, 19], [20, 21], [21, 23], [23, 25], [26, 28], [29, 31], [32, 39], [40, 44], [44, 45], [46, 51], [52, 56], [57, 64], [65, 66], [66, 67], [68, 73], [74, 78], [79, 86], [87, 91], [92, 101], [101, 102], [103, 105], [106, 108], [109, 112], [113, 115], [116, 120], [121, 123], [124, 135], [135, 136], [137, 145], [146, 149], [150, 157], [158, 163], [163, 164], [165, 172], [173, 175], [176, 186], [187, 195], [196, 200], [201, 203], [204, 210], [210, 211]]}
{"doc_key": "ai-train-28", "ner": [[3, 7, "product"], [23, 25, "field"], [29, 30, "task"], [32, 34, "task"], [36, 37, "task"], [39, 40, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 7, 23, 25, "related-to", "", false, false], [29, 30, 23, 25, "part-of", "", false, false], [32, 34, 23, 25, "part-of", "", false, false], [36, 37, 23, 25, "part-of", "", false, false], [39, 40, 23, 25, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "design", "of", "voice", "-", "based", "user", "interfaces", "that", "interpret", "and", "manage", "conversational", "state", "is", "challenging", "because", "of", "the", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "The design of voice-based user interfaces that interpret and manage conversational state is challenging because of the difficulty of integrating complex natural language processing tasks such as coreference resolution, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 3], [4, 10], [11, 13], [14, 19], [19, 20], [20, 25], [26, 30], [31, 41], [42, 46], [47, 56], [57, 60], [61, 67], [68, 82], [83, 88], [89, 91], [92, 103], [104, 111], [112, 114], [115, 118], [119, 129], [130, 132], [133, 144], [145, 152], [153, 160], [161, 169], [170, 180], [181, 186], [187, 191], [192, 194], [195, 206], [207, 217], [217, 218], [219, 224], [225, 231], [232, 243], [243, 244], [245, 256], [257, 266], [267, 270], [271, 279], [280, 290], [290, 291]]}
{"doc_key": "ai-train-29", "ner": [[5, 6, "algorithm"], [9, 12, "algorithm"], [15, 16, "researcher"], [22, 27, "organisation"], [37, 38, "field"], [40, 41, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 15, 16, "origin", "", false, false], [5, 6, 37, 38, "part-of", "", false, false], [5, 6, 40, 41, "part-of", "", false, false], [9, 12, 15, 16, "origin", "", false, false], [9, 12, 37, 38, "part-of", "", false, false], [9, 12, 40, 41, "part-of", "", false, false], [15, 16, 22, 27, "physical", "", false, false], [15, 16, 22, 27, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "Artificial", "Intelligence", "Laboratory", "(", "IDSIA", ")", "won", "eight", "international", "competitions", "in", "the", "field", "of", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in J\u00fcrgen Schmidhuber's research group at the Swiss Artificial Intelligence Laboratory (IDSIA) won eight international competitions in the field of pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 69], [70, 76], [77, 85], [86, 95], [96, 98], [99, 105], [106, 117], [117, 119], [120, 128], [129, 134], [135, 137], [138, 141], [142, 147], [148, 158], [159, 171], [172, 182], [183, 184], [184, 189], [189, 190], [191, 194], [195, 200], [201, 214], [215, 227], [228, 230], [231, 234], [235, 240], [241, 243], [244, 251], [252, 263], [264, 267], [268, 275], [276, 284], [284, 285]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[8, 12, "misc"], [13, 16, "field"], [17, 20, "university"], [26, 30, "field"], [32, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 12, 13, 16, "topic", "topic_of_award", false, false], [8, 12, 17, 20, "origin", "", true, false], [26, 30, 32, 35, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "has", "received", "two", "honorary", "degrees", ",", "a", "S.V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "the", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He has received two honorary degrees, a S.V. della laurea ad honorem in psychology from the University of Padua in 1995 and a doctorate in industrial design and engineering from the Delft University of Technology.", "token2charspan": [[0, 2], [3, 6], [7, 15], [16, 19], [20, 28], [29, 36], [36, 37], [38, 39], [40, 44], [45, 50], [51, 57], [58, 60], [61, 68], [69, 71], [72, 82], [83, 87], [88, 91], [92, 102], [103, 105], [106, 111], [112, 114], [115, 119], [120, 123], [124, 125], [126, 135], [136, 138], [139, 149], [150, 156], [157, 160], [161, 172], [173, 177], [178, 181], [182, 187], [188, 198], [199, 201], [202, 212], [212, 213]]}
{"doc_key": "ai-train-32", "ner": [[0, 4, "researcher"], [13, 17, "organisation"], [18, 18, "location"], [20, 20, "researcher"], [32, 33, "misc"], [45, 47, "misc"], [46, 67, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 4, 13, 17, "physical", "", false, false], [0, 4, 13, 17, "role", "", false, false], [13, 17, 18, 18, "physical", "", false, false], [20, 20, 32, 33, "related-to", "works_with", true, false], [20, 20, 45, 47, "related-to", "works_with", true, false], [20, 20, 46, 67, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["With", "Laurent", "Cohen", ",", "a", "long", "-", "time", "collaborator", "and", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "in", "whom", "injuries", "to", "different", "regions", "of", "the", "parietal", "lobe", "reduced", "constriction", "but", "preserved", "subtraction", "(", "related", "to", "injuries", "to", "the", "inferior", "parietal", "lobe", ")", "and", "those", "in", "whom", "subtraction", "was", "reduced", "but", "constriction", "was", "preserved", "(", "related", "to", "injuries", "to", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "With Laurent Cohen, a long-time collaborator and neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients in whom injuries to different regions of the parietal lobe reduced constriction but preserved subtraction (related to injuries to the inferior parietal lobe) and those in whom subtraction was reduced but constriction was preserved (related to injuries to the intraparietal sulcus).", "token2charspan": [[0, 4], [5, 12], [13, 18], [18, 19], [20, 21], [22, 26], [26, 27], [27, 31], [32, 44], [45, 48], [49, 60], [61, 63], [64, 67], [68, 73], [73, 74], [74, 85], [86, 94], [95, 97], [98, 103], [103, 104], [105, 112], [113, 117], [118, 128], [129, 137], [138, 140], [141, 145], [146, 154], [155, 157], [158, 167], [168, 175], [176, 178], [179, 182], [183, 191], [192, 196], [197, 204], [205, 217], [218, 221], [222, 231], [232, 243], [244, 245], [245, 252], [253, 255], [256, 264], [265, 267], [268, 271], [272, 280], [281, 289], [290, 294], [294, 295], [296, 299], [300, 305], [306, 308], [309, 313], [314, 325], [326, 329], [330, 337], [338, 341], [342, 354], [355, 358], [359, 368], [369, 370], [370, 377], [378, 380], [381, 389], [390, 392], [393, 396], [397, 410], [411, 417], [417, 418], [418, 419]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [13, 16, "misc"], [18, 19, "misc"], [28, 29, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 8, "topic", "", false, false], [18, 19, 6, 8, "topic", "", false, false], [28, 29, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "as", "well", "as", "the", "2016", "TV", "adaptation", "of", "Westworld", ",", "have", "also", "attracted", "public", "sympathy", "for", "robots", "."], "sentence-detokenized": "More recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina, as well as the 2016 TV adaptation of Westworld, have also attracted public sympathy for robots.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 56], [57, 68], [69, 75], [76, 78], [79, 84], [85, 89], [90, 92], [93, 96], [96, 97], [98, 108], [109, 121], [122, 125], [126, 128], [129, 136], [136, 137], [138, 140], [141, 145], [146, 148], [149, 152], [153, 157], [158, 160], [161, 171], [172, 174], [175, 184], [184, 185], [186, 190], [191, 195], [196, 205], [206, 212], [213, 221], [222, 225], [226, 232], [232, 233]]}
{"doc_key": "ai-train-34", "ner": [[5, 7, "field"], [9, 11, "algorithm"], [13, 14, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 5, 7, "part-of", "", false, false], [13, 14, 5, 7, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "two", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "The two main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 20], [21, 25], [26, 28], [29, 41], [42, 50], [51, 54], [55, 64], [65, 74], [75, 83], [84, 87], [88, 95], [96, 104], [104, 105]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [22, 23, "misc"], [28, 29, "misc"], [31, 33, "person"], [38, 39, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[22, 23, 0, 3, "artifact", "", false, false], [28, 29, 0, 3, "artifact", "", false, false], [28, 29, 31, 33, "role", "director_of", false, false], [28, 29, 38, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "started", "to", "make", "more", "pronounced", "use", "of", "3D", "films", "in", "special", "locations", "to", "impress", "audiences", ":", "the", "Magic", "Voyages", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "are", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also started to make more pronounced use of 3D films in special locations to impress audiences: the Magic Voyages (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) are notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 36], [37, 39], [40, 44], [45, 49], [50, 60], [61, 64], [65, 67], [68, 70], [71, 76], [77, 79], [80, 87], [88, 97], [98, 100], [101, 108], [109, 118], [118, 119], [120, 123], [124, 129], [130, 137], [138, 139], [139, 143], [143, 144], [145, 148], [149, 156], [157, 159], [160, 161], [161, 168], [169, 173], [174, 181], [181, 182], [183, 187], [187, 188], [189, 197], [198, 205], [206, 213], [213, 214], [215, 218], [219, 226], [227, 235], [235, 236]]}
{"doc_key": "ai-train-36", "ner": [[9, 12, "field"], [16, 21, "task"], [23, 24, "task"], [26, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[16, 21, 9, 12, "part-of", "", false, false], [23, 24, 9, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "labelling", "and", "syntactic", "analysis", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in natural language processing for tasks such as part-of-speech labelling and syntactic analysis (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 61], [62, 70], [71, 81], [82, 85], [86, 91], [92, 96], [97, 99], [100, 104], [104, 105], [105, 107], [107, 108], [108, 114], [115, 124], [125, 128], [129, 138], [139, 147], [148, 149], [149, 156], [156, 157], [158, 162], [162, 163], [163, 164]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [10, 13, "organisation"], [15, 21, "organisation"], [18, 18, "country"], [22, 25, "product"], [29, 31, "researcher"], [35, 36, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[10, 13, 2, 3, "role", "introduces_to_market", true, false], [15, 21, 2, 3, "role", "introduces_to_market", true, false], [15, 21, 18, 18, "physical", "", false, false], [22, 25, 35, 36, "related-to", "sold_to", true, false], [29, 31, 22, 25, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "in", "1963", "by", "the", "Fuji", "Yusoki", "Kogyo", "Company", ",", "KUKA", "Robotics", "of", "Germany", ",", "and", "the", "programmable", "universal", "assembly", "machine", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", "and", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced in 1963 by the Fuji Yusoki Kogyo Company, KUKA Robotics of Germany, and the programmable universal assembly machine was invented by Victor Scheinman in 1976 and sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 57], [58, 62], [63, 69], [70, 75], [76, 83], [83, 84], [85, 89], [90, 98], [99, 101], [102, 109], [109, 110], [111, 114], [115, 118], [119, 131], [132, 141], [142, 150], [151, 158], [159, 162], [163, 171], [172, 174], [175, 181], [182, 191], [192, 194], [195, 199], [200, 203], [204, 208], [209, 211], [212, 221], [221, 222]]}
{"doc_key": "ai-train-38", "ner": [[10, 10, "conference"], [12, 12, "researcher"], [21, 22, "field"], [32, 33, "researcher"], [39, 43, "researcher"], [51, 52, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[12, 12, 10, 10, "role", "president_of", false, false], [12, 12, 32, 33, "role", "colleagues", false, false], [21, 22, 51, 52, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "when", "he", "was", "president", "of", "the", "AAAI", ",", "Hayes", "launched", "a", "series", "of", "attacks", "on", "critics", "of", "artificial", "intelligence", ",", "mostly", "ironically", ",", "and", "(", "with", "his", "colleague", "Kenneth", "Ford", ")", "invented", "an", "award", "named", "after", "Simon", "Newcomb", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "artificial", "intelligence", "."], "sentence-detokenized": "In the mid-1990s, when he was president of the AAAI, Hayes launched a series of attacks on critics of artificial intelligence, mostly ironically, and (with his colleague Kenneth Ford) invented an award named after Simon Newcomb for the most ridiculous argument refuting the possibility of artificial intelligence.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 22], [23, 25], [26, 29], [30, 39], [40, 42], [43, 46], [47, 51], [51, 52], [53, 58], [59, 67], [68, 69], [70, 76], [77, 79], [80, 87], [88, 90], [91, 98], [99, 101], [102, 112], [113, 125], [125, 126], [127, 133], [134, 144], [144, 145], [146, 149], [150, 151], [151, 155], [156, 159], [160, 169], [170, 177], [178, 182], [182, 183], [184, 192], [193, 195], [196, 201], [202, 207], [208, 213], [214, 219], [220, 227], [228, 231], [232, 235], [236, 240], [241, 251], [252, 260], [261, 269], [270, 273], [274, 285], [286, 288], [289, 299], [300, 312], [312, 313]]}
{"doc_key": "ai-train-39", "ner": [[14, 16, "algorithm"], [38, 41, "algorithm"], [54, 56, "algorithm"], [60, 62, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 16, 38, 41, "named", "same", false, false], [54, 56, 14, 16, "type-of", "", false, false], [60, 62, 14, 16, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "of", "math", "\\", "alpha", "/", "math", "can", "be", "found", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "magnitude", "of", "math", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "usually", "by", "a", "line", "search", "in", "the", "interval", "math0", "\\", "alpha", "1", "/", "math", "or", "by", "a", "backward", "line", "search", "such", "as", "the", "Armijo", "line", "search", "."], "sentence-detokenized": "The optimal value of math\\ alpha / math can be found using a line search algorithm, i.e. the magnitude of math\\ alpha / math is determined by finding the value that minimizes S, usually by a line search in the interval math0\\ alpha 1 / math or by a backward line search such as the Armijo line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 20], [21, 25], [25, 26], [27, 32], [33, 34], [35, 39], [40, 43], [44, 46], [47, 52], [53, 58], [59, 60], [61, 65], [66, 72], [73, 82], [82, 83], [84, 88], [89, 92], [93, 102], [103, 105], [106, 110], [110, 111], [112, 117], [118, 119], [120, 124], [125, 127], [128, 138], [139, 141], [142, 149], [150, 153], [154, 159], [160, 164], [165, 174], [175, 176], [176, 177], [178, 185], [186, 188], [189, 190], [191, 195], [196, 202], [203, 205], [206, 209], [210, 218], [219, 224], [224, 225], [226, 231], [232, 233], [234, 235], [236, 240], [241, 243], [244, 246], [247, 248], [249, 257], [258, 262], [263, 269], [270, 274], [275, 277], [278, 281], [282, 288], [289, 293], [294, 300], [300, 301]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 10, "algorithm"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "Breadth", "-", "first", "search", "and", "Depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "embody", "a", "lot", "of", "technical", "knowledge", ",", "but", "do", "not", "shed", "light", "on", "the", "mental", "processes", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses Breadth-first search and Depth-first search techniques, but ultimately concludes that the results represent expert systems that embody a lot of technical knowledge, but do not shed light on the mental processes people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 67], [67, 68], [69, 72], [73, 83], [84, 93], [94, 98], [99, 102], [103, 110], [111, 120], [121, 127], [128, 135], [136, 140], [141, 147], [148, 149], [150, 153], [154, 156], [157, 166], [167, 176], [176, 177], [178, 181], [182, 184], [185, 188], [189, 193], [194, 199], [200, 202], [203, 206], [207, 213], [214, 223], [224, 230], [231, 234], [235, 237], [238, 243], [244, 248], [249, 256], [256, 257]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "synthesis", "is", "concerned", "with", "how", "spoken", "language", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and synthesis is concerned with how spoken language can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 32], [33, 35], [36, 45], [46, 50], [51, 54], [55, 61], [62, 70], [71, 74], [75, 77], [78, 88], [89, 91], [92, 99], [100, 105], [106, 115], [115, 116]]}
{"doc_key": "ai-train-42", "ner": [[7, 8, "algorithm"], [21, 22, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "is", "usually", "estimated", "using", "the", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "/", "math", ")", "or", "Maximum", "A", "Posteriori", "(", "math\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "procedure", "."], "sentence-detokenized": "This math is usually estimated using the Maximum Likelihood (math\\ theta ^ {*} / math) or Maximum A Posteriori (math\\ theta ^ {*} =\\ theta ^ {ML} / math) procedure.", "token2charspan": [[0, 4], [5, 9], [10, 12], [13, 20], [21, 30], [31, 36], [37, 40], [41, 48], [49, 59], [60, 61], [61, 65], [65, 66], [67, 72], [73, 74], [75, 76], [76, 78], [79, 80], [81, 85], [85, 86], [87, 89], [90, 97], [98, 99], [100, 110], [111, 112], [112, 117], [118, 123], [124, 125], [126, 127], [127, 129], [130, 132], [133, 138], [139, 140], [141, 142], [142, 144], [144, 145], [146, 147], [148, 152], [152, 153], [154, 163], [163, 164]]}
{"doc_key": "ai-train-43", "ner": [[8, 9, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "common", "languages", "use", "the", "open", "source", "eSpeak", "synthesizer", "for", "speech", ",", "which", "produces", "a", "robotic", ",", "clumsy", "sound", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less common languages use the open source eSpeak synthesizer for speech, which produces a robotic, clumsy sound that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 26], [27, 30], [31, 34], [35, 39], [40, 46], [47, 53], [54, 65], [66, 69], [70, 76], [76, 77], [78, 83], [84, 92], [93, 94], [95, 102], [102, 103], [104, 110], [111, 116], [117, 121], [122, 125], [126, 128], [129, 138], [139, 141], [142, 152], [152, 153]]}
{"doc_key": "ai-train-44", "ner": [[1, 20, "programlang"], [36, 37, "programlang"], [39, 39, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 20, 36, 37, "compare", "", false, false], [1, 20, 39, 39, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "used", "primarily", "by", "statisticians", "and", "other", "professionals", "who", "need", "a", "statistical", "computing", "and", "software", "development", "environment", ",", "R", "can", "also", "function", "as", "a", "general", "matrix", "computing", "toolkit", "-", "with", "benchmarks", "as", "powerful", "as", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is used primarily by statisticians and other professionals who need a statistical computing and software development environment, R can also function as a general matrix computing toolkit - with benchmarks as powerful as GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 18], [19, 28], [29, 31], [32, 45], [46, 49], [50, 55], [56, 69], [70, 73], [74, 78], [79, 80], [81, 92], [93, 102], [103, 106], [107, 115], [116, 127], [128, 139], [139, 140], [141, 142], [143, 146], [147, 151], [152, 160], [161, 163], [164, 165], [166, 173], [174, 180], [181, 190], [191, 198], [199, 200], [201, 205], [206, 216], [217, 219], [220, 228], [229, 231], [232, 235], [236, 242], [243, 245], [246, 252], [252, 253]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 4, "part-of", "", false, false], [0, 0, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "mixing", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden that creates new frequencies by mixing two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [76, 79], [80, 88], [89, 97], [98, 107], [108, 112], [113, 120], [121, 124], [125, 136], [137, 139], [140, 146], [147, 150], [151, 162], [162, 163]]}
{"doc_key": "ai-train-46", "ner": [[15, 17, "person"], [18, 18, "misc"], [22, 24, "organisation"], [27, 27, "organisation"], [29, 31, "misc"], [32, 34, "person"], [37, 37, "organisation"], [39, 42, "misc"], [43, 44, "person"], [46, 47, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[15, 17, 18, 18, "role", "actor_in", false, false], [18, 18, 22, 24, "artifact", "", false, false], [29, 31, 27, 27, "artifact", "", false, false], [32, 34, 29, 31, "role", "actor_in", false, false], [39, 42, 37, 37, "artifact", "", false, false], [43, 44, 39, 42, "role", "actor_in", false, false], [46, 47, 39, 42, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Several", "other", "films", "helped", "to", "put", "3D", "back", "on", "the", "map", "that", "month", ":", "the", "John", "Wayne", "film", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", ",", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Several other films helped to put 3D back on the map that month: the John Wayne film Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth, and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 7], [8, 13], [14, 19], [20, 26], [27, 29], [30, 33], [34, 36], [37, 41], [42, 44], [45, 48], [49, 52], [53, 57], [58, 63], [63, 64], [65, 68], [69, 73], [74, 79], [80, 84], [85, 90], [91, 92], [92, 103], [104, 106], [107, 113], [114, 118], [118, 119], [119, 120], [120, 121], [122, 130], [130, 132], [133, 137], [138, 143], [144, 152], [153, 157], [158, 162], [163, 171], [171, 172], [173, 176], [177, 186], [186, 188], [189, 194], [195, 199], [200, 204], [205, 209], [210, 214], [215, 221], [222, 225], [226, 231], [232, 237], [237, 238]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 4, "field"], [5, 6, "task"], [11, 11, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 6, "general-affiliation", "", false, false], [0, 0, 11, 11, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "Facebook", "research", "team", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a Facebook research team.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 84], [85, 89], [89, 90]]}
{"doc_key": "ai-train-48", "ner": [[0, 3, "field"], [8, 8, "conference"], [16, 20, "field"], [26, 29, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 3, 16, 20, "part-of", "subfield", false, false], [8, 8, 0, 3, "topic", "", false, false], [26, 29, 0, 3, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometric", "processing", "is", "a", "frequent", "research", "topic", "at", "SIGGRAPH", ",", "the", "most", "important", "scientific", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "theme", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometric processing is a frequent research topic at SIGGRAPH, the most important scientific conference on computer graphics, and the main theme of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 9], [10, 20], [21, 23], [24, 25], [26, 34], [35, 43], [44, 49], [50, 52], [53, 61], [61, 62], [63, 66], [67, 71], [72, 81], [82, 92], [93, 103], [104, 106], [107, 115], [116, 124], [124, 125], [126, 129], [130, 133], [134, 138], [139, 144], [145, 147], [148, 151], [152, 158], [159, 168], [169, 171], [172, 180], [181, 191], [191, 192]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [14, 14, "algorithm"], [15, 17, "algorithm"], [21, 21, "algorithm"], [20, 24, "algorithm"], [27, 28, "algorithm"], [29, 31, "algorithm"], [36, 36, "misc"], [41, 43, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[14, 14, 36, 36, "general-affiliation", "", false, false], [15, 17, 14, 14, "named", "", false, false], [21, 21, 36, 36, "general-affiliation", "", false, false], [20, 24, 21, 21, "named", "", false, false], [27, 28, 36, 36, "general-affiliation", "", false, false], [29, 31, 27, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "with", "Principal", "Component", "Analysis", "(", "PCA", ")", ",", "Linear", "Discriminant", "Analysis", "(", "LDA", ")", "or", "Canonical", "Correlation", "Analysis", "(", "CCA", ")", "techniques", "as", "a", "preprocessing", "step", ",", "followed", "by", "k", "-", "NN", "clustering", "on", "feature", "vectors", "in", "the", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step with Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) or Canonical Correlation Analysis (CCA) techniques as a preprocessing step, followed by k -NN clustering on feature vectors in the reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 85], [86, 95], [96, 105], [106, 114], [115, 116], [116, 119], [119, 120], [120, 121], [122, 128], [129, 141], [142, 150], [151, 152], [152, 155], [155, 156], [157, 159], [160, 169], [170, 181], [182, 190], [191, 192], [192, 195], [195, 196], [197, 207], [208, 210], [211, 212], [213, 226], [227, 231], [231, 232], [233, 241], [242, 244], [245, 246], [247, 248], [248, 250], [251, 261], [262, 264], [265, 272], [273, 280], [281, 283], [284, 287], [288, 295], [296, 307], [308, 313], [313, 314]]}
{"doc_key": "ai-train-50", "ner": [[0, 3, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 3, 9, 10, "related-to", "good_at", true, false], [0, 3, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [36, 39, "algorithm"], [40, 41, "researcher"], [43, 45, "researcher"], [47, 53, "misc"], [55, 64, "conference"], [66, 66, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [47, 53, 36, 39, "topic", "", false, false], [47, 53, 40, 41, "artifact", "", false, false], [47, 53, 43, 45, "artifact", "", false, false], [47, 53, 55, 64, "temporal", "", false, false], [66, 66, 55, 64, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histograms", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as histograms of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 143], [144, 150], [151, 154], [155, 160], [161, 169], [170, 174], [175, 177], [178, 188], [189, 191], [192, 200], [201, 210], [211, 213], [214, 219], [219, 220], [221, 222], [222, 223], [224, 230], [230, 231], [232, 242], [243, 245], [246, 254], [255, 264], [265, 268], [269, 274], [275, 284], [284, 285], [286, 290], [291, 299], [300, 307], [308, 318], [319, 321], [322, 330], [331, 337], [338, 341], [342, 349], [350, 361], [362, 363], [363, 367], [367, 368], [368, 369], [370, 375], [376, 377], [377, 378], [379, 386], [386, 387], [388, 392], [393, 404], [404, 405]]}
{"doc_key": "ai-train-52", "ner": [[1, 2, "algorithm"], [6, 8, "algorithm"], [12, 15, "task"], [16, 17, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 2, 6, 8, "type-of", "", false, false], [12, 15, 1, 2, "usage", "", true, false], [12, 15, 16, 17, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "for", "learning", "feature", "learning", "in", "an", "unsupervised", "learning", "fashion", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network used for learning feature learning in an unsupervised learning fashion.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 62], [63, 71], [72, 79], [80, 88], [89, 91], [92, 94], [95, 107], [108, 116], [117, 124], [124, 125]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [6, 8, "organisation"], [11, 12, "field"], [14, 15, "field"], [26, 26, "organisation"], [19, 24, "organisation"], [25, 35, "field"], [37, 40, "field"], [42, 44, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 2, 6, 8, "role", "fellow_of", false, false], [0, 2, 11, 12, "related-to", "contributes_to", false, false], [0, 2, 14, 15, "related-to", "contributes_to", false, false], [0, 2, 26, 26, "role", "fellow_of", false, false], [0, 2, 25, 35, "related-to", "contributes_to", false, false], [0, 2, 37, 40, "related-to", "contributes_to", false, false], [19, 24, 26, 26, "named", "", false, false], [42, 44, 26, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "the", "IEEE", "for", "his", "work", "in", "computer", "vision", "and", "image", "processing", ",", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "work", "in", "pattern", "recognition", "and", "image", "processing", "and", "his", "service", "to", "the", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of the IEEE for his work in computer vision and image processing, and a Fellow of the International Association for Pattern Recognition (IAPR) for his work in pattern recognition and image processing and his service to the IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 40], [41, 45], [46, 48], [49, 57], [58, 64], [65, 68], [69, 74], [75, 85], [85, 86], [87, 90], [91, 92], [93, 99], [100, 102], [103, 106], [107, 120], [121, 132], [133, 136], [137, 144], [145, 156], [157, 158], [158, 162], [162, 163], [164, 167], [168, 171], [172, 176], [177, 179], [180, 187], [188, 199], [200, 203], [204, 209], [210, 220], [221, 224], [225, 228], [229, 236], [237, 239], [240, 243], [244, 248], [248, 249]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [12, 14, "algorithm"], [16, 16, "algorithm"], [22, 23, "researcher"], [25, 26, "organisation"], [29, 30, "researcher"], [32, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 9, 12, 14, "usage", "", false, false], [12, 14, 22, 23, "origin", "", true, false], [12, 14, 29, 30, "origin", "", true, false], [16, 16, 12, 14, "named", "", false, false], [22, 23, 25, 26, "physical", "", false, false], [22, 23, 25, 26, "role", "", false, false], [29, 30, 32, 36, "physical", "", false, false], [29, 30, 32, 36, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "the", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "based", "systems", "presented", "by", "Alex", "Graves", "(", "Google", "DeepMind", ")", "and", "Navdeep", "Jaitly", "(", "University", "of", "Toronto", ")", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was the Connectionist Temporal Classification (CTC) based systems presented by Alex Graves (Google DeepMind) and Navdeep Jaitly (University of Toronto) in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 43], [44, 57], [58, 66], [67, 81], [82, 83], [83, 86], [86, 87], [88, 93], [94, 101], [102, 111], [112, 114], [115, 119], [120, 126], [127, 128], [128, 134], [135, 143], [143, 144], [145, 148], [149, 156], [157, 163], [164, 165], [165, 175], [176, 178], [179, 186], [186, 187], [188, 190], [191, 195], [195, 196]]}
{"doc_key": "ai-train-55", "ner": [[0, 7, "algorithm"], [12, 12, "algorithm"], [11, 14, "algorithm"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[11, 14, 12, 12, "named", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["Linear", "-", "Fragmentary", "Programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "Linear", "Programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-Fragmentary Programming (LFP) is a generalisation of Linear Programming (LP).", "token2charspan": [[0, 6], [6, 7], [7, 18], [19, 30], [31, 32], [32, 35], [35, 36], [37, 39], [40, 41], [42, 56], [57, 59], [60, 66], [67, 78], [79, 80], [80, 82], [82, 83], [83, 84]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 14, "misc"], [16, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 14, "win-defeat", "", false, false], [8, 14, 16, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "2011", "and", "2012", "International", "Conference", "on", "Machine", "Learning", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the 2011 and 2012 International Conference on Machine Learning,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 84], [85, 88], [89, 93], [94, 107], [108, 118], [119, 121], [122, 129], [130, 138], [138, 139]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [25, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "emergence", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "deploy", "the", "developed", "neural", "network", "as", "inheritable", "components", "in", "these", "frameworks", "."], "sentence-detokenized": "With the emergence of component-based frameworks such as .NET and Java, component-based development environments are able to deploy the developed neural network as inheritable components in these frameworks.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 31], [31, 32], [32, 37], [38, 48], [49, 53], [54, 56], [57, 61], [62, 65], [66, 70], [70, 71], [72, 81], [81, 82], [82, 87], [88, 99], [100, 112], [113, 116], [117, 121], [122, 124], [125, 131], [132, 135], [136, 145], [146, 152], [153, 160], [161, 163], [164, 175], [176, 186], [187, 189], [190, 195], [196, 206], [206, 207]]}
{"doc_key": "ai-train-58", "ner": [[1, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Like", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "the", "algorithm", "first", "creates", "a", "match", "(", "see", "figures", ")", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "."], "sentence-detokenized": "Like BLEU, the basic unit of evaluation is the sentence, the algorithm first creates a match (see figures) between two sentences, the candidate translation string and the reference translation string.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 14], [15, 20], [21, 25], [26, 28], [29, 39], [40, 42], [43, 46], [47, 55], [55, 56], [57, 60], [61, 70], [71, 76], [77, 84], [85, 86], [87, 92], [93, 94], [94, 97], [98, 105], [105, 106], [107, 114], [115, 118], [119, 128], [128, 129], [130, 133], [134, 143], [144, 155], [156, 162], [163, 166], [167, 170], [171, 180], [181, 192], [193, 199], [199, 200]]}
{"doc_key": "ai-train-59", "ner": [[0, 6, "conference"], [16, 16, "task"], [18, 19, "task"], [28, 29, "metrics"], [31, 37, "metrics"], [44, 45, "conference"], [42, 47, "conference"], [50, 50, "location"], [52, 52, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 6, 16, 16, "related-to", "subject_at", false, false], [0, 6, 18, 19, "related-to", "subject_at", false, false], [28, 29, 0, 6, "temporal", "", false, false], [31, 37, 28, 29, "named", "", true, false], [42, 47, 44, 45, "named", "", false, false], [50, 50, 52, 52, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["At", "the", "annual", "NIST", "Document", "Understanding", "Conferences", ",", "where", "research", "teams", "submit", "their", "systems", "for", "both", "summarization", "and", "translation", "tasks", ",", "one", "of", "the", "metrics", "used", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "At the annual NIST Document Understanding Conferences, where research teams submit their systems for both summarization and translation tasks, one of the metrics used is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 18], [19, 27], [28, 41], [42, 53], [53, 54], [55, 60], [61, 69], [70, 75], [76, 82], [83, 88], [89, 96], [97, 100], [101, 105], [106, 119], [120, 123], [124, 135], [136, 141], [141, 142], [143, 146], [147, 149], [150, 153], [154, 161], [162, 166], [167, 169], [170, 173], [174, 179], [180, 186], [187, 188], [188, 194], [194, 195], [195, 203], [204, 214], [215, 218], [219, 226], [227, 237], [237, 238], [239, 241], [242, 250], [251, 253], [254, 260], [261, 272], [273, 283], [284, 291], [292, 293], [293, 297], [297, 298], [298, 299], [300, 308], [308, 309], [310, 316], [316, 317], [318, 326], [327, 328], [329, 333], [333, 334]]}
{"doc_key": "ai-train-60", "ner": [[4, 5, "programlang"], [6, 7, "product"], [10, 11, "programlang"], [14, 14, "product"], [20, 20, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[4, 5, 10, 11, "type-of", "", false, false], [4, 5, 20, 20, "named", "", false, false], [6, 7, 10, 11, "part-of", "", false, false], [6, 7, 14, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "run", "in", "Java", "with", "JShell", "(", "minimum", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, run in Java with JShell (minimum Java 9): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 24], [25, 27], [28, 32], [33, 37], [38, 44], [45, 46], [46, 53], [54, 58], [59, 60], [60, 61], [61, 62], [63, 73], [74, 84], [85, 86], [87, 106], [107, 111], [112, 113], [114, 118]]}
{"doc_key": "ai-train-61", "ner": [[1, 6, "metrics"], [3, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[1, 6, 3, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLEU", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLEU metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "restriction", "that", "edges", "can", "only", "be", "from", "a", "limited", "set", "of", "possible", "connections", "to", "facilitate", "algebras", "in", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called Knowledge Graphs, which are semantic networks, but with the restriction that edges can only be from a limited set of possible connections to facilitate algebras in the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 116], [117, 118], [119, 126], [127, 133], [134, 143], [144, 150], [150, 151], [152, 157], [158, 161], [162, 170], [171, 179], [179, 180], [181, 184], [185, 189], [190, 193], [194, 205], [206, 210], [211, 216], [217, 220], [221, 225], [226, 228], [229, 233], [234, 235], [236, 243], [244, 247], [248, 250], [251, 259], [260, 271], [272, 274], [275, 285], [286, 294], [295, 297], [298, 301], [302, 307], [307, 308]]}
{"doc_key": "ai-train-63", "ner": [[0, 2, "product"], [17, 18, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "text", "editor", ",", "but", "they", "are", "also", "available", "as", "stand", "-", "alone", "applications", "that", "can", "be", "activated", "from", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program, such as a text editor, but they are also available as stand-alone applications that can be activated from programs that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [76, 77], [78, 82], [83, 85], [86, 87], [88, 92], [93, 99], [99, 100], [101, 104], [105, 109], [110, 113], [114, 118], [119, 128], [129, 131], [132, 137], [137, 138], [138, 143], [144, 156], [157, 161], [162, 165], [166, 168], [169, 178], [179, 183], [184, 192], [193, 197], [198, 202], [203, 207], [208, 216], [217, 221], [221, 222]]}
{"doc_key": "ai-train-64", "ner": [[5, 12, "organisation"], [15, 24, "conference"], [25, 29, "organisation"], [33, 35, "conference"], [37, 39, "conference"], [42, 44, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "Fellow", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "an", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a Fellow of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence, and the Cognitive Science Society, and an editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [137, 138], [139, 142], [143, 146], [147, 156], [157, 164], [165, 172], [172, 173], [174, 177], [178, 180], [181, 187], [188, 190], [191, 193], [194, 203], [204, 213], [213, 214], [215, 217], [218, 226], [227, 235], [235, 236], [237, 240], [241, 243], [244, 251], [252, 260], [260, 261]]}
{"doc_key": "ai-train-65", "ner": [[1, 2, "algorithm"], [0, 4, "algorithm"], [10, 11, "task"], [18, 19, "researcher"], [21, 22, "university"], [25, 26, "researcher"], [28, 31, "organisation"], [33, 35, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 2, 10, 11, "type-of", "", false, false], [1, 2, 18, 19, "origin", "", false, false], [1, 2, 25, 26, "origin", "", false, false], [0, 4, 1, 2, "named", "", false, false], [18, 19, 21, 22, "physical", "", false, false], [18, 19, 21, 22, "role", "", false, false], [25, 26, 28, 31, "role", "", false, false], [33, 35, 28, 31, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "with", "the", "work", "of", "Fumitada", "Itakura", "(", "Nagoya", "University", ")", "and", "Shuzo", "Saito", "(", "Nippon", "Telegraph", "and", "Telephone", ",", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear predictive coding (LPC), a form of speech coding, began with the work of Fumitada Itakura (Nagoya University) and Shuzo Saito (Nippon Telegraph and Telephone, NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 67], [68, 71], [72, 76], [77, 79], [80, 88], [89, 96], [97, 98], [98, 104], [105, 115], [115, 116], [117, 120], [121, 126], [127, 132], [133, 134], [134, 140], [141, 150], [151, 154], [155, 164], [164, 165], [166, 169], [169, 170], [171, 173], [174, 178], [178, 179]]}
{"doc_key": "ai-train-66", "ner": [[59, 61, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "ergodic", ",", "then", "all", "sample", "paths", "have", "the", "same", "time", "average", ",", "and", "so", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=", "\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "terms", "of", "the", "mean", "squared", "error", "."], "sentence-detokenized": "If the signal is ergodic, then all sample paths have the same time average, and so mathR _ x ^ {n / T _ 0} (\\ tau) = \\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in terms of the mean squared error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 24], [24, 25], [26, 30], [31, 34], [35, 41], [42, 47], [48, 52], [53, 56], [57, 61], [62, 66], [67, 74], [74, 75], [76, 79], [80, 82], [83, 88], [89, 90], [91, 92], [93, 94], [95, 96], [96, 97], [98, 99], [100, 101], [102, 103], [104, 105], [105, 106], [107, 108], [108, 109], [110, 113], [113, 114], [115, 116], [117, 118], [119, 126], [127, 128], [128, 129], [129, 130], [131, 132], [133, 134], [135, 136], [137, 138], [138, 139], [140, 141], [142, 143], [144, 145], [146, 147], [147, 148], [149, 150], [150, 151], [152, 155], [155, 156], [157, 158], [159, 163], [164, 166], [167, 172], [173, 175], [176, 179], [180, 184], [185, 192], [193, 198], [198, 199]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [14, 14, "algorithm"], [15, 17, "algorithm"], [21, 21, "algorithm"], [20, 24, "algorithm"], [28, 28, "algorithm"], [27, 31, "algorithm"], [34, 38, "algorithm"], [43, 45, "algorithm"], [47, 50, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12], "relations": [[15, 17, 14, 14, "named", "", false, false], [20, 24, 21, 21, "named", "", false, false], [27, 31, 28, 28, "named", "", false, false], [43, 45, 47, 50, "related-to", "", true, false]], "relations_mapping_to_source": [1, 3, 5, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "Principal", "Component", "Analysis", "(", "PCA", ")", ",", "Linear", "Discriminant", "Analysis", "(", "LDA", ")", ",", "Canonical", "Correlation", "Analysis", "(", "CCA", ")", "or", "Non-Negative", "Matrix", "Factorization", "(", "NMF", ")", "techniques", "followed", "by", "K", "-", "NN", "clustering", "on", "feature", "vectors", "in", "the", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Canonical Correlation Analysis (CCA) or Non-Negative Matrix Factorization (NMF) techniques followed by K-NN clustering on feature vectors in the reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [157, 158], [159, 168], [169, 180], [181, 189], [190, 191], [191, 194], [194, 195], [196, 198], [199, 211], [212, 218], [219, 232], [233, 234], [234, 237], [237, 238], [239, 249], [250, 258], [259, 261], [262, 263], [263, 264], [264, 266], [267, 277], [278, 280], [281, 288], [289, 296], [297, 299], [300, 303], [304, 311], [312, 323], [324, 329], [329, 330]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [14, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[14, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [14, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [14, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [14, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [11, 13, "task"], [31, 32, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 11, 13, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognizing", "named", "entities", "in", "the", "text", "is", "called", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognizing named entities in the text is called Named Entity Recognition, while the task of determining the identity of named entities mentioned in the text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 60], [61, 66], [67, 73], [74, 85], [85, 86], [87, 92], [93, 96], [97, 101], [102, 104], [105, 116], [117, 120], [121, 129], [130, 132], [133, 138], [139, 147], [148, 157], [158, 160], [161, 164], [165, 169], [170, 172], [173, 179], [180, 186], [187, 194], [194, 195]]}
{"doc_key": "ai-train-70", "ner": [[1, 1, "algorithm"], [31, 37, "algorithm"]], "ner_mapping_to_source": [0, 2], "relations": [[1, 1, 31, 37, "part-of", "", true, false]], "relations_mapping_to_source": [0], "sentence": ["The", "sigmoid", "functions", "and", "derivations", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "but", "as", "of", "version", "0.8.0", "they", "have", "been", "released", "in", "a", "separate", "R", "package", ",", "sigmoid", ",", "with", "the", "intention", "of", "allowing", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivations used in the package were originally included in the package, but as of version 0.8.0 they have been released in a separate R package, sigmoid, with the intention of allowing more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 102], [103, 105], [106, 108], [109, 116], [117, 122], [123, 127], [128, 132], [133, 137], [138, 146], [147, 149], [150, 151], [152, 160], [161, 162], [163, 170], [170, 171], [172, 179], [179, 180], [181, 185], [186, 189], [190, 199], [200, 202], [203, 211], [212, 216], [217, 224], [225, 228], [228, 229]]}
{"doc_key": "ai-train-71", "ner": [[0, 1, "programlang"], [16, 20, "organisation"], [22, 22, "organisation"], [25, 25, "location"], [27, 27, "location"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 14, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 1, 7, 8, "artifact", "", true, false], [0, 1, 10, 11, "artifact", "", true, false], [0, 1, 13, 14, "artifact", "", true, false], [22, 22, 16, 20, "named", "", false, false], [22, 22, 25, 25, "physical", "", false, false], [25, 25, 27, 27, "physical", "", false, false], [7, 8, 16, 20, "role", "", false, false], [10, 11, 16, 20, "role", "", false, false], [13, 14, 16, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "designed", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", "in", "Cambridge", ",", "Massachusetts", "."], "sentence-detokenized": "The logo was designed in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert at Bolt, Beranek and Newman (BBN) in Cambridge, Massachusetts.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 21], [22, 24], [25, 29], [30, 32], [33, 38], [39, 47], [47, 48], [49, 56], [57, 64], [65, 68], [69, 76], [77, 83], [84, 86], [87, 91], [91, 92], [93, 100], [101, 104], [105, 111], [112, 113], [113, 116], [116, 117], [118, 120], [121, 130], [130, 131], [132, 145], [145, 146]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [17, 18, "field"], [22, 24, "algorithm"], [26, 29, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 17, 18, "compare", "", false, false], [22, 24, 17, 18, "part-of", "", false, false], [26, 29, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "contrasted", "with", "traditional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and can be contrasted with traditional deep learning techniques that use gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 86], [87, 89], [90, 100], [101, 105], [106, 117], [118, 122], [123, 131], [132, 142], [143, 147], [148, 151], [152, 160], [161, 168], [169, 171], [172, 173], [174, 180], [181, 188], [189, 193], [194, 195], [196, 201], [202, 210], [210, 211]]}
{"doc_key": "ai-train-73", "ner": [[0, 5, "algorithm"], [67, 68, "metrics"], [66, 70, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[66, 70, 67, 68, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "least", "squares", "to", "fit", "a", "hyperplane", "function", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "we", "can", "evaluate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use least squares to fit a hyperplane function \u0177 = a + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264n / sub) sub 1 \u2264 i \u2264n / sub) sub 1 \u2264 i \u2264n / sub, we can evaluate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 15], [16, 23], [24, 26], [27, 30], [31, 32], [33, 43], [44, 52], [53, 54], [55, 56], [57, 58], [59, 60], [61, 62], [63, 67], [68, 69], [70, 73], [74, 75], [76, 78], [79, 82], [83, 87], [88, 89], [89, 90], [91, 94], [95, 96], [97, 98], [99, 102], [102, 103], [104, 105], [106, 109], [110, 111], [112, 113], [114, 117], [117, 118], [119, 122], [123, 124], [125, 126], [127, 128], [129, 131], [132, 133], [134, 137], [137, 138], [139, 142], [143, 144], [145, 146], [147, 148], [149, 151], [152, 153], [154, 157], [157, 158], [159, 162], [163, 164], [165, 166], [167, 168], [169, 171], [172, 173], [174, 177], [177, 178], [179, 181], [182, 185], [186, 194], [195, 198], [199, 202], [203, 208], [209, 212], [213, 217], [218, 225], [226, 231], [232, 233], [233, 236], [236, 237], [237, 238]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [31, 54, "country"], [33, 33, "country"], [35, 35, "country"], [37, 49, "country"], [40, 53, "country"], [42, 42, "country"], [44, 44, "country"], [46, 56, "country"], [59, 59, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "locations", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "the", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", ",", "South", "Africa", ",", "Spain", ",", "the", "Philippines", ",", "Turkey", "and", "the", "UK", "."], "sentence-detokenized": "The company has international locations in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, the Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey, South Africa, Spain, the Philippines, Turkey and the UK.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 39], [40, 42], [43, 52], [52, 53], [54, 60], [60, 61], [62, 68], [68, 69], [70, 75], [75, 76], [77, 84], [84, 85], [86, 91], [91, 92], [93, 98], [98, 99], [100, 105], [105, 106], [107, 112], [112, 113], [114, 123], [123, 124], [125, 131], [131, 132], [133, 141], [141, 142], [143, 146], [147, 158], [158, 159], [160, 166], [166, 167], [168, 177], [177, 178], [179, 184], [185, 191], [191, 192], [193, 198], [198, 199], [200, 206], [206, 207], [208, 216], [216, 217], [218, 224], [224, 225], [226, 231], [232, 238], [238, 239], [240, 245], [245, 246], [247, 250], [251, 262], [262, 263], [264, 270], [271, 274], [275, 278], [279, 281], [281, 282]]}
{"doc_key": "ai-train-75", "ner": [[2, 2, "misc"], [3, 9, "field"], [10, 10, "organisation"], [12, 16, "university"], [26, 28, "organisation"], [30, 36, "university"], [41, 42, "university"], [44, 45, "university"], [48, 50, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[2, 2, 3, 9, "topic", "", false, false], [2, 2, 10, 10, "origin", "", false, false], [2, 2, 12, 16, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "Electrical", "and", "Computer", "Engineering", "from", "Inria", "and", "Sophia", "Antipolis", "University", "of", "Nice", "(", "2000", ")", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "and", "as", "a", "visiting", "fellow", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in Electrical and Computer Engineering from Inria and Sophia Antipolis University of Nice (2000) and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, and as a visiting fellow at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 58], [59, 64], [65, 68], [69, 75], [76, 85], [86, 96], [97, 99], [100, 104], [105, 106], [106, 110], [110, 111], [112, 115], [116, 119], [120, 124], [125, 134], [135, 144], [145, 147], [148, 155], [156, 165], [166, 176], [176, 177], [178, 183], [184, 187], [188, 193], [194, 203], [203, 204], [205, 208], [209, 211], [212, 213], [214, 222], [223, 229], [230, 232], [233, 240], [241, 251], [251, 252], [253, 257], [258, 268], [269, 272], [273, 276], [277, 287], [288, 290], [291, 298], [298, 299]]}
{"doc_key": "ai-train-76", "ner": [[6, 7, "researcher"], [9, 9, "researcher"], [13, 16, "product"], [17, 18, "country"], [20, 22, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 9, 6, 7, "role", "licensing_patent_to", false, false], [9, 9, 17, 18, "physical", "", false, false], [20, 22, 9, 9, "artifact", "", false, false], [20, 22, 13, 16, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Licensing", "the", "original", "patent", "of", "inventor", "George", "Devol", ",", "Engelberger", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Licensing the original patent of inventor George Devol, Engelberger developed the first industrial robot in the United States, Unimate, in the 1950s.", "token2charspan": [[0, 9], [10, 13], [14, 22], [23, 29], [30, 32], [33, 41], [42, 48], [49, 54], [54, 55], [56, 67], [68, 77], [78, 81], [82, 87], [88, 98], [99, 104], [105, 107], [108, 111], [112, 118], [119, 125], [125, 126], [127, 134], [134, 135], [136, 138], [139, 142], [143, 148], [148, 149]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[2, 2, "programlang"], [4, 4, "programlang"], [12, 12, "programlang"], [15, 15, "programlang"], [25, 26, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 2, 12, 12, "named", "", false, false], [4, 4, 2, 2, "origin", "descendant_of", false, false], [4, 4, 15, 15, "general-affiliation", "", false, false], [4, 4, 25, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Descendants", "of", "CLIPS", "include", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "rewritten", "in", "Java", ",", "later", "evolved", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by", "JESS", "."], "sentence-detokenized": "Descendants of CLIPS include Jess (the rule-based part of CLIPS rewritten in Java, later evolved in a different direction), JESS was originally inspired by JESS.", "token2charspan": [[0, 11], [12, 14], [15, 20], [21, 28], [29, 33], [34, 35], [35, 38], [39, 43], [43, 44], [44, 49], [50, 54], [55, 57], [58, 63], [64, 73], [74, 76], [77, 81], [81, 82], [83, 88], [89, 96], [97, 99], [100, 101], [102, 111], [112, 121], [121, 122], [122, 123], [124, 128], [129, 132], [133, 143], [144, 152], [153, 155], [156, 160], [160, 161]]}
{"doc_key": "ai-train-79", "ner": [[11, 13, "product"], [15, 16, "organisation"], [21, 22, "product"], [42, 43, "product"], [40, 47, "product"], [64, 65, "misc"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6], "relations": [[15, 16, 11, 13, "usage", "", false, false], [21, 22, 15, 16, "artifact", "", false, false], [42, 43, 15, 16, "origin", "", true, false], [42, 43, 64, 65, "related-to", "", true, false], [40, 47, 15, 16, "origin", "", true, false], [40, 47, 64, 65, "related-to", "", true, false]], "relations_mapping_to_source": [1, 2, 3, 4, 5, 6], "sentence": ["He", "also", "created", "flexible", ",", "intelligent", "AGV", "applications", ",", "designed", "the", "Motivity", "control", "system", "that", "RMT", "Robotics", "used", "to", "develop", "the", "ADAM", "iAGV", "(", "self", "-", "driving", "vehicle", ")", ",", "which", "is", "used", "for", "complex", "pick", "and", "place", "operations", ",", "along", "with", "gantry", "systems", "and", "industrial", "robotic", "arms", "used", "in", "first", "-", "class", "automotive", "assembly", "plants", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "He also created flexible, intelligent AGV applications, designed the Motivity control system that RMT Robotics used to develop the ADAM iAGV (self-driving vehicle), which is used for complex pick and place operations, along with gantry systems and industrial robotic arms used in first-class automotive assembly plants to move products from process to process in non-linear layouts.", "token2charspan": [[0, 2], [3, 7], [8, 15], [16, 24], [24, 25], [26, 37], [38, 41], [42, 54], [54, 55], [56, 64], [65, 68], [69, 77], [78, 85], [86, 92], [93, 97], [98, 101], [102, 110], [111, 115], [116, 118], [119, 126], [127, 130], [131, 135], [136, 140], [141, 142], [142, 146], [146, 147], [147, 154], [155, 162], [162, 163], [163, 164], [165, 170], [171, 173], [174, 178], [179, 182], [183, 190], [191, 195], [196, 199], [200, 205], [206, 216], [216, 217], [218, 223], [224, 228], [229, 235], [236, 243], [244, 247], [248, 258], [259, 266], [267, 271], [272, 276], [277, 279], [280, 285], [285, 286], [286, 291], [292, 302], [303, 311], [312, 318], [319, 321], [322, 326], [327, 335], [336, 340], [341, 348], [349, 351], [352, 359], [360, 362], [363, 373], [374, 381], [381, 382]]}
{"doc_key": "ai-train-80", "ner": [[6, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameters", "\u03b2", "are", "usually", "estimated", "with", "maximum", "likelihood", "."], "sentence-detokenized": "The parameters \u03b2 are usually estimated with maximum likelihood.", "token2charspan": [[0, 3], [4, 14], [15, 16], [17, 20], [21, 28], [29, 38], [39, 43], [44, 51], [52, 62], [62, 63]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [5, 5, "metrics"], [7, 7, "metrics"], [9, 9, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 0, 1, "part-of", "", false, false], [7, 7, 0, 1, "part-of", "", false, false], [9, 9, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", "such", "as", "accuracy", "and", "recall", "or", "DCG", "are", "useful", "to", "assess", "the", "quality", "of", "the", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics such as accuracy and recall or DCG are useful to assess the quality of the recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [30, 34], [35, 37], [38, 46], [47, 50], [51, 57], [58, 60], [61, 64], [65, 68], [69, 75], [76, 78], [79, 85], [86, 89], [90, 97], [98, 100], [101, 104], [105, 119], [120, 126], [126, 127]]}
{"doc_key": "ai-train-82", "ner": [[7, 8, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "hundreds", "of", "industrial", "robots", "work", "on", "fully", "automated", "production", "lines", ",", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "In a typical factory, hundreds of industrial robots work on fully automated production lines, one robot for every ten human workers.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 30], [31, 33], [34, 44], [45, 51], [52, 56], [57, 59], [60, 65], [66, 75], [76, 86], [87, 92], [92, 93], [94, 97], [98, 103], [104, 107], [108, 113], [114, 117], [118, 123], [124, 131], [131, 132]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [14, 15, "field"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [28, 29, "task"], [31, 32, "task"], [34, 35, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[14, 15, 5, 5, "usage", "", false, true], [19, 20, 14, 15, "part-of", "", false, false], [22, 23, 14, 15, "part-of", "", false, false], [25, 26, 14, 15, "part-of", "", false, false], [28, 29, 14, 15, "part-of", "", false, false], [31, 32, 14, 15, "part-of", "", false, false], [34, 35, 14, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "wide", "range", "of", "image", "processing", "applications", ",", "including", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growing", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a wide range of image processing applications, including image segmentation, feature generation, face extraction, motion detection, region growing and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 52], [53, 58], [59, 61], [62, 67], [68, 78], [79, 91], [91, 92], [93, 102], [103, 108], [109, 121], [121, 122], [123, 130], [131, 141], [141, 142], [143, 147], [148, 158], [158, 159], [160, 166], [167, 176], [176, 177], [178, 184], [185, 192], [193, 196], [197, 202], [203, 212], [212, 213]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [14, 17, "field"], [22, 25, "misc"], [27, 36, "conference"], [32, 32, "conference"], [40, 42, "misc"], [43, 49, "conference"], [50, 51, "conference"], [54, 57, "conference"], [53, 59, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 14, 17, "related-to", "contributes_to", false, false], [0, 0, 22, 25, "win-defeat", "", false, false], [0, 0, 40, 42, "win-defeat", "", false, false], [22, 25, 27, 36, "temporal", "", false, false], [32, 32, 27, 36, "named", "", false, false], [40, 42, 43, 49, "temporal", "", false, false], [40, 42, 54, 57, "temporal", "", false, false], [50, 51, 43, 49, "named", "", false, false], [53, 59, 54, 57, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", ",", "and", "won", "the", "best", "paper", "award", "at", "the", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "international", "conference", ",", "and", "the", "best", "reviewer", "award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision, and won the best paper award at the Non-Photorealistic Rendering and Animation (NPAR) 2012 international conference, and the best reviewer award at the Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [110, 111], [112, 115], [116, 119], [120, 123], [124, 128], [129, 134], [135, 140], [141, 143], [144, 147], [148, 166], [167, 176], [177, 180], [181, 190], [191, 192], [192, 196], [196, 197], [198, 202], [203, 216], [217, 227], [227, 228], [229, 232], [233, 236], [237, 241], [242, 250], [251, 256], [257, 259], [260, 263], [264, 269], [270, 280], [281, 283], [284, 292], [293, 299], [300, 304], [305, 309], [310, 313], [314, 327], [328, 338], [339, 341], [342, 350], [351, 357], [358, 359], [359, 363], [363, 364], [365, 369], [369, 370]]}
{"doc_key": "ai-train-85", "ner": [[0, 1, "programlang"], [6, 7, "field"], [9, 10, "field"], [3, 10, "misc"], [14, 14, "researcher"], [12, 20, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 6, 7, "part-of", "", false, false], [0, 1, 9, 10, "part-of", "", false, false], [0, 1, 3, 10, "type-of", "", false, false], [12, 20, 0, 1, "usage", "", false, false], [12, 20, 14, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "is", "an", "ontological", "language", "in", "computer", "science", "and", "artificial", "intelligence", ",", "used", "by", "Doug", "Lenat", "'s", "Cyc", "Artificial", "Intelligence", "project", "."], "sentence-detokenized": "CycL is an ontological language in computer science and artificial intelligence, used by Doug Lenat's Cyc Artificial Intelligence project.", "token2charspan": [[0, 4], [5, 7], [8, 10], [11, 22], [23, 31], [32, 34], [35, 43], [44, 51], [52, 55], [56, 66], [67, 79], [79, 80], [81, 85], [86, 88], [89, 93], [94, 99], [99, 101], [102, 105], [106, 116], [117, 129], [130, 137], [137, 138]]}
{"doc_key": "ai-train-86", "ner": [[0, 2, "task"], [5, 7, "metrics"], [13, 17, "metrics"], [10, 26, "metrics"], [35, 38, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 0, 2, "part-of", "", false, false], [13, 17, 5, 7, "named", "", false, false], [10, 26, 5, 7, "named", "", false, false], [35, 38, 5, 7, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "mean", "of", "the", "squared", "deviations", "of", "the", "predictions", "from", "TRUE", "in", "the", "out", "-", "of", "-", "sample", "test", "space", "generated", "by", "the", "model", "estimated", "in", "a", "given", "sample", "space", "."], "sentence-detokenized": "In regression analysis, the mean squared error, often referred to as the mean squared prediction error or out-of-sample mean squared error, can refer to the mean of the squared deviations of the predictions from TRUE in the out-of-sample test space generated by the model estimated in a given sample space.", "token2charspan": [[0, 2], [3, 13], [14, 22], [22, 23], [24, 27], [28, 32], [33, 40], [41, 46], [46, 47], [48, 53], [54, 62], [63, 65], [66, 68], [69, 72], [73, 77], [78, 85], [86, 96], [97, 102], [103, 105], [106, 109], [109, 110], [110, 112], [112, 113], [113, 119], [120, 124], [125, 132], [133, 138], [138, 139], [140, 143], [144, 149], [150, 152], [153, 156], [157, 161], [162, 164], [165, 168], [169, 176], [177, 187], [188, 190], [191, 194], [195, 206], [207, 211], [212, 216], [217, 219], [220, 223], [224, 227], [227, 228], [228, 230], [230, 231], [231, 237], [238, 242], [243, 248], [249, 258], [259, 261], [262, 265], [266, 271], [272, 281], [282, 284], [285, 286], [287, 292], [293, 299], [300, 305], [305, 306]]}
{"doc_key": "ai-train-87", "ner": [[5, 7, "algorithm"], [9, 10, "algorithm"], [17, 20, "algorithm"], [34, 35, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 7, 9, 10, "compare", "", false, false], [5, 7, 17, 20, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "terms", "of", "results", ",", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparably", ",", "with", "C", "-", "HOG", "descriptors", "having", "a", "slight", "advantage", "in", "detection", "error", "rates", "in", "both", "datasets", "with", "fixed", "FALSE", "positive", "rates", "."], "sentence-detokenized": "In terms of results, C-HOG and R-HOG block descriptors perform comparably, with C-HOG descriptors having a slight advantage in detection error rates in both datasets with fixed FALSE positive rates.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 19], [19, 20], [21, 22], [22, 23], [23, 26], [27, 30], [31, 33], [33, 36], [37, 42], [43, 54], [55, 62], [63, 73], [73, 74], [75, 79], [80, 81], [81, 82], [82, 85], [86, 97], [98, 104], [105, 106], [107, 113], [114, 123], [124, 126], [127, 136], [137, 142], [143, 148], [149, 151], [152, 156], [157, 165], [166, 170], [171, 176], [177, 182], [183, 191], [192, 197], [197, 198]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [18, 19, "algorithm"], [21, 23, "algorithm"], [25, 27, "algorithm"], [29, 30, "misc"], [35, 37, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 29, 30, "usage", "", false, false], [14, 15, 29, 30, "usage", "", false, false], [18, 19, 29, 30, "usage", "", false, false], [21, 23, 29, 30, "usage", "", false, false], [25, 27, 29, 30, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "flexible", "fitting", "using", "the", "Fisherface", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neurally", "motivated", "dynamic", "link", "fitting", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, flexible fitting using the Fisherface algorithm, hidden Markov model, multilinear subspace learning using tensor representation, and neurally motivated dynamic link fitting.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 124], [125, 132], [133, 138], [139, 142], [143, 153], [154, 163], [163, 164], [165, 171], [172, 178], [179, 184], [184, 185], [186, 197], [198, 206], [207, 215], [216, 221], [222, 228], [229, 243], [243, 244], [245, 248], [249, 257], [258, 267], [268, 275], [276, 280], [281, 288], [288, 289]]}
{"doc_key": "ai-train-89", "ner": [[0, 6, "misc"], [18, 21, "location"], [42, 44, "location"], [59, 59, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[18, 21, 0, 6, "temporal", "", false, false], [42, 44, 0, 6, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["From", "the", "2019", "Toronto", "International", "Film", "Festival", "onwards", ",", "films", "may", "be", "restricted", "from", "being", "screened", "at", "the", "Scotiabank", "Theatre", "in", "Toronto", "-", "one", "of", "the", "main", "venues", "of", "the", "festival", "-", "and", "may", "be", "screened", "elsewhere", "(", "such", "as", "at", "the", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "by", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "From the 2019 Toronto International Film Festival onwards, films may be restricted from being screened at the Scotiabank Theatre in Toronto - one of the main venues of the festival - and may be screened elsewhere (such as at the TIFF Bell Lightbox and other local cinemas) if they are distributed by a service such as Netflix.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 21], [22, 35], [36, 40], [41, 49], [50, 57], [57, 58], [59, 64], [65, 68], [69, 71], [72, 82], [83, 87], [88, 93], [94, 102], [103, 105], [106, 109], [110, 120], [121, 128], [129, 131], [132, 139], [140, 141], [142, 145], [146, 148], [149, 152], [153, 157], [158, 164], [165, 167], [168, 171], [172, 180], [181, 182], [183, 186], [187, 190], [191, 193], [194, 202], [203, 212], [213, 214], [214, 218], [219, 221], [222, 224], [225, 228], [229, 233], [234, 238], [239, 247], [248, 251], [252, 257], [258, 263], [264, 271], [271, 272], [273, 275], [276, 280], [281, 284], [285, 296], [297, 299], [300, 301], [302, 309], [310, 314], [315, 317], [318, 325], [325, 326]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 4, "researcher"], [5, 7, "organisation"], [11, 16, "researcher"], [23, 26, "product"], [36, 37, "researcher"], [41, 43, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 0, 5, 7, "related-to", "purchases", false, false], [2, 4, 11, 16, "named", "same", false, false], [2, 4, 36, 37, "named", "same", false, false], [5, 7, 2, 4, "origin", "founded_by", false, false], [23, 26, 0, 0, "artifact", "", false, false], [41, 43, 36, 37, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Unimation", "acquired", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "manufacturing", "a", "programmable", "universal", "assembly", "machine", ",", "a", "new", "model", "of", "robotic", "arm", ",", "using", "Scheinman", "'s", "cutting", "-", "edge", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation acquired Victor Scheinman's Vicarm Inc. in 1977, and with Scheinman's help, the company created and began manufacturing a programmable universal assembly machine, a new model of robotic arm, using Scheinman's cutting-edge VAL programming language.", "token2charspan": [[0, 9], [10, 18], [19, 25], [26, 35], [35, 37], [38, 44], [45, 49], [50, 52], [53, 57], [57, 58], [59, 62], [63, 67], [68, 77], [77, 79], [80, 84], [84, 85], [86, 89], [90, 97], [98, 105], [106, 109], [110, 115], [116, 129], [130, 131], [132, 144], [145, 154], [155, 163], [164, 171], [171, 172], [173, 174], [175, 178], [179, 184], [185, 187], [188, 195], [196, 199], [199, 200], [201, 206], [207, 216], [216, 218], [219, 226], [226, 227], [227, 231], [232, 235], [236, 247], [248, 256], [256, 257]]}
{"doc_key": "ai-train-91", "ner": [[0, 3, "product"], [6, 6, "programlang"], [10, 11, "algorithm"], [12, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 3, 6, 6, "general-affiliation", "", false, false], [0, 3, 10, 11, "origin", "implementation_of", false, false], [0, 3, 12, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "the", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is the open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 10], [11, 15], [16, 22], [23, 27], [28, 42], [43, 45], [46, 49], [50, 54], [55, 64], [65, 67], [68, 71], [72, 76], [77, 81], [82, 88], [89, 93], [93, 94]]}
{"doc_key": "ai-train-92", "ner": [[13, 14, "product"], [22, 31, "misc"]], "ner_mapping_to_source": [1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", "according", "to", "Google", "Scholar", ",", "and", "in", "2016", "it", "won", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", ",", "indicating", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20,000 times according to Google Scholar, and in 2016 it won the IEEE Signal Processing Society Sustained Impact Award, indicating an unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 51], [52, 57], [58, 67], [68, 70], [71, 77], [78, 85], [85, 86], [87, 90], [91, 93], [94, 98], [99, 101], [102, 105], [106, 109], [110, 114], [115, 121], [122, 132], [133, 140], [141, 150], [151, 157], [158, 163], [163, 164], [165, 175], [176, 178], [179, 188], [189, 193], [194, 200], [201, 204], [205, 207], [208, 213], [214, 216], [217, 222], [223, 228], [229, 240], [240, 241]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [16, 17, "product"], [35, 40, "product"], [44, 44, "organisation"], [45, 45, "product"], [43, 43, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 44, 44, "artifact", "", false, false], [16, 17, 0, 1, "related-to", "performs", false, false], [16, 17, 35, 40, "part-of", "", false, false], [44, 44, 43, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "on", "the", "verge", "of", "becoming", "completely", "indistinguishable", "from", "the", "real", "human", "voice", "with", "Adobe", "Voco", "voice", "editing", "and", "generation", "software", ",", "launched", "in", "2016", ",", "and", "prototyped", "by", "Adobe", "Voco", ",", "designed", "as", "part", "of", "Adobe", "Creative", "Suite", ",", "and", "Google", "DeepMind", "WaveNet", "."], "sentence-detokenized": "Speech synthesis is on the verge of becoming completely indistinguishable from the real human voice with Adobe Voco voice editing and generation software, launched in 2016, and prototyped by Adobe Voco, designed as part of Adobe Creative Suite, and Google DeepMind WaveNet.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 22], [23, 26], [27, 32], [33, 35], [36, 44], [45, 55], [56, 73], [74, 78], [79, 82], [83, 87], [88, 93], [94, 99], [100, 104], [105, 110], [111, 115], [116, 121], [122, 129], [130, 133], [134, 144], [145, 153], [153, 154], [155, 163], [164, 166], [167, 171], [171, 172], [173, 176], [177, 187], [188, 190], [191, 196], [197, 201], [201, 202], [203, 211], [212, 214], [215, 219], [220, 222], [223, 228], [229, 237], [238, 243], [243, 244], [245, 248], [249, 255], [256, 264], [265, 272], [272, 273]]}
{"doc_key": "ai-train-94", "ner": [[0, 6, "researcher"], [7, 9, "organisation"], [15, 20, "organisation"], [27, 27, "conference"], [34, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 6, 7, 9, "role", "", false, false], [0, 6, 15, 20, "role", "", false, false], [0, 6, 27, 27, "role", "", false, false], [0, 6, 34, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "Honorary", "Fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "Fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "member", "of", "the", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an Honorary Fellow of the Neuroscience Research Program, a Fellow of the American Academy of Arts and Sciences, a founding member of the AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 123], [124, 132], [133, 139], [140, 142], [143, 146], [147, 151], [152, 155], [156, 157], [158, 166], [167, 173], [174, 176], [177, 180], [181, 189], [190, 199], [200, 203], [204, 209], [210, 218], [218, 219]]}
{"doc_key": "ai-train-95", "ner": [[9, 10, "task"], [12, 13, "task"], [17, 18, "task"], [25, 25, "misc"], [26, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 10, 17, 18, "cause-effect", "", false, false], [12, 13, 17, 18, "cause-effect", "", false, false], [26, 27, 17, 18, "topic", "", false, false], [26, 27, 25, 25, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "building", "on", "the", "success", "of", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "into", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, building on the success of speech recognition and speech synthesis, research into speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 22], [23, 25], [26, 29], [30, 37], [38, 40], [41, 47], [48, 59], [60, 63], [64, 70], [71, 80], [80, 81], [82, 90], [91, 95], [96, 102], [103, 114], [115, 120], [121, 125], [126, 129], [130, 141], [142, 144], [145, 148], [149, 155], [156, 165], [166, 173], [173, 174]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 16, "algorithm"], [21, 22, "algorithm"], [26, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 16, 3, 4, "origin", "", false, false], [15, 16, 8, 9, "origin", "", false, false], [15, 16, 11, 12, "origin", "", false, false], [15, 16, 26, 26, "part-of", "", false, false], [21, 22, 15, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisor", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forget", "gate", "(", "also", "known", "as", "keep", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisor J\u00fcrgen Schmidhuber and Fred Cummins introduced the forget gate (also known as keep gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 35], [36, 42], [43, 54], [55, 58], [59, 63], [64, 71], [72, 82], [83, 86], [87, 93], [94, 98], [99, 100], [100, 104], [105, 110], [111, 113], [114, 118], [119, 123], [123, 124], [125, 129], [130, 133], [134, 138], [139, 151], [151, 152]]}
{"doc_key": "ai-train-97", "ner": [[0, 3, "field"], [5, 6, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 12, 0, 3, "part-of", "", false, false], [9, 12, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sine", "function", "is", "usually", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sine function is usually defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103], [104, 111]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [20, 24, "conference"], [28, 30, "organisation"], [26, 32, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 20, 24, "role", "", false, false], [9, 10, 28, 30, "role", "", false, false], [26, 32, 28, 30, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "who", "was", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "for", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hays, who was a founding member of both the Association for Computational Linguistics and the International Committee for Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 72], [72, 73], [74, 77], [78, 81], [82, 83], [84, 92], [93, 99], [100, 102], [103, 107], [108, 111], [112, 123], [124, 127], [128, 141], [142, 153], [154, 157], [158, 161], [162, 175], [176, 185], [186, 189], [190, 203], [204, 215], [216, 217], [217, 221], [221, 222], [222, 223]]}
{"doc_key": "ai-train-99", "ner": [[8, 13, "misc"], [18, 18, "misc"], [35, 36, "metrics"], [34, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[34, 38, 35, 36, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one", "-dimensional", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPD", ",", "in", "order", "to", "solve", "the", "coefficients", "of", "the", "digital", "polynomial", "prior", "and", "minimize", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "allows", "the", "nonlinear", "products", "of", "the", "digital", "prior", "order", "to", "be", "captured", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional polynomial-based memory (or memoryless) DPD, in order to solve the coefficients of the digital polynomial prior and minimize the mean squared error (MSE), the distorted output of the nonlinear system must be oversampled at a rate that allows the nonlinear products of the digital prior order to be captured.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 35], [35, 47], [48, 58], [58, 59], [59, 64], [65, 71], [72, 73], [73, 75], [76, 86], [86, 87], [88, 91], [91, 92], [93, 95], [96, 101], [102, 104], [105, 110], [111, 114], [115, 127], [128, 130], [131, 134], [135, 142], [143, 153], [154, 159], [160, 163], [164, 172], [173, 176], [177, 181], [182, 189], [190, 195], [196, 197], [197, 200], [200, 201], [201, 202], [203, 206], [207, 216], [217, 223], [224, 226], [227, 230], [231, 240], [241, 247], [248, 252], [253, 255], [256, 267], [268, 270], [271, 272], [273, 277], [278, 282], [283, 289], [290, 293], [294, 303], [304, 312], [313, 315], [316, 319], [320, 327], [328, 333], [334, 339], [340, 342], [343, 345], [346, 354], [354, 355]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 9, "location"], [11, 12, "location"], [14, 14, "country"], [18, 18, "location"], [20, 20, "country"], [34, 39, "organisation"], [43, 46, "organisation"], [47, 48, "location"], [52, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 9, 9, "physical", "", false, false], [0, 1, 43, 46, "physical", "", false, false], [0, 1, 52, 54, "role", "", false, false], [9, 9, 11, 12, "physical", "", false, false], [11, 12, 14, 14, "physical", "", false, false], [34, 39, 43, 46, "part-of", "", false, false], [43, 46, 47, 48, "physical", "", false, false], [52, 54, 34, 39, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", "(", "born", "October", "5", ",", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldavian", "SSR", ",", "USSR", ",", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "a", "senior", "US", "researcher", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", ",", "leading", "the", "InfoLab", "group", "at", "the", "laboratory", "."], "sentence-detokenized": "Boris Katz (born October 5, 1947 in Chi\u0219in\u0103u, Moldavian SSR, USSR, (now Chi\u0219in\u0103u, Moldova)) is a senior US researcher (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge, leading the InfoLab group at the laboratory.", "token2charspan": [[0, 5], [6, 10], [11, 12], [12, 16], [17, 24], [25, 26], [26, 27], [28, 32], [33, 35], [36, 44], [44, 45], [46, 55], [56, 59], [59, 60], [61, 65], [65, 66], [67, 68], [68, 71], [72, 80], [80, 81], [82, 89], [89, 90], [90, 91], [92, 94], [95, 96], [97, 103], [104, 106], [107, 117], [118, 119], [119, 127], [128, 137], [137, 138], [139, 141], [142, 145], [146, 149], [150, 158], [159, 166], [167, 170], [171, 181], [182, 194], [195, 205], [206, 208], [209, 212], [213, 226], [227, 236], [237, 239], [240, 250], [251, 253], [254, 263], [263, 264], [265, 272], [273, 276], [277, 284], [285, 290], [291, 293], [294, 297], [298, 308], [308, 309]]}
