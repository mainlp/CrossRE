{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [29, 30, "field"], [31, 33, "researcher"], [35, 37, "researcher"], [39, 40, "researcher"], [42, 43, "researcher"], [47, 47, "researcher"], [49, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 20, "part-of", "", false, false], [3, 7, 19, 20, "usage", "", false, false], [3, 7, 29, 30, "part-of", "", false, false], [3, 7, 29, 30, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "for", "opinion", "-", "based", "recommender", "systems", "use", "various", "techniques", "such", "as", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches for opinion-based recommender systems use various techniques such as text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019), 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 30], [30, 31], [31, 36], [37, 48], [49, 56], [57, 60], [61, 68], [69, 79], [80, 84], [85, 87], [88, 92], [93, 99], [99, 100], [101, 112], [113, 122], [122, 123], [124, 133], [134, 142], [143, 144], [144, 147], [148, 152], [153, 163], [164, 173], [174, 182], [182, 183], [184, 187], [188, 192], [193, 201], [202, 205], [205, 206], [207, 211], [211, 212], [213, 214], [214, 215], [216, 221], [221, 222], [223, 227], [228, 231], [231, 232], [233, 237], [238, 243], [243, 244], [245, 246], [246, 247], [248, 251], [251, 252], [253, 257], [258, 263], [263, 264], [265, 269], [270, 274], [274, 275], [276, 278], [279, 281], [281, 282], [283, 284], [284, 288], [288, 289], [289, 290], [291, 293], [294, 295], [295, 296], [296, 297], [297, 298], [299, 305], [305, 306]]}
{"doc_key": "ai-train-2", "ner": [[11, 11, "university"], [16, 17, "researcher"], [9, 20, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[16, 17, 11, 11, "physical", "", false, false], [16, 17, 11, 11, "role", "", false, false], [9, 20, 11, 11, "physical", "", false, false], [9, 20, 11, 11, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "proponents", "of", "procedural", "representation", "were", "mainly", "to", "be", "found", "at", "MIT", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The proponents of procedural representation were mainly to be found at MIT under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 3], [4, 14], [15, 17], [18, 28], [29, 43], [44, 48], [49, 55], [56, 58], [59, 61], [62, 67], [68, 70], [71, 74], [75, 80], [81, 84], [85, 95], [96, 98], [99, 105], [106, 112], [113, 116], [117, 124], [125, 131], [131, 132]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [22, 22, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 22, 22, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "to", "numerically", "solve", "linear", "and", "non-linear", "problems", "and", "perform", "other", "numerical", "experiments", "with", "a", "program", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps to numerically solve linear and non-linear problems and perform other numerical experiments with a program that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 27], [28, 33], [34, 40], [41, 44], [45, 55], [56, 64], [65, 68], [69, 76], [77, 82], [83, 92], [93, 104], [105, 109], [110, 111], [112, 119], [120, 124], [125, 127], [128, 134], [135, 145], [146, 150], [151, 157], [157, 158]]}
{"doc_key": "ai-train-5", "ner": [[3, 4, "algorithm"], [9, 10, "misc"], [12, 13, "researcher"], [18, 20, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 4, 12, 13, "origin", "", false, false], [9, 10, 12, 13, "origin", "", false, false], [12, 13, 18, 20, "physical", "", false, false], [12, 13, 18, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "backpropagation", "algorithm", ",", "as", "well", "as", "unsupervised", "methods", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", ",", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the backpropagation algorithm, as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto, can be used to train deep, highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 31], [32, 41], [41, 42], [43, 45], [46, 50], [51, 53], [54, 66], [67, 74], [75, 77], [78, 83], [84, 90], [91, 94], [95, 105], [106, 108], [109, 112], [113, 123], [124, 126], [127, 134], [134, 135], [136, 139], [140, 142], [143, 147], [148, 150], [151, 156], [157, 161], [161, 162], [163, 169], [170, 179], [180, 186], [187, 200], [200, 201], [202, 203], [203, 204], [204, 208], [209, 216]]}
{"doc_key": "ai-train-6", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalent", "in", "DCG", "notation", ":"], "sentence-detokenized": "or equivalent in DCG notation:", "token2charspan": [[0, 2], [3, 13], [14, 16], [17, 20], [21, 29], [29, 30]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 11, "algorithm"], [14, 15, "algorithm"], [20, 22, "algorithm"], [25, 25, "algorithm"], [27, 28, "algorithm"], [40, 42, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 11, "type-of", "", false, false], [0, 3, 14, 15, "usage", "part-of?", true, false], [14, 15, 20, 22, "compare", "", false, false], [25, 25, 20, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "use", "competitive", "learning", "(", "as", "opposed", "to", "error", "correction", "learning", "such", "as", "backpropagation", "with", "gradient", "descent", ")", "and", "that", "they", "use", "a", "neighbourhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks in that they use competitive learning (as opposed to error correction learning such as backpropagation with gradient descent) and that they use a neighbourhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 82], [83, 94], [95, 103], [104, 105], [105, 107], [108, 115], [116, 118], [119, 124], [125, 135], [136, 144], [145, 149], [150, 152], [153, 168], [169, 173], [174, 182], [183, 190], [190, 191], [192, 195], [196, 200], [201, 205], [206, 209], [210, 211], [212, 225], [226, 234], [235, 237], [238, 246], [247, 250], [251, 262], [263, 273], [274, 276], [277, 280], [281, 286], [287, 292], [292, 293]]}
{"doc_key": "ai-train-8", "ner": [[15, 17, "organisation"], [27, 28, "misc"], [38, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "it", "has", "been", "recommended", "by", "various", "bodies", ",", "including", "the", "Audio", "Engineering", "Society", ",", "that", "dynamic", "range", "measurements", "be", "made", "with", "the", "audio", "signal", "present", ",", "which", "is", "then", "filtered", "out", "in", "the", "noise", "level", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "mutes", "."], "sentence-detokenized": "Since the early 1990s, it has been recommended by various bodies, including the Audio Engineering Society, that dynamic range measurements be made with the audio signal present, which is then filtered out in the noise level measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or mutes.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 25], [26, 29], [30, 34], [35, 46], [47, 49], [50, 57], [58, 64], [64, 65], [66, 75], [76, 79], [80, 85], [86, 97], [98, 105], [105, 106], [107, 111], [112, 119], [120, 125], [126, 138], [139, 141], [142, 146], [147, 151], [152, 155], [156, 161], [162, 168], [169, 176], [176, 177], [178, 183], [184, 186], [187, 191], [192, 200], [201, 204], [205, 207], [208, 211], [212, 217], [218, 223], [224, 235], [236, 240], [241, 243], [244, 253], [254, 261], [262, 267], [267, 268], [269, 273], [274, 280], [281, 293], [294, 306], [307, 312], [313, 315], [316, 319], [320, 323], [324, 326], [327, 332], [333, 338], [339, 341], [342, 347], [347, 348]]}
{"doc_key": "ai-train-9", "ner": [[4, 6, "misc"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [29, 34, "task"], [36, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[4, 6, 17, 18, "part-of", "concept_used_in", true, false], [4, 6, 20, 21, "part-of", "concept_used_in", false, false], [4, 6, 23, 24, "part-of", "concept_used_in", false, false], [4, 6, 26, 27, "part-of", "concept_used_in", false, false], [4, 6, 29, 34, "part-of", "concept_used_in", false, false], [4, 6, 36, 38, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["The", "technique", "of", "creating", "self", "-", "faces", "and", "using", "them", "for", "recognition", "is", "also", "used", "outside", "of", "face", "recognition", ":", "Handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "The technique of creating self-faces and using them for recognition is also used outside of face recognition: Handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation and medical image analysis.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 25], [26, 30], [30, 31], [31, 36], [37, 40], [41, 46], [47, 51], [52, 55], [56, 67], [68, 70], [71, 75], [76, 80], [81, 88], [89, 91], [92, 96], [97, 108], [108, 109], [110, 121], [122, 133], [133, 134], [135, 138], [139, 146], [146, 147], [148, 153], [154, 165], [165, 166], [167, 171], [172, 180], [180, 181], [181, 185], [186, 193], [194, 208], [209, 212], [213, 220], [221, 226], [227, 235], [235, 236]]}
{"doc_key": "ai-train-10", "ner": [[1, 3, "organisation"], [9, 13, "organisation"], [15, 15, "organisation"], [19, 22, "organisation"], [25, 29, "organisation"], [32, 35, "organisation"], [38, 42, "organisation"], [44, 44, "organisation"], [48, 51, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[9, 13, 1, 3, "part-of", "", false, false], [15, 15, 9, 13, "named", "", false, false], [19, 22, 1, 3, "part-of", "", false, false], [25, 29, 1, 3, "part-of", "", false, false], [32, 35, 1, 3, "part-of", "", false, false], [38, 42, 1, 3, "part-of", "", false, false], [44, 44, 38, 42, "named", "", false, false], [48, 51, 1, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "an", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "coordinated", "studies", "to", "assist", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research coordinated studies to assist strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 38], [39, 47], [48, 51], [52, 55], [56, 64], [65, 76], [77, 80], [81, 86], [87, 101], [102, 103], [103, 107], [107, 108], [108, 109], [110, 113], [114, 116], [117, 127], [128, 130], [131, 137], [137, 138], [139, 142], [143, 145], [146, 156], [157, 159], [160, 168], [169, 173], [173, 174], [175, 178], [179, 181], [182, 192], [193, 195], [196, 203], [203, 204], [205, 208], [209, 216], [217, 225], [226, 234], [235, 243], [244, 250], [251, 252], [252, 257], [257, 258], [259, 262], [263, 266], [267, 273], [274, 276], [277, 282], [283, 291], [292, 303], [304, 311], [312, 314], [315, 321], [322, 331], [332, 340], [341, 343], [344, 349], [350, 363], [363, 364]]}
{"doc_key": "ai-train-11", "ner": [[10, 11, "algorithm"], [15, 17, "researcher"], [23, 23, "researcher"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[15, 17, 23, 23, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [1], "sentence": ["A", "rapid", "method", "for", "calculating", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "in", "1935", "as", "an", "appendix", "to", "Bliss", "'", "paper", "."], "sentence-detokenized": "A rapid method for calculating maximum likelihood estimates for the probit model was proposed by Ronald Fisher in 1935 as an appendix to Bliss' paper.", "token2charspan": [[0, 1], [2, 7], [8, 14], [15, 18], [19, 30], [31, 38], [39, 49], [50, 59], [60, 63], [64, 67], [68, 74], [75, 80], [81, 84], [85, 93], [94, 96], [97, 103], [104, 110], [111, 113], [114, 118], [119, 121], [122, 124], [125, 133], [134, 136], [137, 142], [142, 143], [144, 149], [149, 150]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 15, "product"], [18, 18, "organisation"], [20, 20, "product"], [22, 22, "organisation"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[20, 20, 14, 15, "usage", "uses_software", false, false], [20, 20, 18, 18, "artifact", "", false, false], [20, 20, 24, 24, "named", "", false, false], [24, 24, 22, 22, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "of", "these", "programmes", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "powers", "AltaVista", "'s", "BabelFish", "(", "Yahoo", "'s", "Babelfish", "since", "9", "May", "2008", ")", "."], "sentence-detokenized": "Several of these programmes are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (Yahoo's Babelfish since 9 May 2008).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 27], [28, 31], [32, 41], [42, 48], [48, 49], [50, 54], [55, 57], [58, 64], [65, 74], [75, 78], [79, 82], [83, 90], [91, 97], [98, 102], [103, 109], [110, 119], [119, 121], [122, 131], [132, 133], [133, 138], [138, 140], [141, 150], [151, 156], [157, 158], [159, 162], [163, 167], [167, 168], [168, 169]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 22, "field"], [26, 27, "misc"], [23, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 23, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 23, 32, "related-to", "", true, false], [10, 11, 20, 22, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 23, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[14, 20, "metrics"]], "ner_mapping_to_source": [1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "most", "common", "method", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "measure", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "."], "sentence-detokenized": "The most common method is to use the so-called ROUGE measure (Recall-Oriented Understudy for Gisting Evaluation).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 22], [23, 25], [26, 28], [29, 32], [33, 36], [37, 39], [39, 40], [40, 46], [47, 52], [53, 60], [61, 62], [62, 68], [68, 69], [69, 77], [78, 88], [89, 92], [93, 100], [101, 111], [111, 112], [112, 113]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 3, 4], "relations": [[18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["RapidMiner", "provides", "learning", "schemes", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "with", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "November", "13", ",", "2013", "."], "sentence-detokenized": "RapidMiner provides learning schemes, models and algorithms and can be extended with R and Python scripts. David Norris, Bloor Research, November 13, 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 84], [85, 86], [87, 90], [91, 97], [98, 105], [105, 106], [107, 112], [113, 119], [119, 120], [121, 126], [127, 135], [135, 136], [137, 145], [146, 148], [148, 149], [150, 154], [154, 155]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [13, 14, "task"], [18, 20, "misc"], [37, 38, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 13, 14, "related-to", "", false, false], [0, 0, 37, 38, "related-to", "", true, false], [18, 20, 0, 0, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["tity", "contains", "a", "collection", "of", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modelling", "as", "well", "as", "graphical", "user", "interfaces", "for", "easy", "access", "to", "these", "functions", ".", "The", "newer", ",", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "development", "of", "which", "began", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "especially", "in", "teaching", "and", "research", "."], "sentence-detokenized": "tity contains a collection of visualisation tools and algorithms for data analysis and predictive modelling as well as graphical user interfaces for easy access to these functions. The newer, fully Java-based version (Weka 3), development of which began in 1997, is now used in many different application areas, especially in teaching and research.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 107], [108, 110], [111, 115], [116, 118], [119, 128], [129, 133], [134, 144], [145, 148], [149, 153], [154, 160], [161, 163], [164, 169], [170, 179], [179, 180], [181, 184], [185, 190], [190, 191], [192, 197], [198, 202], [202, 203], [203, 208], [209, 216], [217, 218], [218, 222], [223, 224], [224, 225], [225, 226], [227, 238], [239, 241], [242, 247], [248, 253], [254, 256], [257, 261], [261, 262], [263, 265], [266, 269], [270, 274], [275, 277], [278, 282], [283, 292], [293, 304], [305, 310], [310, 311], [312, 322], [323, 325], [326, 334], [335, 338], [339, 347], [347, 348]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [12, 20, "misc"], [21, 25, "misc"], [26, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 20, 0, 0, "topic", "", false, false], [12, 20, 21, 25, "win-defeat", "", false, false], [21, 25, 26, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "won", "great", "acclaim", "when", "his", "paper", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "was", "awarded", "the", "Best", "Paper", "Award", "by", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "in", "1982", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and won great acclaim when his paper Heuretics: Theoretical and Study of Heuristic Rules was awarded the Best Paper Award by the Association for the Advancement of Artificial Intelligence in 1982.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 49], [50, 55], [56, 63], [64, 68], [69, 72], [73, 78], [79, 88], [88, 89], [90, 101], [102, 105], [106, 111], [112, 114], [115, 124], [125, 130], [131, 134], [135, 142], [143, 146], [147, 151], [152, 157], [158, 163], [164, 166], [167, 170], [171, 182], [183, 186], [187, 190], [191, 202], [203, 205], [206, 216], [217, 229], [230, 232], [233, 237], [237, 238]]}
{"doc_key": "ai-train-18", "ner": [[8, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "account", "for", "multiple", "units", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To account for multiple units, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 10], [11, 14], [15, 23], [24, 29], [29, 30], [31, 32], [33, 41], [42, 47], [48, 52], [53, 55], [56, 66], [67, 70], [71, 75], [76, 83], [83, 84]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 13, "product"], [15, 16, "product"], [18, 19, "product"], [21, 23, "product"], [25, 26, "product"], [35, 40, "product"], [43, 44, "product"], [46, 47, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 25, 26, "type-of", "", false, false], [12, 13, 25, 26, "type-of", "", false, false], [15, 16, 25, 26, "type-of", "", false, false], [18, 19, 25, 26, "type-of", "", false, false], [21, 23, 25, 26, "type-of", "", false, false], [43, 44, 35, 40, "type-of", "", false, false], [46, 47, 35, 40, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "voice", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "smart", "far", "-", "field", "voice", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of voice assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, voice portals can now be accessed via mobile devices and smart far-field voice speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 24], [25, 35], [36, 40], [41, 43], [44, 49], [49, 51], [52, 56], [56, 57], [58, 64], [65, 70], [70, 71], [72, 78], [79, 88], [88, 89], [90, 99], [100, 107], [108, 111], [112, 119], [119, 121], [122, 127], [127, 128], [129, 134], [135, 142], [143, 146], [147, 150], [151, 153], [154, 162], [163, 166], [167, 173], [174, 181], [182, 185], [186, 191], [192, 195], [195, 196], [196, 201], [202, 207], [208, 216], [217, 221], [222, 224], [225, 231], [232, 236], [237, 240], [241, 247], [248, 252], [252, 253]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 14, "algorithm"], [16, 16, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "type-of", "", false, false], [9, 11, 2, 3, "type-of", "", false, false], [13, 14, 2, 3, "type-of", "", false, false], [16, 16, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "Naive", "Bayes", "classifier", ",", "Support", "Vector", "Machine", ",", "Gaussian", "Mixtures", "and", "Network", "."], "sentence-detokenized": "Examples of supervised learning are Naive Bayes classifier, Support Vector Machine, Gaussian Mixtures and Network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 41], [42, 47], [48, 58], [58, 59], [60, 67], [68, 74], [75, 82], [82, 83], [84, 92], [93, 101], [102, 105], [106, 113], [113, 114]]}
{"doc_key": "ai-train-21", "ner": [[4, 6, "algorithm"], [27, 29, "algorithm"], [31, 31, "task"], [34, 35, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 6, 27, 29, "part-of", "", true, false], [34, 35, 31, 31, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "derive", "math", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "math", "regret", "bounds", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "for", "classification", "using", "the", "hinge", "loss", "math", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to derive math O (\\ sqrt {T}) / math regret bounds for the online version of the support vector machine for classification using the hinge loss math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 39], [40, 44], [45, 46], [47, 48], [48, 49], [50, 54], [55, 56], [56, 57], [57, 58], [58, 59], [60, 61], [62, 66], [67, 73], [74, 80], [81, 84], [85, 88], [89, 95], [96, 103], [104, 106], [107, 110], [111, 118], [119, 125], [126, 133], [134, 137], [138, 152], [153, 158], [159, 162], [163, 168], [169, 173], [174, 178], [179, 180], [181, 183], [184, 185], [185, 186], [186, 187], [188, 190], [191, 194], [194, 195], [196, 197], [197, 198], [198, 199], [200, 201], [202, 203], [204, 205], [206, 208], [209, 210], [210, 211], [211, 212], [213, 217], [218, 219], [220, 222], [222, 223], [223, 225], [226, 227], [228, 232]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", "and", "matchmaking", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, individual wildlife identification and matchmaking.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 102], [102, 103], [104, 111], [112, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 151], [152, 160], [161, 175], [176, 179], [180, 191], [191, 192]]}
{"doc_key": "ai-train-23", "ner": [[8, 9, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [27, 32, "university"], [36, 36, "university"], [38, 40, "university"], [42, 43, "university"], [45, 50, "university"], [52, 52, "university"], [55, 59, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[8, 9, 14, 15, "related-to", "", true, false], [8, 9, 17, 19, "related-to", "", true, false], [8, 9, 21, 22, "related-to", "", true, false], [8, 9, 24, 25, "related-to", "", true, false], [8, 9, 27, 32, "related-to", "", true, false], [8, 9, 36, 36, "related-to", "", true, false], [8, 9, 38, 40, "related-to", "", true, false], [8, 9, 42, 43, "related-to", "", true, false], [8, 9, 45, 50, "related-to", "", true, false], [8, 9, 52, 52, "related-to", "", true, false], [8, 9, 55, 59, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "researching", "pose", "estimation", ",", "including", "groups", "from", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbr\u00fccken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are researching pose estimation, including groups from Brown University, Carnegie Mellon University, MPI Saarbr\u00fccken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 48], [49, 53], [54, 64], [64, 65], [66, 75], [76, 82], [83, 87], [88, 93], [94, 104], [104, 105], [106, 114], [115, 121], [122, 132], [132, 133], [134, 137], [138, 149], [149, 150], [151, 159], [160, 170], [170, 171], [172, 182], [183, 185], [186, 196], [196, 197], [198, 201], [202, 207], [207, 208], [209, 219], [220, 222], [223, 230], [230, 231], [232, 237], [238, 246], [247, 252], [252, 253], [254, 257], [258, 264], [264, 265], [266, 274], [275, 285], [286, 288], [289, 297], [298, 301], [302, 312], [313, 314], [314, 318], [318, 319], [320, 323], [324, 334], [335, 337], [338, 348], [348, 349], [350, 356], [356, 357]]}
{"doc_key": "ai-train-24", "ner": [[0, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Sigmoid", "function", "cross", "entropy", "loss", "is", "used", "for", "the", "prediction", "of", "K", "independent", "probability", "values", "in", "math", "0.1", "/", "math", "."], "sentence-detokenized": "Sigmoid function cross entropy loss is used for the prediction of K independent probability values in math 0.1 / math.", "token2charspan": [[0, 7], [8, 16], [17, 22], [23, 30], [31, 35], [36, 38], [39, 43], [44, 47], [48, 51], [52, 62], [63, 65], [66, 67], [68, 79], [80, 91], [92, 98], [99, 101], [102, 106], [107, 110], [111, 112], [113, 117], [117, 118]]}
{"doc_key": "ai-train-25", "ner": [[3, 5, "misc"], [7, 7, "field"], [9, 10, "field"], [13, 15, "university"], [18, 18, "country"], [21, 23, "misc"], [26, 29, "university"], [31, 31, "country"], [37, 37, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 5, 7, 7, "topic", "", false, false], [3, 5, 9, 10, "topic", "", false, false], [3, 5, 13, 15, "physical", "", true, false], [13, 15, 18, 18, "physical", "", false, false], [21, 23, 26, 29, "physical", "", true, false], [26, 29, 31, 31, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "a", "professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Chair in Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan before becoming a professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 34], [35, 37], [38, 49], [50, 53], [54, 62], [63, 70], [71, 73], [74, 77], [78, 88], [89, 91], [92, 101], [102, 104], [105, 108], [109, 120], [121, 124], [125, 128], [129, 136], [137, 144], [145, 150], [151, 153], [154, 157], [158, 163], [164, 173], [174, 176], [177, 187], [188, 190], [191, 196], [197, 203], [204, 212], [213, 214], [215, 224], [225, 227], [228, 237], [237, 238]]}
{"doc_key": "ai-train-26", "ner": [[6, 7, "algorithm"], [14, 16, "algorithm"], [19, 20, "researcher"], [22, 23, "researcher"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[6, 7, 14, 16, "usage", "", true, false], [14, 16, 19, 20, "origin", "", false, false], [14, 16, 22, 23, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Another", "technique", "used", "in", "particular", "for", "recurrent", "neural", "networks", "is", "the", "LSTM", "network", "(", "Long", "Short", "Memory", ")", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "from", "1997", "."], "sentence-detokenized": "Another technique used in particular for recurrent neural networks is the LSTM network (Long Short Memory) by Sepp Hochreiter & J\u00fcrgen Schmidhuber from 1997.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 25], [26, 36], [37, 40], [41, 50], [51, 57], [58, 66], [67, 69], [70, 73], [74, 78], [79, 86], [87, 88], [88, 92], [93, 98], [99, 105], [105, 106], [107, 109], [110, 114], [115, 125], [126, 127], [128, 134], [135, 146], [147, 151], [152, 156], [156, 157]]}
{"doc_key": "ai-train-27", "ner": [[8, 9, "product"], [15, 15, "product"], [44, 44, "product"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["The", "integration", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "versatile", ",", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "mode", "similar", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The integration of a C++ interpreter (CINT up to version 5.34, Cling from version 6) makes this package very versatile, as it can be used in interactive, scripted and compiled mode similar to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 15], [16, 18], [19, 20], [21, 22], [22, 24], [25, 36], [37, 38], [38, 40], [40, 42], [43, 45], [46, 48], [49, 56], [57, 61], [61, 62], [63, 68], [69, 73], [74, 81], [82, 83], [83, 84], [85, 90], [91, 95], [96, 103], [104, 108], [109, 118], [118, 119], [120, 122], [123, 125], [126, 129], [130, 132], [133, 137], [138, 140], [141, 152], [152, 153], [154, 162], [163, 166], [167, 175], [176, 180], [181, 188], [189, 191], [192, 202], [203, 211], [212, 216], [217, 219], [220, 226], [226, 227]]}
{"doc_key": "ai-train-28", "ner": [[0, 2, "product"], [18, 20, "field"], [24, 25, "task"], [27, 28, "task"], [30, 31, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 2, 18, 20, "related-to", "", false, false], [24, 25, 18, 20, "part-of", "", false, false], [27, 28, 18, 20, "part-of", "", false, false], [30, 31, 18, 20, "part-of", "", false, false], [33, 34, 18, 20, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Speech-driven", "user", "interfaces", "that", "interpret", "and", "manage", "conversational", "state", "are", "challenging", "because", "it", "is", "difficult", "to", "integrate", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "name", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Speech-driven user interfaces that interpret and manage conversational state are challenging because it is difficult to integrate complex natural language processing tasks such as coreference resolution, name recognition, information retrieval and dialogue management.", "token2charspan": [[0, 13], [14, 18], [19, 29], [30, 34], [35, 44], [45, 48], [49, 55], [56, 70], [71, 76], [77, 80], [81, 92], [93, 100], [101, 103], [104, 106], [107, 116], [117, 119], [120, 129], [130, 137], [138, 145], [146, 154], [155, 165], [166, 171], [172, 176], [177, 179], [180, 191], [192, 202], [202, 203], [204, 208], [209, 220], [220, 221], [222, 233], [234, 243], [244, 247], [248, 256], [257, 267], [267, 268]]}
{"doc_key": "ai-train-29", "ner": [[6, 7, "algorithm"], [10, 12, "algorithm"], [16, 17, "researcher"], [23, 26, "organisation"], [32, 33, "field"], [35, 36, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[6, 7, 16, 17, "origin", "", false, false], [6, 7, 32, 33, "part-of", "", false, false], [6, 7, 35, 36, "part-of", "", false, false], [10, 12, 16, 17, "origin", "", false, false], [10, 12, 32, 33, "part-of", "", false, false], [10, 12, 35, 36, "part-of", "", false, false], [16, 17, 23, 26, "physical", "", false, false], [16, 17, 23, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "the", "recurrent", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "AI", "Lab", "IDSIA", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in J\u00fcrgen Schmidhuber's research group at the Swiss AI Lab IDSIA won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 26], [27, 36], [37, 43], [44, 52], [53, 56], [57, 61], [62, 73], [74, 80], [81, 89], [90, 99], [100, 102], [103, 109], [110, 121], [121, 123], [124, 132], [133, 138], [139, 141], [142, 145], [146, 151], [152, 154], [155, 158], [159, 164], [165, 168], [169, 174], [175, 188], [189, 201], [202, 204], [205, 212], [213, 224], [225, 228], [229, 236], [237, 245], [245, 246]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 9, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 9, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 12, "misc"], [14, 14, "field"], [17, 20, "university"], [26, 29, "field"], [32, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 12, 14, 14, "topic", "topic_of_award", false, false], [7, 12, 17, 20, "origin", "", true, false], [26, 29, 32, 35, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "an", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "the", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, an S. V. della laurea ad honorem in psychology from the University of Padua in 1995 and a doctorate in industrial design and engineering from the Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 36], [37, 39], [40, 42], [43, 48], [49, 55], [56, 58], [59, 66], [67, 69], [70, 80], [81, 85], [86, 89], [90, 100], [101, 103], [104, 109], [110, 112], [113, 117], [118, 121], [122, 123], [124, 133], [134, 136], [137, 147], [148, 154], [155, 158], [159, 170], [171, 175], [176, 179], [180, 185], [186, 196], [197, 199], [200, 210], [210, 211]]}
{"doc_key": "ai-train-32", "ner": [[7, 8, "researcher"], [14, 18, "organisation"], [19, 19, "location"], [21, 21, "researcher"], [32, 34, "misc"], [49, 51, "misc"], [70, 71, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[7, 8, 14, 18, "physical", "", false, false], [7, 8, 14, 18, "role", "", false, false], [14, 18, 19, 19, "physical", "", false, false], [21, 21, 32, 34, "related-to", "works_with", true, false], [21, 21, 49, 51, "related-to", "works_with", true, false], [21, 21, 70, 71, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "in", "which", "multiplication", "was", "impaired", "but", "subtraction", "was", "preserved", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobe", ")", "and", "others", "in", "which", "subtraction", "was", "impaired", "but", "multiplication", "was", "preserved", "(", "associated", "with", "lesions", "in", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Together with his long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe in which multiplication was impaired but subtraction was preserved (associated with lesions of the inferior parietal lobe) and others in which subtraction was impaired but multiplication was preserved (associated with lesions in the intraparietal sulcus).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [41, 48], [49, 54], [54, 55], [56, 57], [58, 69], [70, 72], [73, 76], [77, 82], [82, 83], [83, 94], [95, 103], [104, 106], [107, 112], [112, 113], [114, 121], [122, 126], [127, 137], [138, 146], [147, 151], [152, 159], [160, 162], [163, 172], [173, 180], [181, 183], [184, 187], [188, 196], [197, 201], [202, 204], [205, 210], [211, 225], [226, 229], [230, 238], [239, 242], [243, 254], [255, 258], [259, 268], [269, 270], [270, 280], [281, 285], [286, 293], [294, 296], [297, 300], [301, 309], [310, 318], [319, 323], [323, 324], [325, 328], [329, 335], [336, 338], [339, 344], [345, 356], [357, 360], [361, 369], [370, 373], [374, 388], [389, 392], [393, 402], [403, 404], [404, 414], [415, 419], [420, 427], [428, 430], [431, 434], [435, 448], [449, 455], [455, 456], [456, 457]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [13, 16, "misc"], [18, 19, "misc"], [29, 29, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 8, "topic", "", false, false], [18, 19, 6, 8, "topic", "", false, false], [29, 29, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "as", "well", "as", "in", "the", "television", "adaptation", "of", "Westworld", "(", "2016", ")", ",", "have", "made", "viewers", "sympathetic", "to", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina, as well as in the television adaptation of Westworld (2016), have made viewers sympathetic to the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 56], [57, 68], [69, 75], [76, 78], [79, 84], [85, 89], [90, 92], [93, 96], [96, 97], [98, 108], [109, 121], [122, 125], [126, 128], [129, 136], [136, 137], [138, 140], [141, 145], [146, 148], [149, 151], [152, 155], [156, 166], [167, 177], [178, 180], [181, 190], [191, 192], [192, 196], [196, 197], [197, 198], [199, 203], [204, 208], [209, 216], [217, 228], [229, 231], [232, 235], [236, 242], [243, 253], [253, 254]]}
{"doc_key": "ai-train-34", "ner": [[8, 9, "field"], [11, 13, "algorithm"], [15, 16, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[11, 13, 8, 9, "part-of", "", false, false], [15, 16, 8, 9, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "most", "important", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the most important methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 25], [26, 33], [34, 38], [39, 41], [42, 54], [55, 63], [64, 67], [68, 77], [78, 87], [88, 96], [97, 100], [101, 108], [109, 117], [117, 118]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [17, 18, "misc"], [23, 24, "misc"], [26, 28, "person"], [33, 34, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[17, 18, 0, 3, "artifact", "", false, false], [23, 24, 0, 3, "artifact", "", false, false], [23, 24, 26, 28, "role", "director_of", false, false], [23, 24, 33, 34, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "in", "special", "locations", "to", "impress", "audiences", ".", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "are", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films in special locations to impress audiences. Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) are notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 53], [54, 61], [62, 71], [72, 74], [75, 82], [83, 92], [92, 93], [94, 99], [100, 108], [109, 110], [110, 114], [114, 115], [116, 119], [120, 127], [128, 130], [131, 132], [132, 139], [140, 144], [145, 152], [152, 153], [154, 158], [158, 159], [160, 168], [169, 176], [177, 184], [184, 185], [186, 189], [190, 197], [198, 206], [206, 207]]}
{"doc_key": "ai-train-36", "ner": [[12, 15, "field"], [19, 24, "task"], [26, 27, "task"], [29, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 15, "part-of", "", false, false], [26, 27, 12, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing for tasks such as part-of-speech tagging and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [95, 98], [99, 104], [105, 109], [110, 112], [113, 117], [117, 118], [118, 120], [120, 121], [121, 127], [128, 135], [136, 139], [140, 149], [150, 157], [158, 159], [159, 166], [166, 167], [168, 172], [172, 173], [173, 174]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [10, 15, "organisation"], [16, 19, "product"], [23, 25, "researcher"], [33, 33, "organisation"]], "ner_mapping_to_source": [0, 1, 4, 5, 6], "relations": [[10, 15, 2, 3, "role", "introduces_to_market", true, false], [16, 19, 33, 33, "related-to", "sold_to", true, false], [23, 25, 16, 19, "origin", "", false, false]], "relations_mapping_to_source": [0, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "in", "1963", "by", "the", "Fuji", "Yusoki", "Kogyo", "Company", ".", "The", "universal", "programmable", "assembly", "machine", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. The universal programmable assembly machine was invented by Victor Scheinman in 1976 and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 57], [58, 62], [63, 69], [70, 75], [76, 83], [83, 84], [85, 88], [89, 98], [99, 111], [112, 120], [121, 128], [129, 132], [133, 141], [142, 144], [145, 151], [152, 161], [162, 164], [165, 169], [170, 173], [174, 177], [178, 184], [185, 188], [189, 193], [194, 196], [197, 206], [206, 207]]}
{"doc_key": "ai-train-38", "ner": [[11, 11, "conference"], [13, 13, "researcher"], [22, 22, "field"], [36, 37, "researcher"], [43, 46, "researcher"], [57, 57, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[13, 13, 11, 11, "role", "president_of", false, false], [13, 13, 36, 37, "role", "colleagues", false, false], [22, 22, 57, 57, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "during", "his", "tenure", "as", "president", "of", "the", "AAAI", ",", "Hayes", "began", "a", "series", "of", "attacks", "on", "critics", "of", "AI", ",", "mostly", "tongue", "-", "in", "-", "cheek", ",", "and", "invented", "(", "with", "colleague", "Kenneth", "Ford", ")", "a", "prize", "named", "after", "Simon", "Newcomb", "to", "be", "awarded", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, during his tenure as president of the AAAI, Hayes began a series of attacks on critics of AI, mostly tongue-in-cheek, and invented (with colleague Kenneth Ford) a prize named after Simon Newcomb to be awarded for the most ridiculous argument refuting the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 24], [25, 28], [29, 35], [36, 38], [39, 48], [49, 51], [52, 55], [56, 60], [60, 61], [62, 67], [68, 73], [74, 75], [76, 82], [83, 85], [86, 93], [94, 96], [97, 104], [105, 107], [108, 110], [110, 111], [112, 118], [119, 125], [125, 126], [126, 128], [128, 129], [129, 134], [134, 135], [136, 139], [140, 148], [149, 150], [150, 154], [155, 164], [165, 172], [173, 177], [177, 178], [179, 180], [181, 186], [187, 192], [193, 198], [199, 204], [205, 212], [213, 215], [216, 218], [219, 226], [227, 230], [231, 234], [235, 239], [240, 250], [251, 259], [260, 268], [269, 272], [273, 284], [285, 287], [288, 290], [290, 291]]}
{"doc_key": "ai-train-39", "ner": [[14, 16, "algorithm"], [40, 41, "algorithm"], [53, 55, "algorithm"], [59, 61, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 16, 40, 41, "named", "same", false, false], [53, 55, 14, 16, "type-of", "", false, false], [59, 61, 14, 16, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "optimal", "value", "for", "math", "\\", "alpha", "/", "math", "can", "be", "found", "using", "a", "row", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimises", "S", ",", "usually", "using", "a", "row", "search", "in", "the", "interval", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "backtracking", "row", "search", "such", "as", "the", "Armijo", "row", "search", "."], "sentence-detokenized": "An optimal value for math\\ alpha / math can be found using a row search algorithm, i.e. the size of math\\ alpha / math is determined by finding the value that minimises S, usually using a row search in the interval math0\\ alpha 1 / math or a backtracking row search such as the Armijo row search.", "token2charspan": [[0, 2], [3, 10], [11, 16], [17, 20], [21, 25], [25, 26], [27, 32], [33, 34], [35, 39], [40, 43], [44, 46], [47, 52], [53, 58], [59, 60], [61, 64], [65, 71], [72, 81], [81, 82], [83, 87], [88, 91], [92, 96], [97, 99], [100, 104], [104, 105], [106, 111], [112, 113], [114, 118], [119, 121], [122, 132], [133, 135], [136, 143], [144, 147], [148, 153], [154, 158], [159, 168], [169, 170], [170, 171], [172, 179], [180, 185], [186, 187], [188, 191], [192, 198], [199, 201], [202, 205], [206, 214], [215, 220], [220, 221], [222, 227], [228, 229], [230, 231], [232, 236], [237, 239], [240, 241], [242, 254], [255, 258], [259, 265], [266, 270], [271, 273], [274, 277], [278, 284], [285, 288], [289, 295], [295, 296]]}
{"doc_key": "ai-train-40", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "breadth", "-", "first", "and", "depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "embody", "a", "lot", "of", "technical", "knowledge", ",", "but", "do", "not", "shed", "much", "light", "on", "the", "mental", "processes", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses breadth-first and depth-first search techniques, but ultimately concludes that the results represent expert systems that embody a lot of technical knowledge, but do not shed much light on the mental processes people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 30], [31, 36], [36, 37], [37, 42], [43, 49], [50, 60], [60, 61], [62, 65], [66, 76], [77, 86], [87, 91], [92, 95], [96, 103], [104, 113], [114, 120], [121, 128], [129, 133], [134, 140], [141, 142], [143, 146], [147, 149], [150, 159], [160, 169], [169, 170], [171, 174], [175, 177], [178, 181], [182, 186], [187, 191], [192, 197], [198, 200], [201, 204], [205, 211], [212, 221], [222, 228], [229, 232], [233, 235], [236, 241], [242, 246], [247, 254], [254, 255]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "are", "concerned", "with", "how", "spoken", "language", "can", "be", "understood", "or", "generated", "with", "the", "help", "of", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis are concerned with how spoken language can be understood or generated with the help of computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 43], [44, 53], [54, 58], [59, 62], [63, 69], [70, 78], [79, 82], [83, 85], [86, 96], [97, 99], [100, 109], [110, 114], [115, 118], [119, 123], [124, 126], [127, 136], [136, 137]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^", "{", "*}", "/", "math", "is", "usually", "estimated", "using", "a", "maximum", "likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "maximum", "a", "posteriori", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "MAP", "}", "/", "math", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta ^ {*} / math is usually estimated using a maximum likelihood (math\\ theta ^ {*} =\\ theta ^ {ML} / math) or maximum a posteriori (math\\ theta ^ {*} =\\ theta ^ {MAP} / math) procedure.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [17, 18], [19, 20], [20, 22], [23, 24], [25, 29], [30, 32], [33, 40], [41, 50], [51, 56], [57, 58], [59, 66], [67, 77], [78, 79], [79, 83], [83, 84], [85, 90], [91, 92], [93, 94], [94, 96], [97, 99], [100, 105], [106, 107], [108, 109], [109, 111], [111, 112], [113, 114], [115, 119], [119, 120], [121, 123], [124, 131], [132, 133], [134, 144], [145, 146], [146, 150], [150, 151], [152, 157], [158, 159], [160, 161], [161, 163], [164, 166], [167, 172], [173, 174], [175, 176], [176, 179], [179, 180], [181, 182], [183, 187], [187, 188], [189, 198], [198, 199]]}
{"doc_key": "ai-train-43", "ner": [[6, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "common", "languages", "use", "the", "open", "-", "source", "synthesiser", "eSpeak", "for", "their", "speech", ",", "which", "produces", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less common languages use the open-source synthesiser eSpeak for their speech, which produces a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 26], [27, 30], [31, 34], [35, 39], [39, 40], [40, 46], [47, 58], [59, 65], [66, 69], [70, 75], [76, 82], [82, 83], [84, 89], [90, 98], [99, 100], [101, 108], [108, 109], [110, 117], [118, 123], [124, 128], [129, 132], [133, 135], [136, 145], [146, 148], [149, 159], [159, 160]]}
{"doc_key": "ai-train-44", "ner": [[1, 1, "programlang"], [38, 39, "programlang"], [41, 41, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 38, 39, "compare", "", false, false], [1, 1, 41, 41, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "mainly", "used", "by", "statisticians", "and", "other", "practitioners", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "it", "can", "also", "be", "used", "as", "a", "general", "toolbox", "for", "matrix", "calculations", "-", "with", "performance", "comparisons", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is mainly used by statisticians and other practitioners who need an environment for statistical computing and software development, it can also be used as a general toolbox for matrix calculations - with performance comparisons to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 20], [21, 25], [26, 28], [29, 42], [43, 46], [47, 52], [53, 66], [67, 70], [71, 75], [76, 78], [79, 90], [91, 94], [95, 106], [107, 116], [117, 120], [121, 129], [130, 141], [141, 142], [143, 145], [146, 149], [150, 154], [155, 157], [158, 162], [163, 165], [166, 167], [168, 175], [176, 183], [184, 187], [188, 194], [195, 207], [208, 209], [210, 214], [215, 226], [227, 238], [239, 241], [242, 245], [246, 252], [253, 255], [256, 262], [262, 263]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [8, 11, "misc"], [12, 14, "researcher"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 0, 12, 14, "origin", "", false, false], [8, 11, 12, 14, "named", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", "in", "which", "new", "frequencies", "are", "created", "by", "combining", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden in which new frequencies are created by combining two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [76, 79], [80, 88], [89, 97], [98, 107], [108, 110], [111, 116], [117, 120], [121, 132], [133, 136], [137, 144], [145, 147], [148, 157], [158, 161], [162, 173], [173, 174]]}
{"doc_key": "ai-train-46", "ner": [[15, 15, "person"], [18, 18, "misc"], [22, 24, "organisation"], [29, 31, "misc"], [33, 34, "person"], [38, 40, "misc"], [42, 43, "person"], [45, 46, "person"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 7, 8, 9], "relations": [[15, 15, 18, 18, "role", "actor_in", false, false], [18, 18, 22, 24, "artifact", "", false, false], [33, 34, 29, 31, "role", "actor_in", false, false], [42, 43, 38, 40, "role", "actor_in", false, false], [45, 46, 38, 40, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 3, 5, 6], "sentence": ["Some", "other", "films", "that", "helped", "put", "3D", "back", "on", "the", "map", "this", "month", "were", "the", "John", "Wayne", "film", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Some other films that helped put 3D back on the map this month were the John Wayne film Hondo (distributed by Warner Bros. ), Columbia's Miss Sadie Thompson with Rita Hayworth and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 4], [5, 10], [11, 16], [17, 21], [22, 28], [29, 32], [33, 35], [36, 40], [41, 43], [44, 47], [48, 51], [52, 56], [57, 62], [63, 67], [68, 71], [72, 76], [77, 82], [83, 87], [88, 93], [94, 95], [95, 106], [107, 109], [110, 116], [117, 121], [121, 122], [123, 124], [124, 125], [126, 134], [134, 136], [137, 141], [142, 147], [148, 156], [157, 161], [162, 166], [167, 175], [176, 179], [180, 189], [189, 191], [192, 197], [198, 202], [203, 207], [208, 212], [213, 217], [218, 224], [225, 228], [229, 234], [235, 240], [240, 241]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [14, 14, "organisation"]], "ner_mapping_to_source": [0, 3], "relations": [[0, 0, 14, 14, "artifact", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["DeepFace", "is", "a", "deep", "learning", "face", "recognition", "system", "developed", "by", "a", "research", "group", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep learning face recognition system developed by a research group at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 32], [33, 44], [45, 51], [52, 61], [62, 64], [65, 66], [67, 75], [76, 81], [82, 84], [85, 93], [93, 94]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [15, 16, "field"], [25, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [25, 27, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "frequent", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "theme", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a frequent research topic at SIGGRAPH, the leading academic conference on computer graphics, and the main theme of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 33], [34, 42], [43, 48], [49, 51], [52, 60], [60, 61], [62, 65], [66, 73], [74, 82], [83, 93], [94, 96], [97, 105], [106, 114], [114, 115], [116, 119], [120, 123], [124, 128], [129, 134], [135, 137], [138, 141], [142, 148], [149, 157], [158, 168], [169, 178], [178, 179]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [35, 36, "misc"], [42, 44, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 15, 35, 36, "general-affiliation", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 35, 36, "general-affiliation", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 35, 36, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "one", "step", "by", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "as", "a", "preprocessing", "step", ",", "followed", "by", "clustering", "by", "k", "-", "NN", "on", "the", "feature", "vectors", "in", "the", "dimension", "reduced", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in one step by using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) as a preprocessing step, followed by clustering by k -NN on the feature vectors in the dimension reduced space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 65], [66, 70], [71, 73], [74, 79], [80, 89], [90, 99], [100, 108], [109, 110], [110, 113], [113, 114], [114, 115], [116, 122], [123, 135], [136, 144], [145, 146], [146, 149], [149, 150], [151, 153], [154, 163], [164, 175], [176, 184], [185, 186], [186, 189], [189, 190], [191, 193], [194, 195], [196, 209], [210, 214], [214, 215], [216, 224], [225, 227], [228, 238], [239, 241], [242, 243], [244, 245], [245, 247], [248, 250], [251, 254], [255, 262], [263, 270], [271, 273], [274, 277], [278, 287], [288, 295], [296, 301], [301, 302]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [8, 9, "field"], [11, 12, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 8, 9, "related-to", "good_at", true, false], [0, 2, 11, 12, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "characterised", "by", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models characterised by machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 65], [66, 68], [69, 76], [77, 85], [86, 89], [90, 97], [98, 109], [109, 110]]}
{"doc_key": "ai-train-51", "ner": [[0, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [36, 39, "algorithm"], [40, 41, "researcher"], [43, 45, "researcher"], [47, 53, "misc"], [55, 64, "conference"], [66, 66, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 0, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [47, 53, 36, 39, "topic", "", false, false], [47, 53, 40, 41, "artifact", "", false, false], [47, 53, 43, 45, "artifact", "", false, false], [47, 53, 55, 64, "temporal", "", false, false], [66, 66, 55, 64, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["C", ".", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "Histogram", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": "C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as Histogram of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [1, 2], [3, 15], [16, 19], [20, 22], [23, 29], [29, 30], [31, 32], [33, 42], [43, 53], [54, 63], [64, 70], [70, 71], [72, 85], [86, 93], [94, 96], [97, 105], [106, 112], [113, 114], [114, 118], [118, 119], [119, 120], [121, 126], [127, 128], [128, 129], [130, 132], [132, 133], [133, 135], [135, 136], [137, 141], [142, 148], [149, 152], [153, 158], [159, 167], [168, 172], [173, 175], [176, 185], [186, 188], [189, 197], [198, 207], [208, 210], [211, 216], [216, 217], [218, 219], [219, 220], [221, 227], [227, 228], [229, 239], [240, 242], [243, 251], [252, 261], [262, 265], [266, 271], [272, 281], [281, 282], [283, 287], [288, 296], [297, 304], [305, 315], [316, 318], [319, 327], [328, 334], [335, 338], [339, 346], [347, 358], [359, 360], [360, 364], [364, 365], [365, 366], [367, 372], [373, 374], [374, 375], [376, 383], [383, 384], [385, 389], [390, 401], [401, 402]]}
{"doc_key": "ai-train-52", "ner": [[1, 1, "algorithm"], [6, 8, "algorithm"], [12, 12, "task"], [11, 11, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 1, 6, 8, "type-of", "", false, false], [12, 12, 1, 1, "usage", "", true, false], [12, 12, 11, 11, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "for", "unsupervised", "feature", "learning", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network used for unsupervised feature learning.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 62], [63, 75], [76, 83], [84, 92], [92, 93]]}
{"doc_key": "ai-train-53", "ner": [[0, 0, "researcher"], [6, 7, "organisation"], [11, 12, "field"], [14, 17, "field"], [21, 25, "organisation"], [27, 27, "organisation"], [33, 34, "field"], [36, 37, "field"], [44, 44, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 0, 6, 7, "role", "fellow_of", false, false], [0, 0, 11, 12, "related-to", "contributes_to", false, false], [0, 0, 14, 17, "related-to", "contributes_to", false, false], [0, 0, 21, 25, "role", "fellow_of", false, false], [0, 0, 33, 34, "related-to", "contributes_to", false, false], [0, 0, 36, 37, "related-to", "contributes_to", false, false], [27, 27, 21, 25, "named", "", false, false], [44, 44, 21, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "the", "IEEE", "for", "his", "contributions", "in", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "in", "pattern", "recognition", "and", "image", "processing", "and", "for", "his", "service", "to", "the", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of the IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition and image processing and for his service to the IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 40], [41, 54], [55, 57], [58, 66], [67, 73], [74, 77], [78, 83], [84, 94], [95, 98], [99, 100], [101, 107], [108, 110], [111, 114], [115, 128], [129, 140], [141, 144], [145, 152], [153, 164], [165, 166], [166, 170], [170, 171], [172, 175], [176, 179], [180, 193], [194, 196], [197, 204], [205, 216], [217, 220], [221, 226], [227, 237], [238, 241], [242, 245], [246, 249], [250, 257], [258, 260], [261, 264], [265, 269], [269, 270]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [12, 14, "algorithm"], [16, 18, "algorithm"], [22, 23, "researcher"], [25, 26, "organisation"], [28, 30, "researcher"], [32, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 9, 12, 14, "usage", "", false, false], [12, 14, 22, 23, "origin", "", true, false], [12, 14, 28, 30, "origin", "", true, false], [16, 18, 12, 14, "named", "", false, false], [22, 23, 25, 26, "physical", "", false, false], [22, 23, 25, 26, "role", "", false, false], [28, 30, 32, 36, "physical", "", false, false], [28, 30, 32, 36, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "the", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "based", "system", "presented", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was the Connectionist Temporal Classification (CTC) based system presented by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 43], [44, 57], [58, 66], [67, 81], [82, 83], [83, 86], [86, 87], [88, 93], [94, 100], [101, 110], [111, 113], [114, 118], [119, 125], [126, 128], [129, 135], [136, 144], [145, 148], [149, 156], [157, 163], [164, 166], [167, 170], [171, 181], [182, 184], [185, 192], [193, 195], [196, 200], [200, 201]]}
{"doc_key": "ai-train-55", "ner": [[0, 1, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 1, "named", "", false, false], [10, 11, 0, 1, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear fractional programming (LFP) is a generalisation of linear programming (LP).", "token2charspan": [[0, 6], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 1, "researcher"], [8, 11, "misc"], [14, 22, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 8, 11, "win-defeat", "", false, false], [8, 11, 14, 22, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "of", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "in", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test of Time awards at the International Conference on Machine Learning in 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [58, 60], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 104], [105, 107], [108, 115], [116, 124], [125, 127], [128, 132], [133, 136], [137, 141], [141, 142]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [25, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "deploy", "the", "developed", "neural", "network", "in", "these", "frameworks", "as", "inheritable", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to deploy the developed neural network in these frameworks as inheritable components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 128], [129, 132], [133, 142], [143, 149], [150, 157], [158, 160], [161, 166], [167, 177], [178, 180], [181, 192], [193, 203], [203, 204]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ".", "The", "algorithm", "first", "creates", "an", "alignment", "(", "see", "figures", ")", "between", "two", "sentences", ",", "the", "candidate", "translation", "and", "the", "reference", "translation", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence. The algorithm first creates an alignment (see figures) between two sentences, the candidate translation and the reference translation.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 73], [74, 79], [80, 87], [88, 90], [91, 100], [101, 102], [102, 105], [106, 113], [113, 114], [115, 122], [123, 126], [127, 136], [136, 137], [138, 141], [142, 151], [152, 163], [164, 167], [168, 171], [172, 181], [182, 193], [193, 194]]}
{"doc_key": "ai-train-59", "ner": [[6, 13, "conference"], [21, 21, "task"], [23, 24, "task"], [28, 29, "metrics"], [31, 37, "metrics"], [42, 45, "conference"], [47, 47, "conference"], [50, 50, "location"], [52, 52, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 13, 21, 21, "related-to", "subject_at", false, false], [6, 13, 23, 24, "related-to", "subject_at", false, false], [28, 29, 6, 13, "temporal", "", false, false], [31, 37, 28, 29, "named", "", true, false], [47, 47, 42, 45, "named", "", false, false], [50, 50, 52, 52, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "NIST", "'s", "annual", "Document", "Understanding", "Conferences", ",", "where", "research", "groups", "submit", "their", "systems", "for", "both", "summary", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "2014", "."], "sentence-detokenized": "One of the metrics used at NIST's annual Document Understanding Conferences, where research groups submit their systems for both summary and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 31], [31, 33], [34, 40], [41, 49], [50, 63], [64, 75], [75, 76], [77, 82], [83, 91], [92, 98], [99, 105], [106, 111], [112, 119], [120, 123], [124, 128], [129, 136], [137, 140], [141, 152], [153, 158], [158, 159], [160, 162], [163, 166], [167, 172], [173, 179], [180, 181], [181, 187], [187, 188], [188, 196], [197, 207], [208, 211], [212, 219], [220, 230], [230, 231], [232, 234], [235, 243], [244, 246], [247, 253], [254, 265], [266, 276], [277, 284], [285, 286], [286, 290], [290, 291], [291, 292], [293, 301], [301, 302], [303, 309], [309, 310], [311, 319], [320, 324], [324, 325]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [12, 13, "programlang"], [16, 16, "product"], [22, 22, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 12, 13, "type-of", "", false, false], [6, 6, 22, 22, "named", "", false, false], [8, 8, 12, 13, "part-of", "", false, false], [8, 8, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "for", "execution", "in", "Java", "with", "JShell", "(", "at", "least", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, for execution in Java with JShell (at least Java 9): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 24], [25, 34], [35, 37], [38, 42], [43, 47], [48, 54], [55, 56], [56, 58], [59, 64], [65, 69], [70, 71], [71, 72], [72, 73], [74, 84], [85, 95], [96, 97], [98, 117], [118, 122], [123, 124], [125, 129]]}
{"doc_key": "ai-train-61", "ner": [[0, 3, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 3, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "method", "is", "based", "on", "the", "BLEU", "method", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST method is based on the BLEU method, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 32, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 32, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", "but", "where", "edges", "are", "restricted", "to", "a", "limited", "number", "of", "possible", "relations", "to", "facilitate", "algebras", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called Knowledge Graphs, which are semantic networks but where edges are restricted to a limited number of possible relations to facilitate algebras on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 116], [117, 118], [119, 126], [127, 133], [134, 143], [144, 150], [150, 151], [152, 157], [158, 161], [162, 170], [171, 179], [180, 183], [184, 189], [190, 195], [196, 199], [200, 210], [211, 213], [214, 215], [216, 223], [224, 230], [231, 233], [234, 242], [243, 252], [253, 255], [256, 266], [267, 275], [276, 278], [279, 282], [283, 288], [288, 289]]}
{"doc_key": "ai-train-63", "ner": [[0, 1, "product"], [14, 15, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 14, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "usually", "implemented", "as", "part", "of", "a", "larger", "program", ",", "e.g.", "a", "word", "processor", ",", "but", "are", "also", "available", "as", "a", "stand", "-", "alone", "application", "that", "can", "be", "activated", "from", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are usually implemented as part of a larger program, e.g. a word processor, but are also available as a stand-alone application that can be activated from programs that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 28], [29, 40], [41, 43], [44, 48], [49, 51], [52, 53], [54, 60], [61, 68], [68, 69], [70, 74], [75, 76], [77, 81], [82, 91], [91, 92], [93, 96], [97, 100], [101, 105], [106, 115], [116, 118], [119, 120], [121, 126], [126, 127], [127, 132], [133, 144], [145, 149], [150, 153], [154, 156], [157, 166], [167, 171], [172, 180], [181, 185], [186, 190], [191, 195], [196, 204], [205, 209], [209, 210]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 20, "conference"], [24, 29, "organisation"], [34, 36, "conference"], [38, 39, "conference"], [43, 45, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "Fellow", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "an", "editor", "of", "the", "journals", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a Fellow of the American Association for the Advancement of Science, the Association for the Advancement Artificial Intelligence, and the Cognitive Science Society, and an editor of the journals J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 121], [122, 134], [134, 135], [136, 139], [140, 143], [144, 153], [154, 161], [162, 169], [169, 170], [171, 174], [175, 177], [178, 184], [185, 187], [188, 191], [192, 200], [201, 203], [204, 213], [214, 223], [223, 224], [225, 227], [228, 236], [237, 245], [245, 246], [247, 250], [251, 253], [254, 261], [262, 270], [270, 271]]}
{"doc_key": "ai-train-65", "ner": [[3, 5, "algorithm"], [7, 7, "algorithm"], [13, 14, "task"], [23, 25, "researcher"], [26, 27, "university"], [29, 31, "researcher"], [32, 34, "organisation"], [37, 37, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[3, 5, 13, 14, "type-of", "", false, false], [3, 5, 23, 25, "origin", "", false, false], [3, 5, 29, 31, "origin", "", false, false], [7, 7, 3, 5, "named", "", false, false], [23, 25, 26, 27, "physical", "", false, false], [23, 25, 26, 27, "role", "", false, false], [29, 31, 32, 34, "role", "", false, false], [37, 37, 32, 34, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "development", "of", "linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "in", "1966", "with", "the", "work", "of", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "."], "sentence-detokenized": "The development of linear predictive coding (LPC), a form of speech coding, began in 1966 with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT).", "token2charspan": [[0, 3], [4, 15], [16, 18], [19, 25], [26, 36], [37, 43], [44, 45], [45, 48], [48, 49], [49, 50], [51, 52], [53, 57], [58, 60], [61, 67], [68, 74], [74, 75], [76, 81], [82, 84], [85, 89], [90, 94], [95, 98], [99, 103], [104, 106], [107, 115], [116, 123], [124, 126], [127, 133], [134, 144], [145, 148], [149, 154], [155, 160], [161, 163], [164, 170], [171, 180], [181, 184], [185, 194], [195, 196], [196, 199], [199, 200], [200, 201]]}
{"doc_key": "ai-train-66", "ner": [[57, 60, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "still", "ergodic", ",", "all", "sample", "paths", "have", "the", "same", "time", "average", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "the", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is still ergodic, all sample paths have the same time average and thus mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the sense of the mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 22], [23, 30], [30, 31], [32, 35], [36, 42], [43, 48], [49, 53], [54, 57], [58, 62], [63, 67], [68, 75], [76, 79], [80, 84], [85, 90], [91, 92], [93, 94], [95, 96], [97, 98], [98, 99], [100, 101], [102, 103], [104, 105], [106, 107], [107, 108], [109, 110], [110, 111], [112, 115], [115, 116], [117, 119], [120, 127], [128, 129], [129, 130], [130, 131], [132, 133], [134, 135], [136, 137], [138, 139], [139, 140], [141, 142], [143, 144], [145, 146], [147, 148], [148, 149], [150, 151], [151, 152], [153, 156], [156, 157], [158, 159], [160, 164], [165, 167], [168, 171], [172, 177], [178, 180], [181, 184], [185, 189], [190, 196], [197, 202], [202, 203]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [34, 36, "algorithm"], [38, 38, "algorithm"], [42, 43, "misc"], [49, 51, "algorithm"], [53, 54, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[13, 15, 42, 43, "related-to", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 42, 43, "related-to", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 42, 43, "related-to", "", false, false], [31, 31, 27, 29, "named", "", false, false], [34, 36, 42, 43, "related-to", "", false, false], [38, 38, 34, 36, "named", "", false, false], [49, 51, 53, 54, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "one", "step", "by", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorisation", "(", "NMF", ")", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "by", "K", "-", "NN", "on", "feature", "vectors", "in", "the", "reduced", "dimension", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in one step by using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorisation (NMF) as a pre-processing step, followed by clustering by K-NN on feature vectors in the reduced dimension space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 65], [66, 70], [71, 73], [74, 79], [80, 89], [90, 99], [100, 108], [109, 110], [110, 113], [113, 114], [114, 115], [116, 122], [123, 135], [136, 144], [145, 146], [146, 149], [149, 150], [150, 151], [152, 161], [162, 173], [174, 182], [183, 184], [184, 187], [187, 188], [189, 191], [192, 204], [205, 211], [212, 225], [226, 227], [227, 230], [230, 231], [232, 234], [235, 236], [237, 251], [252, 256], [256, 257], [258, 266], [267, 269], [270, 280], [281, 283], [284, 285], [285, 286], [286, 288], [289, 291], [292, 299], [300, 307], [308, 310], [311, 314], [315, 322], [323, 332], [333, 338], [338, 339]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [11, 13, "task"], [31, 32, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 11, 13, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognising", "named", "entities", "in", "the", "text", "is", "called", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognising named entities in the text is called Named Entity Recognition, while the task of determining the identity of named entities mentioned in the text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 60], [61, 66], [67, 73], [74, 85], [85, 86], [87, 92], [93, 96], [97, 101], [102, 104], [105, 116], [117, 120], [121, 129], [130, 132], [133, 138], [139, 147], [148, 157], [158, 160], [161, 164], [165, 169], [170, 172], [173, 179], [180, 186], [187, 194], [194, 195]]}
{"doc_key": "ai-train-70", "ner": [[29, 29, "algorithm"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "functions", "and", "derivations", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ".", "As", "of", "version", "0.8.0", ",", "they", "were", "published", "in", "a", "separate", "R", "package", "sigmoid", "to", "allow", "for", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivations used in the package were originally included in the package. As of version 0.8.0, they were published in a separate R package sigmoid to allow for more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 101], [102, 104], [105, 112], [113, 118], [118, 119], [120, 124], [125, 129], [130, 139], [140, 142], [143, 144], [145, 153], [154, 155], [156, 163], [164, 171], [172, 174], [175, 180], [181, 184], [185, 189], [190, 197], [198, 201], [201, 202]]}
{"doc_key": "ai-train-71", "ner": [[0, 0, "programlang"], [11, 19, "organisation"], [21, 21, "organisation"], [28, 28, "location"], [30, 30, "location"], [6, 7, "researcher"], [9, 10, "researcher"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 0, 6, 7, "artifact", "", true, false], [0, 0, 9, 10, "artifact", "", true, false], [0, 0, 12, 13, "artifact", "", true, false], [21, 21, 11, 19, "named", "", false, false], [21, 21, 28, 28, "physical", "", false, false], [28, 28, 30, 30, "physical", "", false, false], [6, 7, 11, 19, "role", "", false, false], [9, 10, 11, 19, "role", "", false, false], [12, 13, 11, 19, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Logo", "was", "developed", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "research", "firm", "in", "Cambridge", ",", "Massachusetts", "."], "sentence-detokenized": "Logo was developed in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert at Bolt, Beranek and Newman (BBN), a research firm in Cambridge, Massachusetts.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 26], [27, 29], [30, 35], [36, 44], [44, 45], [46, 53], [54, 61], [62, 65], [66, 73], [74, 80], [81, 83], [84, 88], [88, 89], [90, 97], [98, 101], [102, 108], [109, 110], [110, 113], [113, 114], [114, 115], [116, 117], [118, 126], [127, 131], [132, 134], [135, 144], [144, 145], [146, 159], [159, 160]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [22, 24, "algorithm"], [26, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Neuroevolution", "is", "often", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "compared", "to", "conventional", "deep", "learning", "techniques", "that", "apply", "gradient", "descent", "to", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is often used as part of the reinforcement learning paradigm and can be compared to conventional deep learning techniques that apply gradient descent to a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 23], [24, 28], [29, 31], [32, 36], [37, 39], [40, 43], [44, 57], [58, 66], [67, 75], [76, 79], [80, 83], [84, 86], [87, 95], [96, 98], [99, 111], [112, 116], [117, 125], [126, 136], [137, 141], [142, 147], [148, 156], [157, 164], [165, 167], [168, 169], [170, 176], [177, 184], [185, 189], [190, 191], [192, 197], [198, 206], [206, 207]]}
{"doc_key": "ai-train-73", "ner": [[3, 5, "algorithm"], [50, 52, "metrics"], [54, 54, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[54, 54, 50, 52, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["When", "we", "use", "least", "squares", "to", "fit", "a", "hyperplane", "function", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "we", "can", "evaluate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "When we use least squares to fit a hyperplane function \u0177 = a + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264n / sub, we can evaluate the fit using the mean squared error (MSE).", "token2charspan": [[0, 4], [5, 7], [8, 11], [12, 17], [18, 25], [26, 28], [29, 32], [33, 34], [35, 45], [46, 54], [55, 56], [57, 58], [59, 60], [61, 62], [63, 64], [65, 69], [70, 71], [72, 75], [76, 77], [78, 80], [81, 84], [85, 89], [90, 91], [91, 92], [93, 96], [97, 98], [99, 100], [101, 104], [104, 105], [106, 107], [108, 111], [112, 113], [114, 115], [116, 119], [119, 120], [121, 124], [125, 126], [127, 128], [129, 130], [131, 133], [134, 135], [136, 139], [139, 140], [141, 143], [144, 147], [148, 156], [157, 160], [161, 164], [165, 170], [171, 174], [175, 179], [180, 187], [188, 193], [194, 195], [195, 198], [198, 199], [199, 200]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [31, 31, "country"], [33, 33, "country"], [35, 35, "country"], [37, 38, "country"], [40, 40, "country"], [42, 42, "country"], [44, 44, "country"], [46, 46, "country"], [49, 50, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "locations", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "the", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international locations in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, the Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 39], [40, 42], [43, 52], [52, 53], [54, 60], [60, 61], [62, 68], [68, 69], [70, 75], [75, 76], [77, 84], [84, 85], [86, 91], [91, 92], [93, 98], [98, 99], [100, 105], [105, 106], [107, 112], [112, 113], [114, 123], [123, 124], [125, 131], [131, 132], [133, 141], [141, 142], [143, 146], [147, 158], [158, 159], [160, 166], [166, 167], [168, 177], [177, 178], [179, 184], [185, 191], [191, 192], [193, 198], [198, 199], [200, 206], [206, 207], [208, 216], [216, 217], [218, 224], [225, 228], [229, 232], [233, 239], [240, 247], [247, 248]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 8, "field"], [13, 13, "organisation"], [16, 23, "university"], [28, 30, "organisation"], [32, 35, "university"], [40, 41, "university"], [43, 44, "university"], [47, 49, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 8, "topic", "", false, false], [3, 3, 13, 13, "origin", "", false, false], [3, 3, 16, 23, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", ".", "He", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering (2000) from Inria and the University of Nice Sophia Antipolis. He has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech and visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 90], [91, 93], [94, 98], [99, 105], [106, 115], [115, 116], [117, 119], [120, 123], [124, 128], [129, 138], [139, 148], [149, 151], [152, 159], [160, 169], [170, 180], [180, 181], [182, 187], [188, 191], [192, 197], [198, 207], [208, 211], [212, 220], [221, 230], [231, 233], [234, 241], [242, 252], [252, 253], [254, 258], [259, 269], [270, 273], [274, 277], [278, 288], [289, 291], [292, 299], [299, 300]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [0, 0, "researcher"], [13, 14, "product"], [17, 18, "country"], [21, 21, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 8, "role", "licensing_patent_to", false, false], [0, 0, 17, 18, "physical", "", false, false], [21, 21, 0, 0, "artifact", "", false, false], [21, 21, 13, 14, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberger", "licensed", "the", "original", "patent", "from", "inventor", "George", "Devol", "and", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "the", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Engelberger licensed the original patent from inventor George Devol and developed the first industrial robot in the United States, the Unimate, in the 1950s.", "token2charspan": [[0, 11], [12, 20], [21, 24], [25, 33], [34, 40], [41, 45], [46, 54], [55, 61], [62, 67], [68, 71], [72, 81], [82, 85], [86, 91], [92, 102], [103, 108], [109, 111], [112, 115], [116, 122], [123, 129], [129, 130], [131, 134], [135, 142], [142, 143], [144, 146], [147, 150], [151, 156], [156, 157]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [3, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[7, 7, "programlang"], [15, 15, "programlang"], [19, 19, "programlang"], [29, 29, "programlang"]], "ner_mapping_to_source": [1, 2, 3, 4], "relations": [[7, 7, 19, 19, "general-affiliation", "", false, false], [7, 7, 29, 29, "named", "", false, false]], "relations_mapping_to_source": [2, 3], "sentence": ["The", "descendants", "of", "the", "CLIPS", "language", "include", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "was", "rewritten", "in", "Java", "and", "later", "evolved", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by"], "sentence-detokenized": "The descendants of the CLIPS language include Jess (the rule-based part of CLIPS was rewritten in Java and later evolved in a different direction), JESS was originally inspired by", "token2charspan": [[0, 3], [4, 15], [16, 18], [19, 22], [23, 28], [29, 37], [38, 45], [46, 50], [51, 52], [52, 55], [56, 60], [60, 61], [61, 66], [67, 71], [72, 74], [75, 80], [81, 84], [85, 94], [95, 97], [98, 102], [103, 106], [107, 112], [113, 120], [121, 123], [124, 125], [126, 135], [136, 145], [145, 146], [146, 147], [148, 152], [153, 156], [157, 167], [168, 176], [177, 179]]}
{"doc_key": "ai-train-79", "ner": [[12, 14, "product"], [17, 18, "organisation"], [22, 23, "product"], [45, 46, "product"], [48, 50, "product"], [67, 68, "misc"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6], "relations": [[17, 18, 12, 14, "usage", "", false, false], [22, 23, 17, 18, "artifact", "", false, false], [45, 46, 17, 18, "origin", "", true, false], [45, 46, 67, 68, "related-to", "", true, false], [48, 50, 17, 18, "origin", "", true, false], [48, 50, 67, 68, "related-to", "", true, false]], "relations_mapping_to_source": [1, 2, 3, 4, 5, 6], "sentence": ["The", "company", "has", "also", "developed", "flexible", "intelligent", "AGV", "applications", "and", "designed", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", ",", "which", "is", "used", "for", "complex", "pick", "-", "and", "-", "place", "operations", "in", "conjunction", "with", "gantry", "systems", "and", "industrial", "robot", "arms", "used", "in", "world", "-", "class", "automotive", "supply", "operations", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "The company has also developed flexible intelligent AGV applications and designed the Motivity control system used by RMT Robotics to develop the ADAM iAGV (Self-Guided Vehicle), which is used for complex pick-and-place operations in conjunction with gantry systems and industrial robot arms used in world-class automotive supply operations to move products from process to process in non-linear layouts.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 20], [21, 30], [31, 39], [40, 51], [52, 55], [56, 68], [69, 72], [73, 81], [82, 85], [86, 94], [95, 102], [103, 109], [110, 114], [115, 117], [118, 121], [122, 130], [131, 133], [134, 141], [142, 145], [146, 150], [151, 155], [156, 157], [157, 161], [161, 162], [162, 168], [169, 176], [176, 177], [177, 178], [179, 184], [185, 187], [188, 192], [193, 196], [197, 204], [205, 209], [209, 210], [210, 213], [213, 214], [214, 219], [220, 230], [231, 233], [234, 245], [246, 250], [251, 257], [258, 265], [266, 269], [270, 280], [281, 286], [287, 291], [292, 296], [297, 299], [300, 305], [305, 306], [306, 311], [312, 322], [323, 329], [330, 340], [341, 343], [344, 348], [349, 357], [358, 362], [363, 370], [371, 373], [374, 381], [382, 384], [385, 395], [396, 403], [403, 404]]}
{"doc_key": "ai-train-80", "ner": [[9, 11, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameters", "\u03b2", "are", "usually", "estimated", "according", "to", "the", "maximum", "likelihood", "method", "."], "sentence-detokenized": "The parameters \u03b2 are usually estimated according to the maximum likelihood method.", "token2charspan": [[0, 3], [4, 14], [15, 16], [17, 20], [21, 28], [29, 38], [39, 48], [49, 51], [52, 55], [56, 63], [64, 74], [75, 81], [81, 82]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [5, 5, "metrics"], [7, 7, "metrics"], [9, 9, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 0, 1, "part-of", "", false, false], [7, 7, 0, 1, "part-of", "", false, false], [9, 9, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", "such", "as", "Precision", "and", "Recall", "or", "DCG", "are", "useful", "to", "assess", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics such as Precision and Recall or DCG are useful to assess the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [30, 34], [35, 37], [38, 47], [48, 51], [52, 58], [59, 61], [62, 65], [66, 69], [70, 76], [77, 79], [80, 86], [87, 90], [91, 98], [99, 101], [102, 103], [104, 118], [119, 125], [125, 126]]}
{"doc_key": "ai-train-82", "ner": [[7, 8, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "hundreds", "of", "industrial", "robots", "work", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "In a typical factory, hundreds of industrial robots work on fully automated production lines, with one robot for every ten human workers.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 30], [31, 33], [34, 44], [45, 51], [52, 56], [57, 59], [60, 65], [66, 75], [76, 86], [87, 92], [92, 93], [94, 98], [99, 102], [103, 108], [109, 112], [113, 118], [119, 122], [123, 128], [129, 136], [136, 137]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [30, 31, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7], "relations": [], "relations_mapping_to_source": [], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", "including", ":", "Image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growing", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications including: Image segmentation, feature generation, face extraction, motion detection, region growing and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [89, 98], [98, 99], [100, 105], [106, 118], [118, 119], [120, 127], [128, 138], [138, 139], [140, 144], [145, 155], [155, 156], [157, 163], [164, 173], [173, 174], [175, 181], [182, 189], [190, 193], [194, 199], [200, 209], [209, 210]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 18, "field"], [21, 23, "misc"], [26, 31, "conference"], [33, 33, "conference"], [38, 40, "misc"], [43, 49, "conference"], [50, 51, "conference"], [53, 57, "conference"], [59, 59, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 16, 18, "related-to", "contributes_to", false, false], [0, 0, 21, 23, "win-defeat", "", false, false], [0, 0, 38, 40, "win-defeat", "", false, false], [21, 23, 26, 31, "temporal", "", false, false], [33, 33, 26, 31, "named", "", false, false], [38, 40, 43, 49, "temporal", "", false, false], [38, 40, 53, 57, "temporal", "", false, false], [50, 51, 43, 49, "named", "", false, false], [59, 59, 53, 57, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "international", "conference", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "and", "the", "Best", "Reviewer", "Award", "at", "the", "international", "conferences", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision and won the Best Paper Award at the international conference Non-Photorealistic Rendering and Animation (NPAR) 2012 and the Best Reviewer Award at the international conferences Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 160], [161, 171], [172, 190], [191, 200], [201, 204], [205, 214], [215, 216], [216, 220], [220, 221], [222, 226], [227, 230], [231, 234], [235, 239], [240, 248], [249, 254], [255, 257], [258, 261], [262, 275], [276, 287], [288, 293], [294, 304], [305, 307], [308, 316], [317, 323], [324, 328], [329, 333], [334, 337], [338, 351], [352, 362], [363, 365], [366, 374], [375, 381], [382, 383], [383, 387], [387, 388], [389, 393], [393, 394]]}
{"doc_key": "ai-train-85", "ner": [[7, 8, "programlang"], [1, 2, "field"], [4, 5, "field"], [10, 11, "misc"], [14, 14, "researcher"], [17, 19, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[7, 8, 1, 2, "part-of", "", false, false], [7, 8, 4, 5, "part-of", "", false, false], [7, 8, 10, 11, "type-of", "", false, false], [17, 19, 7, 8, "usage", "", false, false], [17, 19, 14, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "computer", "science", "and", "artificial", "intelligence", ",", "CycL", "is", "an", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "In computer science and artificial intelligence, CycL is an ontology language used by Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 2], [3, 11], [12, 19], [20, 23], [24, 34], [35, 47], [47, 48], [49, 53], [54, 56], [57, 59], [60, 68], [69, 77], [78, 82], [83, 85], [86, 90], [91, 96], [96, 98], [99, 102], [103, 113], [114, 121], [121, 122]]}
{"doc_key": "ai-train-86", "ner": [[2, 3, "task"], [5, 7, "metrics"], [13, 16, "metrics"], [10, 25, "metrics"], [34, 36, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "part-of", "", false, false], [13, 16, 5, 7, "named", "", false, false], [10, 25, 5, 7, "named", "", false, false], [34, 36, 5, 7, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "mean", "squared", "error", ",", "often", "referred", "to", "as", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "mean", "of", "the", "squared", "deviations", "of", "the", "predictions", "from", "the", "TRUE", "values", "over", "an", "out", "-", "of", "-", "sample", "test", "space", "generated", "by", "a", "model", "estimated", "over", "a", "given", "sample", "space", "."], "sentence-detokenized": "Also in regression analysis, mean squared error, often referred to as mean squared prediction error or out-of-sample mean squared error, can refer to the mean of the squared deviations of the predictions from the TRUE values over an out-of-sample test space generated by a model estimated over a given sample space.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 33], [34, 41], [42, 47], [47, 48], [49, 54], [55, 63], [64, 66], [67, 69], [70, 74], [75, 82], [83, 93], [94, 99], [100, 102], [103, 106], [106, 107], [107, 109], [109, 110], [110, 116], [117, 121], [122, 129], [130, 135], [135, 136], [137, 140], [141, 146], [147, 149], [150, 153], [154, 158], [159, 161], [162, 165], [166, 173], [174, 184], [185, 187], [188, 191], [192, 203], [204, 208], [209, 212], [213, 217], [218, 224], [225, 229], [230, 232], [233, 236], [236, 237], [237, 239], [239, 240], [240, 246], [247, 251], [252, 257], [258, 267], [268, 270], [271, 272], [273, 278], [279, 288], [289, 293], [294, 295], [296, 301], [302, 308], [309, 314], [314, 315]]}
{"doc_key": "ai-train-87", "ner": [[18, 22, "algorithm"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "results", "show", "that", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptors", "having", "a", "slight", "advantage", "in", "error", "detection", "rate", "at", "fixed", "FALSE", "positive", "rates", "in", "both", "datasets", "."], "sentence-detokenized": "The results show that the C-HOG and R-HOG block descriptors perform comparably, with the C-HOG descriptors having a slight advantage in error detection rate at fixed FALSE positive rates in both datasets.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 21], [22, 25], [26, 27], [27, 28], [28, 31], [32, 35], [36, 38], [38, 41], [42, 47], [48, 59], [60, 67], [68, 78], [78, 79], [80, 84], [85, 88], [89, 90], [90, 91], [91, 94], [95, 106], [107, 113], [114, 115], [116, 122], [123, 132], [133, 135], [136, 141], [142, 151], [152, 156], [157, 159], [160, 165], [166, 171], [172, 180], [181, 186], [187, 189], [190, 194], [195, 203], [203, 204]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 16, "algorithm"], [17, 18, "algorithm"], [20, 22, "algorithm"], [24, 26, "algorithm"], [28, 29, "misc"], [34, 36, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 28, 29, "usage", "", false, false], [14, 16, 28, 29, "usage", "", false, false], [17, 18, 28, 29, "usage", "", false, false], [20, 22, 28, 29, "usage", "", false, false], [24, 26, 28, 29, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "are", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "Fisherface", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neuronally", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms are principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using Fisherface algorithm, hidden Markov model, multilinear subspace learning using tensor representation, and neuronally motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 34], [35, 44], [45, 54], [55, 63], [64, 69], [70, 80], [80, 81], [82, 88], [89, 101], [102, 110], [110, 111], [112, 119], [120, 128], [129, 134], [135, 145], [146, 155], [155, 156], [157, 163], [164, 170], [171, 176], [176, 177], [178, 189], [190, 198], [199, 207], [208, 213], [214, 220], [221, 235], [235, 236], [237, 240], [241, 251], [252, 261], [262, 269], [270, 274], [275, 283], [283, 284]]}
{"doc_key": "ai-train-89", "ner": [[3, 7, "misc"], [19, 22, "location"], [39, 41, "location"], [56, 56, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 22, 3, 7, "temporal", "", false, false], [39, 41, 3, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "will", "no", "longer", "be", "allowed", "to", "be", "screened", "at", "Toronto", "'s", "Scotiabank", "Theatre", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "but", "only", "at", "other", "theatres", "(", "e.g.", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "through", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films will no longer be allowed to be screened at Toronto's Scotiabank Theatre - one of the festival's main venues - but only at other theatres (e.g. TIFF Bell Lightbox and other local cinemas) if they are distributed through a service such as Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 70], [71, 73], [74, 80], [81, 83], [84, 91], [92, 94], [95, 97], [98, 106], [107, 109], [110, 117], [117, 119], [120, 130], [131, 138], [139, 140], [141, 144], [145, 147], [148, 151], [152, 160], [160, 162], [163, 167], [168, 174], [175, 176], [177, 180], [181, 185], [186, 188], [189, 194], [195, 203], [204, 205], [205, 209], [210, 214], [215, 219], [220, 228], [229, 232], [233, 238], [239, 244], [245, 252], [252, 253], [254, 256], [257, 261], [262, 265], [266, 277], [278, 285], [286, 287], [288, 295], [296, 300], [301, 303], [304, 311], [311, 312]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [5, 7, "researcher"], [2, 3, "organisation"], [21, 25, "product"], [36, 37, "researcher"], [39, 41, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 6], "relations": [[0, 0, 2, 3, "related-to", "purchases", false, false], [5, 7, 36, 37, "named", "same", false, false], [2, 3, 5, 7, "origin", "founded_by", false, false], [21, 25, 0, 0, "artifact", "", false, false], [39, 41, 36, 37, "artifact", "", true, false]], "relations_mapping_to_source": [0, 2, 3, 4, 5], "sentence": ["Unimation", "bought", "Vicarm", "Inc.", "from", "Victor", "Scheinman", "in", "1977", ".", "With", "Scheinman", "'s", "help", ",", "the", "company", "developed", "and", "produced", "the", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "a", "new", "model", "of", "robotic", "arm", ",", "and", "used", "Scheinman", "'s", "innovative", "programming", "language", "VAL", "."], "sentence-detokenized": "Unimation bought Vicarm Inc. from Victor Scheinman in 1977. With Scheinman's help, the company developed and produced the Programmable Universal Machine for Assembly, a new model of robotic arm, and used Scheinman's innovative programming language VAL.", "token2charspan": [[0, 9], [10, 16], [17, 23], [24, 28], [29, 33], [34, 40], [41, 50], [51, 53], [54, 58], [58, 59], [60, 64], [65, 74], [74, 76], [77, 81], [81, 82], [83, 86], [87, 94], [95, 104], [105, 108], [109, 117], [118, 121], [122, 134], [135, 144], [145, 152], [153, 156], [157, 165], [165, 166], [167, 168], [169, 172], [173, 178], [179, 181], [182, 189], [190, 193], [193, 194], [195, 198], [199, 203], [204, 213], [213, 215], [216, 226], [227, 238], [239, 247], [248, 251], [251, 252]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [10, 11, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 1, 10, 11, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "data", "mining", "tool", "Weka", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the data mining tool Weka.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 82], [83, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[15, 16, "product"], [22, 30, "misc"]], "ner_mapping_to_source": [1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "SSIM", "publication", "from", "2004", "has", "been", "cited", "more", "than", "20,000", "times", ",", "according", "to", "Google", "Scholar", ".", "It", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "'s", "Sustained", "Impact", "Award", "in", "2016", ",", "which", "recognises", "a", "paper", "that", "has", "had", "an", "unusually", "high", "impact", "at", "least", "10", "years", "after", "its", "publication", "."], "sentence-detokenized": "The SSIM publication from 2004 has been cited more than 20,000 times, according to Google Scholar. It also received the IEEE Signal Processing Society's Sustained Impact Award in 2016, which recognises a paper that has had an unusually high impact at least 10 years after its publication.", "token2charspan": [[0, 3], [4, 8], [9, 20], [21, 25], [26, 30], [31, 34], [35, 39], [40, 45], [46, 50], [51, 55], [56, 62], [63, 68], [68, 69], [70, 79], [80, 82], [83, 89], [90, 97], [97, 98], [99, 101], [102, 106], [107, 115], [116, 119], [120, 124], [125, 131], [132, 142], [143, 150], [150, 152], [153, 162], [163, 169], [170, 175], [176, 178], [179, 183], [183, 184], [185, 190], [191, 201], [202, 203], [204, 209], [210, 214], [215, 218], [219, 222], [223, 225], [226, 235], [236, 240], [241, 247], [248, 250], [251, 256], [257, 259], [260, 265], [266, 271], [272, 275], [276, 287], [287, 288]]}
{"doc_key": "ai-train-93", "ner": [[0, 3, "task"], [18, 19, "product"], [29, 31, "product"], [34, 34, "organisation"], [35, 35, "product"], [40, 43, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 3, 34, 34, "artifact", "", false, false], [18, 19, 0, 3, "related-to", "performs", false, false], [18, 19, 29, 31, "part-of", "", false, false], [34, 34, 40, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "has", "become", "almost", "indistinguishable", "from", "a", "real", "human", "voice", "with", "the", "speech", "editing", "and", "generation", "software", "Adobe", "Voco", ",", "a", "prototype", "that", "will", "be", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", ",", "unveiled", "in", "2016", "."], "sentence-detokenized": "Speech synthesis has become almost indistinguishable from a real human voice with the speech editing and generation software Adobe Voco, a prototype that will be part of the Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google, unveiled in 2016.", "token2charspan": [[0, 6], [7, 16], [17, 20], [21, 27], [28, 34], [35, 52], [53, 57], [58, 59], [60, 64], [65, 70], [71, 76], [77, 81], [82, 85], [86, 92], [93, 100], [101, 104], [105, 115], [116, 124], [125, 130], [131, 135], [135, 136], [137, 138], [139, 148], [149, 153], [154, 158], [159, 161], [162, 166], [167, 169], [170, 173], [174, 179], [180, 188], [189, 194], [194, 195], [196, 199], [200, 208], [209, 216], [216, 217], [218, 219], [220, 229], [230, 234], [235, 241], [241, 242], [243, 251], [252, 254], [255, 259], [259, 260]]}
{"doc_key": "ai-train-94", "ner": [[0, 0, "researcher"], [7, 9, "organisation"], [15, 23, "organisation"], [26, 26, "conference"], [33, 37, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 9, "role", "", false, false], [0, 0, 15, 23, "role", "", false, false], [0, 0, 26, 26, "role", "", false, false], [0, 0, 33, 37, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "member", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "member", "of", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary member of the Neuroscience Research Program, a member of the American Academy of Arts and Sciences, a founding member of AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 123], [124, 132], [133, 139], [140, 142], [143, 147], [148, 151], [152, 153], [154, 162], [163, 169], [170, 172], [173, 176], [177, 185], [186, 195], [196, 199], [200, 205], [206, 214], [214, 215]]}
{"doc_key": "ai-train-95", "ner": [[9, 10, "task"], [12, 13, "task"], [17, 18, "task"], [25, 25, "misc"], [26, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 10, 17, 18, "cause-effect", "", false, false], [12, 13, 17, 18, "cause-effect", "", false, false], [26, 27, 17, 18, "topic", "", false, false], [26, 27, 25, 25, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "encouraged", "by", "the", "successes", "in", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "into", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, encouraged by the successes in speech recognition and speech synthesis, research into speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 31], [32, 41], [42, 44], [45, 51], [52, 63], [64, 67], [68, 74], [75, 84], [84, 85], [86, 94], [95, 99], [100, 106], [107, 118], [119, 124], [125, 129], [130, 133], [134, 145], [146, 148], [149, 152], [153, 159], [160, 169], [170, 177], [177, 178]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 16, "algorithm"], [20, 21, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 16, 3, 4, "origin", "", false, false], [15, 16, 8, 9, "origin", "", false, false], [15, 16, 11, 12, "origin", "", false, false], [20, 21, 15, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisors", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forgetting", "gate", "(", "also", "called", "keep", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisors J\u00fcrgen Schmidhuber and Fred Cummins introduced the forgetting gate (also called keep gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 87], [88, 98], [99, 103], [104, 105], [105, 109], [110, 116], [117, 121], [122, 126], [126, 127], [128, 132], [133, 136], [137, 141], [142, 154], [154, 155]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 12, 1, 3, "part-of", "", false, false], [9, 12, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalised", "sine", "function", "is", "usually", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalised sine function is usually defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103], [104, 111]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [18, 20, "conference"], [21, 28, "organisation"], [30, 30, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 18, 20, "role", "", false, false], [9, 10, 21, 28, "role", "", false, false], [30, 30, 21, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 72], [72, 73], [74, 75], [76, 84], [85, 91], [92, 94], [95, 99], [100, 103], [104, 115], [116, 119], [120, 133], [134, 145], [146, 149], [150, 153], [154, 167], [168, 177], [178, 180], [181, 194], [195, 206], [207, 208], [208, 212], [212, 213], [213, 214]]}
{"doc_key": "ai-train-99", "ner": [[8, 13, "misc"], [18, 18, "misc"], [62, 64, "metrics"], [66, 66, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[66, 66, 62, 64, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one", "-dimensional", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPD", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "allows", "the", "nonlinear", "products", "of", "the", "order", "of", "the", "digital", "predistorter", "to", "be", "captured", "in", "order", "to", "solve", "the", "coefficients", "of", "the", "digital", "predistorter", "polynomials", "and", "minimise", "the", "mean", "square", "error", "(", "MSE", ")", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional polynomial-based memory (or memoryless) DPD, the distorted output of the nonlinear system must be oversampled at a rate that allows the nonlinear products of the order of the digital predistorter to be captured in order to solve the coefficients of the digital predistorter polynomials and minimise the mean square error (MSE).", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 35], [35, 47], [48, 58], [58, 59], [59, 64], [65, 71], [72, 73], [73, 75], [76, 86], [86, 87], [88, 91], [91, 92], [93, 96], [97, 106], [107, 113], [114, 116], [117, 120], [121, 130], [131, 137], [138, 142], [143, 145], [146, 157], [158, 160], [161, 162], [163, 167], [168, 172], [173, 179], [180, 183], [184, 193], [194, 202], [203, 205], [206, 209], [210, 215], [216, 218], [219, 222], [223, 230], [231, 243], [244, 246], [247, 249], [250, 258], [259, 261], [262, 267], [268, 270], [271, 276], [277, 280], [281, 293], [294, 296], [297, 300], [301, 308], [309, 321], [322, 333], [334, 337], [338, 346], [347, 350], [351, 355], [356, 362], [363, 368], [369, 370], [370, 373], [373, 374], [374, 375]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 9, "location"], [11, 12, "location"], [14, 15, "country"], [19, 19, "location"], [21, 21, "country"], [35, 41, "organisation"], [44, 47, "organisation"], [49, 50, "location"], [56, 57, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 9, 9, "physical", "", false, false], [0, 1, 44, 47, "physical", "", false, false], [0, 1, 56, 57, "role", "", false, false], [9, 9, 11, 12, "physical", "", false, false], [11, 12, 14, 15, "physical", "", false, false], [35, 41, 44, 47, "part-of", "", false, false], [44, 47, 49, 50, "physical", "", false, false], [56, 57, 35, 41, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "5", "October", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldavian", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "a", "senior", "American", "researcher", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "and", "head", "of", "the", "Laboratory", "'s", "InfoLab", "Group", "."], "sentence-detokenized": "Boris Katz, (born 5 October 1947 in Chi\u0219in\u0103u, Moldavian SSR, Soviet Union, (now Chi\u0219in\u0103u, Moldova)) is a senior American researcher (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and head of the Laboratory's InfoLab Group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 19], [20, 27], [28, 32], [33, 35], [36, 44], [44, 45], [46, 55], [56, 59], [59, 60], [61, 67], [68, 73], [73, 74], [75, 76], [76, 79], [80, 88], [88, 89], [90, 97], [97, 98], [98, 99], [100, 102], [103, 104], [105, 111], [112, 120], [121, 131], [132, 133], [133, 141], [142, 151], [151, 152], [153, 155], [156, 159], [160, 163], [164, 172], [173, 180], [181, 184], [185, 195], [196, 208], [209, 219], [220, 222], [223, 226], [227, 240], [241, 250], [251, 253], [254, 264], [265, 267], [268, 277], [278, 281], [282, 286], [287, 289], [290, 293], [294, 304], [304, 306], [307, 314], [315, 320], [320, 321]]}
