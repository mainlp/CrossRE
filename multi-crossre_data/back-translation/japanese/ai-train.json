{"doc_key": "ai-train-1", "ner": [[3, 6, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [23, 26, "task"], [11, 30, "field"], [31, 33, "researcher"], [35, 37, "researcher"], [39, 40, "researcher"], [42, 43, "researcher"], [45, 47, "researcher"], [49, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 6, 13, 14, "part-of", "", false, false], [3, 6, 13, 14, "usage", "", false, false], [3, 6, 16, 17, "part-of", "", false, false], [3, 6, 16, 17, "usage", "", false, false], [3, 6, 19, 20, "part-of", "", false, false], [3, 6, 19, 20, "usage", "", false, false], [3, 6, 11, 30, "part-of", "", false, false], [3, 6, 11, 30, "usage", "", false, false], [23, 26, 19, 20, "part-of", "", false, false], [23, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["General", "approaches", "to", "opinion", "-", "based", "recommendation", "systems", "utilise", "various", "techniques", "such", "as", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.-C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", "21", "(", "5", ")", ":", "see", "e12957", "."], "sentence-detokenized": "General approaches to opinion-based recommendation systems utilise various techniques such as text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.-C. Liang, R.C. Guan, D. Xu, (2019), 21 (5): see e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 50], [51, 58], [59, 66], [67, 74], [75, 85], [86, 90], [91, 93], [94, 98], [99, 105], [105, 106], [107, 118], [119, 128], [128, 129], [130, 139], [140, 148], [149, 150], [150, 153], [154, 158], [159, 169], [170, 179], [180, 188], [188, 189], [190, 193], [194, 198], [199, 207], [208, 211], [211, 212], [213, 217], [217, 218], [219, 220], [220, 221], [222, 227], [227, 228], [229, 233], [234, 237], [237, 238], [239, 243], [244, 249], [249, 250], [251, 252], [252, 253], [254, 257], [257, 258], [259, 264], [265, 270], [270, 271], [272, 276], [277, 281], [281, 282], [283, 285], [286, 288], [288, 289], [290, 291], [291, 295], [295, 296], [296, 297], [298, 300], [301, 302], [302, 303], [303, 304], [304, 305], [306, 309], [310, 316], [316, 317]]}
{"doc_key": "ai-train-2", "ner": [[15, 15, "university"], [4, 13, "researcher"]], "ner_mapping_to_source": [0, 2], "relations": [[4, 13, 15, 15, "physical", "", false, false], [4, 13, 15, 15, "role", "", false, false]], "relations_mapping_to_source": [2, 3], "sentence": ["Advocates", "of", "procedural", "representation", "were", "led", "by", "Marvin", "Minsky", "and", "Seymour", "Papert", ",", "mainly", "at", "MIT", "."], "sentence-detokenized": "Advocates of procedural representation were led by Marvin Minsky and Seymour Papert, mainly at MIT.", "token2charspan": [[0, 9], [10, 12], [13, 23], [24, 38], [39, 43], [44, 47], [48, 50], [51, 57], [58, 64], [65, 68], [69, 76], [77, 83], [83, 84], [85, 91], [92, 94], [95, 98], [98, 99]]}
{"doc_key": "ai-train-3", "ner": [[6, 8, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "and", "calculator", "interfaces", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard and calculator interfaces are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 16], [17, 27], [28, 38], [39, 42], [43, 50], [51, 53], [54, 58], [58, 59]]}
{"doc_key": "ai-train-4", "ner": [[0, 1, "product"], [5, 5, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 5, 5, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "is", "largely", "compatible", "with", "MATLAB", "and", "helps", "you", "solve", "linear", "and", "non-linear", "problems", "numerically", "and", "perform", "other", "numerical", "experiments", "."], "sentence-detokenized": "Octave is largely compatible with MATLAB and helps you solve linear and non-linear problems numerically and perform other numerical experiments.", "token2charspan": [[0, 6], [7, 9], [10, 17], [18, 28], [29, 33], [34, 40], [41, 44], [45, 50], [51, 54], [55, 60], [61, 67], [68, 71], [72, 82], [83, 91], [92, 103], [104, 107], [108, 115], [116, 121], [122, 131], [132, 143], [143, 144]]}
{"doc_key": "ai-train-5", "ner": [[3, 5, "algorithm"], [6, 8, "misc"], [9, 10, "researcher"], [16, 18, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 5, 9, 10, "origin", "", false, false], [6, 8, 9, 10, "origin", "", false, false], [9, 10, 16, 18, "physical", "", false, false], [9, 10, 16, 18, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "backpropagation", "algorithm", "and", "unsupervised", "methods", "by", "Geoff", "Hinton", "et", "al", ".", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "learn", "deep", "and", "highly", "non-linear", "neural", "architectures", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the backpropagation algorithm and unsupervised methods by Geoff Hinton et al. at the University of Toronto can be used to learn deep and highly non-linear neural architectures {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 31], [32, 41], [42, 45], [46, 58], [59, 66], [67, 69], [70, 75], [76, 82], [83, 85], [86, 88], [88, 89], [90, 92], [93, 96], [97, 107], [108, 110], [111, 118], [119, 122], [123, 125], [126, 130], [131, 133], [134, 139], [140, 144], [145, 148], [149, 155], [156, 166], [167, 173], [174, 187], [188, 189], [189, 190], [190, 194], [195, 202]]}
{"doc_key": "ai-train-6", "ner": [[4, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalently", "use", "the", "DCG", "notation", "."], "sentence-detokenized": "or equivalently use the DCG notation.", "token2charspan": [[0, 2], [3, 15], [16, 19], [20, 23], [24, 27], [28, 36], [36, 37]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 12, "algorithm"], [13, 15, "algorithm"], [16, 22, "algorithm"], [25, 28, "algorithm"], [33, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 5, 6], "relations": [[0, 3, 7, 12, "type-of", "", false, false], [0, 3, 13, 15, "usage", "part-of?", true, false], [13, 15, 16, 22, "compare", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "use", "competitive", "learning", "rather", "than", "error", "correction", "learning", "such", "as", "backpropagation", "or", "gradient", "descent", ",", "and", "neighbourhood", "functions", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks in that they use competitive learning rather than error correction learning such as backpropagation or gradient descent, and neighbourhood functions to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 82], [83, 94], [95, 103], [104, 110], [111, 115], [116, 121], [122, 132], [133, 141], [142, 146], [147, 149], [150, 165], [166, 168], [169, 177], [178, 185], [185, 186], [187, 190], [191, 204], [205, 214], [215, 217], [218, 226], [227, 230], [231, 242], [243, 253], [254, 256], [257, 260], [261, 266], [267, 272], [272, 273]]}
{"doc_key": "ai-train-8", "ner": [[14, 17, "organisation"], [27, 34, "misc"], [35, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "it", "has", "been", "recommended", "by", "several", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "that", "dynamic", "range", "measurements", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", "and", "that", "the", "noise", "floor", "used", "for", "dynamic", "range", "measurements", "be", "filtered", ".", "This", "avoids", "questionable", "measurements", "using", "empty", "media", "or", "muting", "circuits", "."], "sentence-detokenized": "Since the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that dynamic range measurements be made in the presence of an audio signal and that the noise floor used for dynamic range measurements be filtered. This avoids questionable measurements using empty media or muting circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 25], [26, 29], [30, 34], [35, 46], [47, 49], [50, 57], [58, 69], [69, 70], [71, 80], [81, 84], [85, 90], [91, 102], [103, 110], [110, 111], [112, 116], [117, 124], [125, 130], [131, 143], [144, 146], [147, 151], [152, 154], [155, 158], [159, 167], [168, 170], [171, 173], [174, 179], [180, 186], [187, 190], [191, 195], [196, 199], [200, 205], [206, 211], [212, 216], [217, 220], [221, 228], [229, 234], [235, 247], [248, 250], [251, 259], [259, 260], [261, 265], [266, 272], [273, 285], [286, 298], [299, 304], [305, 310], [311, 316], [317, 319], [320, 326], [327, 335], [335, 336]]}
{"doc_key": "ai-train-9", "ner": [[11, 12, "misc"], [0, 4, "task"], [20, 20, "task"], [22, 24, "task"], [26, 27, "task"], [29, 30, "task"], [32, 33, "task"], [17, 37, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[11, 12, 0, 4, "part-of", "concept_used_in", true, false], [11, 12, 20, 20, "part-of", "concept_used_in", false, false], [11, 12, 22, 24, "part-of", "concept_used_in", false, false], [11, 12, 26, 27, "part-of", "concept_used_in", false, false], [11, 12, 29, 30, "part-of", "concept_used_in", false, false], [11, 12, 32, 33, "part-of", "concept_used_in", false, false], [11, 12, 17, 37, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["In", "addition", "to", "face", "recognition", ",", "technologies", "for", "creating", "and", "using", "unique", "faces", "for", "recognition", "are", "also", "used", "for", "handwriting", "recognition", ",", "lip", "-", "reading", ",", "speech", "recognition", ",", "sign", "language", "and", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "In addition to face recognition, technologies for creating and using unique faces for recognition are also used for handwriting recognition, lip-reading, speech recognition, sign language and gesture interpretation and medical image analysis.", "token2charspan": [[0, 2], [3, 11], [12, 14], [15, 19], [20, 31], [31, 32], [33, 45], [46, 49], [50, 58], [59, 62], [63, 68], [69, 75], [76, 81], [82, 85], [86, 97], [98, 101], [102, 106], [107, 111], [112, 115], [116, 127], [128, 139], [139, 140], [141, 144], [144, 145], [145, 152], [152, 153], [154, 160], [161, 172], [172, 173], [174, 178], [179, 187], [188, 191], [192, 199], [200, 214], [215, 218], [219, 226], [227, 232], [233, 241], [241, 242]]}
{"doc_key": "ai-train-10", "ner": [[0, 5, "organisation"], [10, 16, "organisation"], [20, 25, "organisation"], [26, 30, "organisation"], [33, 36, "organisation"], [39, 45, "organisation"], [49, 53, "organisation"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 7, 8], "relations": [[20, 25, 0, 5, "part-of", "", false, false], [26, 30, 0, 5, "part-of", "", false, false], [33, 36, 0, 5, "part-of", "", false, false], [49, 53, 0, 5, "part-of", "", false, false]], "relations_mapping_to_source": [2, 3, 4, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "under", "the", "umbrella", "of", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defence", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Naval", "Research", "Laboratory", ",", "which", "coordinated", "research", "to", "inform", "strategic", "planners", "'", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was under the umbrella of the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, the Defence Advanced Research Projects Agency (DARPA) and the Naval Research Laboratory, which coordinated research to inform strategic planners' deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 41], [42, 45], [46, 54], [55, 57], [58, 61], [62, 70], [71, 82], [83, 86], [87, 92], [93, 107], [108, 109], [109, 113], [113, 114], [114, 115], [116, 119], [120, 122], [123, 133], [134, 136], [137, 143], [143, 144], [145, 148], [149, 151], [152, 162], [163, 165], [166, 174], [175, 179], [179, 180], [181, 184], [185, 187], [188, 198], [199, 201], [202, 209], [209, 210], [211, 214], [215, 222], [223, 231], [232, 240], [241, 249], [250, 256], [257, 258], [258, 263], [263, 264], [265, 268], [269, 272], [273, 278], [279, 287], [288, 298], [298, 299], [300, 305], [306, 317], [318, 326], [327, 329], [330, 336], [337, 346], [347, 355], [355, 356], [357, 370], [370, 371]]}
{"doc_key": "ai-train-11", "ner": [[5, 6, "metrics"], [9, 10, "algorithm"], [13, 15, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[5, 6, 9, 10, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["A", "fast", "method", "for", "calculating", "maximum", "likelihood", "estimates", "of", "probit", "models", "was", "proposed", "by", "Fisher", "in", "1935", "as", "an", "appendix", "to", "Bliss", "'s", "work", "."], "sentence-detokenized": "A fast method for calculating maximum likelihood estimates of probit models was proposed by Fisher in 1935 as an appendix to Bliss's work.", "token2charspan": [[0, 1], [2, 6], [7, 13], [14, 17], [18, 29], [30, 37], [38, 48], [49, 58], [59, 61], [62, 68], [69, 75], [76, 79], [80, 88], [89, 91], [92, 98], [99, 101], [102, 106], [107, 109], [110, 112], [113, 121], [122, 124], [125, 130], [130, 132], [133, 137], [137, 138]]}
{"doc_key": "ai-train-12", "ner": [[10, 10, "product"], [7, 16, "product"], [20, 20, "product"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 3, 5], "relations": [[20, 20, 7, 16, "usage", "uses_software", false, false], [20, 20, 24, 24, "named", "", false, false]], "relations_mapping_to_source": [0, 2], "sentence": ["Some", "of", "these", "programmes", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "powers", "AltaVista", "'s", "BabelFish", "(", "Yahoo", "'s", "Babelfish", "as", "of", "9", "May", "2008", ")", "."], "sentence-detokenized": "Some of these programmes are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (Yahoo's Babelfish as of 9 May 2008).", "token2charspan": [[0, 4], [5, 7], [8, 13], [14, 24], [25, 28], [29, 38], [39, 45], [45, 46], [47, 51], [52, 54], [55, 61], [62, 71], [72, 75], [76, 79], [80, 87], [88, 94], [95, 99], [100, 106], [107, 116], [116, 118], [119, 128], [129, 130], [130, 135], [135, 137], [138, 147], [148, 150], [151, 153], [154, 155], [156, 159], [160, 164], [164, 165], [165, 166]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [5, 11, "researcher"], [20, 22, "field"], [26, 26, "misc"], [23, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 26, "related-to", "", true, false], [3, 3, 23, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 26, "related-to", "", true, false], [7, 8, 23, 32, "related-to", "", true, false], [5, 11, 20, 22, "related-to", "", true, false], [5, 11, 26, 26, "related-to", "", true, false], [5, 11, 23, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 19, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 19, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "method", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "indicator", "."], "sentence-detokenized": "The most common method is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) indicator.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 22], [23, 25], [26, 28], [29, 32], [33, 36], [37, 39], [39, 40], [40, 46], [47, 52], [53, 54], [54, 60], [60, 61], [61, 69], [70, 80], [81, 84], [85, 92], [93, 103], [103, 104], [105, 114], [114, 115]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [15, 16, "programlang"], [17, 18, "researcher"], [20, 21, "organisation"]], "ner_mapping_to_source": [0, 2, 3, 4], "relations": [[0, 0, 15, 16, "related-to", "", false, false], [17, 18, 20, 21, "role", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["RapidMiner", "provides", "learning", "schemes", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "R", "and", "Python", "scripts", "David", "Norris", ",", "Bloor", "Research", ",", "13", "Nov", "2013", "."], "sentence-detokenized": "RapidMiner provides learning schemes, models and algorithms and can be extended using R and Python scripts David Norris, Bloor Research, 13 Nov 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 85], [86, 87], [88, 91], [92, 98], [99, 106], [107, 112], [113, 119], [119, 120], [121, 126], [127, 135], [135, 136], [137, 139], [140, 143], [144, 148], [148, 149]]}
{"doc_key": "ai-train-16", "ner": [[7, 7, "field"], [10, 11, "task"], [15, 18, "misc"], [0, 1, "product"]], "ner_mapping_to_source": [1, 2, 3, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["Weka", "has", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modelling", ",", "and", "a", "graphical", "user", "interface", "for", "easy", "access", "to", "these", "functions", "."], "sentence-detokenized": "Weka has visualisation tools and algorithms for data analysis and predictive modelling, and a graphical user interface for easy access to these functions.", "token2charspan": [[0, 4], [5, 8], [9, 22], [23, 28], [29, 32], [33, 43], [44, 47], [48, 52], [53, 61], [62, 65], [66, 76], [77, 86], [86, 87], [88, 91], [92, 93], [94, 103], [104, 108], [109, 118], [119, 122], [123, 127], [128, 134], [135, 137], [138, 143], [144, 153], [153, 154]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [13, 39, "misc"], [40, 42, "misc"], [25, 30, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 39, 0, 0, "topic", "", false, false], [13, 39, 40, 42, "win-defeat", "", false, false], [40, 42, 25, 30, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "received", "great", "recognition", ",", "among", "them", "the", "paper", "'", "Heuristics", "'", ",", "which", "won", "the", "Best", "Paper", "Award", "at", "the", "1982", "Artificial", "Intelligence", "Society", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", ",", "which", "won", "the", "Best", "Paper", "Award", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and received great recognition, among them the paper 'Heuristics', which won the Best Paper Award at the 1982 Artificial Intelligence Society Theoretical and Study of Heuristic Rules, which won the Best Paper Award.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 54], [55, 60], [61, 72], [72, 73], [74, 79], [80, 84], [85, 88], [89, 94], [95, 96], [96, 106], [106, 107], [107, 108], [109, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 151], [152, 162], [163, 175], [176, 183], [184, 195], [196, 199], [200, 205], [206, 208], [209, 218], [219, 224], [224, 225], [226, 231], [232, 235], [236, 239], [240, 244], [245, 250], [251, 256], [256, 257]]}
{"doc_key": "ai-train-18", "ner": [[6, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Multiple", "entities", "are", "considered", "and", "individual", "hinge", "losses", "are", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "Multiple entities are considered and individual hinge losses are calculated for each capsule.", "token2charspan": [[0, 8], [9, 17], [18, 21], [22, 32], [33, 36], [37, 47], [48, 53], [54, 60], [61, 64], [65, 75], [76, 79], [80, 84], [85, 92], [92, 93]]}
{"doc_key": "ai-train-19", "ner": [[4, 6, "product"], [8, 9, "product"], [11, 12, "product"], [14, 15, "product"], [2, 22, "product"], [47, 50, "product"], [31, 51, "product"], [36, 37, "product"], [34, 40, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 47, 50, "type-of", "", false, false], [8, 9, 47, 50, "type-of", "", false, false], [11, 12, 47, 50, "type-of", "", false, false], [14, 15, 47, 50, "type-of", "", false, false], [2, 22, 47, 50, "type-of", "", false, false], [36, 37, 31, 51, "type-of", "", false, false], [34, 40, 31, 51, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Conversational", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", "are", "now", "available", "and", "can", "be", "used", "on", "mobile", "devices", "and", "distant", "voice", "smarts", "such", "as", "Amazon", "Echo", "and", "Google", "Home", ".", "It", "is", "now", "possible", "to", "access", "voice", "portals", "from", "speakers", "."], "sentence-detokenized": "Conversational assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby are now available and can be used on mobile devices and distant voice smarts such as Amazon Echo and Google Home. It is now possible to access voice portals from speakers.", "token2charspan": [[0, 14], [15, 25], [26, 30], [31, 33], [34, 39], [39, 41], [42, 46], [46, 47], [48, 54], [55, 60], [60, 61], [62, 68], [69, 78], [78, 79], [80, 89], [90, 97], [98, 101], [102, 109], [109, 111], [112, 117], [118, 121], [122, 125], [126, 135], [136, 139], [140, 143], [144, 146], [147, 151], [152, 154], [155, 161], [162, 169], [170, 173], [174, 181], [182, 187], [188, 194], [195, 199], [200, 202], [203, 209], [210, 214], [215, 218], [219, 225], [226, 230], [230, 231], [232, 234], [235, 237], [238, 241], [242, 250], [251, 253], [254, 260], [261, 266], [267, 274], [275, 279], [280, 288], [288, 289]]}
{"doc_key": "ai-train-20", "ner": [[1, 4, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 14, "algorithm"], [16, 16, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 1, 4, "type-of", "", false, false], [9, 11, 1, 4, "type-of", "", false, false], [13, 14, 1, 4, "type-of", "", false, false], [16, 16, 1, 4, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "include", "na\u00efve", "Bayes", "classifiers", ",", "support", "vector", "machines", ",", "Gaussian", "mixtures", "and", "networks", "."], "sentence-detokenized": "Examples of supervised learning include na\u00efve Bayes classifiers, support vector machines, Gaussian mixtures and networks.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 39], [40, 45], [46, 51], [52, 63], [63, 64], [65, 72], [73, 79], [80, 88], [88, 89], [90, 98], [99, 107], [108, 111], [112, 120], [120, 121]]}
{"doc_key": "ai-train-21", "ner": [[0, 5, "algorithm"], [11, 12, "algorithm"], [14, 15, "task"], [16, 17, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 5, 11, 12, "part-of", "", true, false], [16, 17, 14, 15, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Using", "the", "OSD", "algorithm", ",", "the", "online", "version", "of", "the", "Support", "vector", "machine", "for", "classification", "with", "hinge", "loss", "math", "v", "_t", "(", "w", ")", "=", "max", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "maths", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "math", "regret", "It", "is", "possible", "to", "derive", "bounds", ".", "/", "math"], "sentence-detokenized": "Using the OSD algorithm, the online version of the Support vector machine for classification with hinge loss math v _t (w) = max {0, 1 - y _t (w\\ cdot x _t)\\} maths O (\\ sqrt {T}) / math regret It is possible to derive bounds. / math", "token2charspan": [[0, 5], [6, 9], [10, 13], [14, 23], [23, 24], [25, 28], [29, 35], [36, 43], [44, 46], [47, 50], [51, 58], [59, 65], [66, 73], [74, 77], [78, 92], [93, 97], [98, 103], [104, 108], [109, 113], [114, 115], [116, 118], [119, 120], [120, 121], [121, 122], [123, 124], [125, 128], [129, 130], [130, 131], [131, 132], [133, 134], [135, 136], [137, 138], [139, 141], [142, 143], [143, 144], [144, 145], [146, 150], [151, 152], [153, 155], [155, 156], [156, 158], [159, 164], [165, 166], [167, 168], [168, 169], [170, 174], [175, 176], [176, 177], [177, 178], [178, 179], [180, 181], [182, 186], [187, 193], [194, 196], [197, 199], [200, 208], [209, 211], [212, 218], [219, 225], [225, 226], [227, 228], [229, 233]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 5, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [17, 17, "task"], [19, 20, "task"], [22, 23, "task"], [25, 25, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "wildlife", "identification", "and", "matchmoving", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, wildlife identification and matchmoving.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 102], [102, 103], [104, 111], [112, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 149], [150, 164], [165, 168], [169, 180], [180, 181]]}
{"doc_key": "ai-train-23", "ner": [[54, 55, "task"], [0, 1, "university"], [3, 5, "university"], [7, 8, "university"], [10, 11, "university"], [13, 17, "university"], [19, 21, "university"], [23, 25, "university"], [27, 28, "university"], [33, 33, "university"], [30, 37, "university"], [40, 45, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[54, 55, 0, 1, "related-to", "", true, false], [54, 55, 3, 5, "related-to", "", true, false], [54, 55, 7, 8, "related-to", "", true, false], [54, 55, 10, 11, "related-to", "", true, false], [54, 55, 13, 17, "related-to", "", true, false], [54, 55, 19, 21, "related-to", "", true, false], [54, 55, 23, 25, "related-to", "", true, false], [54, 55, 27, 28, "related-to", "", true, false], [54, 55, 33, 33, "related-to", "", true, false], [54, 55, 30, 37, "related-to", "", true, false], [54, 55, 40, 45, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbr\u00fccken", ",", "Stanford", "University", ",", "University", "of", "California", "San", "Diego", ",", "University", "of", "Toronto", ",", "Institut", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", ",", "University", "of", "California", ",", "Irvine", "and", "many", "other", "groups", "and", "companies", "are", "working", "on", "attitude", "Estimation", "is", "being", "studied", "."], "sentence-detokenized": "Brown University, Carnegie Mellon University, MPI Saarbr\u00fccken, Stanford University, University of California San Diego, University of Toronto, Institut Centrale Paris, ETH Zurich, National University of Science and Technology (NUST), University of California, Irvine and many other groups and companies are working on attitude Estimation is being studied.", "token2charspan": [[0, 5], [6, 16], [16, 17], [18, 26], [27, 33], [34, 44], [44, 45], [46, 49], [50, 61], [61, 62], [63, 71], [72, 82], [82, 83], [84, 94], [95, 97], [98, 108], [109, 112], [113, 118], [118, 119], [120, 130], [131, 133], [134, 141], [141, 142], [143, 151], [152, 160], [161, 166], [166, 167], [168, 171], [172, 178], [178, 179], [180, 188], [189, 199], [200, 202], [203, 210], [211, 214], [215, 225], [226, 227], [227, 231], [231, 232], [232, 233], [234, 244], [245, 247], [248, 258], [258, 259], [260, 266], [267, 270], [271, 275], [276, 281], [282, 288], [289, 292], [293, 302], [303, 306], [307, 314], [315, 317], [318, 326], [327, 337], [338, 340], [341, 346], [347, 354], [354, 355]]}
{"doc_key": "ai-train-24", "ner": [[0, 6, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "Math", "0,1", "/", "Math", "."], "sentence-detokenized": "The sigmoid function Cross entropy loss is used to predict K independent probability values in Math 0,1 / Math.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 26], [27, 34], [35, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 99], [100, 103], [104, 105], [106, 110], [110, 111]]}
{"doc_key": "ai-train-25", "ner": [[3, 6, "misc"], [9, 11, "field"], [13, 14, "university"], [17, 17, "country"], [20, 23, "misc"], [24, 29, "university"], [30, 33, "country"], [35, 37, "university"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 6, 9, 11, "topic", "", false, false], [3, 6, 13, 14, "physical", "", true, false], [13, 14, 17, 17, "physical", "", false, false], [20, 23, 24, 29, "physical", "", true, false], [24, 29, 30, 33, "physical", "", false, false]], "relations_mapping_to_source": [1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johan", "Bernoulli", "Chair", "in", "Mathematics", "and", "Informatics", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "a", "professor", "at", "Cambridge", "University", "."], "sentence-detokenized": "He held the Johan Bernoulli Chair in Mathematics and Informatics at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan before becoming a professor at Cambridge University.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 17], [18, 27], [28, 33], [34, 36], [37, 48], [49, 52], [53, 64], [65, 67], [68, 71], [72, 82], [83, 85], [86, 95], [96, 98], [99, 102], [103, 114], [115, 118], [119, 122], [123, 130], [131, 138], [139, 144], [145, 147], [148, 151], [152, 157], [158, 167], [168, 170], [171, 181], [182, 184], [185, 190], [191, 197], [198, 206], [207, 208], [209, 218], [219, 221], [222, 231], [232, 242], [242, 243]]}
{"doc_key": "ai-train-26", "ner": [[12, 15, "algorithm"], [10, 10, "algorithm"], [20, 20, "researcher"], [18, 25, "researcher"]], "ner_mapping_to_source": [1, 2, 3, 4], "relations": [[12, 15, 20, 20, "origin", "", false, false], [12, 15, 18, 25, "origin", "", false, false], [10, 10, 12, 15, "named", "", false, false]], "relations_mapping_to_source": [1, 2, 3], "sentence": ["Another", "technique", "used", "specifically", "for", "recurrent", "neural", "networks", "is", "the", "LSTM", "(", "long", "-", "short", "memory", ")", "network", "developed", "by", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another technique used specifically for recurrent neural networks is the LSTM (long-short memory) network developed by Sepp Hochreiter and J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 35], [36, 39], [40, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 77], [78, 79], [79, 83], [83, 84], [84, 89], [90, 96], [96, 97], [98, 105], [106, 115], [116, 118], [119, 123], [124, 134], [135, 138], [139, 145], [146, 157], [158, 160], [161, 165], [165, 166]]}
{"doc_key": "ai-train-27", "ner": [[4, 6, "programlang"], [9, 10, "product"], [16, 16, "product"], [44, 45, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[9, 10, 4, 6, "general-affiliation", "", false, false], [9, 10, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "+", "+", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "it", "a", "very", "versatile", "package", "that", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "modes", ",", "just", "like", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C + + interpreter (CINT up to version 5.34, Cling from version 6) makes it a very versatile package that can be used in interactive, scripted and compiled modes, just like commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [21, 22], [23, 24], [25, 36], [37, 38], [38, 40], [40, 42], [43, 45], [46, 48], [49, 56], [57, 61], [61, 62], [63, 68], [69, 73], [74, 81], [82, 83], [83, 84], [85, 90], [91, 93], [94, 95], [96, 100], [101, 110], [111, 118], [119, 123], [124, 127], [128, 130], [131, 135], [136, 138], [139, 150], [150, 151], [152, 160], [161, 164], [165, 173], [174, 179], [179, 180], [181, 185], [186, 190], [191, 201], [202, 210], [211, 215], [216, 218], [219, 225], [225, 226]]}
{"doc_key": "ai-train-28", "ner": [[0, 3, "product"], [21, 24, "field"], [27, 28, "task"], [30, 32, "task"], [34, 35, "task"], [37, 37, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 3, 21, 24, "related-to", "", false, false], [27, 28, 21, 24, "part-of", "", false, false], [30, 32, 21, 24, "part-of", "", false, false], [34, 35, 21, 24, "part-of", "", false, false], [37, 37, 21, 24, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Spoken", "user", "interfaces", "that", "interpret", "and", "manage", "conversational", "state", "are", "difficult", "to", "design", "due", "to", "the", "inherent", "difficulties", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Spoken user interfaces that interpret and manage conversational state are difficult to design due to the inherent difficulties of integrating complex natural language processing tasks such as coreference resolution, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 6], [7, 11], [12, 22], [23, 27], [28, 37], [38, 41], [42, 48], [49, 63], [64, 69], [70, 73], [74, 83], [84, 86], [87, 93], [94, 97], [98, 100], [101, 104], [105, 113], [114, 126], [127, 129], [130, 141], [142, 149], [150, 157], [158, 166], [167, 177], [178, 183], [184, 188], [189, 191], [192, 203], [204, 214], [214, 215], [216, 221], [222, 228], [229, 240], [240, 241], [242, 253], [254, 263], [264, 267], [268, 276], [277, 287], [287, 288]]}
{"doc_key": "ai-train-29", "ner": [[16, 18, "researcher"], [21, 27, "organisation"], [34, 34, "field"], [37, 37, "field"]], "ner_mapping_to_source": [2, 3, 4, 5], "relations": [[16, 18, 21, 27, "physical", "", false, false], [16, 18, 21, 27, "role", "", false, false]], "relations_mapping_to_source": [6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "the", "recurrent", "and", "deep", "feed", "-", "forward", "neural", "networks", "developed", "by", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "AI", "Lab", "IDSIA", "won", "eight", "awards", "in", "international", "competitions", "for", "pattern", "recognition", "and", "machine", "learning", "Awards", "."], "sentence-detokenized": "Between 2009 and 2012, the recurrent and deep feed-forward neural networks developed by J\u00fcrgen Schmidhuber's research group at the Swiss AI Lab IDSIA won eight awards in international competitions for pattern recognition and machine learning Awards.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 26], [27, 36], [37, 40], [41, 45], [46, 50], [50, 51], [51, 58], [59, 65], [66, 74], [75, 84], [85, 87], [88, 94], [95, 106], [106, 108], [109, 117], [118, 123], [124, 126], [127, 130], [131, 136], [137, 139], [140, 143], [144, 149], [150, 153], [154, 159], [160, 166], [167, 169], [170, 183], [184, 196], [197, 200], [201, 208], [209, 220], [221, 224], [225, 232], [233, 241], [242, 248], [248, 249]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [11, 12, "product"], [14, 15, "product"], [6, 6, "task"], [9, 10, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 11, 12, "usage", "", false, false], [1, 3, 14, 15, "usage", "", false, false], [1, 3, 6, 6, "usage", "", true, false], [1, 3, 9, 10, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "support", "speech", "synthesis", "and", "voice", "using", "SAPI", "4", "and", "SAPI", "5", "components", "."], "sentence-detokenized": "Modern Windows desktop systems can support speech synthesis and voice using SAPI 4 and SAPI 5 components.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 42], [43, 49], [50, 59], [60, 63], [64, 69], [70, 75], [76, 80], [81, 82], [83, 86], [87, 91], [92, 93], [94, 104], [104, 105]]}
{"doc_key": "ai-train-31", "ner": [[3, 8, "misc"], [9, 10, "field"], [13, 15, "university"], [21, 25, "field"], [26, 29, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 8, 9, 10, "topic", "topic_of_award", false, false], [3, 8, 13, 15, "origin", "", true, false], [21, 25, 26, 29, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "the", "S.V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "PhD", "in", "industrial", "design", "and", "engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received the S.V. della laurea ad honorem in psychology from the University of Padua in 1995 and a PhD in industrial design and engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 20], [21, 26], [27, 33], [34, 36], [37, 44], [45, 47], [48, 58], [59, 63], [64, 67], [68, 78], [79, 81], [82, 87], [88, 90], [91, 95], [96, 99], [100, 101], [102, 105], [106, 108], [109, 119], [120, 126], [127, 130], [131, 142], [143, 147], [148, 153], [154, 164], [165, 167], [168, 178], [178, 179]]}
{"doc_key": "ai-train-32", "ner": [[2, 10, "researcher"], [16, 20, "location"], [0, 0, "researcher"], [32, 33, "misc"], [47, 49, "misc"], [65, 67, "misc"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6], "relations": [[0, 0, 32, 33, "related-to", "works_with", true, false], [0, 0, 47, 49, "related-to", "works_with", true, false], [0, 0, 65, 67, "related-to", "works_with", true, false]], "relations_mapping_to_source": [3, 4, 5], "sentence": ["Dehaene", ",", "together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Hospital", "Paris", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", ",", "also", "identified", "patients", "with", "lesions", "in", "different", "areas", "of", "the", "parietal", "lobes", ",", "with", "impaired", "multiplication", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "to", "the", "inferior", "parietal", "lobule", ")", "and", "with", "impaired", "subtraction", "but", "patients", "with", "preserved", "multiplication", "(", "associated", "with", "lesions", "to", "the", "intraparietal", "sulcus", ")", "were", "also", "identified", "."], "sentence-detokenized": "Dehaene, together with his long-time collaborator Laurent Cohen, a neurologist at the Hospital Paris Piti\u00e9-Salp\u00eatri\u00e8re, also identified patients with lesions in different areas of the parietal lobes, with impaired multiplication but preserved subtraction (associated with lesions to the inferior parietal lobule) and with impaired subtraction but patients with preserved multiplication (associated with lesions to the intraparietal sulcus) were also identified.", "token2charspan": [[0, 7], [7, 8], [9, 17], [18, 22], [23, 26], [27, 31], [31, 32], [32, 36], [37, 49], [50, 57], [58, 63], [63, 64], [65, 66], [67, 78], [79, 81], [82, 85], [86, 94], [95, 100], [101, 106], [106, 107], [107, 118], [118, 119], [120, 124], [125, 135], [136, 144], [145, 149], [150, 157], [158, 160], [161, 170], [171, 176], [177, 179], [180, 183], [184, 192], [193, 198], [198, 199], [200, 204], [205, 213], [214, 228], [229, 232], [233, 242], [243, 254], [255, 256], [256, 266], [267, 271], [272, 279], [280, 282], [283, 286], [287, 295], [296, 304], [305, 311], [311, 312], [313, 316], [317, 321], [322, 330], [331, 342], [343, 346], [347, 355], [356, 360], [361, 370], [371, 385], [386, 387], [387, 397], [398, 402], [403, 410], [411, 413], [414, 417], [418, 431], [432, 438], [438, 439], [440, 444], [445, 449], [450, 460], [460, 461]]}
{"doc_key": "ai-train-33", "ner": [[6, 12, "product"], [14, 17, "misc"], [19, 20, "misc"], [27, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 17, 6, 12, "topic", "", false, false], [19, 20, 6, 12, "topic", "", false, false], [27, 27, 6, 12, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "artificial", "intelligence", "robots", ",", "such", "as", "the", "films", "A.I", ".", "Artificial", "Intelligence", ",", "Ex", "Machina", "and", "the", "2016", "TV", "adaptation", "of", "Westworld", ",", "have", "generated", "audience", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of artificial intelligence robots, such as the films A.I. Artificial Intelligence, Ex Machina and the 2016 TV adaptation of Westworld, have generated audience sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 54], [55, 67], [68, 74], [74, 75], [76, 80], [81, 83], [84, 87], [88, 93], [94, 97], [97, 98], [99, 109], [110, 122], [122, 123], [124, 126], [127, 134], [135, 138], [139, 142], [143, 147], [148, 150], [151, 161], [162, 164], [165, 174], [174, 175], [176, 180], [181, 190], [191, 199], [200, 208], [209, 212], [213, 216], [217, 223], [224, 234], [234, 235]]}
{"doc_key": "ai-train-34", "ner": [[4, 6, "field"], [8, 9, "algorithm"], [7, 13, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[8, 9, 4, 6, "part-of", "", false, false], [7, 13, 4, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "main", "methods", "used", "in", "unsupervised", "learning", "include", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "The main methods used in unsupervised learning include principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 8], [9, 16], [17, 21], [22, 24], [25, 37], [38, 46], [47, 54], [55, 64], [65, 74], [75, 83], [84, 87], [88, 95], [96, 104], [104, 105]]}
{"doc_key": "ai-train-35", "ner": [[0, 4, "organisation"], [22, 23, "misc"], [28, 29, "misc"], [31, 33, "person"], [37, 39, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[22, 23, 0, 4, "artifact", "", false, false], [28, 29, 0, 4, "artifact", "", false, false], [28, 29, 31, 33, "role", "director_of", false, false], [28, 29, 37, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "more", "prominently", "in", "special", "occasions", "to", "impress", "audiences", ",", "for", "example", "in", "Magic", "Journey", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films more prominently in special occasions to impress audiences, for example in Magic Journey (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 55], [56, 67], [68, 70], [71, 78], [79, 88], [89, 91], [92, 99], [100, 109], [109, 110], [111, 114], [115, 122], [123, 125], [126, 131], [132, 139], [140, 141], [141, 145], [145, 146], [147, 150], [151, 158], [159, 161], [162, 163], [163, 170], [171, 175], [176, 183], [183, 184], [185, 189], [189, 190], [191, 199], [200, 207], [208, 215], [215, 216], [216, 217]]}
{"doc_key": "ai-train-36", "ner": [[10, 15, "field"], [19, 24, "task"], [16, 26, "task"], [28, 28, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 10, 15, "part-of", "", false, false], [16, 26, 10, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "learning", "has", "been", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron learning has been popular in the field of natural language processing for tasks such as part-of-speech tagging and parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 40], [41, 48], [49, 51], [52, 55], [56, 61], [62, 64], [65, 72], [73, 81], [82, 92], [93, 96], [97, 102], [103, 107], [108, 110], [111, 115], [115, 116], [116, 118], [118, 119], [119, 125], [126, 133], [134, 137], [138, 145], [146, 147], [147, 154], [154, 155], [156, 160], [160, 161], [161, 162]]}
{"doc_key": "ai-train-37", "ner": [[2, 4, "product"], [5, 30, "organisation"], [24, 24, "organisation"], [25, 28, "country"], [16, 21, "product"], [31, 34, "researcher"], [39, 39, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[5, 30, 2, 4, "role", "introduces_to_market", true, false], [24, 24, 2, 4, "role", "introduces_to_market", true, false], [24, 24, 25, 28, "physical", "", false, false], [16, 21, 39, 39, "related-to", "sold_to", true, false], [31, 34, 16, 21, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "launched", "by", "Fuji", "Spinning", "Co.", "in", "1963", ",", "and", "the", "first", "programmable", "universal", "machine", "for", "assembly", "was", "invented", "by", "KUKA", ",", "Germany", ",", "in", "1976", "by", "Victor", "Scheinman", ",", "whose", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was launched by Fuji Spinning Co. in 1963, and the first programmable universal machine for assembly was invented by KUKA, Germany, in 1976 by Victor Scheinman, whose design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 40], [41, 43], [44, 48], [49, 57], [58, 61], [62, 64], [65, 69], [69, 70], [71, 74], [75, 78], [79, 84], [85, 97], [98, 107], [108, 115], [116, 119], [120, 128], [129, 132], [133, 141], [142, 144], [145, 149], [149, 150], [151, 158], [158, 159], [160, 162], [163, 167], [168, 170], [171, 177], [178, 187], [187, 188], [189, 194], [195, 201], [202, 205], [206, 210], [211, 213], [214, 223], [223, 224]]}
{"doc_key": "ai-train-38", "ner": [[7, 8, "conference"], [10, 10, "researcher"], [15, 16, "field"], [19, 22, "researcher"], [29, 32, "researcher"], [39, 40, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[10, 10, 7, 8, "role", "president_of", false, false], [10, 10, 19, 22, "role", "colleagues", false, false], [15, 16, 39, 40, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "while", "president", "of", "the", "AAAI", ",", "Hayes", "began", "cynically", "attacking", "critics", "of", "AI", "and", ",", "with", "colleague", "Kenneth", "Ford", ",", "initiated", "an", "award", "named", "after", "Simon", "Newcombe", "for", "the", "most", "ridiculous", "argument", "against", "the", "potential", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, while president of the AAAI, Hayes began cynically attacking critics of AI and, with colleague Kenneth Ford, initiated an award named after Simon Newcombe for the most ridiculous argument against the potential of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 33], [34, 36], [37, 40], [41, 45], [45, 46], [47, 52], [53, 58], [59, 68], [69, 78], [79, 86], [87, 89], [90, 92], [93, 96], [96, 97], [98, 102], [103, 112], [113, 120], [121, 125], [125, 126], [127, 136], [137, 139], [140, 145], [146, 151], [152, 157], [158, 163], [164, 172], [173, 176], [177, 180], [181, 185], [186, 196], [197, 205], [206, 213], [214, 217], [218, 227], [228, 230], [231, 233], [233, 234]]}
{"doc_key": "ai-train-39", "ner": [[10, 14, "algorithm"], [40, 41, "algorithm"], [38, 52, "algorithm"], [55, 58, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[10, 14, 40, 41, "named", "same", false, false], [38, 52, 10, 14, "type-of", "", false, false], [55, 58, 10, 14, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "of", "math", "\u03b1", "/", "math", "can", "be", "found", "using", "line", "search", "algorithms", ".", "That", "is", ",", "the", "size", "of", "math", "\u03b1", "/", "math", "can", "be", "determined", "by", "finding", "the", "value", "that", "minimises", "S", ",", "usually", "using", "a", "line", "search", "on", "the", "interval", "math0\u03b11", "/", "math", "or", "a", "backtracking", "line", "search", "such", "as", "Armijo", "-", "line", "search", "."], "sentence-detokenized": "The optimal value of math \u03b1 / math can be found using line search algorithms. That is, the size of math \u03b1 / math can be determined by finding the value that minimises S, usually using a line search on the interval math0\u03b11 / math or a backtracking line search such as Armijo-line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 20], [21, 25], [26, 27], [28, 29], [30, 34], [35, 38], [39, 41], [42, 47], [48, 53], [54, 58], [59, 65], [66, 76], [76, 77], [78, 82], [83, 85], [85, 86], [87, 90], [91, 95], [96, 98], [99, 103], [104, 105], [106, 107], [108, 112], [113, 116], [117, 119], [120, 130], [131, 133], [134, 141], [142, 145], [146, 151], [152, 156], [157, 166], [167, 168], [168, 169], [170, 177], [178, 183], [184, 185], [186, 190], [191, 197], [198, 200], [201, 204], [205, 213], [214, 221], [222, 223], [224, 228], [229, 231], [232, 233], [234, 246], [247, 251], [252, 258], [259, 263], [264, 266], [267, 273], [273, 274], [274, 278], [279, 285], [285, 286]]}
{"doc_key": "ai-train-40", "ner": [[6, 6, "product"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["However", ",", "the", "results", "conclude", "that", "expert", "systems", "embody", "a", "lot", "of", "technical", "knowledge", ",", "but", "shed", "little", "light", "on", "the", "mental", "processes", "that", "humans", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "However, the results conclude that expert systems embody a lot of technical knowledge, but shed little light on the mental processes that humans use to solve such puzzles.", "token2charspan": [[0, 7], [7, 8], [9, 12], [13, 20], [21, 29], [30, 34], [35, 41], [42, 49], [50, 56], [57, 58], [59, 62], [63, 65], [66, 75], [76, 85], [85, 86], [87, 90], [91, 95], [96, 102], [103, 108], [109, 111], [112, 115], [116, 122], [123, 132], [133, 137], [138, 144], [145, 148], [149, 151], [152, 157], [158, 162], [163, 170], [170, 171]]}
{"doc_key": "ai-train-41", "ner": [[0, 0, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deal", "with", "the", "use", "of", "computers", "to", "understand", "and", "produce", "spoken", "language", "."], "sentence-detokenized": "Speech recognition and speech synthesis deal with the use of computers to understand and produce spoken language.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 44], [45, 49], [50, 53], [54, 57], [58, 60], [61, 70], [71, 73], [74, 84], [85, 88], [89, 96], [97, 103], [104, 112], [112, 113]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"], [33, 35, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "theta", "^", "{", "*}", "is", "/", "is", "usually", "estimated", "using", "the", "maximum", "likelihood", "method", "(", "math", "Filter", "^", "{", "*}", "=", "Theta", "^", "{", "ML", "}", "/", "math", ")", "or", "the", "maximum", "posterior", "method", "(", "math", "Filter", "^", "{", "*}", "=", "Theta", "^", "{", "MAP", "}", "/", "math", ")", "."], "sentence-detokenized": "This math theta ^ {*} is / is usually estimated using the maximum likelihood method (math Filter ^ {*} = Theta ^ {ML} / math) or the maximum posterior method (math Filter ^ {*} = Theta ^ {MAP} / math).", "token2charspan": [[0, 4], [5, 9], [10, 15], [16, 17], [18, 19], [19, 21], [22, 24], [25, 26], [27, 29], [30, 37], [38, 47], [48, 53], [54, 57], [58, 65], [66, 76], [77, 83], [84, 85], [85, 89], [90, 96], [97, 98], [99, 100], [100, 102], [103, 104], [105, 110], [111, 112], [113, 114], [114, 116], [116, 117], [118, 119], [120, 124], [124, 125], [126, 128], [129, 132], [133, 140], [141, 150], [151, 157], [158, 159], [159, 163], [164, 170], [171, 172], [173, 174], [174, 176], [177, 178], [179, 184], [185, 186], [187, 188], [188, 191], [191, 192], [193, 194], [195, 199], [199, 200], [200, 201]]}
{"doc_key": "ai-train-43", "ner": [[4, 13, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Less", "widely", "used", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesiser", ",", "which", "has", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Less widely used languages use the open-source eSpeak synthesiser, which has a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 11], [12, 16], [17, 26], [27, 30], [31, 34], [35, 39], [39, 40], [40, 46], [47, 53], [54, 65], [65, 66], [67, 72], [73, 76], [77, 78], [79, 86], [86, 87], [88, 95], [96, 101], [102, 106], [107, 110], [111, 113], [114, 123], [124, 126], [127, 137], [137, 138]]}
{"doc_key": "ai-train-44", "ner": [[0, 1, "programlang"], [37, 37, "programlang"], [40, 40, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 37, 37, "compare", "", false, false], [0, 1, 40, 40, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["R", "is", "mainly", "used", "by", "statisticians", "and", "other", "practitioners", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "but", "it", "also", "works", "as", "a", "toolbox", "for", "general", "matrix", "calculations", "and", "has", "performance", "benchmarks", "comparable", "to", "GNU", "Octave", "and", "MATLAB", "."], "sentence-detokenized": "R is mainly used by statisticians and other practitioners who need an environment for statistical computing and software development, but it also works as a toolbox for general matrix calculations and has performance benchmarks comparable to GNU Octave and MATLAB.", "token2charspan": [[0, 1], [2, 4], [5, 11], [12, 16], [17, 19], [20, 33], [34, 37], [38, 43], [44, 57], [58, 61], [62, 66], [67, 69], [70, 81], [82, 85], [86, 97], [98, 107], [108, 111], [112, 120], [121, 132], [132, 133], [134, 137], [138, 140], [141, 145], [146, 151], [152, 154], [155, 156], [157, 164], [165, 168], [169, 176], [177, 183], [184, 196], [197, 200], [201, 204], [205, 216], [217, 227], [228, 238], [239, 241], [242, 245], [246, 252], [253, 256], [257, 263], [263, 264]]}
{"doc_key": "ai-train-45", "ner": [[3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["Heterodyne", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", ",", "which", "creates", "a", "new", "frequency", "by", "mixing", "and", "combining", "two", "frequencies", "."], "sentence-detokenized": "Heterodyne is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden, which creates a new frequency by mixing and combining two frequencies.", "token2charspan": [[0, 10], [11, 13], [14, 15], [16, 22], [23, 33], [34, 43], [44, 52], [53, 55], [56, 64], [65, 73], [74, 77], [78, 86], [87, 95], [96, 105], [105, 106], [107, 112], [113, 120], [121, 122], [123, 126], [127, 136], [137, 139], [140, 146], [147, 150], [151, 160], [161, 164], [165, 176], [176, 177]]}
{"doc_key": "ai-train-46", "ner": [[9, 11, "person"], [12, 12, "misc"], [15, 17, "organisation"], [20, 21, "organisation"], [22, 25, "misc"], [26, 27, "person"], [29, 30, "organisation"], [31, 48, "misc"], [33, 33, "person"], [36, 37, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[9, 11, 12, 12, "role", "actor_in", false, false], [12, 12, 15, 17, "artifact", "", false, false], [22, 25, 20, 21, "artifact", "", false, false], [26, 27, 22, 25, "role", "actor_in", false, false], [31, 48, 29, 30, "artifact", "", false, false], [33, 33, 31, 48, "role", "actor_in", false, false], [36, 37, 31, 48, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "month", "brought", "3D", "back", "into", "the", "limelight", "with", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Brothers", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", ",", "Paramount", "'s", "Money", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "From", "Home", "'", ",", "which", "starred", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "The month brought 3D back into the limelight with John Wayne's Hondo (distributed by Warner Brothers), Columbia's Miss Sadie Thompson with Rita Hayworth, Paramount's Money with Dean Martin and Jerry Lewis From Home', which starred Dean Martin and Jerry Lewis.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 25], [26, 30], [31, 34], [35, 44], [45, 49], [50, 54], [55, 60], [60, 62], [63, 68], [69, 70], [70, 81], [82, 84], [85, 91], [92, 100], [100, 101], [101, 102], [103, 111], [111, 113], [114, 118], [119, 124], [125, 133], [134, 138], [139, 143], [144, 152], [152, 153], [154, 163], [163, 165], [166, 171], [172, 176], [177, 181], [182, 188], [189, 192], [193, 198], [199, 204], [205, 209], [210, 214], [214, 215], [215, 216], [217, 222], [223, 230], [231, 235], [236, 242], [243, 246], [247, 252], [253, 258], [258, 259]]}
{"doc_key": "ai-train-47", "ner": [[3, 4, "field"], [5, 6, "task"], [11, 11, "organisation"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "face", "recognition", "system", "created", "by", "a", "Facebook", "research", "group", "."], "sentence-detokenized": "DeepFace is a deep learning face recognition system created by a Facebook research group.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 32], [33, 44], [45, 51], [52, 59], [60, 62], [63, 64], [65, 73], [74, 82], [83, 88], [88, 89]]}
{"doc_key": "ai-train-48", "ner": [[0, 4, "field"], [9, 9, "conference"], [13, 14, "field"], [25, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 4, 13, 14, "part-of", "subfield", false, false], [9, 9, 0, 4, "topic", "", false, false], [25, 27, 0, 4, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "also", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "premier", "computer", "graphics", "conference", ",", "and", "the", "main", "topic", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometry processing is also a common research topic at SIGGRAPH, the premier computer graphics conference, and the main topic of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 27], [28, 29], [30, 36], [37, 45], [46, 51], [52, 54], [55, 63], [63, 64], [65, 68], [69, 76], [77, 85], [86, 94], [95, 105], [105, 106], [107, 110], [111, 114], [115, 119], [120, 125], [126, 128], [129, 132], [133, 139], [140, 149], [150, 152], [153, 161], [162, 172], [172, 173]]}
{"doc_key": "ai-train-49", "ner": [[5, 5, "task"], [8, 10, "task"], [16, 20, "algorithm"], [24, 24, "algorithm"], [23, 27, "algorithm"], [12, 36, "algorithm"], [37, 37, "misc"], [42, 45, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 7, 8, 9], "relations": [[24, 24, 37, 37, "general-affiliation", "", false, false], [23, 27, 24, 24, "named", "", false, false]], "relations_mapping_to_source": [2, 3], "sentence": ["It", "is", "possible", "to", "perform", "feature", "extraction", "and", "dimensionality", "reduction", "at", "once", "using", "methods", "such", "as", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "and", "canonical", "correlation", "analysis", "(", "CCA", ")", "as", "pre-processing", ",", "and", "clustering", "by", "k", "-", "NN", "on", "the", "feature", "vector", "after", "dimensionality", "reduction", "."], "sentence-detokenized": "It is possible to perform feature extraction and dimensionality reduction at once using methods such as principal component analysis (PCA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA) as pre-processing, and clustering by k -NN on the feature vector after dimensionality reduction.", "token2charspan": [[0, 2], [3, 5], [6, 14], [15, 17], [18, 25], [26, 33], [34, 44], [45, 48], [49, 63], [64, 73], [74, 76], [77, 81], [82, 87], [88, 95], [96, 100], [101, 103], [104, 113], [114, 123], [124, 132], [133, 134], [134, 137], [137, 138], [138, 139], [140, 146], [147, 159], [160, 168], [169, 170], [170, 173], [173, 174], [175, 178], [179, 188], [189, 200], [201, 209], [210, 211], [211, 214], [214, 215], [216, 218], [219, 233], [233, 234], [235, 238], [239, 249], [250, 252], [253, 254], [255, 256], [256, 258], [259, 261], [262, 265], [266, 273], [274, 280], [281, 286], [287, 301], [302, 311], [311, 312]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 9, "field"], [7, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 9, "related-to", "good_at", true, false], [0, 2, 7, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "at", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel at machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[36, 40, "misc"], [16, 17, "conference"], [18, 22, "conference"], [0, 51, "algorithm"], [53, 54, "researcher"], [56, 58, "researcher"], [48, 66, "misc"], [68, 76, "conference"], [77, 79, "conference"]], "ner_mapping_to_source": [2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[36, 40, 16, 17, "temporal", "", false, false], [18, 22, 16, 17, "named", "", false, false], [48, 66, 0, 51, "topic", "", false, false], [48, 66, 53, 54, "artifact", "", false, false], [48, 66, 56, 58, "artifact", "", false, false], [48, 66, 68, 76, "temporal", "", false, false], [77, 79, 68, 76, "named", "", false, false]], "relations_mapping_to_source": [2, 3, 4, 5, 6, 7, 8], "sentence": ["N", ".", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "Oriented", "Gradients", "for", "Human", "Detection", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "et", "al", ",", "as", "a", "trainable", "system", "for", "pedestrian", "detection", "using", "local", "features", "such", "as", "Histograms", "of", "Oriented", "Gradients", ".", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "Oriented", "Gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", ",", "which", "are", "local", "features", "for", "pedestrian", "detection", "."], "sentence-detokenized": "N. Dalal, B. Triggs, Histograms of Oriented Gradients for Human Detection, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 et al, as a trainable system for pedestrian detection using local features such as Histograms of Oriented Gradients. N. Dalal, B. Triggs, Histograms of Oriented Gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors, which are local features for pedestrian detection.", "token2charspan": [[0, 1], [1, 2], [3, 8], [8, 9], [10, 11], [11, 12], [13, 19], [19, 20], [21, 31], [32, 34], [35, 43], [44, 53], [54, 57], [58, 63], [64, 73], [73, 74], [75, 88], [89, 96], [97, 99], [100, 108], [109, 115], [116, 117], [117, 121], [121, 122], [122, 123], [124, 129], [130, 131], [131, 132], [133, 135], [135, 136], [136, 138], [138, 139], [140, 144], [145, 147], [148, 150], [150, 151], [152, 154], [155, 156], [157, 166], [167, 173], [174, 177], [178, 188], [189, 198], [199, 204], [205, 210], [211, 219], [220, 224], [225, 227], [228, 238], [239, 241], [242, 250], [251, 260], [260, 261], [262, 264], [265, 270], [270, 271], [272, 273], [273, 274], [275, 281], [281, 282], [283, 293], [294, 296], [297, 305], [306, 315], [316, 319], [320, 325], [326, 335], [335, 336], [337, 341], [342, 350], [351, 358], [359, 369], [370, 372], [373, 381], [382, 388], [389, 392], [393, 400], [401, 412], [413, 414], [414, 418], [418, 419], [419, 420], [421, 426], [427, 428], [428, 429], [430, 437], [437, 438], [439, 443], [444, 455], [455, 456], [457, 462], [463, 466], [467, 472], [473, 481], [482, 485], [486, 496], [497, 506], [506, 507]]}
{"doc_key": "ai-train-52", "ner": [[0, 0, "algorithm"], [4, 7, "algorithm"], [11, 12, "task"], [13, 14, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 4, 7, "type-of", "", false, false], [11, 12, 0, 0, "usage", "", true, false], [11, 12, 13, 14, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Autoencoders", "are", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "features", "in", "unsupervised", "learning", "."], "sentence-detokenized": "Autoencoders are a type of artificial neural network used to learn features in unsupervised learning.", "token2charspan": [[0, 12], [13, 16], [17, 18], [19, 23], [24, 26], [27, 37], [38, 44], [45, 52], [53, 57], [58, 60], [61, 66], [67, 75], [76, 78], [79, 91], [92, 100], [100, 101]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [3, 5, "organisation"], [8, 8, "field"], [11, 12, "field"], [15, 21, "organisation"], [27, 27, "field"], [30, 31, "field"], [24, 34, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 5, 6, 7, 8], "relations": [[0, 2, 3, 5, "role", "fellow_of", false, false], [0, 2, 8, 8, "related-to", "contributes_to", false, false], [0, 2, 11, 12, "related-to", "contributes_to", false, false], [0, 2, 27, 27, "related-to", "contributes_to", false, false], [0, 2, 30, 31, "related-to", "contributes_to", false, false]], "relations_mapping_to_source": [0, 1, 2, 4, 5], "sentence": ["She", "is", "an", "IEEE", "Fellow", "for", "contributions", "in", "computer", "vision", "and", "image", "processing", "and", "an", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "Fellow", "for", "contributions", "in", "pattern", "recognition", "and", "image", "processing", "and", "to", "IAPR", "."], "sentence-detokenized": "She is an IEEE Fellow for contributions in computer vision and image processing and an International Association for Pattern Recognition (IAPR) Fellow for contributions in pattern recognition and image processing and to IAPR.", "token2charspan": [[0, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 25], [26, 39], [40, 42], [43, 51], [52, 58], [59, 62], [63, 68], [69, 79], [80, 83], [84, 86], [87, 100], [101, 112], [113, 116], [117, 124], [125, 136], [137, 138], [138, 142], [142, 143], [144, 150], [151, 154], [155, 168], [169, 171], [172, 179], [180, 191], [192, 195], [196, 201], [202, 212], [213, 216], [217, 219], [220, 224], [224, 225]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [12, 16, "algorithm"], [23, 23, "researcher"], [26, 27, "organisation"], [29, 32, "researcher"], [34, 36, "university"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6], "relations": [[23, 23, 26, 27, "physical", "", false, false], [23, 23, 26, 27, "role", "", false, false], [29, 32, 34, 36, "physical", "", false, false], [29, 32, 34, 36, "role", "", false, false]], "relations_mapping_to_source": [4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "the", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "-", "based", "system", "presented", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was the Connectionist Temporal Classification (CTC)-based system presented by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 43], [44, 57], [58, 66], [67, 81], [82, 83], [83, 86], [86, 87], [87, 88], [88, 93], [94, 100], [101, 110], [111, 113], [114, 118], [119, 125], [126, 128], [129, 135], [136, 144], [145, 148], [149, 156], [157, 163], [164, 166], [167, 170], [171, 181], [182, 184], [185, 192], [193, 195], [196, 200], [200, 201]]}
{"doc_key": "ai-train-55", "ner": [[0, 7, "algorithm"], [11, 11, "algorithm"], [10, 13, "algorithm"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[10, 13, 11, 11, "named", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["Linear", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear fractional programming (LFP) is a generalisation of linear programming (LP).", "token2charspan": [[0, 6], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 13, "misc"], [14, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 13, "win-defeat", "", false, false], [8, 13, 14, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "won", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "2011", "&", "2012", "."], "sentence-detokenized": "Lafferty has won numerous awards, including two Test-of-Time awards at the International Conference on Machine Learning 2011 & 2012.", "token2charspan": [[0, 8], [9, 12], [13, 16], [17, 25], [26, 32], [32, 33], [34, 43], [44, 47], [48, 52], [52, 53], [53, 55], [55, 56], [56, 60], [61, 67], [68, 70], [71, 74], [75, 88], [89, 99], [100, 102], [103, 110], [111, 119], [120, 124], [125, 126], [127, 131], [131, 132]]}
{"doc_key": "ai-train-57", "ner": [[0, 0, "product"], [3, 5, "programlang"], [20, 21, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": [".", "NET", ",", "Java", "and", "other", "component", "-", "based", "frameworks", ",", "component", "-", "based", "development", "environments", "can", "now", "deploy", "developed", "neural", "networks", "as", "inheritable", "components", "in", "these", "frameworks", "."], "sentence-detokenized": ". NET, Java and other component-based frameworks, component-based development environments can now deploy developed neural networks as inheritable components in these frameworks.", "token2charspan": [[0, 1], [2, 5], [5, 6], [7, 11], [12, 15], [16, 21], [22, 31], [31, 32], [32, 37], [38, 48], [48, 49], [50, 59], [59, 60], [60, 65], [66, 77], [78, 90], [91, 94], [95, 98], [99, 105], [106, 115], [116, 122], [123, 131], [132, 134], [135, 146], [147, 157], [158, 160], [161, 166], [167, 177], [177, 178]]}
{"doc_key": "ai-train-58", "ner": [[1, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "and", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "figure", ")", "between", "two", "sentences", ",", "a", "candidate", "translation", "string", "and", "a", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, and the algorithm first creates an alignment (see figure) between two sentences, a candidate translation string and a reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 67], [68, 77], [78, 83], [84, 91], [92, 94], [95, 104], [105, 106], [106, 109], [110, 116], [116, 117], [118, 125], [126, 129], [130, 139], [139, 140], [141, 142], [143, 152], [153, 164], [165, 171], [172, 175], [176, 177], [178, 187], [188, 199], [200, 206], [206, 207]]}
{"doc_key": "ai-train-59", "ner": [[0, 8, "conference"], [14, 18, "task"], [27, 28, "metrics"], [30, 36, "metrics"], [41, 46, "conference"], [49, 49, "location"], [51, 51, "country"]], "ner_mapping_to_source": [0, 2, 3, 4, 6, 7, 8], "relations": [[0, 8, 14, 18, "related-to", "subject_at", false, false], [27, 28, 0, 8, "temporal", "", false, false], [30, 36, 27, 28, "named", "", true, false], [49, 49, 51, 51, "physical", "", false, false]], "relations_mapping_to_source": [1, 2, 3, 5], "sentence": ["At", "NIST", "'s", "annual", "Document", "Understanding", "Conference", ",", "where", "research", "groups", "submit", "systems", "for", "both", "summarisation", "and", "translation", "tasks", ",", "one", "of", "the", "metrics", "used", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", ")", "."], "sentence-detokenized": "At NIST's annual Document Understanding Conference, where research groups submit systems for both summarisation and translation tasks, one of the metrics used is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014).", "token2charspan": [[0, 2], [3, 7], [7, 9], [10, 16], [17, 25], [26, 39], [40, 50], [50, 51], [52, 57], [58, 66], [67, 73], [74, 80], [81, 88], [89, 92], [93, 97], [98, 111], [112, 115], [116, 127], [128, 133], [133, 134], [135, 138], [139, 141], [142, 145], [146, 153], [154, 158], [159, 161], [162, 165], [166, 171], [172, 178], [179, 180], [180, 186], [186, 187], [187, 195], [196, 206], [207, 210], [211, 218], [219, 229], [229, 230], [231, 233], [234, 242], [243, 245], [246, 252], [253, 264], [265, 275], [276, 283], [284, 285], [285, 289], [289, 290], [290, 291], [292, 300], [300, 301], [302, 308], [308, 309], [310, 318], [319, 320], [321, 325], [325, 326], [326, 327]]}
{"doc_key": "ai-train-60", "ner": [[5, 6, "programlang"], [6, 8, "product"], [10, 11, "programlang"], [16, 16, "product"], [22, 22, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 10, 11, "type-of", "", false, false], [5, 6, 22, 22, "named", "", false, false], [6, 8, 10, 11, "part-of", "", false, false], [6, 8, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["To", "run", "the", "same", "implementation", "in", "Java", "using", "JShell", "(", "Java", "9", "and", "above", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "To run the same implementation in Java using JShell (Java 9 and above): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 2], [3, 6], [7, 10], [11, 15], [16, 30], [31, 33], [34, 38], [39, 44], [45, 51], [52, 53], [53, 57], [58, 59], [60, 63], [64, 69], [69, 70], [70, 71], [72, 82], [83, 93], [94, 95], [96, 115], [116, 120], [121, 122], [123, 127]]}
{"doc_key": "ai-train-61", "ner": [[0, 3, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 3, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "indicators", "are", "based", "on", "the", "BLEU", "indicators", ",", "but", "with", "slight", "modifications", "."], "sentence-detokenized": "The NIST indicators are based on the BLEU indicators, but with slight modifications.", "token2charspan": [[0, 3], [4, 8], [9, 19], [20, 23], [24, 29], [30, 32], [33, 36], [37, 41], [42, 52], [52, 53], [54, 57], [58, 62], [63, 69], [70, 83], [83, 84]]}
{"doc_key": "ai-train-62", "ner": [[16, 16, "country"], [8, 10, "university"], [22, 30, "product"], [31, 32, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[8, 10, 16, 16, "physical", "", false, false], [22, 30, 8, 10, "origin", "", false, false], [22, 30, 31, 32, "type-of", "", false, false]], "relations_mapping_to_source": [0, 2, 4], "sentence": ["In", "the", "late", "1980s", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", "in", "the", "Netherlands", "started", "a", "joi", "nt", "project", "called", "'", "Knowledge", "Graphs", "'", ".", "This", "is", "a", "semantic", "network", ",", "but", "with", "the", "added", "restriction", "that", "edges", "are", "restricted", "to", "a", "limited", "set", "of", "possible", "relations", "in", "order", "to", "facilitate", "algebra", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, the University of Groningen and the University of Twente in the Netherlands started a joint project called 'Knowledge Graphs'. This is a semantic network, but with the added restriction that edges are restricted to a limited set of possible relations in order to facilitate algebra on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 33], [34, 36], [37, 46], [47, 50], [51, 54], [55, 65], [66, 68], [69, 75], [76, 78], [79, 82], [83, 94], [95, 102], [103, 104], [105, 108], [108, 110], [111, 118], [119, 125], [126, 127], [127, 136], [137, 143], [143, 144], [144, 145], [146, 150], [151, 153], [154, 155], [156, 164], [165, 172], [172, 173], [174, 177], [178, 182], [183, 186], [187, 192], [193, 204], [205, 209], [210, 215], [216, 219], [220, 230], [231, 233], [234, 235], [236, 243], [244, 247], [248, 250], [251, 259], [260, 269], [270, 272], [273, 278], [279, 281], [282, 292], [293, 300], [301, 303], [304, 307], [308, 313], [313, 314]]}
{"doc_key": "ai-train-63", "ner": [[0, 2, "product"], [12, 16, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 12, 16, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "larger", "programmes", ",", "such", "as", "word", "processors", ",", "but", "they", "are", "also", "available", "as", "stand", "-", "alone", "applications", "that", "can", "be", "launched", "from", "within", "programmes", "dealing", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of larger programmes, such as word processors, but they are also available as stand-alone applications that can be launched from within programmes dealing with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 66], [67, 77], [77, 78], [79, 83], [84, 86], [87, 91], [92, 102], [102, 103], [104, 107], [108, 112], [113, 116], [117, 121], [122, 131], [132, 134], [135, 140], [140, 141], [141, 146], [147, 159], [160, 164], [165, 168], [169, 171], [172, 180], [181, 185], [186, 192], [193, 203], [204, 211], [212, 216], [217, 225], [226, 230], [230, 231]]}
{"doc_key": "ai-train-64", "ner": [[0, 9, "organisation"], [12, 17, "conference"], [18, 20, "organisation"], [29, 31, "conference"], [33, 35, "conference"], [37, 40, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["Member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Society", "for", "Artificial", "Intelligence", "and", "the", "Cognitive", "Science", "Society", ",", "and", "serves", "on", "the", "editorial", "boards", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", "and", "J.", "Applied", "Ontology", "journals", "."], "sentence-detokenized": "Member of the American Association for the Advancement of Science, the Society for Artificial Intelligence and the Cognitive Science Society, and serves on the editorial boards of J. Automated Reasoning, J. Learning Sciences and J. Applied Ontology journals.", "token2charspan": [[0, 6], [7, 9], [10, 13], [14, 22], [23, 34], [35, 38], [39, 42], [43, 54], [55, 57], [58, 65], [65, 66], [67, 70], [71, 78], [79, 82], [83, 93], [94, 106], [107, 110], [111, 114], [115, 124], [125, 132], [133, 140], [140, 141], [142, 145], [146, 152], [153, 155], [156, 159], [160, 169], [170, 176], [177, 179], [180, 182], [183, 192], [193, 202], [202, 203], [204, 206], [207, 215], [216, 224], [225, 228], [229, 231], [232, 239], [240, 248], [249, 257], [257, 258]]}
{"doc_key": "ai-train-65", "ner": [[0, 7, "algorithm"], [9, 13, "task"], [19, 21, "researcher"], [22, 23, "university"], [25, 26, "researcher"], [30, 31, "organisation"], [28, 29, "organisation"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6, 7], "relations": [[19, 21, 22, 23, "physical", "", false, false], [19, 21, 22, 23, "role", "", false, false], [25, 26, 30, 31, "role", "", false, false], [28, 29, 30, 31, "part-of", "", false, false]], "relations_mapping_to_source": [4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", ",", "a", "type", "of", "speech", "coding", ",", "was", "first", "developed", "in", "1966", "by", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "."], "sentence-detokenized": "Linear Predictive Coding (LPC), a type of speech coding, was first developed in 1966 by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT).", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 60], [61, 66], [67, 76], [77, 79], [80, 84], [85, 87], [88, 96], [97, 104], [105, 107], [108, 114], [115, 125], [126, 129], [130, 135], [136, 141], [142, 144], [145, 151], [152, 161], [162, 165], [166, 175], [176, 177], [177, 180], [180, 181], [181, 182]]}
{"doc_key": "ai-train-66", "ner": [[55, 58, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Furthermore", ",", "for", "ergodic", "signals", ",", "all", "sample", "paths", "show", "the", "same", "time", "average", ",", "so", "that", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "mean", "square", "error", "sense", "."], "sentence-detokenized": "Furthermore, for ergodic signals, all sample paths show the same time average, so that mathR _ x ^ {n / T _ 0} (\\ tau) = widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the mean square error sense.", "token2charspan": [[0, 11], [11, 12], [13, 16], [17, 24], [25, 32], [32, 33], [34, 37], [38, 44], [45, 50], [51, 55], [56, 59], [60, 64], [65, 69], [70, 77], [77, 78], [79, 81], [82, 86], [87, 92], [93, 94], [95, 96], [97, 98], [99, 100], [100, 101], [102, 103], [104, 105], [106, 107], [108, 109], [109, 110], [111, 112], [112, 113], [114, 117], [117, 118], [119, 120], [121, 128], [129, 130], [130, 131], [131, 132], [133, 134], [135, 136], [137, 138], [139, 140], [140, 141], [142, 143], [144, 145], [146, 147], [148, 149], [149, 150], [151, 152], [152, 153], [154, 157], [157, 158], [159, 160], [161, 165], [166, 168], [169, 172], [173, 177], [178, 184], [185, 190], [191, 196], [196, 197]]}
{"doc_key": "ai-train-67", "ner": [[5, 5, "task"], [8, 10, "task"], [13, 17, "algorithm"], [21, 22, "algorithm"], [20, 24, "algorithm"], [27, 31, "algorithm"], [12, 40, "algorithm"], [41, 41, "misc"], [46, 49, "algorithm"], [50, 52, "misc"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 7, 9, 10, 11, 12], "relations": [[21, 22, 41, 41, "related-to", "", false, false], [20, 24, 21, 22, "named", "", false, false], [46, 49, 50, 52, "related-to", "", true, false]], "relations_mapping_to_source": [2, 3, 8], "sentence": ["It", "is", "possible", "to", "perform", "feature", "extraction", "and", "dimensionality", "reduction", "at", "once", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "and", "non-negative", "matrix", "factorisation", "(", "NMF", ")", "as", "pre-processing", ",", "and", "clustering", "by", "K", "-", "NN", "on", "the", "feature", "vector", "after", "dimensionality", "reduction", "."], "sentence-detokenized": "It is possible to perform feature extraction and dimensionality reduction at once using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) and non-negative matrix factorisation (NMF) as pre-processing, and clustering by K-NN on the feature vector after dimensionality reduction.", "token2charspan": [[0, 2], [3, 5], [6, 14], [15, 17], [18, 25], [26, 33], [34, 44], [45, 48], [49, 63], [64, 73], [74, 76], [77, 81], [82, 87], [88, 97], [98, 107], [108, 116], [117, 118], [118, 121], [121, 122], [122, 123], [124, 130], [131, 143], [144, 152], [153, 154], [154, 157], [157, 158], [158, 159], [160, 169], [170, 181], [182, 190], [191, 192], [192, 195], [195, 196], [197, 200], [201, 213], [214, 220], [221, 234], [235, 236], [236, 239], [239, 240], [241, 243], [244, 258], [258, 259], [260, 263], [264, 274], [275, 277], [278, 279], [279, 280], [280, 282], [283, 285], [286, 289], [290, 297], [298, 304], [305, 310], [311, 325], [326, 335], [335, 336]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "and", ".NET", "can", "be", "called", "directly", "from", "MATLAB", "."], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX and .NET can be called directly from MATLAB.", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 44], [45, 49], [50, 53], [54, 56], [57, 63], [64, 72], [73, 77], [78, 84], [84, 85]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [10, 12, "task"], [26, 28, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 10, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognising", "named", "entities", "in", "text", "is", "called", "Named", "Entity", "Recognition", "and", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "in", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognising named entities in text is called Named Entity Recognition and the task of determining the identity of named entities in text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 46], [47, 49], [50, 56], [57, 62], [63, 69], [70, 81], [82, 85], [86, 89], [90, 94], [95, 97], [98, 109], [110, 113], [114, 122], [123, 125], [126, 131], [132, 140], [141, 143], [144, 148], [149, 151], [152, 158], [159, 165], [166, 173], [173, 174]]}
{"doc_key": "ai-train-70", "ner": [[0, 1, "algorithm"], [27, 27, "programlang"], [23, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 23, 30, "part-of", "", true, false], [23, 30, 27, 27, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "this", "package", "were", "originally", "included", "in", "the", "package", ",", "but", "since", "version", "0.8.0", "they", "have", "been", "published", "as", "a", "separate", "R", "package", "sigmoid", "for", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in this package were originally included in the package, but since version 0.8.0 they have been published as a separate R package sigmoid for more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 50], [51, 58], [59, 63], [64, 74], [75, 83], [84, 86], [87, 90], [91, 98], [98, 99], [100, 103], [104, 109], [110, 117], [118, 123], [124, 128], [129, 133], [134, 138], [139, 148], [149, 151], [152, 153], [154, 162], [163, 164], [165, 172], [173, 180], [181, 184], [185, 189], [190, 197], [198, 201], [201, 202]]}
{"doc_key": "ai-train-71", "ner": [[0, 1, "programlang"], [10, 12, "organisation"], [12, 16, "organisation"], [17, 17, "location"], [18, 21, "location"], [22, 23, "researcher"], [25, 26, "researcher"], [28, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 1, 22, 23, "artifact", "", true, false], [0, 1, 25, 26, "artifact", "", true, false], [0, 1, 28, 29, "artifact", "", true, false], [12, 16, 10, 12, "named", "", false, false], [12, 16, 17, 17, "physical", "", false, false], [17, 17, 18, 21, "physical", "", false, false], [22, 23, 10, 12, "role", "", false, false], [25, 26, 10, 12, "role", "", false, false], [28, 29, 10, 12, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "at", "the", "research", "firm", "Bolt", "Beranek", "Newman", "(", "BBN", ")", "in", "Cambridge", ",", "Massachusetts", ",", "by", "Wally", "Foerzeg", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The logo was created in 1967 at the research firm Bolt Beranek Newman (BBN) in Cambridge, Massachusetts, by Wally Foerzeg, Cynthia Solomon and Seymour Papert.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 35], [36, 44], [45, 49], [50, 54], [55, 62], [63, 69], [70, 71], [71, 74], [74, 75], [76, 78], [79, 88], [88, 89], [90, 103], [103, 104], [105, 107], [108, 113], [114, 121], [121, 122], [123, 130], [131, 138], [139, 142], [143, 150], [151, 157], [157, 158]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [17, 20, "field"], [21, 24, "algorithm"], [25, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 17, 20, "compare", "", false, false], [21, 24, 17, 20, "part-of", "", false, false], [25, 27, 17, 20, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "contrasted", "with", "traditional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "for", "neural", "networks", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and can be contrasted with traditional deep learning techniques that use gradient descent for neural networks with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 86], [87, 89], [90, 100], [101, 105], [106, 117], [118, 122], [123, 131], [132, 142], [143, 147], [148, 151], [152, 160], [161, 168], [169, 172], [173, 179], [180, 188], [189, 193], [194, 195], [196, 201], [202, 210], [210, 211]]}
{"doc_key": "ai-train-73", "ner": [[41, 45, "algorithm"], [54, 55, "metrics"], [52, 57, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[52, 57, 54, 55, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["For", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "a", "function", "of", "the", "form", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "can", "be", "fitted", "by", "the", "least", "squares", "method", "to", "assess", "goodness", "of", "fit", "in", "terms", "of", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "For data (x sub i / sub, y sub i / sub) 1 \u2264 i \u2264n / sub, a function of the form hyperplane \u0177 = a + \u03b2 supT / sup x can be fitted by the least squares method to assess goodness of fit in terms of mean squared error (MSE).", "token2charspan": [[0, 3], [4, 8], [9, 10], [10, 11], [12, 15], [16, 17], [18, 19], [20, 23], [23, 24], [25, 26], [27, 30], [31, 32], [33, 34], [35, 38], [38, 39], [40, 41], [42, 43], [44, 45], [46, 48], [49, 50], [51, 54], [54, 55], [56, 57], [58, 66], [67, 69], [70, 73], [74, 78], [79, 89], [90, 91], [92, 93], [94, 95], [96, 97], [98, 99], [100, 104], [105, 106], [107, 110], [111, 112], [113, 116], [117, 119], [120, 126], [127, 129], [130, 133], [134, 139], [140, 147], [148, 154], [155, 157], [158, 164], [165, 173], [174, 176], [177, 180], [181, 183], [184, 189], [190, 192], [193, 197], [198, 205], [206, 211], [212, 213], [213, 216], [216, 217], [217, 218]]}
{"doc_key": "ai-train-74", "ner": [[5, 5, "country"], [7, 7, "country"], [9, 9, "country"], [11, 11, "country"], [13, 13, "country"], [15, 15, "country"], [17, 17, "country"], [19, 19, "country"], [21, 21, "country"], [23, 23, "country"], [25, 25, "country"], [27, 27, "country"], [29, 29, "country"], [31, 31, "country"], [33, 33, "country"], [35, 36, "country"], [38, 38, "country"], [40, 40, "country"], [42, 42, "country"], [44, 44, "country"], [47, 47, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["It", "has", "overseas", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "UK", "."], "sentence-detokenized": "It has overseas offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the UK.", "token2charspan": [[0, 2], [3, 6], [7, 15], [16, 23], [24, 26], [27, 36], [36, 37], [38, 44], [44, 45], [46, 52], [52, 53], [54, 59], [59, 60], [61, 68], [68, 69], [70, 75], [75, 76], [77, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 107], [107, 108], [109, 115], [115, 116], [117, 125], [125, 126], [127, 138], [138, 139], [140, 146], [146, 147], [148, 157], [157, 158], [159, 164], [165, 171], [171, 172], [173, 178], [178, 179], [180, 186], [186, 187], [188, 196], [196, 197], [198, 204], [205, 208], [209, 212], [213, 215], [215, 216]]}
{"doc_key": "ai-train-75", "ner": [[0, 21, "misc"], [5, 9, "field"], [14, 17, "university"], [27, 29, "organisation"], [32, 34, "university"], [39, 40, "university"], [42, 43, "university"], [46, 47, "university"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 8], "relations": [[0, 21, 5, 9, "topic", "", false, false], [0, 21, 14, 17, "origin", "", false, false]], "relations_mapping_to_source": [0, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", ",", "and", "has", "held", "full", "-", "time", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering from Inria and the University of Nice Sophia Antipolis, and has held full-time positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech and visiting positions at Rutgers University, Yale University and University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 58], [59, 64], [65, 68], [69, 72], [73, 83], [84, 86], [87, 91], [92, 98], [99, 108], [108, 109], [110, 113], [114, 117], [118, 122], [123, 127], [127, 128], [128, 132], [133, 142], [143, 145], [146, 153], [154, 163], [164, 174], [174, 175], [176, 181], [182, 185], [186, 191], [192, 201], [202, 205], [206, 214], [215, 224], [225, 227], [228, 235], [236, 246], [246, 247], [248, 252], [253, 263], [264, 267], [268, 278], [279, 281], [282, 289], [289, 290]]}
{"doc_key": "ai-train-76", "ner": [[8, 9, "researcher"], [0, 0, "researcher"], [17, 20, "product"], [21, 23, "country"], [12, 13, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 8, 9, "role", "licensing_patent_to", false, false], [0, 0, 21, 23, "physical", "", false, false], [12, 13, 0, 0, "artifact", "", false, false], [12, 13, 17, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberger", "licensed", "the", "original", "patent", "granted", "to", "inventor", "George", "DeVol", "and", "developed", "the", "Unimate", ",", "the", "first", "industrial", "robot", "in", "the", "USA", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Engelberger licensed the original patent granted to inventor George DeVol and developed the Unimate, the first industrial robot in the USA, in the 1950s.", "token2charspan": [[0, 11], [12, 20], [21, 24], [25, 33], [34, 40], [41, 48], [49, 51], [52, 60], [61, 67], [68, 73], [74, 77], [78, 87], [88, 91], [92, 99], [99, 100], [101, 104], [105, 110], [111, 121], [122, 127], [128, 130], [131, 134], [135, 138], [138, 139], [140, 142], [143, 146], [147, 152], [152, 153]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [3, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[0, 0, "programlang"], [3, 3, "programlang"], [15, 15, "programlang"], [6, 6, "programlang"], [26, 26, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 15, 15, "named", "", false, false], [3, 3, 0, 0, "origin", "descendant_of", false, false], [3, 3, 6, 6, "general-affiliation", "", false, false], [3, 3, 26, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["CLIPS", "descendants", "include", "Jess", "(", "a", "Java", "rewrite", "of", "the", "rule", "-", "based", "part", "of", "CLIPS", ",", "which", "later", "evolved", "in", "a", "different", "direction", ")", "and", "JESS", ",", "which", "was", "originally"], "sentence-detokenized": "CLIPS descendants include Jess (a Java rewrite of the rule-based part of CLIPS, which later evolved in a different direction) and JESS, which was originally", "token2charspan": [[0, 5], [6, 17], [18, 25], [26, 30], [31, 32], [32, 33], [34, 38], [39, 46], [47, 49], [50, 53], [54, 58], [58, 59], [59, 64], [65, 69], [70, 72], [73, 78], [78, 79], [80, 85], [86, 91], [92, 99], [100, 102], [103, 104], [105, 114], [115, 124], [124, 125], [126, 129], [130, 134], [134, 135], [136, 141], [142, 145], [146, 156]]}
{"doc_key": "ai-train-79", "ner": [[51, 51, "product"], [3, 6, "product"], [9, 10, "organisation"], [14, 15, "product"], [53, 60, "product"], [39, 42, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 5, 6], "relations": [[3, 6, 51, 51, "type-of", "", false, false], [9, 10, 3, 6, "usage", "", false, false], [14, 15, 9, 10, "artifact", "", false, false], [53, 60, 9, 10, "origin", "", true, false], [53, 60, 39, 42, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 5, 6], "sentence": ["It", "also", "designed", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "self", "-", "guided", "vehicle", ")", "used", "for", "complex", "pick", "and", "place", "tasks", "and", "to", "move", "products", "from", "process", "to", "process", "in", "a", "non-linear", "layout", "in", "a", "first", "class", "automotive", "supply", "plant", "Developed", "flexible", "intelligent", "AGV", "applications", "linked", "to", "gantry", "systems", "and", "industrial", "robotic", "arms", "."], "sentence-detokenized": "It also designed the Motivity control system used by RMT Robotics to develop the ADAM iAGV (self-guided vehicle) used for complex pick and place tasks and to move products from process to process in a non-linear layout in a first class automotive supply plant Developed flexible intelligent AGV applications linked to gantry systems and industrial robotic arms.", "token2charspan": [[0, 2], [3, 7], [8, 16], [17, 20], [21, 29], [30, 37], [38, 44], [45, 49], [50, 52], [53, 56], [57, 65], [66, 68], [69, 76], [77, 80], [81, 85], [86, 90], [91, 92], [92, 96], [96, 97], [97, 103], [104, 111], [111, 112], [113, 117], [118, 121], [122, 129], [130, 134], [135, 138], [139, 144], [145, 150], [151, 154], [155, 157], [158, 162], [163, 171], [172, 176], [177, 184], [185, 187], [188, 195], [196, 198], [199, 200], [201, 211], [212, 218], [219, 221], [222, 223], [224, 229], [230, 235], [236, 246], [247, 253], [254, 259], [260, 269], [270, 278], [279, 290], [291, 294], [295, 307], [308, 314], [315, 317], [318, 324], [325, 332], [333, 336], [337, 347], [348, 355], [356, 360], [360, 361]]}
{"doc_key": "ai-train-80", "ner": [[7, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameter", "\u03b2", "is", "generally", "estimated", "by", "the", "maximum", "likelihood", "method", "."], "sentence-detokenized": "The parameter \u03b2 is generally estimated by the maximum likelihood method.", "token2charspan": [[0, 3], [4, 13], [14, 15], [16, 18], [19, 28], [29, 38], [39, 41], [42, 45], [46, 53], [54, 64], [65, 71], [71, 72]]}
{"doc_key": "ai-train-81", "ner": [[0, 2, "task"], [7, 7, "metrics"], [3, 10, "metrics"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[7, 7, 0, 2, "part-of", "", false, false], [3, 10, 0, 2, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["Information", "retrieval", "indicators", "such", "as", "accuracy", ",", "reproducibility", "and", "DCG", "are", "useful", "for", "assessing", "the", "quality", "of", "recommendation", "methods", "."], "sentence-detokenized": "Information retrieval indicators such as accuracy, reproducibility and DCG are useful for assessing the quality of recommendation methods.", "token2charspan": [[0, 11], [12, 21], [22, 32], [33, 37], [38, 40], [41, 49], [49, 50], [51, 66], [67, 70], [71, 74], [75, 78], [79, 85], [86, 89], [90, 99], [100, 103], [104, 111], [112, 114], [115, 129], [130, 137], [137, 138]]}
{"doc_key": "ai-train-82", "ner": [[7, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "hundreds", "of", "industrial", "robots", "work", "on", "fully", "automated", "production", "lines", ",", "which", "means", "that", "there", "is", "one", "robot", "for", "every", "10", "humans", "."], "sentence-detokenized": "In a typical factory, hundreds of industrial robots work on fully automated production lines, which means that there is one robot for every 10 humans.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 30], [31, 33], [34, 44], [45, 51], [52, 56], [57, 59], [60, 65], [66, 75], [76, 86], [87, 92], [92, 93], [94, 99], [100, 105], [106, 110], [111, 116], [117, 119], [120, 123], [124, 129], [130, 133], [134, 139], [140, 142], [143, 149], [149, 150]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [13, 14, "field"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [30, 31, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[13, 14, 5, 5, "usage", "", false, true], [18, 19, 13, 14, "part-of", "", false, false], [21, 22, 13, 14, "part-of", "", false, false], [24, 25, 13, 14, "part-of", "", false, false], [27, 28, 13, 14, "part-of", "", false, false], [30, 31, 13, 14, "part-of", "", false, false], [33, 34, 13, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNN", "has", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", ",", "including", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "expansion", "and", "noise", "removal", "."], "sentence-detokenized": "Over the past decade, PCNN has been used in a variety of image processing applications, including image segmentation, feature generation, face extraction, motion detection, region expansion and noise removal.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 26], [27, 30], [31, 35], [36, 40], [41, 43], [44, 45], [46, 53], [54, 56], [57, 62], [63, 73], [74, 86], [86, 87], [88, 97], [98, 103], [104, 116], [116, 117], [118, 125], [126, 136], [136, 137], [138, 142], [143, 153], [153, 154], [155, 161], [162, 171], [171, 172], [173, 179], [180, 189], [190, 193], [194, 199], [200, 207], [207, 208]]}
{"doc_key": "ai-train-84", "ner": [[15, 17, "field"], [20, 25, "misc"], [26, 30, "conference"], [28, 33, "conference"], [46, 51, "misc"], [38, 42, "conference"], [43, 43, "conference"], [52, 54, "conference"], [55, 57, "conference"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[20, 25, 26, 30, "temporal", "", false, false], [28, 33, 26, 30, "named", "", false, false], [46, 51, 38, 42, "temporal", "", false, false], [46, 51, 52, 54, "temporal", "", false, false], [43, 43, 38, 42, "named", "", false, false], [55, 57, 52, 54, "named", "", false, false]], "relations_mapping_to_source": [3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", ",", "including", "the", "Best", "Paper", "Award", "at", "the", "International", "Conference", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", ",", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "Best", "Reviewer", "Award", "at", "the", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision, including the Best Paper Award at the International Conference Non-Photorealistic Rendering and Animation (NPAR) 2012, the Asian Conference on Computer Vision ACCV 2012 and Best Reviewer Award at the International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [110, 111], [112, 121], [122, 125], [126, 130], [131, 136], [137, 142], [143, 145], [146, 149], [150, 163], [164, 174], [175, 193], [194, 203], [204, 207], [208, 217], [218, 219], [219, 223], [223, 224], [225, 229], [229, 230], [231, 234], [235, 240], [241, 251], [252, 254], [255, 263], [264, 270], [271, 275], [276, 280], [281, 284], [285, 289], [290, 298], [299, 304], [305, 307], [308, 311], [312, 325], [326, 336], [337, 339], [340, 348], [349, 355], [356, 357], [357, 361], [361, 362], [363, 367], [367, 368]]}
{"doc_key": "ai-train-85", "ner": [[5, 5, "field"], [9, 12, "misc"], [13, 13, "researcher"], [16, 18, "misc"]], "ner_mapping_to_source": [2, 3, 4, 5], "relations": [[16, 18, 13, 13, "origin", "", false, false]], "relations_mapping_to_source": [4], "sentence": ["CycL", "in", "Computer", "Science", "and", "Artificial", "Intelligence", "is", "an", "ontology", "language", "used", "in", "Doug", "Lenat", "'s", "Cyc", "Artificial", "Project", "."], "sentence-detokenized": "CycL in Computer Science and Artificial Intelligence is an ontology language used in Doug Lenat's Cyc Artificial Project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 67], [68, 76], [77, 81], [82, 84], [85, 89], [90, 95], [95, 97], [98, 101], [102, 112], [113, 120], [120, 121]]}
{"doc_key": "ai-train-86", "ner": [[0, 2, "task"], [4, 6, "metrics"], [12, 15, "metrics"], [17, 24, "metrics"], [31, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[4, 6, 0, 2, "part-of", "", false, false], [12, 15, 4, 6, "named", "", false, false], [17, 24, 4, 6, "named", "", false, false], [31, 37, 4, 6, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "regression", "analysis", ",", "mean", "squared", "error", ",", "often", "referred", "to", "as", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "also", "refer", "to", "the", "average", "squared", "deviation", "of", "the", "predictions", "from", "the", "true", "values", "over", "the", "out", "-", "of", "-", "sample", "test", "space", "generated", "by", "the", "model", "estimated", "over", "a", "particular", "sample", "space", "."], "sentence-detokenized": "In regression analysis, mean squared error, often referred to as mean squared prediction error or out-of-sample mean squared error, can also refer to the average squared deviation of the predictions from the true values over the out-of-sample test space generated by the model estimated over a particular sample space.", "token2charspan": [[0, 2], [3, 13], [14, 22], [22, 23], [24, 28], [29, 36], [37, 42], [42, 43], [44, 49], [50, 58], [59, 61], [62, 64], [65, 69], [70, 77], [78, 88], [89, 94], [95, 97], [98, 101], [101, 102], [102, 104], [104, 105], [105, 111], [112, 116], [117, 124], [125, 130], [130, 131], [132, 135], [136, 140], [141, 146], [147, 149], [150, 153], [154, 161], [162, 169], [170, 179], [180, 182], [183, 186], [187, 198], [199, 203], [204, 207], [208, 212], [213, 219], [220, 224], [225, 228], [229, 232], [232, 233], [233, 235], [235, 236], [236, 242], [243, 247], [248, 253], [254, 263], [264, 266], [267, 270], [271, 276], [277, 286], [287, 291], [292, 293], [294, 304], [305, 311], [312, 317], [317, 318]]}
{"doc_key": "ai-train-87", "ner": [[5, 7, "algorithm"], [9, 10, "algorithm"], [19, 22, "algorithm"], [34, 35, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 7, 9, 10, "compare", "", false, false], [5, 7, 19, 22, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "results", "show", "that", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparatively", "well", ",", "with", "the", "C", "-", "HOG", "descriptor", "maintaining", "a", "slight", "advantage", "in", "the", "detection", "error", "rate", "when", "the", "FALSE", "positive", "rate", "is", "fixed", "for", "both", "data", "sets", "."], "sentence-detokenized": "The results show that the C-HOG and R-HOG block descriptors perform comparatively well, with the C-HOG descriptor maintaining a slight advantage in the detection error rate when the FALSE positive rate is fixed for both data sets.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 21], [22, 25], [26, 27], [27, 28], [28, 31], [32, 35], [36, 38], [38, 41], [42, 47], [48, 59], [60, 67], [68, 81], [82, 86], [86, 87], [88, 92], [93, 96], [97, 98], [98, 99], [99, 102], [103, 113], [114, 125], [126, 127], [128, 134], [135, 144], [145, 147], [148, 151], [152, 161], [162, 167], [168, 172], [173, 177], [178, 181], [182, 187], [188, 196], [197, 201], [202, 204], [205, 210], [211, 214], [215, 219], [220, 224], [225, 229], [229, 230]]}
{"doc_key": "ai-train-88", "ner": [[6, 8, "algorithm"], [9, 10, "misc"], [12, 14, "algorithm"], [16, 17, "algorithm"], [18, 21, "algorithm"], [23, 25, "algorithm"], [27, 29, "algorithm"], [30, 32, "misc"], [5, 36, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 8, 9, 10, "usage", "", false, false], [12, 14, 30, 32, "usage", "", false, false], [16, 17, 30, 32, "usage", "", false, false], [18, 21, 30, 32, "usage", "", false, false], [23, 25, 30, 32, "usage", "", false, false], [27, 29, 30, 32, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Well", "-", "known", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "Elastic", "matching", "using", "the", "Fisherface", "algorithm", ",", "hidden", "Markov", "models", ",", "multi-linear", "subspace", "learning", "using", "tensor", "representations", "and", "dynamic", "link", "matching", "using", "neurons", "."], "sentence-detokenized": "Well-known recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, Elastic matching using the Fisherface algorithm, hidden Markov models, multi-linear subspace learning using tensor representations and dynamic link matching using neurons.", "token2charspan": [[0, 4], [4, 5], [5, 10], [11, 22], [23, 33], [34, 41], [42, 51], [52, 61], [62, 70], [71, 76], [77, 87], [87, 88], [89, 95], [96, 108], [109, 117], [117, 118], [119, 126], [127, 135], [136, 141], [142, 145], [146, 156], [157, 166], [166, 167], [168, 174], [175, 181], [182, 188], [188, 189], [190, 202], [203, 211], [212, 220], [221, 226], [227, 233], [234, 249], [250, 253], [254, 261], [262, 266], [267, 275], [276, 281], [282, 289], [289, 290]]}
{"doc_key": "ai-train-89", "ner": [[0, 7, "misc"], [31, 33, "location"], [44, 45, "location"], [12, 17, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[31, 33, 0, 7, "temporal", "", false, false], [44, 45, 0, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "distributed", "through", "services", "such", "as", "Netflix", "will", "be", "restricted", "to", "screening", "at", "one", "of", "the", "festival", "'s", "main", "venues", ",", "the", "Scotiabank", "Theatre", "Toronto", ",", "and", "may", "be", "screened", "elsewhere", "(", "such", "as", "the", "TIFF", "Bell", "Lightbox", "and", "local", "cinemas", ")", "The", "festival", "has", "now", "become", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films distributed through services such as Netflix will be restricted to screening at one of the festival's main venues, the Scotiabank Theatre Toronto, and may be screened elsewhere (such as the TIFF Bell Lightbox and local cinemas) The festival has now become.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 77], [78, 85], [86, 94], [95, 99], [100, 102], [103, 110], [111, 115], [116, 118], [119, 129], [130, 132], [133, 142], [143, 145], [146, 149], [150, 152], [153, 156], [157, 165], [165, 167], [168, 172], [173, 179], [179, 180], [181, 184], [185, 195], [196, 203], [204, 211], [211, 212], [213, 216], [217, 220], [221, 223], [224, 232], [233, 242], [243, 244], [244, 248], [249, 251], [252, 255], [256, 260], [261, 265], [266, 274], [275, 278], [279, 284], [285, 292], [292, 293], [294, 297], [298, 306], [307, 310], [311, 314], [315, 321], [321, 322]]}
{"doc_key": "ai-train-90", "ner": [[3, 3, "organisation"], [5, 7, "researcher"], [8, 12, "organisation"], [13, 13, "researcher"], [26, 30, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 3, 8, 12, "related-to", "purchases", false, false], [5, 7, 13, 13, "named", "same", false, false], [8, 12, 5, 7, "origin", "founded_by", false, false], [26, 30, 3, 3, "artifact", "", false, false]], "relations_mapping_to_source": [0, 1, 3, 4], "sentence": ["In", "1977", ",", "Unimation", "acquired", "Victor", "Scheinman", "'s", "Vikarum", "company", "and", ",", "with", "Scheinman", "'s", "help", ",", "developed", "a", "new", "model", "of", "robotic", "arm", ",", "the", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "was", "developed", "and", "put", "into", "production", "."], "sentence-detokenized": "In 1977, Unimation acquired Victor Scheinman's Vikarum company and, with Scheinman's help, developed a new model of robotic arm, the Programmable Universal Machine for Assembly, was developed and put into production.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 18], [19, 27], [28, 34], [35, 44], [44, 46], [47, 54], [55, 62], [63, 66], [66, 67], [68, 72], [73, 82], [82, 84], [85, 89], [89, 90], [91, 100], [101, 102], [103, 106], [107, 112], [113, 115], [116, 123], [124, 127], [127, 128], [129, 132], [133, 145], [146, 155], [156, 163], [164, 167], [168, 176], [176, 177], [178, 181], [182, 191], [192, 195], [196, 199], [200, 204], [205, 215], [215, 216]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [10, 12, "algorithm"], [13, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 10, 12, "origin", "implementation_of", false, false], [0, 1, 13, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "of", "the", "data", "mining", "tool", "Weka", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm of the data mining tool Weka.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 82], [83, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[2, 5, "metrics"], [13, 20, "product"], [21, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[2, 5, 13, 20, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20", "000", "times", "according", "to", "Google", "Scholar", ".", "It", "also", "won", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", "in", "2016", ",", "which", "is", "given", "to", "papers", "that", "have", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20 000 times according to Google Scholar. It also won the IEEE Signal Processing Society Sustained Impact Award in 2016, which is given to papers that have an unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 47], [48, 51], [52, 57], [58, 67], [68, 70], [71, 77], [78, 85], [85, 86], [87, 89], [90, 94], [95, 98], [99, 102], [103, 107], [108, 114], [115, 125], [126, 133], [134, 143], [144, 150], [151, 156], [157, 159], [160, 164], [164, 165], [166, 171], [172, 174], [175, 180], [181, 183], [184, 190], [191, 195], [196, 200], [201, 203], [204, 213], [214, 218], [219, 225], [226, 229], [230, 232], [233, 238], [239, 241], [242, 247], [248, 253], [254, 265], [265, 266]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [21, 24, "product"], [30, 38, "product"], [44, 44, "organisation"], [17, 45, "product"], [42, 43, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 44, 44, "artifact", "", false, false], [21, 24, 0, 1, "related-to", "performs", false, false], [21, 24, 30, 38, "part-of", "", false, false], [44, 44, 42, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "has", "reached", "the", "point", "where", "it", "is", "completely", "indistinguishable", "from", "the", "actual", "human", "voice", ",", "with", "the", "introduction", "of", "Adobe", "Voco", ",", "a", "speech", "editing", "and", "generation", "software", "to", "be", "included", "in", "the", "Adobe", "Creative", "Suite", "in", "2016", ",", "and", "Google", "'s", "DeepMind", "WaveNet", "prototype", "."], "sentence-detokenized": "Speech synthesis has reached the point where it is completely indistinguishable from the actual human voice, with the introduction of Adobe Voco, a speech editing and generation software to be included in the Adobe Creative Suite in 2016, and Google's DeepMind WaveNet prototype.", "token2charspan": [[0, 6], [7, 16], [17, 20], [21, 28], [29, 32], [33, 38], [39, 44], [45, 47], [48, 50], [51, 61], [62, 79], [80, 84], [85, 88], [89, 95], [96, 101], [102, 107], [107, 108], [109, 113], [114, 117], [118, 130], [131, 133], [134, 139], [140, 144], [144, 145], [146, 147], [148, 154], [155, 162], [163, 166], [167, 177], [178, 186], [187, 189], [190, 192], [193, 201], [202, 204], [205, 208], [209, 214], [215, 223], [224, 229], [230, 232], [233, 237], [237, 238], [239, 242], [243, 249], [249, 251], [252, 260], [261, 268], [269, 278], [278, 279]]}
{"doc_key": "ai-train-94", "ner": [[0, 2, "researcher"], [7, 9, "organisation"], [14, 20, "organisation"], [16, 27, "conference"], [34, 36, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 2, 7, 9, "role", "", false, false], [0, 2, 14, 20, "role", "", false, false], [0, 2, 16, 27, "role", "", false, false], [0, 2, 34, 36, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "Honorary", "Member", "of", "the", "Neuroscience", "Research", "Programme", ",", "a", "member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "Fellow", "of", "the", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Brain", "Institute", "."], "sentence-detokenized": "Poggio is an Honorary Member of the Neuroscience Research Programme, a member of the American Academy of Arts and Sciences, a founding Fellow of the AAAI and a founding member of the McGovern Brain Institute.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 67], [67, 68], [69, 70], [71, 77], [78, 80], [81, 84], [85, 93], [94, 101], [102, 104], [105, 109], [110, 113], [114, 122], [122, 123], [124, 125], [126, 134], [135, 141], [142, 144], [145, 148], [149, 153], [154, 157], [158, 159], [160, 168], [169, 175], [176, 178], [179, 182], [183, 191], [192, 197], [198, 207], [207, 208]]}
{"doc_key": "ai-train-95", "ner": [[9, 9, "task"], [12, 15, "task"], [23, 25, "task"], [16, 16, "misc"], [17, 19, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 9, 23, 25, "cause-effect", "", false, false], [12, 15, 23, 25, "cause-effect", "", false, false], [17, 19, 23, 25, "topic", "", false, false], [17, 19, 16, 16, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "boosted", "by", "the", "success", "of", "speech", "recognition", "and", "speech", "synthesis", ",", "the", "German", "Verbmobil", "project", "was", "developed", "and", "research", "into", "speech", "translation", "began", "."], "sentence-detokenized": "In the 1990s, boosted by the success of speech recognition and speech synthesis, the German Verbmobil project was developed and research into speech translation began.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 21], [22, 24], [25, 28], [29, 36], [37, 39], [40, 46], [47, 58], [59, 62], [63, 69], [70, 79], [79, 80], [81, 84], [85, 91], [92, 101], [102, 109], [110, 113], [114, 123], [124, 127], [128, 136], [137, 141], [142, 148], [149, 160], [161, 166], [166, 167]]}
{"doc_key": "ai-train-96", "ner": [[3, 3, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [16, 16, "algorithm"], [18, 24, "algorithm"], [26, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 8, 9, "role", "", false, false], [16, 16, 3, 3, "origin", "", false, false], [16, 16, 8, 9, "origin", "", false, false], [16, 16, 11, 12, "origin", "", false, false], [16, 16, 26, 26, "part-of", "", false, false], [18, 24, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisers", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forget", "gate", "(", "also", "known", "as", "keep", "gate", ")", "in", "the", "LSTM", "architecture", "."], "sentence-detokenized": "In 1999, Felix Gers and his advisers J\u00fcrgen Schmidhuber and Fred Cummins introduced the forget gate (also known as keep gate) in the LSTM architecture.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 87], [88, 94], [95, 99], [100, 101], [101, 105], [106, 111], [112, 114], [115, 119], [120, 124], [124, 125], [126, 128], [129, 132], [133, 137], [138, 150], [150, 151]]}
{"doc_key": "ai-train-97", "ner": [[1, 2, "field"], [5, 6, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 12, 1, 2, "part-of", "", false, false], [9, 12, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalised", "sinc", "function", "is", "generally", "defined", "as"], "sentence-detokenized": "In digital signal processing and information theory, the normalised sinc function is generally defined as", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 94], [95, 102], [103, 105]]}
{"doc_key": "ai-train-98", "ner": [[0, 3, "field"], [9, 12, "researcher"], [18, 20, "conference"], [19, 29, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 4], "relations": [[0, 3, 9, 12, "origin", "coined_term", false, false], [9, 12, 18, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hayes", ",", "a", "founding", "member", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Commission", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hayes, a founding member of the Association for Computational Linguistics and the International Commission on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 73], [73, 74], [75, 76], [77, 85], [86, 92], [93, 95], [96, 99], [100, 111], [112, 115], [116, 129], [130, 141], [142, 145], [146, 149], [150, 163], [164, 174], [175, 177], [178, 191], [192, 203], [204, 205], [205, 209], [209, 210], [210, 211]]}
{"doc_key": "ai-train-99", "ner": [[7, 11, "misc"], [16, 16, "misc"], [22, 23, "metrics"], [20, 27, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[20, 27, 22, 23, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "In", "1D", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPDs", ",", "to", "minimise", "the", "mean", "squared", "error", "(", "MSE", ")", "by", "solving", "for", "the", "polynomial", "coefficients", "of", "the", "digital", "pre-distorter", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "It", "must", "be", "oversampled", "at", "a", "rate", "that", "can", "capture", "the", "non-linear", "product", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. In 1D polynomial-based memory (or memoryless) DPDs, to minimise the mean squared error (MSE) by solving for the polynomial coefficients of the digital pre-distorter, the distorted output of the nonlinear system It must be oversampled at a rate that can capture the non-linear product.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 26], [27, 29], [30, 40], [40, 41], [41, 46], [47, 53], [54, 55], [55, 57], [58, 68], [68, 69], [70, 74], [74, 75], [76, 78], [79, 87], [88, 91], [92, 96], [97, 104], [105, 110], [111, 112], [112, 115], [115, 116], [117, 119], [120, 127], [128, 131], [132, 135], [136, 146], [147, 159], [160, 162], [163, 166], [167, 174], [175, 188], [188, 189], [190, 193], [194, 203], [204, 210], [211, 213], [214, 217], [218, 227], [228, 234], [235, 237], [238, 242], [243, 245], [246, 257], [258, 260], [261, 262], [263, 267], [268, 272], [273, 276], [277, 284], [285, 288], [289, 299], [300, 307], [307, 308]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [13, 15, "location"], [8, 8, "location"], [10, 11, "country"], [35, 41, "organisation"], [30, 34, "organisation"], [42, 42, "location"], [49, 51, "organisation"]], "ner_mapping_to_source": [0, 2, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 30, 34, "physical", "", false, false], [0, 1, 49, 51, "role", "", false, false], [35, 41, 30, 34, "part-of", "", false, false], [30, 34, 42, 42, "physical", "", false, false], [49, 51, 35, 41, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2, 5, 6, 7], "sentence": ["Boris", "Katz", "(", "born", "5", "October", "1947", "in", "Kishtinau", "(", "now", "Moldova", ")", ",", "Moldavian", "SSR", ",", "USSR", ")", "is", "an", "American", "principal", "investigator", "(", "computer", "scientist", ")", "at", "the", "Massachusetts", "Institute", "of", "Technology", "'s", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "in", "Cambridge", ".", "He", "is", "head", "of", "the", "Information", "Lab", "Group", "."], "sentence-detokenized": "Boris Katz (born 5 October 1947 in Kishtinau (now Moldova), Moldavian SSR, USSR) is an American principal investigator (computer scientist) at the Massachusetts Institute of Technology's Computer Science and Artificial Intelligence Laboratory in Cambridge. He is head of the Information Lab Group.", "token2charspan": [[0, 5], [6, 10], [11, 12], [12, 16], [17, 18], [19, 26], [27, 31], [32, 34], [35, 44], [45, 46], [46, 49], [50, 57], [57, 58], [58, 59], [60, 69], [70, 73], [73, 74], [75, 79], [79, 80], [81, 83], [84, 86], [87, 95], [96, 105], [106, 118], [119, 120], [120, 128], [129, 138], [138, 139], [140, 142], [143, 146], [147, 160], [161, 170], [171, 173], [174, 184], [184, 186], [187, 195], [196, 203], [204, 207], [208, 218], [219, 231], [232, 242], [243, 245], [246, 255], [255, 256], [257, 259], [260, 262], [263, 267], [268, 270], [271, 274], [275, 286], [287, 290], [291, 296], [296, 297]]}
