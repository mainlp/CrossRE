{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [15, 16, "field"], [18, 19, "task"], [21, 22, "task"], [26, 28, "task"], [32, 33, "field"], [34, 36, "researcher"], [38, 40, "researcher"], [42, 43, "researcher"], [45, 46, "researcher"], [48, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"], [58, 59, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 15, 16, "part-of", "", false, false], [3, 7, 15, 16, "usage", "", false, false], [3, 7, 18, 19, "part-of", "", false, false], [3, 7, 18, 19, "usage", "", false, false], [3, 7, 21, 22, "part-of", "", false, false], [3, 7, 21, 22, "usage", "", false, false], [3, 7, 32, 33, "part-of", "", false, false], [3, 7, 32, 33, "usage", "", false, false], [26, 28, 21, 22, "part-of", "", false, false], [26, 28, 21, 22, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "opinion", "-", "based", "recommender", "systems", "use", "a", "variety", "of", "techniques", ",", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", ",", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to opinion-based recommender systems use a variety of techniques, including text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis), and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 55], [56, 59], [60, 61], [62, 69], [70, 72], [73, 83], [83, 84], [85, 94], [95, 99], [100, 106], [106, 107], [108, 119], [120, 129], [129, 130], [131, 140], [141, 149], [150, 151], [151, 154], [155, 159], [160, 170], [171, 180], [181, 189], [189, 190], [190, 191], [192, 195], [196, 200], [201, 209], [210, 213], [213, 214], [215, 219], [219, 220], [221, 222], [222, 223], [224, 229], [229, 230], [231, 235], [236, 239], [239, 240], [241, 245], [246, 251], [251, 252], [253, 254], [254, 255], [256, 259], [259, 260], [261, 265], [266, 271], [271, 272], [273, 277], [278, 282], [282, 283], [284, 286], [287, 289], [289, 290], [291, 292], [292, 296], [296, 297], [297, 298], [298, 299], [300, 302], [303, 304], [304, 305], [305, 306], [306, 307], [308, 314], [314, 315]]}
{"doc_key": "ai-train-2", "ner": [[8, 8, "university"], [13, 14, "researcher"], [16, 17, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[13, 14, 8, 8, "physical", "", false, false], [13, 14, 8, 8, "role", "", false, false], [16, 17, 8, 8, "physical", "", false, false], [16, 17, 8, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "proponents", "of", "procedural", "representations", "focused", "mainly", "on", "MIT", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The proponents of procedural representations focused mainly on MIT under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 3], [4, 14], [15, 17], [18, 28], [29, 44], [45, 52], [53, 59], [60, 62], [63, 66], [67, 72], [73, 76], [77, 87], [88, 90], [91, 97], [98, 104], [105, 108], [109, 116], [117, 123], [123, 124]]}
{"doc_key": "ai-train-3", "ner": [[11, 11, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "user", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard user interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 17], [18, 27], [28, 31], [32, 35], [36, 46], [47, 56], [57, 60], [61, 68], [69, 71], [72, 76], [76, 77]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [21, 21, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 21, 21, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "you", "solve", "linear", "and", "non-linear", "problems", "numerically", "and", "perform", "other", "numerical", "experiments", "using", "software", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps you solve linear and non-linear problems numerically and perform other numerical experiments using software that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 16], [17, 22], [23, 29], [30, 33], [34, 44], [45, 53], [54, 65], [66, 69], [70, 77], [78, 83], [84, 93], [94, 105], [106, 111], [112, 120], [121, 125], [126, 128], [129, 135], [136, 146], [147, 151], [152, 158], [158, 159]]}
{"doc_key": "ai-train-5", "ner": [[2, 6, "algorithm"], [8, 9, "misc"], [12, 13, "researcher"], [16, 20, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[2, 6, 12, 13, "origin", "", false, false], [8, 9, 12, 13, "origin", "", false, false], [12, 13, 16, 20, "physical", "", false, false], [12, 13, 16, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variations", "of", "the", "back", "-", "propagation", "algorithm", "and", "unsupervised", "methods", "used", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "train", "deep", ",", "highly", "non-linear", "neuroarchitectures", ",", "{{", "cite", "journal"], "sentence-detokenized": "Variations of the back-propagation algorithm and unsupervised methods used by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly non-linear neuroarchitectures, {{cite journal", "token2charspan": [[0, 10], [11, 13], [14, 17], [18, 22], [22, 23], [23, 34], [35, 44], [45, 48], [49, 61], [62, 69], [70, 74], [75, 77], [78, 83], [84, 90], [91, 94], [95, 105], [106, 108], [109, 112], [113, 123], [124, 126], [127, 134], [135, 138], [139, 141], [142, 146], [147, 149], [150, 155], [156, 160], [160, 161], [162, 168], [169, 179], [180, 198], [198, 199], [200, 202], [202, 206], [207, 214]]}
{"doc_key": "ai-train-6", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "in", "a", "similar", "way", "with", "DCG", "labelling", ":"], "sentence-detokenized": "or in a similar way with DCG labelling:", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 15], [16, 19], [20, 24], [25, 28], [29, 38], [38, 39]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 11, "algorithm"], [14, 17, "algorithm"], [20, 22, "algorithm"], [26, 26, "algorithm"], [27, 29, "algorithm"], [42, 45, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 11, "type-of", "", false, false], [0, 3, 14, 17, "usage", "part-of?", true, false], [14, 17, 20, 22, "compare", "", false, false], [26, 26, 20, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "apply", "competitive", "learning", "(", "as", "opposed", "to", "error", "-correction", "learning", ",", "such", "as", "backpropagation", "with", "gradient", "descent", ")", ",", "and", "in", "that", "they", "use", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps differ from other artificial neural networks in that they apply competitive learning (as opposed to error-correction learning, such as backpropagation with gradient descent), and in that they use a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 84], [85, 96], [97, 105], [106, 107], [107, 109], [110, 117], [118, 120], [121, 126], [126, 137], [138, 146], [146, 147], [148, 152], [153, 155], [156, 171], [172, 176], [177, 185], [186, 193], [193, 194], [194, 195], [196, 199], [200, 202], [203, 207], [208, 212], [213, 216], [217, 218], [219, 231], [232, 240], [241, 243], [244, 252], [253, 256], [257, 268], [269, 279], [280, 282], [283, 286], [287, 292], [293, 298], [298, 299]]}
{"doc_key": "ai-train-8", "ner": [[10, 12, "organisation"], [22, 25, "misc"], [30, 32, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "several", "bodies", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "be", "made", "with", "the", "audio", "signal", "filtered", "out", "in", "the", "noise", "floor", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "mute", "circuits", "."], "sentence-detokenized": "Since the early 1990s, several bodies, including the Audio Engineering Society, have recommended that dynamic range measurements be made with the audio signal filtered out in the noise floor measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or mute circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 58], [59, 70], [71, 78], [78, 79], [80, 84], [85, 96], [97, 101], [102, 109], [110, 115], [116, 128], [129, 131], [132, 136], [137, 141], [142, 145], [146, 151], [152, 158], [159, 167], [168, 171], [172, 174], [175, 178], [179, 184], [185, 190], [191, 202], [203, 207], [208, 210], [211, 220], [221, 228], [229, 234], [234, 235], [236, 240], [241, 247], [248, 260], [261, 273], [274, 279], [280, 282], [283, 286], [287, 290], [291, 293], [294, 299], [300, 305], [306, 308], [309, 313], [314, 322], [322, 323]]}
{"doc_key": "ai-train-9", "ner": [[7, 8, "misc"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 23, "task"], [25, 30, "task"], [32, 34, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[7, 8, 13, 14, "part-of", "concept_used_in", true, false], [7, 8, 16, 17, "part-of", "concept_used_in", false, false], [7, 8, 19, 20, "part-of", "concept_used_in", false, false], [7, 8, 22, 23, "part-of", "concept_used_in", false, false], [7, 8, 25, 30, "part-of", "concept_used_in", false, false], [7, 8, 32, 34, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["The", "technology", "used", "to", "create", "and", "recognise", "distinctive", "faces", "is", "also", "used", "beyond", "facial", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "imaging", "analysis", "."], "sentence-detokenized": "The technology used to create and recognise distinctive faces is also used beyond facial recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation and medical imaging analysis.", "token2charspan": [[0, 3], [4, 14], [15, 19], [20, 22], [23, 29], [30, 33], [34, 43], [44, 55], [56, 61], [62, 64], [65, 69], [70, 74], [75, 81], [82, 88], [89, 100], [100, 101], [102, 113], [114, 125], [125, 126], [127, 130], [131, 138], [138, 139], [140, 145], [146, 157], [157, 158], [159, 163], [164, 172], [172, 173], [173, 177], [178, 185], [186, 200], [201, 204], [205, 212], [213, 220], [221, 229], [229, 230]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [10, 14, "organisation"], [16, 16, "organisation"], [20, 23, "organisation"], [26, 31, "organisation"], [35, 38, "organisation"], [41, 45, "organisation"], [47, 47, "organisation"], [51, 55, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 14, 0, 3, "part-of", "", false, false], [16, 16, 10, 14, "named", "", false, false], [20, 23, 0, 3, "part-of", "", false, false], [26, 31, 0, 3, "part-of", "", false, false], [35, 38, 0, 3, "part-of", "", false, false], [41, 45, 0, 3, "part-of", "", false, false], [47, 47, 41, 45, "named", "", false, false], [51, 55, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "acted", "as", "an", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "(", "NIST", ")", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "to", "coordinate", "studies", "that", "helped", "inform", "strategic", "planners", "'", "thinking", "."], "sentence-detokenized": "The National Science Foundation acted as an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce (NIST), the US Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research to coordinate studies that helped inform strategic planners' thinking.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 37], [38, 40], [41, 43], [44, 52], [53, 56], [57, 60], [61, 69], [70, 81], [82, 85], [86, 91], [92, 106], [107, 108], [108, 112], [112, 113], [113, 114], [115, 118], [119, 121], [122, 132], [133, 135], [136, 142], [142, 143], [144, 147], [148, 150], [151, 161], [162, 164], [165, 173], [174, 175], [175, 179], [179, 180], [180, 181], [182, 185], [186, 188], [189, 199], [200, 202], [203, 210], [210, 211], [212, 215], [216, 223], [224, 232], [233, 241], [242, 250], [251, 257], [258, 259], [259, 264], [264, 265], [266, 269], [270, 273], [274, 280], [281, 283], [284, 289], [290, 298], [299, 301], [302, 312], [313, 320], [321, 325], [326, 332], [333, 339], [340, 349], [350, 358], [358, 359], [360, 368], [368, 369]]}
{"doc_key": "ai-train-11", "ner": [[8, 9, "metrics"], [13, 14, "algorithm"], [0, 1, "researcher"], [19, 20, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 13, 14, "part-of", "", false, false], [0, 1, 19, 20, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Ronald", "Fisher", "proposed", "a", "fast", "method", "for", "calculating", "maximum", "likelihood", "estimates", "of", "the", "probit", "model", "as", "an", "appendix", "to", "Bliss", "in", "1935", "."], "sentence-detokenized": "Ronald Fisher proposed a fast method for calculating maximum likelihood estimates of the probit model as an appendix to Bliss in 1935.", "token2charspan": [[0, 6], [7, 13], [14, 22], [23, 24], [25, 29], [30, 36], [37, 40], [41, 52], [53, 60], [61, 71], [72, 81], [82, 84], [85, 88], [89, 95], [96, 101], [102, 104], [105, 107], [108, 116], [117, 119], [120, 125], [126, 128], [129, 133], [133, 134]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 15, "product"], [21, 21, "product"], [31, 31, "product"]], "ner_mapping_to_source": [0, 1, 3, 5], "relations": [[21, 21, 14, 15, "usage", "uses_software", false, false], [21, 21, 31, 31, "named", "", false, false]], "relations_mapping_to_source": [0, 2], "sentence": ["Several", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", ",", "which", "uses", "AltaVista", "'s", "BabelFish", "(", "as", "of", "9", "May", "2008", ",", "Yahoo", "'s", "Babelfish", ")", "."], "sentence-detokenized": "Several of these programs are available online, such as Google Translate and the SYSTRAN system, which uses AltaVista's BabelFish (as of 9 May 2008, Yahoo's Babelfish).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 39], [40, 46], [46, 47], [48, 52], [53, 55], [56, 62], [63, 72], [73, 76], [77, 80], [81, 88], [89, 95], [95, 96], [97, 102], [103, 107], [108, 117], [117, 119], [120, 129], [130, 131], [131, 133], [134, 136], [137, 138], [139, 142], [143, 147], [147, 148], [149, 154], [154, 156], [157, 166], [166, 167], [167, 168]]}
{"doc_key": "ai-train-13", "ner": [[0, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 24, "field"], [26, 27, "misc"], [32, 33, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 3, 20, 24, "related-to", "", true, false], [0, 3, 26, 27, "related-to", "", true, false], [0, 3, 32, 33, "related-to", "", true, false], [7, 8, 20, 24, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 32, 33, "related-to", "", true, false], [10, 11, 20, 24, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 32, 33, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-", "based", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-based reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 196], [196, 201], [202, 215], [216, 224], [224, 225]]}
{"doc_key": "ai-train-14", "ner": [[13, 19, "metrics"]], "ner_mapping_to_source": [1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "most", "common", "method", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measure", "."], "sentence-detokenized": "The most common method is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 22], [23, 25], [26, 28], [29, 32], [33, 36], [37, 39], [39, 40], [40, 46], [47, 52], [53, 54], [54, 60], [60, 61], [61, 69], [70, 80], [81, 84], [85, 92], [93, 103], [103, 104], [105, 112], [112, 113]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [14, 14, "programlang"], [19, 20, "researcher"], [22, 23, "organisation"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[0, 0, 14, 14, "related-to", "", false, false], [19, 20, 22, 23, "role", "", false, false]], "relations_mapping_to_source": [0, 2], "sentence": ["RapidMiner", "provides", "learning", "plans", ",", "templates", "and", "algorithms", ",", "and", "can", "be", "extended", "with", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides learning plans, templates and algorithms, and can be extended with R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 34], [34, 35], [36, 45], [46, 49], [50, 60], [60, 61], [62, 65], [66, 69], [70, 72], [73, 81], [82, 86], [87, 88], [89, 92], [93, 99], [100, 107], [107, 108], [109, 114], [115, 121], [121, 122], [123, 128], [129, 137], [137, 138], [139, 141], [142, 150], [151, 155], [155, 156]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [13, 14, "task"], [19, 23, "misc"], [40, 45, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 13, 14, "related-to", "", false, false], [0, 0, 40, 45, "related-to", "", true, false], [19, 23, 0, 0, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["tity", "includes", "a", "collection", "of", "visualization", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modeling", ",", "as", "well", "as", "graphical", "user", "interfaces", "to", "make", "these", "functions", "easy", "to", "use", ".", "but", "the", "newer", ",", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "which", "was", "first", "developed", "in", "1997", ",", "is", "now", "used", "in", "a", "wide", "range", "of", "applications", ",", "particularly", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "tity includes a collection of visualization tools and algorithms for data analysis and predictive modeling, as well as graphical user interfaces to make these functions easy to use. but the newer, fully Java-based version (Weka 3), which was first developed in 1997, is now used in a wide range of applications, particularly for educational and research purposes.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 106], [106, 107], [108, 110], [111, 115], [116, 118], [119, 128], [129, 133], [134, 144], [145, 147], [148, 152], [153, 158], [159, 168], [169, 173], [174, 176], [177, 180], [180, 181], [182, 185], [186, 189], [190, 195], [195, 196], [197, 202], [203, 207], [207, 208], [208, 213], [214, 221], [222, 223], [223, 227], [228, 229], [229, 230], [230, 231], [232, 237], [238, 241], [242, 247], [248, 257], [258, 260], [261, 265], [265, 266], [267, 269], [270, 273], [274, 278], [279, 281], [282, 283], [284, 288], [289, 294], [295, 297], [298, 310], [310, 311], [312, 324], [325, 328], [329, 340], [341, 344], [345, 353], [354, 362], [362, 363]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [12, 20, "misc"], [25, 27, "misc"], [29, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 20, 0, 0, "topic", "", false, false], [12, 20, 25, 27, "win-defeat", "", false, false], [25, 27, 29, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "enjoyed", "considerable", "acclaim", "with", "his", "paper", "Heuretics", ":", "The", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", ",", "which", "won", "the", "best", "paper", "award", "at", "the", "1982", "Association", "for", "Advancement", "of", "Artificial", "Intelligence", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and enjoyed considerable acclaim with his paper Heuretics: The Theoretical and Study of Heuristic Rules, which won the best paper award at the 1982 Association for Advancement of Artificial Intelligence.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 53], [54, 66], [67, 74], [75, 79], [80, 83], [84, 89], [90, 99], [99, 100], [101, 104], [105, 116], [117, 120], [121, 126], [127, 129], [130, 139], [140, 145], [145, 146], [147, 152], [153, 156], [157, 160], [161, 165], [166, 171], [172, 177], [178, 180], [181, 184], [185, 189], [190, 201], [202, 205], [206, 217], [218, 220], [221, 231], [232, 244], [244, 245]]}
{"doc_key": "ai-train-18", "ner": [[9, 11, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "take", "account", "of", "multiple", "entities", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To take account of multiple entities, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 7], [8, 15], [16, 18], [19, 27], [28, 36], [36, 37], [38, 39], [40, 48], [49, 54], [55, 59], [60, 62], [63, 73], [74, 77], [78, 82], [83, 90], [90, 91]]}
{"doc_key": "ai-train-19", "ner": [[4, 6, "product"], [8, 10, "product"], [12, 13, "product"], [15, 17, "product"], [19, 21, "product"], [23, 25, "product"], [29, 34, "product"], [36, 37, "product"], [39, 40, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 23, 25, "type-of", "", false, false], [8, 10, 23, 25, "type-of", "", false, false], [12, 13, 23, 25, "type-of", "", false, false], [15, 17, 23, 25, "type-of", "", false, false], [19, 21, 23, 25, "type-of", "", false, false], [36, 37, 29, 34, "type-of", "", false, false], [39, 40, 29, 34, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "conversational", "assistants", "like", "Apple", "'s", "Siri", ",", "Amazon", "'s", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "'s", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "through", "mobile", "devices", "and", "smart", "speakers", "like", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With conversational assistants like Apple's Siri, Amazon's Alexa, Google Assistant, Microsoft's Cortana and Samsung's Bixby, voice portals can now be accessed through mobile devices and smart speakers like Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 19], [20, 30], [31, 35], [36, 41], [41, 43], [44, 48], [48, 49], [50, 56], [56, 58], [59, 64], [64, 65], [66, 72], [73, 82], [82, 83], [84, 93], [93, 95], [96, 103], [104, 107], [108, 115], [115, 117], [118, 123], [123, 124], [125, 130], [131, 138], [139, 142], [143, 146], [147, 149], [150, 158], [159, 166], [167, 173], [174, 181], [182, 185], [186, 191], [192, 200], [201, 205], [206, 212], [213, 217], [218, 221], [222, 228], [229, 233], [233, 234]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [6, 8, "algorithm"], [10, 12, "algorithm"], [14, 15, "algorithm"], [18, 18, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 3, "type-of", "", false, false], [10, 12, 2, 3, "type-of", "", false, false], [14, 15, 2, 3, "type-of", "", false, false], [18, 18, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "include", "the", "Naive", "Bayes", "classifier", ",", "support", "vector", "machines", ",", "Gaussian", "mixtures", "and", "the", "web", "."], "sentence-detokenized": "Examples of supervised learning include the Naive Bayes classifier, support vector machines, Gaussian mixtures and the web.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 39], [40, 43], [44, 49], [50, 55], [56, 66], [66, 67], [68, 75], [76, 82], [83, 91], [91, 92], [93, 101], [102, 110], [111, 114], [115, 118], [119, 122], [122, 123]]}
{"doc_key": "ai-train-21", "ner": [[0, 2, "algorithm"], [26, 30, "algorithm"], [31, 31, "task"], [34, 35, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 26, 30, "part-of", "", true, false], [34, 35, 31, 31, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "OSD", "algorithm", "can", "be", "used", "to", "derive", "the", "mathematical", "O", "(", "\\", "sqrt", "{", "T", "})", "/mathematical", "loss", "bounds", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "of", "the", "classification", "using", "the", "hinge", "loss", "math", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "The OSD algorithm can be used to derive the mathematical O(\\ sqrt {T})/mathematical loss bounds for the online version of the support vector machine of the classification using the hinge loss math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 24], [25, 29], [30, 32], [33, 39], [40, 43], [44, 56], [57, 58], [58, 59], [59, 60], [61, 65], [66, 67], [67, 68], [68, 70], [70, 83], [84, 88], [89, 95], [96, 99], [100, 103], [104, 110], [111, 118], [119, 121], [122, 125], [126, 133], [134, 140], [141, 148], [149, 151], [152, 155], [156, 170], [171, 176], [177, 180], [181, 186], [187, 191], [192, 196], [197, 198], [199, 201], [202, 203], [203, 204], [204, 205], [206, 208], [209, 212], [212, 213], [214, 215], [215, 216], [216, 217], [218, 219], [220, 221], [222, 223], [224, 226], [227, 228], [228, 229], [229, 230], [231, 235], [236, 237], [238, 240], [240, 241], [241, 243], [244, 245], [246, 250]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 26, "task"], [28, 29, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "fusion", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "identification", "of", "wild", "animals", "and", "match", "transfer", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image fusion, 3D modelling, gesture recognition, video tracking, individual identification of wild animals and match transfer.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 85], [85, 86], [87, 89], [90, 99], [99, 100], [101, 108], [109, 120], [120, 121], [122, 127], [128, 136], [136, 137], [138, 148], [149, 163], [164, 166], [167, 171], [172, 179], [180, 183], [184, 189], [190, 198], [198, 199]]}
{"doc_key": "ai-train-23", "ner": [[6, 7, "task"], [10, 11, "university"], [13, 15, "university"], [17, 18, "university"], [20, 21, "university"], [23, 27, "university"], [29, 31, "university"], [33, 35, "university"], [37, 38, "university"], [43, 45, "university"], [47, 47, "university"], [50, 54, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[6, 7, 10, 11, "related-to", "", true, false], [6, 7, 13, 15, "related-to", "", true, false], [6, 7, 17, 18, "related-to", "", true, false], [6, 7, 20, 21, "related-to", "", true, false], [6, 7, 23, 27, "related-to", "", true, false], [6, 7, 29, 31, "related-to", "", true, false], [6, 7, 33, 35, "related-to", "", true, false], [6, 7, 37, 38, "related-to", "", true, false], [6, 7, 43, 45, "related-to", "", true, false], [6, 7, 47, 47, "related-to", "", true, false], [6, 7, 50, 54, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Several", "groups", "and", "companies", "are", "studying", "pose", "estimation", ",", "including", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", "San", "Diego", ",", "University", "of", "Toronto", ",", "Paris", "Central", "School", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "Several groups and companies are studying pose estimation, including Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California San Diego, University of Toronto, Paris Central School, ETH Zurich, National University of Sciences and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 7], [8, 14], [15, 18], [19, 28], [29, 32], [33, 41], [42, 46], [47, 57], [57, 58], [59, 68], [69, 74], [75, 85], [85, 86], [87, 95], [96, 102], [103, 113], [113, 114], [115, 118], [119, 131], [131, 132], [133, 141], [142, 152], [152, 153], [154, 164], [165, 167], [168, 178], [179, 182], [183, 188], [188, 189], [190, 200], [201, 203], [204, 211], [211, 212], [213, 218], [219, 226], [227, 233], [233, 234], [235, 238], [239, 245], [245, 246], [247, 255], [256, 266], [267, 269], [270, 278], [279, 282], [283, 293], [294, 295], [295, 299], [299, 300], [301, 304], [305, 315], [316, 318], [319, 329], [329, 330], [331, 337], [337, 338]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "math", "0.1", "/", "math", "."], "sentence-detokenized": "The sigmoid function Cross entropy loss is used to predict K independent probability values in math 0.1 / math.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 26], [27, 34], [35, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 99], [100, 103], [104, 105], [106, 110], [110, 111]]}
{"doc_key": "ai-train-25", "ner": [[3, 6, "misc"], [7, 7, "field"], [9, 10, "field"], [11, 15, "university"], [16, 18, "country"], [21, 24, "misc"], [25, 29, "university"], [30, 31, "country"], [36, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 6, 7, 7, "topic", "", false, false], [3, 6, 9, 10, "topic", "", false, false], [3, 6, 11, 15, "physical", "", true, false], [11, 15, 16, 18, "physical", "", false, false], [21, 24, 25, 29, "physical", "", true, false], [25, 29, 30, 31, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Professorship", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Professorship", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "Professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Professorship in Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Endowed Professorship at the Tokyo Institute of Technology in Japan before becoming Professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 42], [43, 45], [46, 57], [58, 61], [62, 70], [71, 78], [79, 81], [82, 85], [86, 96], [97, 99], [100, 109], [110, 112], [113, 116], [117, 128], [129, 132], [133, 136], [137, 144], [145, 152], [153, 166], [167, 169], [170, 173], [174, 179], [180, 189], [190, 192], [193, 203], [204, 206], [207, 212], [213, 219], [220, 228], [229, 238], [239, 241], [242, 251], [251, 252]]}
{"doc_key": "ai-train-26", "ner": [[5, 5, "algorithm"], [12, 15, "algorithm"], [20, 21, "researcher"], [23, 24, "researcher"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[5, 5, 12, 15, "usage", "", true, false], [12, 15, 20, 21, "origin", "", false, false], [12, 15, 23, 24, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Another", "technique", "used", "specifically", "for", "recurrent", "neural", "networks", "is", "the", "LSTM", "(", "Long", "Short", "Short", "Memory", ")", "network", "developed", "by", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another technique used specifically for recurrent neural networks is the LSTM (Long Short Short Memory) network developed by Sepp Hochreiter and J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 35], [36, 39], [40, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 77], [78, 79], [79, 83], [84, 89], [90, 95], [96, 102], [102, 103], [104, 111], [112, 121], [122, 124], [125, 129], [130, 140], [141, 144], [145, 151], [152, 163], [164, 166], [167, 171], [171, 172]]}
{"doc_key": "ai-train-27", "ner": [[8, 9, "product"], [15, 15, "product"], [48, 48, "product"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", "onwards", ")", "makes", "this", "package", "very", "versatile", ",", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "mode", "in", "the", "same", "way", "as", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT up to version 5.34, Cling from version 6 onwards) makes this package very versatile, as it can be used in interactive, scripted and compiled mode in the same way as commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 43], [44, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [82, 89], [89, 90], [91, 96], [97, 101], [102, 109], [110, 114], [115, 124], [124, 125], [126, 128], [129, 131], [132, 135], [136, 138], [139, 143], [144, 146], [147, 158], [158, 159], [160, 168], [169, 172], [173, 181], [182, 186], [187, 189], [190, 193], [194, 198], [199, 202], [203, 205], [206, 216], [217, 225], [226, 230], [231, 233], [234, 240], [240, 241]]}
{"doc_key": "ai-train-28", "ner": [[0, 1, "product"], [21, 22, "field"], [27, 28, "task"], [30, 32, "task"], [34, 35, "task"], [37, 38, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 21, 22, "related-to", "", false, false], [27, 28, 21, 22, "part-of", "", false, false], [30, 32, 21, 22, "part-of", "", false, false], [34, 35, 21, 22, "part-of", "", false, false], [37, 38, 21, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Speech", "interfaces", "that", "interpret", "and", "manage", "the", "conversational", "space", "are", "challenging", "to", "design", "because", "of", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "extraction", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Speech interfaces that interpret and manage the conversational space are challenging to design because of the inherent difficulty of integrating complex natural language processing tasks such as coreference extraction, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 6], [7, 17], [18, 22], [23, 32], [33, 36], [37, 43], [44, 47], [48, 62], [63, 68], [69, 72], [73, 84], [85, 87], [88, 94], [95, 102], [103, 105], [106, 109], [110, 118], [119, 129], [130, 132], [133, 144], [145, 152], [153, 160], [161, 169], [170, 180], [181, 186], [187, 191], [192, 194], [195, 206], [207, 217], [217, 218], [219, 224], [225, 231], [232, 243], [243, 244], [245, 256], [257, 266], [267, 270], [271, 279], [280, 290], [290, 291]]}
{"doc_key": "ai-train-29", "ner": [[5, 5, "algorithm"], [9, 9, "algorithm"], [15, 17, "researcher"], [21, 26, "organisation"], [34, 35, "field"], [37, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 5, 15, 17, "origin", "", false, false], [5, 5, 34, 35, "part-of", "", false, false], [5, 5, 37, 38, "part-of", "", false, false], [9, 9, 15, 17, "origin", "", false, false], [9, 9, 34, 35, "part-of", "", false, false], [9, 9, 37, 38, "part-of", "", false, false], [15, 17, 21, 26, "physical", "", false, false], [15, 17, 21, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recursive", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "by", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "IDSIA", ",", "a", "Swiss", "AI", "laboratory", ",", "have", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recursive neural networks and deep feedforward neural networks developed by J\u00fcrgen Schmidhuber's research group at IDSIA, a Swiss AI laboratory, have won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 69], [70, 76], [77, 85], [86, 95], [96, 98], [99, 105], [106, 117], [117, 119], [120, 128], [129, 134], [135, 137], [138, 143], [143, 144], [145, 146], [147, 152], [153, 155], [156, 166], [166, 167], [168, 172], [173, 176], [177, 182], [183, 196], [197, 209], [210, 212], [213, 220], [221, 232], [233, 236], [237, 244], [245, 253], [253, 254]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 11, "misc"], [12, 13, "field"], [14, 19, "university"], [25, 30, "field"], [31, 34, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 11, 12, 13, "topic", "topic_of_award", false, false], [7, 11, 14, 19, "origin", "", true, false], [25, 30, 31, 34, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "doctorates", ",", "one", "S.V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "one", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "the", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary doctorates, one S.V. della laurea ad honorem in psychology from the University of Padua in 1995 and one doctorate in industrial design and engineering from the Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 35], [35, 36], [37, 40], [41, 45], [46, 51], [52, 58], [59, 61], [62, 69], [70, 72], [73, 83], [84, 88], [89, 92], [93, 103], [104, 106], [107, 112], [113, 115], [116, 120], [121, 124], [125, 128], [129, 138], [139, 141], [142, 152], [153, 159], [160, 163], [164, 175], [176, 180], [181, 184], [185, 190], [191, 201], [202, 204], [205, 215], [215, 216]]}
{"doc_key": "ai-train-32", "ner": [[6, 9, "researcher"], [11, 16, "organisation"], [17, 18, "location"], [0, 0, "researcher"], [29, 33, "misc"], [45, 47, "misc"], [62, 64, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[6, 9, 11, 16, "physical", "", false, false], [6, 9, 11, 16, "role", "", false, false], [11, 16, 17, 18, "physical", "", false, false], [0, 0, 29, 33, "related-to", "works_with", true, false], [0, 0, 45, 47, "related-to", "works_with", true, false], [0, 0, 62, 64, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Dehaene", "and", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "who", "had", "impaired", "multiplication", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "in", "the", "inferior", "parietal", "lobe", ")", "and", "others", "with", "impaired", "subtraction", "but", "preserved", "multiplication", "(", "associated", "with", "lesions", "in", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Dehaene and long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, also identified patients with lesions in different regions of the parietal lobe who had impaired multiplication but preserved subtraction (associated with lesions in the inferior parietal lobe) and others with impaired subtraction but preserved multiplication (associated with lesions in the intraparietal sulcus).", "token2charspan": [[0, 7], [8, 11], [12, 16], [16, 17], [17, 21], [22, 34], [35, 42], [43, 48], [48, 49], [50, 51], [52, 63], [64, 66], [67, 70], [71, 76], [76, 77], [77, 88], [89, 97], [98, 100], [101, 106], [106, 107], [108, 112], [113, 123], [124, 132], [133, 137], [138, 145], [146, 148], [149, 158], [159, 166], [167, 169], [170, 173], [174, 182], [183, 187], [188, 191], [192, 195], [196, 204], [205, 219], [220, 223], [224, 233], [234, 245], [246, 247], [247, 257], [258, 262], [263, 270], [271, 273], [274, 277], [278, 286], [287, 295], [296, 300], [300, 301], [302, 305], [306, 312], [313, 317], [318, 326], [327, 338], [339, 342], [343, 352], [353, 367], [368, 369], [369, 379], [380, 384], [385, 392], [393, 395], [396, 399], [400, 413], [414, 420], [420, 421], [421, 422]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [12, 15, "misc"], [17, 18, "misc"], [25, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 15, 6, 8, "topic", "", false, false], [17, 18, 6, 8, "topic", "", false, false], [25, 27, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "artificially", "intelligent", "robots", "in", "the", "films", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "and", "the", "2016", "TV", "adaptation", "Westworld", ",", "have", "generated", "public", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of artificially intelligent robots in the films A.I. Artificial Intelligence and Ex Machina, and the 2016 TV adaptation Westworld, have generated public sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 56], [57, 68], [69, 75], [76, 78], [79, 82], [83, 88], [89, 92], [92, 93], [94, 104], [105, 117], [118, 121], [122, 124], [125, 132], [132, 133], [134, 137], [138, 141], [142, 146], [147, 149], [150, 160], [161, 170], [170, 171], [172, 176], [177, 186], [187, 193], [194, 202], [203, 206], [207, 210], [211, 217], [218, 228], [228, 229]]}
{"doc_key": "ai-train-34", "ner": [[6, 7, "field"], [9, 11, "algorithm"], [13, 14, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 6, 7, "part-of", "", false, false], [13, 14, 6, 7, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "two", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "The two main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 20], [21, 25], [26, 28], [29, 41], [42, 50], [51, 54], [55, 64], [65, 74], [75, 83], [84, 87], [88, 95], [96, 104], [104, 105]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [20, 21, "misc"], [26, 27, "misc"], [29, 31, "person"], [36, 37, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[20, 21, 0, 3, "artifact", "", false, false], [26, 27, 0, 3, "artifact", "", false, false], [26, 27, 29, 31, "role", "director_of", false, false], [26, 27, 36, 37, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "more", "prominently", "in", "special", "events", "to", "engage", "audiences", ",", "with", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "being", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films more prominently in special events to engage audiences, with Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) being notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 55], [56, 67], [68, 70], [71, 78], [79, 85], [86, 88], [89, 95], [96, 105], [105, 106], [107, 111], [112, 117], [118, 126], [127, 128], [128, 132], [132, 133], [134, 137], [138, 145], [146, 148], [149, 150], [150, 157], [158, 162], [163, 170], [170, 171], [172, 176], [176, 177], [178, 186], [187, 194], [195, 202], [202, 203], [204, 209], [210, 217], [218, 226], [226, 227]]}
{"doc_key": "ai-train-36", "ner": [[8, 14, "field"], [17, 23, "task"], [25, 26, "task"], [28, 28, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 23, 8, 14, "part-of", "", false, false], [25, 26, 8, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", ",", "e.g.", "in", "the", "annotation", "of", "parts", "of", "speech", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing, e.g. in the annotation of parts of speech and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [94, 95], [96, 100], [101, 103], [104, 107], [108, 118], [119, 121], [122, 127], [128, 130], [131, 137], [138, 141], [142, 151], [152, 159], [160, 161], [161, 168], [168, 169], [170, 174], [174, 175], [175, 176]]}
{"doc_key": "ai-train-37", "ner": [[8, 10, "product"], [0, 12, "organisation"], [13, 14, "organisation"], [15, 16, "country"], [22, 28, "product"], [19, 20, "researcher"], [37, 37, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 12, 8, 10, "role", "introduces_to_market", true, false], [13, 14, 8, 10, "role", "introduces_to_market", true, false], [13, 14, 15, 16, "physical", "", false, false], [22, 28, 37, 37, "related-to", "sold_to", true, false], [19, 20, 22, 28, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "Fuji", "Yusoki", "Kogyo", "Company", "introduced", "the", "first", "palletising", "robot", "in", "1963", ".", "KUKA", "Robotics", "in", "Germany", ",", "and", "Victor", "Scheinman", "invented", "a", "programmable", "universal", "machine", "for", "assembly", "in", "1976", ",", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The Fuji Yusoki Kogyo Company introduced the first palletising robot in 1963. KUKA Robotics in Germany, and Victor Scheinman invented a programmable universal machine for assembly in 1976, and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 21], [22, 29], [30, 40], [41, 44], [45, 50], [51, 62], [63, 68], [69, 71], [72, 76], [76, 77], [78, 82], [83, 91], [92, 94], [95, 102], [102, 103], [104, 107], [108, 114], [115, 124], [125, 133], [134, 135], [136, 148], [149, 158], [159, 166], [167, 170], [171, 179], [180, 182], [183, 187], [187, 188], [189, 192], [193, 196], [197, 203], [204, 207], [208, 212], [213, 215], [216, 225], [225, 226]]}
{"doc_key": "ai-train-38", "ner": [[8, 8, "conference"], [10, 10, "researcher"], [19, 19, "field"], [33, 34, "researcher"], [46, 47, "researcher"], [61, 61, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[10, 10, 8, 8, "role", "president_of", false, false], [10, 10, 33, 34, "role", "colleagues", false, false], [19, 19, 61, 61, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "as", "president", "of", "the", "AAAI", ",", "Hayes", "launched", "a", "series", "of", "attacks", "on", "critics", "of", "AI", ",", "mostly", "in", "an", "ironic", "light", ",", "and", "(", "together", "with", "his", "colleague", "Kenneth", "Ford", ")", "came", "up", "with", "the", "idea", "of", "a", "prize", "named", "after", "Simon", "Newcomb", ",", "to", "be", "awarded", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, as president of the AAAI, Hayes launched a series of attacks on critics of AI, mostly in an ironic light, and (together with his colleague Kenneth Ford) came up with the idea of a prize named after Simon Newcomb, to be awarded for the most ridiculous argument refuting the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 20], [21, 30], [31, 33], [34, 37], [38, 42], [42, 43], [44, 49], [50, 58], [59, 60], [61, 67], [68, 70], [71, 78], [79, 81], [82, 89], [90, 92], [93, 95], [95, 96], [97, 103], [104, 106], [107, 109], [110, 116], [117, 122], [122, 123], [124, 127], [128, 129], [129, 137], [138, 142], [143, 146], [147, 156], [157, 164], [165, 169], [169, 170], [171, 175], [176, 178], [179, 183], [184, 187], [188, 192], [193, 195], [196, 197], [198, 203], [204, 209], [210, 215], [216, 221], [222, 229], [229, 230], [231, 233], [234, 236], [237, 244], [245, 248], [249, 252], [253, 257], [258, 268], [269, 277], [278, 286], [287, 290], [291, 302], [303, 305], [306, 308], [308, 309]]}
{"doc_key": "ai-train-39", "ner": [[13, 16, "algorithm"], [39, 41, "algorithm"], [50, 53, "algorithm"], [56, 59, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 39, 41, "named", "same", false, false], [50, 53, 13, 16, "type-of", "", false, false], [56, 59, 13, 16, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "for", "math", "\\", "alpha", "/", "math", "can", "be", "found", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "magnitude", "of", "math", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "usually", "using", "a", "line", "search", "between", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "backward", "line", "search", "such", "as", "an", "Armijo", "line", "search", "."], "sentence-detokenized": "The optimal value for math\\ alpha / math can be found using a line search algorithm, i.e. the magnitude of math\\ alpha / math is determined by finding the value that minimizes S, usually using a line search between math0\\ alpha 1 / math or a backward line search such as an Armijo line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 21], [22, 26], [26, 27], [28, 33], [34, 35], [36, 40], [41, 44], [45, 47], [48, 53], [54, 59], [60, 61], [62, 66], [67, 73], [74, 83], [83, 84], [85, 89], [90, 93], [94, 103], [104, 106], [107, 111], [111, 112], [113, 118], [119, 120], [121, 125], [126, 128], [129, 139], [140, 142], [143, 150], [151, 154], [155, 160], [161, 165], [166, 175], [176, 177], [177, 178], [179, 186], [187, 192], [193, 194], [195, 199], [200, 206], [207, 214], [215, 220], [220, 221], [222, 227], [228, 229], [230, 231], [232, 236], [237, 239], [240, 241], [242, 250], [251, 255], [256, 262], [263, 267], [268, 270], [271, 273], [274, 280], [281, 285], [286, 292], [292, 293]]}
{"doc_key": "ai-train-40", "ner": [[2, 4, "algorithm"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "Breadth", "-", "first", "and", "Depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "contain", "a", "lot", "of", "technical", "information", ",", "but", "do", "not", "shed", "much", "light", "on", "the", "mental", "processes", "of", "people", "in", "solving", "such", "puzzles", "."], "sentence-detokenized": "He discusses Breadth-first and Depth-first search techniques, but ultimately concludes that the results represent expert systems that contain a lot of technical information, but do not shed much light on the mental processes of people in solving such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 30], [31, 36], [36, 37], [37, 42], [43, 49], [50, 60], [60, 61], [62, 65], [66, 76], [77, 86], [87, 91], [92, 95], [96, 103], [104, 113], [114, 120], [121, 128], [129, 133], [134, 141], [142, 143], [144, 147], [148, 150], [151, 160], [161, 172], [172, 173], [174, 177], [178, 180], [181, 184], [185, 189], [190, 194], [195, 200], [201, 203], [204, 207], [208, 214], [215, 224], [225, 227], [228, 234], [235, 237], [238, 245], [246, 250], [251, 258], [258, 259]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deal", "with", "how", "spoken", "language", "can", "be", "understood", "or", "created", "by", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis deal with how spoken language can be understood or created by computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 44], [45, 49], [50, 53], [54, 60], [61, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 98], [99, 101], [102, 111], [111, 112]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"], [33, 35, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^", "{", "*}", "/", "math", "is", "usually", "estimated", "using", "the", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "Maximum", "A", "Posteriori", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "MAP", "}", "/", "math", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta ^ {*} / math is usually estimated using the Maximum Likelihood (math\\ theta ^ {*} =\\ theta ^ {ML} / math) or Maximum A Posteriori (math\\ theta ^ {*} =\\ theta ^ {MAP} / math) procedure.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [17, 18], [19, 20], [20, 22], [23, 24], [25, 29], [30, 32], [33, 40], [41, 50], [51, 56], [57, 60], [61, 68], [69, 79], [80, 81], [81, 85], [85, 86], [87, 92], [93, 94], [95, 96], [96, 98], [99, 101], [102, 107], [108, 109], [110, 111], [111, 113], [113, 114], [115, 116], [117, 121], [121, 122], [123, 125], [126, 133], [134, 135], [136, 146], [147, 148], [148, 152], [152, 153], [154, 159], [160, 161], [162, 163], [163, 165], [166, 168], [169, 174], [175, 176], [177, 178], [178, 181], [181, 182], [183, 184], [185, 189], [189, 190], [191, 200], [200, 201]]}
{"doc_key": "ai-train-43", "ner": [[6, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "commonly", "spoken", "languages", "use", "an", "open", "-", "source", "eSpeak", "synthesizer", ",", "which", "produces", "a", "robotic", ",", "clunky", "sound", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less commonly spoken languages use an open-source eSpeak synthesizer, which produces a robotic, clunky sound that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 18], [19, 25], [26, 35], [36, 39], [40, 42], [43, 47], [47, 48], [48, 54], [55, 61], [62, 73], [73, 74], [75, 80], [81, 89], [90, 91], [92, 99], [99, 100], [101, 107], [108, 113], [114, 118], [119, 122], [123, 125], [126, 135], [136, 138], [139, 149], [149, 150]]}
{"doc_key": "ai-train-44", "ner": [[1, 2, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 2, 35, 36, "compare", "", false, false], [1, 2, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "mainly", "used", "by", "statisticians", "and", "other", "professionals", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "it", "can", "also", "serve", "as", "a", "general", "matrix", "computing", "toolkit", "with", "performance", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is mainly used by statisticians and other professionals who need an environment for statistical computing and software development, it can also serve as a general matrix computing toolkit with performance comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 20], [21, 25], [26, 28], [29, 42], [43, 46], [47, 52], [53, 66], [67, 70], [71, 75], [76, 78], [79, 90], [91, 94], [95, 106], [107, 116], [117, 120], [121, 129], [130, 141], [141, 142], [143, 145], [146, 149], [150, 154], [155, 160], [161, 163], [164, 165], [166, 173], [174, 180], [181, 190], [191, 198], [199, 203], [204, 215], [216, 226], [227, 229], [230, 233], [234, 240], [241, 243], [244, 250], [250, 251]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [8, 11, "misc"], [12, 14, "researcher"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 0, 12, 14, "origin", "", false, false], [8, 11, 12, 14, "named", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "combining", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden that creates new frequencies by combining two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [76, 79], [80, 88], [89, 97], [98, 107], [108, 112], [113, 120], [121, 124], [125, 136], [137, 139], [140, 149], [150, 153], [154, 165], [165, 166]]}
{"doc_key": "ai-train-46", "ner": [[14, 16, "person"], [17, 17, "misc"], [19, 21, "organisation"], [26, 28, "misc"], [31, 32, "person"], [37, 39, "misc"], [42, 43, "person"], [45, 46, "person"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 7, 8, 9], "relations": [[14, 16, 17, 17, "role", "actor_in", false, false], [17, 17, 19, 21, "artifact", "", false, false], [31, 32, 26, 28, "role", "actor_in", false, false], [42, 43, 37, 39, "role", "actor_in", false, false], [45, 46, 37, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 3, 5, 6], "sentence": ["Several", "other", "films", "that", "helped", "put", "3D", "back", "on", "the", "map", "that", "month", "included", "John", "Wayne", "'s", "Hondo", "(", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", ",", "starring", "Rita", "Hayworth", ",", "and", "Paramount", "'s", "Money", "From", "Home", ",", "starring", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Several other films that helped put 3D back on the map that month included John Wayne's Hondo (Warner Bros. ), Columbia's Miss Sadie Thompson, starring Rita Hayworth, and Paramount's Money From Home, starring Dean Martin and Jerry Lewis.", "token2charspan": [[0, 7], [8, 13], [14, 19], [20, 24], [25, 31], [32, 35], [36, 38], [39, 43], [44, 46], [47, 50], [51, 54], [55, 59], [60, 65], [66, 74], [75, 79], [80, 85], [85, 87], [88, 93], [94, 95], [95, 101], [102, 106], [106, 107], [108, 109], [109, 110], [111, 119], [119, 121], [122, 126], [127, 132], [133, 141], [141, 142], [143, 151], [152, 156], [157, 165], [165, 166], [167, 170], [171, 180], [180, 182], [183, 188], [189, 193], [194, 198], [198, 199], [200, 208], [209, 213], [214, 220], [221, 224], [225, 230], [231, 236], [236, 237]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 4, "field"], [5, 7, "task"], [11, 11, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 7, "general-affiliation", "", false, false], [0, 0, 11, 11, "artifact", "", false, false], [5, 7, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "Facebook", "research", "team", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a Facebook research team.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 84], [85, 89], [89, 90]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [7, 8, "conference"], [15, 16, "field"], [25, 28, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [7, 8, 0, 1, "topic", "", false, false], [25, 28, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "theme", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the leading academic conference on computer graphics, and the main theme of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 91], [92, 94], [95, 103], [104, 112], [112, 113], [114, 117], [118, 121], [122, 126], [127, 132], [133, 135], [136, 139], [140, 146], [147, 156], [157, 159], [160, 168], [169, 179], [179, 180]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 17, "algorithm"], [21, 22, "algorithm"], [20, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [40, 43, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 9], "relations": [[20, 24, 21, 22, "named", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [3, 5], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "as", "a", "pre-processing", "step", ",", "followed", "by", "k", "-", "NN", "clustering", "by", "clustering", "feature", "vectors", "in", "a", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) as a pre-processing step, followed by k-NN clustering by clustering feature vectors in a reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [153, 155], [156, 165], [166, 177], [178, 186], [187, 188], [188, 191], [191, 192], [193, 195], [196, 197], [198, 212], [213, 217], [217, 218], [219, 227], [228, 230], [231, 232], [232, 233], [233, 235], [236, 246], [247, 249], [250, 260], [261, 268], [269, 276], [277, 279], [280, 281], [282, 289], [290, 301], [302, 307], [307, 308]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 11, "field"], [13, 14, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 11, "related-to", "good_at", true, false], [0, 2, 13, 14, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "are", "excellent", "for", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that are excellent for machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 60], [61, 70], [71, 74], [75, 82], [83, 91], [92, 95], [96, 103], [104, 115], [115, 116]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [36, 39, "algorithm"], [41, 42, "researcher"], [44, 46, "researcher"], [48, 54, "misc"], [56, 65, "conference"], [61, 67, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [48, 54, 36, 39, "topic", "", false, false], [48, 54, 41, 42, "artifact", "", false, false], [48, 54, 44, 46, "artifact", "", false, false], [48, 54, 56, 65, "temporal", "", false, false], [61, 67, 56, 65, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histogram", "of", "oriented", "gradients", ",", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", ",", "graphs", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as histogram of oriented gradients, N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005, graphs.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 143], [144, 150], [151, 154], [155, 160], [161, 169], [170, 174], [175, 177], [178, 187], [188, 190], [191, 199], [200, 209], [209, 210], [211, 213], [214, 219], [219, 220], [221, 222], [222, 223], [224, 230], [230, 231], [232, 242], [243, 245], [246, 254], [255, 264], [265, 268], [269, 274], [275, 284], [284, 285], [286, 290], [291, 299], [300, 307], [308, 318], [319, 321], [322, 330], [331, 337], [338, 341], [342, 349], [350, 361], [362, 363], [363, 367], [367, 368], [368, 369], [370, 375], [376, 377], [377, 378], [379, 386], [386, 387], [388, 392], [392, 393], [394, 400], [400, 401]]}
{"doc_key": "ai-train-52", "ner": [[0, 1, "algorithm"], [6, 8, "algorithm"], [12, 14, "task"], [15, 15, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 8, "type-of", "", false, false], [12, 14, 0, 1, "usage", "", true, false], [12, 14, 15, 15, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autocoder", "is", "a", "kind", "of", "artificial", "neural", "network", "used", "for", "learning", "Features", "learning", "by", "unsupervised", "learning", "."], "sentence-detokenized": "An autocoder is a kind of artificial neural network used for learning Features learning by unsupervised learning.", "token2charspan": [[0, 2], [3, 12], [13, 15], [16, 17], [18, 22], [23, 25], [26, 36], [37, 43], [44, 51], [52, 56], [57, 60], [61, 69], [70, 78], [79, 87], [88, 90], [91, 103], [104, 112], [112, 113]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [5, 5, "organisation"], [10, 11, "field"], [13, 14, "field"], [24, 25, "organisation"], [27, 27, "organisation"], [33, 34, "field"], [36, 36, "field"], [41, 41, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 2, 5, 5, "role", "fellow_of", false, false], [0, 2, 10, 11, "related-to", "contributes_to", false, false], [0, 2, 13, 14, "related-to", "contributes_to", false, false], [0, 2, 24, 25, "role", "fellow_of", false, false], [0, 2, 33, 34, "related-to", "contributes_to", false, false], [0, 2, 36, 36, "related-to", "contributes_to", false, false], [27, 27, 24, 25, "named", "", false, false], [41, 41, 24, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "IEEE", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", ",", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "image", "recognition", "and", "processing", "and", "his", "service", "to", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of IEEE for his contributions to computer vision and image processing, and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions to image recognition and processing and his service to IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 28], [29, 32], [33, 36], [37, 50], [51, 53], [54, 62], [63, 69], [70, 73], [74, 79], [80, 90], [90, 91], [92, 95], [96, 97], [98, 104], [105, 107], [108, 111], [112, 125], [126, 137], [138, 141], [142, 149], [150, 161], [162, 163], [163, 167], [167, 168], [169, 172], [173, 176], [177, 190], [191, 193], [194, 199], [200, 211], [212, 215], [216, 226], [227, 230], [231, 234], [235, 242], [243, 245], [246, 250], [250, 251]]}
{"doc_key": "ai-train-54", "ner": [[4, 5, "task"], [8, 10, "algorithm"], [12, 12, "algorithm"], [19, 20, "researcher"], [22, 23, "organisation"], [26, 27, "researcher"], [29, 33, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 5, 8, 10, "usage", "", false, false], [8, 10, 19, 20, "origin", "", true, false], [8, 10, 26, 27, "origin", "", true, false], [12, 12, 8, 10, "named", "", false, false], [19, 20, 22, 23, "physical", "", false, false], [19, 20, 22, 23, "role", "", false, false], [26, 27, 29, 33, "physical", "", false, false], [26, 27, 29, 33, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempts", "to", "complete", "ASR", "were", "the", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "-", "based", "systems", "presented", "by", "Alex", "Graves", "(", "Google", "DeepMind", ")", "and", "Navdeep", "Jaitly", "(", "University", "of", "Toronto", ")", "in", "2014", "."], "sentence-detokenized": "The first attempts to complete ASR were the Connectionist Temporal Classification (CTC)-based systems presented by Alex Graves (Google DeepMind) and Navdeep Jaitly (University of Toronto) in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 18], [19, 21], [22, 30], [31, 34], [35, 39], [40, 43], [44, 57], [58, 66], [67, 81], [82, 83], [83, 86], [86, 87], [87, 88], [88, 93], [94, 101], [102, 111], [112, 114], [115, 119], [120, 126], [127, 128], [128, 134], [135, 143], [143, 144], [145, 148], [149, 156], [157, 163], [164, 165], [165, 175], [176, 178], [179, 186], [186, 187], [188, 190], [191, 195], [195, 196]]}
{"doc_key": "ai-train-55", "ner": [[1, 2, "algorithm"], [0, 7, "algorithm"], [11, 11, "algorithm"], [10, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 7, 1, 2, "named", "", false, false], [11, 11, 1, 2, "type-of", "", false, false], [10, 13, 11, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear", "Fractional", "Programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "Linear", "Programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear Fractional Programming (LFP) is a generalisation of Linear Programming (LP).", "token2charspan": [[0, 6], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 1, "researcher"], [8, 13, "misc"], [14, 24, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 8, 13, "win-defeat", "", false, false], [8, 13, 14, 24, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "in", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the International Conference on Machine Learning in 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 104], [105, 107], [108, 115], [116, 124], [125, 127], [128, 132], [133, 136], [137, 141], [141, 142]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [26, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "take", "advantage", "of", "the", "neural", "network", "developed", "as", "components", "inherited", "from", "these", "frameworks", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to take advantage of the neural network developed as components inherited from these frameworks.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 126], [127, 136], [137, 139], [140, 143], [144, 150], [151, 158], [159, 168], [169, 171], [172, 182], [183, 192], [193, 197], [198, 203], [204, 214], [214, 215]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "in", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "and", "the", "algorithm", "first", "creates", "an", "alignment", "between", "two", "sentences", ",", "a", "translation", "candidate", "and", "a", "string", "of", "reference", "translation", "strings", "(", "see", "figures", ")", "."], "sentence-detokenized": "As in BLEU, the basic unit of evaluation is the sentence, and the algorithm first creates an alignment between two sentences, a translation candidate and a string of reference translation strings (see figures).", "token2charspan": [[0, 2], [3, 5], [6, 10], [10, 11], [12, 15], [16, 21], [22, 26], [27, 29], [30, 40], [41, 43], [44, 47], [48, 56], [56, 57], [58, 61], [62, 65], [66, 75], [76, 81], [82, 89], [90, 92], [93, 102], [103, 110], [111, 114], [115, 124], [124, 125], [126, 127], [128, 139], [140, 149], [150, 153], [154, 155], [156, 162], [163, 165], [166, 175], [176, 187], [188, 195], [196, 197], [197, 200], [201, 208], [208, 209], [209, 210]]}
{"doc_key": "ai-train-59", "ner": [[6, 11, "conference"], [21, 21, "task"], [23, 24, "task"], [28, 29, "metrics"], [31, 37, "metrics"], [44, 45, "conference"], [42, 47, "conference"], [50, 50, "location"], [52, 52, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 11, 21, 21, "related-to", "subject_at", false, false], [6, 11, 23, 24, "related-to", "subject_at", false, false], [28, 29, 6, 11, "temporal", "", false, false], [31, 37, 28, 29, "named", "", true, false], [42, 47, 44, 45, "named", "", false, false], [50, 50, 52, 52, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "NIST", "'s", "annual", "Document", "Understanding", "Conferences", ",", "where", "research", "teams", "present", "their", "systems", "for", "both", "summarization", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used at NIST's annual Document Understanding Conferences, where research teams present their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 31], [31, 33], [34, 40], [41, 49], [50, 63], [64, 75], [75, 76], [77, 82], [83, 91], [92, 97], [98, 105], [106, 111], [112, 119], [120, 123], [124, 128], [129, 142], [143, 146], [147, 158], [159, 164], [164, 165], [166, 168], [169, 172], [173, 178], [179, 185], [186, 187], [187, 193], [193, 194], [194, 202], [203, 213], [214, 217], [218, 225], [226, 236], [236, 237], [238, 240], [241, 249], [250, 252], [253, 259], [260, 271], [272, 282], [283, 290], [291, 292], [292, 296], [296, 297], [297, 298], [299, 307], [307, 308], [309, 315], [315, 316], [317, 325], [326, 327], [328, 332], [332, 333]]}
{"doc_key": "ai-train-60", "ner": [[6, 7, "programlang"], [6, 9, "product"], [11, 12, "programlang"], [17, 17, "product"], [23, 23, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 7, 11, 12, "type-of", "", false, false], [6, 7, 23, 23, "named", "", false, false], [6, 9, 11, 12, "part-of", "", false, false], [6, 9, 17, 17, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "to", "be", "executed", "in", "Java", "with", "JShell", "(", "Java", "9", "or", "later", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, to be executed in Java with JShell (Java 9 or later): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 23], [24, 26], [27, 35], [36, 38], [39, 43], [44, 48], [49, 55], [56, 57], [57, 61], [62, 63], [64, 66], [67, 72], [72, 73], [73, 74], [75, 85], [86, 96], [97, 98], [99, 118], [119, 123], [124, 125], [126, 130]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [6, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 6, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "meter", "is", "based", "on", "the", "BLEU", "meter", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST meter is based on the BLEU meter, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 14], [15, 17], [18, 23], [24, 26], [27, 30], [31, 35], [36, 41], [41, 42], [43, 46], [47, 51], [52, 56], [57, 70], [70, 71]]}
{"doc_key": "ai-train-62", "ner": [[1, 1, "country"], [5, 7, "university"], [10, 12, "university"], [23, 24, "product"], [27, 31, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 1, 1, "physical", "", false, false], [10, 12, 1, 1, "physical", "", false, false], [23, 24, 5, 7, "origin", "", false, false], [23, 24, 10, 12, "origin", "", false, false], [23, 24, 27, 31, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "in", "the", "late", "1980s", "called", "Knowledge", "Graphs", ",", "a", "semantic", "network", "but", "with", "the", "added", "constraint", "that", "edges", "are", "restricted", "to", "belong", "to", "a", "limited", "set", "of", "possible", "relations", "to", "make", "graph", "algebras", "easier", "."], "sentence-detokenized": "Two Dutch universities, the University of Groningen and the University of Twente, jointly started a project in the late 1980s called Knowledge Graphs, a semantic network but with the added constraint that edges are restricted to belong to a limited set of possible relations to make graph algebras easier.", "token2charspan": [[0, 3], [4, 9], [10, 22], [22, 23], [24, 27], [28, 38], [39, 41], [42, 51], [52, 55], [56, 59], [60, 70], [71, 73], [74, 80], [80, 81], [82, 89], [90, 97], [98, 99], [100, 107], [108, 110], [111, 114], [115, 119], [120, 125], [126, 132], [133, 142], [143, 149], [149, 150], [151, 152], [153, 161], [162, 169], [170, 173], [174, 178], [179, 182], [183, 188], [189, 199], [200, 204], [205, 210], [211, 214], [215, 225], [226, 228], [229, 235], [236, 238], [239, 240], [241, 248], [249, 252], [253, 255], [256, 264], [265, 274], [275, 277], [278, 282], [283, 288], [289, 297], [298, 304], [304, 305]]}
{"doc_key": "ai-train-63", "ner": [[0, 2, "product"], [16, 18, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 16, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "word", "processor", ",", "but", "they", "are", "also", "available", "as", "stand", "-", "alone", "applications", "that", "can", "be", "activated", "from", "the", "programs", "that", "handle", "the", "text", "to", "be", "edited", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but they are also available as stand-alone applications that can be activated from the programs that handle the text to be edited.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [76, 77], [78, 82], [83, 85], [86, 87], [88, 92], [93, 102], [102, 103], [104, 107], [108, 112], [113, 116], [117, 121], [122, 131], [132, 134], [135, 140], [140, 141], [141, 146], [147, 159], [160, 164], [165, 168], [169, 171], [172, 181], [182, 186], [187, 190], [191, 199], [200, 204], [205, 211], [212, 215], [216, 220], [221, 223], [224, 226], [227, 233], [233, 234]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 21, "conference"], [5, 26, "organisation"], [32, 34, "conference"], [36, 38, "conference"], [40, 42, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "and", "the", "Cognitive", "Science", "Society", ",", "and", "an", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence and the Cognitive Science Society, and an editor of J. Automated Reasoning, J. Learning Sciences and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [138, 141], [142, 145], [146, 155], [156, 163], [164, 171], [171, 172], [173, 176], [177, 179], [180, 186], [187, 189], [190, 192], [193, 202], [203, 212], [212, 213], [214, 216], [217, 225], [226, 234], [235, 238], [239, 241], [242, 249], [250, 258], [258, 259]]}
{"doc_key": "ai-train-65", "ner": [[1, 2, "algorithm"], [0, 4, "algorithm"], [10, 11, "task"], [20, 22, "researcher"], [23, 23, "university"], [26, 28, "researcher"], [29, 32, "organisation"], [34, 34, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 2, 10, 11, "type-of", "", false, false], [1, 2, 20, 22, "origin", "", false, false], [1, 2, 26, 28, "origin", "", false, false], [0, 4, 1, 2, "named", "", false, false], [20, 22, 23, 23, "physical", "", false, false], [20, 22, 23, 23, "role", "", false, false], [26, 28, 29, 32, "role", "", false, false], [34, 34, 29, 32, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "develop", "with", "the", "work", "of", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear Predictive Coding (LPC), a form of speech coding, began to develop with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 73], [74, 78], [79, 82], [83, 87], [88, 90], [91, 99], [100, 107], [108, 110], [111, 117], [118, 128], [129, 132], [133, 138], [139, 144], [145, 147], [148, 154], [155, 164], [165, 168], [169, 178], [179, 180], [180, 183], [183, 184], [185, 187], [188, 192], [192, 193]]}
{"doc_key": "ai-train-66", "ner": [[57, 64, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Moreover", ",", "if", "the", "signal", "is", "ergodic", ",", "all", "sampling", "points", "have", "the", "same", "time", "-", "averaged", "value", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=", "\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "the", "mean", "squared", "error", "."], "sentence-detokenized": "Moreover, if the signal is ergodic, all sampling points have the same time-averaged value and thus mathR _ x ^ {n / T _ 0} (\\ tau) = \\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the sense of the mean squared error.", "token2charspan": [[0, 8], [8, 9], [10, 12], [13, 16], [17, 23], [24, 26], [27, 34], [34, 35], [36, 39], [40, 48], [49, 55], [56, 60], [61, 64], [65, 69], [70, 74], [74, 75], [75, 83], [84, 89], [90, 93], [94, 98], [99, 104], [105, 106], [107, 108], [109, 110], [111, 112], [112, 113], [114, 115], [116, 117], [118, 119], [120, 121], [121, 122], [123, 124], [124, 125], [126, 129], [129, 130], [131, 132], [133, 134], [135, 142], [143, 144], [144, 145], [145, 146], [147, 148], [149, 150], [151, 152], [153, 154], [154, 155], [156, 157], [158, 159], [160, 161], [162, 163], [163, 164], [165, 166], [166, 167], [168, 171], [171, 172], [173, 174], [175, 179], [180, 182], [183, 186], [187, 192], [193, 195], [196, 199], [200, 204], [205, 212], [213, 218], [218, 219]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 17, "algorithm"], [21, 22, "algorithm"], [20, 24, "algorithm"], [27, 28, "algorithm"], [29, 31, "algorithm"], [34, 35, "algorithm"], [34, 38, "algorithm"], [41, 44, "misc"], [57, 60, "algorithm"], [49, 51, "misc"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[21, 22, 41, 44, "related-to", "", false, false], [20, 24, 21, 22, "named", "", false, false], [27, 28, 41, 44, "related-to", "", false, false], [29, 31, 27, 28, "named", "", false, false], [34, 35, 41, 44, "related-to", "", false, false], [34, 38, 34, 35, "named", "", false, false], [57, 60, 49, 51, "related-to", "", true, false]], "relations_mapping_to_source": [2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorization", "(", "NMF", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "the", "feature", "vectors", "in", "the", "reduced", "dimension", "space", "with", "K", "-", "NN", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorization (NMF) techniques as a pre-processing step, followed by clustering the feature vectors in the reduced dimension space with K-NN.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [152, 153], [154, 163], [164, 175], [176, 184], [185, 186], [186, 189], [189, 190], [191, 193], [194, 206], [207, 213], [214, 227], [228, 229], [229, 232], [232, 233], [234, 244], [245, 247], [248, 249], [250, 264], [265, 269], [269, 270], [271, 279], [280, 282], [283, 293], [294, 297], [298, 305], [306, 313], [314, 316], [317, 320], [321, 328], [329, 338], [339, 344], [345, 349], [350, 351], [351, 352], [352, 354], [354, 355]]}
{"doc_key": "ai-train-68", "ner": [[0, 0, "programlang"], [2, 2, "programlang"], [4, 4, "programlang"], [6, 7, "programlang"], [12, 13, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[12, 13, 0, 0, "related-to", "program_type_compatible_with", false, false], [12, 13, 2, 2, "related-to", "program_type_compatible_with", false, false], [12, 13, 4, 4, "related-to", "program_type_compatible_with", false, false], [12, 13, 6, 7, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "libraries", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Perl, Java, ActiveX or .NET libraries can be called directly from MATLAB,", "token2charspan": [[0, 4], [4, 5], [6, 10], [10, 11], [12, 19], [20, 22], [23, 27], [28, 37], [38, 41], [42, 44], [45, 51], [52, 60], [61, 65], [66, 72], [72, 73]]}
{"doc_key": "ai-train-69", "ner": [[0, 4, "task"], [7, 9, "task"], [23, 24, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 4, 7, 9, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Identifying", "named", "entities", "from", "text", "is", "called", "Named", "Entity", "Recognition", ",", "while", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "Identifying named entities from text is called Named Entity Recognition, while determining the identity of named entities mentioned in text is called Entity Linking.", "token2charspan": [[0, 11], [12, 17], [18, 26], [27, 31], [32, 36], [37, 39], [40, 46], [47, 52], [53, 59], [60, 71], [71, 72], [73, 78], [79, 90], [91, 94], [95, 103], [104, 106], [107, 112], [113, 121], [122, 131], [132, 134], [135, 139], [140, 142], [143, 149], [150, 156], [157, 164], [164, 165]]}
{"doc_key": "ai-train-70", "ner": [[29, 29, "algorithm"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "but", "as", "of", "version", "0.8.0", "they", "were", "released", "as", "a", "separate", "R", "package", "sigmoid", ",", "intended", "to", "allow", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, but as of version 0.8.0 they were released as a separate R package sigmoid, intended to allow more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 102], [103, 105], [106, 108], [109, 116], [117, 122], [123, 127], [128, 132], [133, 141], [142, 144], [145, 146], [147, 155], [156, 157], [158, 165], [166, 173], [173, 174], [175, 183], [184, 186], [187, 192], [193, 197], [198, 205], [206, 209], [209, 210]]}
{"doc_key": "ai-train-71", "ner": [[1, 1, "programlang"], [17, 20, "organisation"], [22, 22, "organisation"], [25, 25, "location"], [26, 27, "location"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 14, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 1, 7, 8, "artifact", "", true, false], [1, 1, 10, 11, "artifact", "", true, false], [1, 1, 13, 14, "artifact", "", true, false], [22, 22, 17, 20, "named", "", false, false], [22, 22, 25, 25, "physical", "", false, false], [25, 25, 26, 27, "physical", "", false, false], [7, 8, 17, 20, "role", "", false, false], [10, 11, 17, 20, "role", "", false, false], [13, 14, 17, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", "in", "Cambridge", ",", "Massachusetts", "."], "sentence-detokenized": "The logo was created in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert at Bolt, Beranek and Newman (BBN) in Cambridge, Massachusetts.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 37], [38, 46], [46, 47], [48, 55], [56, 63], [64, 67], [68, 75], [76, 82], [83, 85], [86, 90], [90, 91], [92, 99], [100, 103], [104, 110], [111, 112], [112, 115], [115, 116], [117, 119], [120, 129], [129, 130], [131, 144], [144, 145]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [23, 25, "algorithm"], [26, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", ",", "and", "can", "be", "compared", "to", "traditional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "in", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm, and can be compared to traditional deep learning techniques that use gradient descent in a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [78, 79], [80, 83], [84, 87], [88, 90], [91, 99], [100, 102], [103, 114], [115, 119], [120, 128], [129, 139], [140, 144], [145, 148], [149, 157], [158, 165], [166, 168], [169, 170], [171, 177], [178, 185], [186, 190], [191, 192], [193, 198], [199, 207], [207, 208]]}
{"doc_key": "ai-train-73", "ner": [[47, 47, "algorithm"], [48, 48, "metrics"], [44, 50, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[44, 50, 48, 48, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "fit", "a", "function", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264", "n", "/", "sub", ",", "we", "can", "estimate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we fit a function \u0177 = a + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264 n / sub, we can estimate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 11], [12, 20], [21, 22], [23, 24], [25, 26], [27, 28], [29, 30], [31, 35], [36, 37], [38, 41], [42, 43], [44, 46], [47, 50], [51, 55], [56, 57], [57, 58], [59, 62], [63, 64], [65, 66], [67, 70], [70, 71], [72, 73], [74, 77], [78, 79], [80, 81], [82, 85], [85, 86], [87, 90], [91, 92], [93, 94], [95, 96], [97, 98], [99, 100], [101, 102], [103, 106], [106, 107], [108, 110], [111, 114], [115, 123], [124, 127], [128, 131], [132, 137], [138, 141], [142, 146], [147, 154], [155, 160], [161, 162], [162, 165], [165, 166], [166, 167]]}
{"doc_key": "ai-train-74", "ner": [[5, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [48, 49, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 152], [152, 153], [154, 160], [160, 161], [162, 171], [171, 172], [173, 178], [179, 185], [185, 186], [187, 192], [192, 193], [194, 200], [200, 201], [202, 210], [210, 211], [212, 218], [219, 222], [223, 226], [227, 233], [234, 241], [241, 242]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 8, "field"], [13, 13, "organisation"], [16, 24, "university"], [28, 30, "organisation"], [32, 37, "university"], [41, 42, "university"], [44, 45, "university"], [48, 50, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 8, "topic", "", false, false], [3, 3, 13, 13, "origin", "", false, false], [3, 3, 16, 24, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "Electrical", "and", "Computer", "Engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", ",", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in Electrical and Computer Engineering (2000) from Inria and the University of Nice Sophia Antipolis, and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, and visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 90], [91, 93], [94, 98], [99, 105], [106, 115], [115, 116], [117, 120], [121, 124], [125, 129], [130, 139], [140, 149], [150, 152], [153, 160], [161, 170], [171, 181], [181, 182], [183, 188], [189, 192], [193, 198], [199, 208], [208, 209], [210, 213], [214, 222], [223, 232], [233, 235], [236, 243], [244, 254], [254, 255], [256, 260], [261, 271], [272, 275], [276, 279], [280, 290], [291, 293], [294, 301], [301, 302]]}
{"doc_key": "ai-train-76", "ner": [[8, 9, "researcher"], [0, 0, "researcher"], [14, 15, "product"], [23, 24, "country"], [18, 19, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 8, 9, "role", "licensing_patent_to", false, false], [0, 0, 23, 24, "physical", "", false, false], [18, 19, 0, 0, "artifact", "", false, false], [18, 19, 14, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberger", "licensed", "the", "original", "patent", "granted", "to", "inventor", "George", "Devol", "and", "developed", "the", "first", "industrial", "robot", ",", "the", "Unimate", "robot", ",", "in", "the", "United", "States", "in", "the", "1950s", "."], "sentence-detokenized": "Engelberger licensed the original patent granted to inventor George Devol and developed the first industrial robot, the Unimate robot, in the United States in the 1950s.", "token2charspan": [[0, 11], [12, 20], [21, 24], [25, 33], [34, 40], [41, 48], [49, 51], [52, 60], [61, 67], [68, 73], [74, 77], [78, 87], [88, 91], [92, 97], [98, 108], [109, 114], [114, 115], [116, 119], [120, 127], [128, 133], [133, 134], [135, 137], [138, 141], [142, 148], [149, 155], [156, 158], [159, 162], [163, 168], [168, 169]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [9, 10, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 60], [61, 70], [70, 71]]}
{"doc_key": "ai-train-78", "ner": [[7, 7, "programlang"], [16, 17, "programlang"], [20, 21, "programlang"], [31, 31, "programlang"]], "ner_mapping_to_source": [1, 2, 3, 4], "relations": [[7, 7, 20, 21, "general-affiliation", "", false, false], [7, 7, 31, 31, "named", "", false, false]], "relations_mapping_to_source": [2, 3], "sentence": ["The", "descendants", "of", "the", "CLIPS", "language", "are", "Jess", "(", "the", "rule", "-", "based", "part", "of", "the", "CLIPS", "language", ",", "rewritten", "in", "Java", ",", "later", "grown", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by"], "sentence-detokenized": "The descendants of the CLIPS language are Jess (the rule-based part of the CLIPS language, rewritten in Java, later grown in a different direction), JESS was originally inspired by", "token2charspan": [[0, 3], [4, 15], [16, 18], [19, 22], [23, 28], [29, 37], [38, 41], [42, 46], [47, 48], [48, 51], [52, 56], [56, 57], [57, 62], [63, 67], [68, 70], [71, 74], [75, 80], [81, 89], [89, 90], [91, 100], [101, 103], [104, 108], [108, 109], [110, 115], [116, 121], [122, 124], [125, 126], [127, 136], [137, 146], [146, 147], [147, 148], [149, 153], [154, 157], [158, 168], [169, 177], [178, 180]]}
{"doc_key": "ai-train-79", "ner": [[11, 13, "product"], [15, 16, "organisation"], [21, 33, "product"], [45, 46, "product"], [48, 50, "product"], [64, 66, "misc"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6], "relations": [[15, 16, 11, 13, "usage", "", false, false], [21, 33, 15, 16, "artifact", "", false, false], [45, 46, 15, 16, "origin", "", true, false], [45, 46, 64, 66, "related-to", "", true, false], [48, 50, 15, 16, "origin", "", true, false], [48, 50, 64, 66, "related-to", "", true, false]], "relations_mapping_to_source": [1, 2, 3, 4, 5, 6], "sentence": ["It", "has", "also", "created", "flexible", "intelligent", "AGV", "applications", "and", "designed", "the", "Motivity", "control", "system", "that", "RMT", "Robotics", "is", "using", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", ".", "The", "ADAM", "iAGV", "is", "used", "for", "complex", "pick", "and", "place", "operations", "in", "combination", "with", "gantry", "systems", "and", "industrial", "robotic", "arms", "used", "in", "tier", "one", "automotive", "plants", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "It has also created flexible intelligent AGV applications and designed the Motivity control system that RMT Robotics is using to develop the ADAM iAGV (Self-Guided Vehicle). The ADAM iAGV is used for complex pick and place operations in combination with gantry systems and industrial robotic arms used in tier one automotive plants to move products from process to process in non-linear layouts.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 19], [20, 28], [29, 40], [41, 44], [45, 57], [58, 61], [62, 70], [71, 74], [75, 83], [84, 91], [92, 98], [99, 103], [104, 107], [108, 116], [117, 119], [120, 125], [126, 128], [129, 136], [137, 140], [141, 145], [146, 150], [151, 152], [152, 156], [156, 157], [157, 163], [164, 171], [171, 172], [172, 173], [174, 177], [178, 182], [183, 187], [188, 190], [191, 195], [196, 199], [200, 207], [208, 212], [213, 216], [217, 222], [223, 233], [234, 236], [237, 248], [249, 253], [254, 260], [261, 268], [269, 272], [273, 283], [284, 291], [292, 296], [297, 301], [302, 304], [305, 309], [310, 313], [314, 324], [325, 331], [332, 334], [335, 339], [340, 348], [349, 353], [354, 361], [362, 364], [365, 372], [373, 375], [376, 386], [387, 394], [394, 395]]}
{"doc_key": "ai-train-80", "ner": [[6, 7, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Parameters", "\u03b2", "are", "typically", "estimated", "using", "maximum", "likelihood", "."], "sentence-detokenized": "Parameters \u03b2 are typically estimated using maximum likelihood.", "token2charspan": [[0, 10], [11, 12], [13, 16], [17, 26], [27, 36], [37, 42], [43, 50], [51, 61], [61, 62]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 0, 1, "part-of", "", false, false], [8, 8, 0, 1, "part-of", "", false, false], [10, 10, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Data", "retrieval", "metrics", ",", "such", "as", "accuracy", "and", "recall", "or", "DCG", ",", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Data retrieval metrics, such as accuracy and recall or DCG, are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 4], [5, 14], [15, 22], [22, 23], [24, 28], [29, 31], [32, 40], [41, 44], [45, 51], [52, 54], [55, 58], [58, 59], [60, 63], [64, 70], [71, 74], [75, 84], [85, 88], [89, 96], [97, 99], [100, 101], [102, 116], [117, 123], [123, 124]]}
{"doc_key": "ai-train-82", "ner": [[6, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "has", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "and", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "A typical factory has hundreds of industrial robots working on fully automated production lines, and one robot for every ten human workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 21], [22, 30], [31, 33], [34, 44], [45, 51], [52, 59], [60, 62], [63, 68], [69, 78], [79, 89], [90, 95], [95, 96], [97, 100], [101, 104], [105, 110], [111, 114], [115, 120], [121, 124], [125, 130], [131, 138], [138, 139]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [28, 29, "task"], [31, 32, "task"], [34, 35, "task"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7], "relations": [], "relations_mapping_to_source": [], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", ",", "such", "as", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "enhancement", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications, such as image segmentation, feature generation, face extraction, motion detection, region enhancement and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [88, 89], [90, 94], [95, 97], [98, 103], [104, 116], [116, 117], [118, 125], [126, 136], [136, 137], [138, 142], [143, 153], [153, 154], [155, 161], [162, 171], [171, 172], [173, 179], [180, 191], [192, 195], [196, 201], [202, 211], [211, 212]]}
{"doc_key": "ai-train-84", "ner": [[0, 1, "researcher"], [13, 20, "field"], [23, 27, "misc"], [28, 34, "conference"], [31, 36, "conference"], [41, 44, "misc"], [45, 50, "conference"], [51, 52, "conference"], [55, 58, "conference"], [54, 60, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 13, 20, "related-to", "contributes_to", false, false], [0, 1, 23, 27, "win-defeat", "", false, false], [0, 1, 41, 44, "win-defeat", "", false, false], [23, 27, 28, 34, "temporal", "", false, false], [31, 36, 28, 34, "named", "", false, false], [41, 44, 45, 50, "temporal", "", false, false], [41, 44, 55, 58, "temporal", "", false, false], [51, 52, 45, 50, "named", "", false, false], [54, 60, 55, 58, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", ",", "and", "has", "won", "the", "best", "paper", "award", "at", "the", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "and", "the", "best", "reviewer", "award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision, and has won the best paper award at the International Conference on Non-Photorealistic Rendering and Animation (NPAR) 2012 and the best reviewer award at the Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [110, 111], [112, 115], [116, 119], [120, 123], [124, 127], [128, 132], [133, 138], [139, 144], [145, 147], [148, 151], [152, 165], [166, 176], [177, 179], [180, 198], [199, 208], [209, 212], [213, 222], [223, 224], [224, 228], [228, 229], [230, 234], [235, 238], [239, 242], [243, 247], [248, 256], [257, 262], [263, 265], [266, 269], [270, 275], [276, 286], [287, 289], [290, 298], [299, 305], [306, 310], [311, 315], [316, 319], [320, 333], [334, 344], [345, 347], [348, 356], [357, 363], [364, 365], [365, 369], [369, 370], [371, 375], [375, 376]]}
{"doc_key": "ai-train-85", "ner": [[7, 7, "programlang"], [0, 2, "field"], [4, 5, "field"], [10, 11, "misc"], [14, 14, "researcher"], [13, 19, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[7, 7, 0, 2, "part-of", "", false, false], [7, 7, 4, 5, "part-of", "", false, false], [7, 7, 10, 11, "type-of", "", false, false], [13, 19, 7, 7, "usage", "", false, false], [13, 19, 14, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "computer", "science", "and", "artificial", "intelligence", ",", "CycL", "is", "an", "ontology", "language", "used", "in", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "In computer science and artificial intelligence, CycL is an ontology language used in Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 2], [3, 11], [12, 19], [20, 23], [24, 34], [35, 47], [47, 48], [49, 53], [54, 56], [57, 59], [60, 68], [69, 77], [78, 82], [83, 85], [86, 90], [91, 96], [96, 98], [99, 102], [103, 113], [114, 121], [121, 122]]}
{"doc_key": "ai-train-86", "ner": [[1, 5, "task"], [6, 8, "metrics"], [15, 18, "metrics"], [20, 27, "metrics"], [35, 39, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 1, 5, "part-of", "", false, false], [15, 18, 6, 8, "named", "", false, false], [20, 27, 6, 8, "named", "", false, false], [35, 39, 6, 8, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "of", "the", "squared", "deviations", "of", "the", "predictions", "produced", "by", "a", "model", "estimated", "over", "a", "given", "sample", "space", "from", "the", "ACTUAL", "values", "in", "the", "out", "-", "of", "-", "sample", "test", "space", "."], "sentence-detokenized": "Also in regression analysis, the mean squared error, often referred to as the mean squared prediction error or out-of-sample mean squared error, can refer to the average of the squared deviations of the predictions produced by a model estimated over a given sample space from the ACTUAL values in the out-of-sample test space.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 32], [33, 37], [38, 45], [46, 51], [51, 52], [53, 58], [59, 67], [68, 70], [71, 73], [74, 77], [78, 82], [83, 90], [91, 101], [102, 107], [108, 110], [111, 114], [114, 115], [115, 117], [117, 118], [118, 124], [125, 129], [130, 137], [138, 143], [143, 144], [145, 148], [149, 154], [155, 157], [158, 161], [162, 169], [170, 172], [173, 176], [177, 184], [185, 195], [196, 198], [199, 202], [203, 214], [215, 223], [224, 226], [227, 228], [229, 234], [235, 244], [245, 249], [250, 251], [252, 257], [258, 264], [265, 270], [271, 275], [276, 279], [280, 286], [287, 293], [294, 296], [297, 300], [301, 304], [304, 305], [305, 307], [307, 308], [308, 314], [315, 319], [320, 325], [325, 326]]}
{"doc_key": "ai-train-87", "ner": [[5, 7, "algorithm"], [17, 21, "algorithm"], [34, 37, "metrics"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[5, 7, 17, 21, "named", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["The", "results", "show", "that", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptors", "maintaining", "a", "slight", "advantage", "in", "the", "number", "of", "undetected", "errors", "with", "fixed", "FALSE", "-", "positive", "error", "rates", "in", "both", "datasets", "."], "sentence-detokenized": "The results show that the C-HOG and R-HOG block descriptors perform comparably, with the C-HOG descriptors maintaining a slight advantage in the number of undetected errors with fixed FALSE-positive error rates in both datasets.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 21], [22, 25], [26, 27], [27, 28], [28, 31], [32, 35], [36, 38], [38, 41], [42, 47], [48, 59], [60, 67], [68, 78], [78, 79], [80, 84], [85, 88], [89, 90], [90, 91], [91, 94], [95, 106], [107, 118], [119, 120], [121, 127], [128, 137], [138, 140], [141, 144], [145, 151], [152, 154], [155, 165], [166, 172], [173, 177], [178, 183], [184, 189], [189, 190], [190, 198], [199, 204], [205, 210], [211, 213], [214, 218], [219, 227], [227, 228]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 9, "misc"], [11, 13, "algorithm"], [15, 16, "algorithm"], [17, 19, "algorithm"], [21, 23, "algorithm"], [25, 27, "algorithm"], [29, 30, "misc"], [35, 37, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 9, "usage", "", false, false], [11, 13, 29, 30, "usage", "", false, false], [15, 16, 29, 30, "usage", "", false, false], [17, 19, 29, 30, "usage", "", false, false], [21, 23, 29, 30, "usage", "", false, false], [25, 27, 29, 30, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "identification", "algorithms", "include", "principal", "component", "analysis", "using", "feature", "surfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "with", "Fisherface", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neuronally", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular identification algorithms include principal component analysis using feature surfaces, linear discriminant analysis, elastic matching with Fisherface algorithm, hidden Markov model, multilinear subspace learning using tensor representation, and neuronally motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 22], [23, 33], [34, 41], [42, 51], [52, 61], [62, 70], [71, 76], [77, 84], [85, 93], [93, 94], [95, 101], [102, 114], [115, 123], [123, 124], [125, 132], [133, 141], [142, 146], [147, 157], [158, 167], [167, 168], [169, 175], [176, 182], [183, 188], [188, 189], [190, 201], [202, 210], [211, 219], [220, 225], [226, 232], [233, 247], [247, 248], [249, 252], [253, 263], [264, 273], [274, 281], [282, 286], [287, 295], [295, 296]]}
{"doc_key": "ai-train-89", "ner": [[2, 7, "misc"], [15, 18, "location"], [37, 40, "location"], [54, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[15, 18, 2, 7, "temporal", "", false, false], [37, 40, 2, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "can", "now", "be", "restricted", "to", "the", "Scotiabank", "Theatre", "Toronto", "-", "one", "of", "the", "main", "venues", "for", "the", "festival", "-", "and", "can", "be", "shown", "elsewhere", "(", "such", "as", "at", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "by", "a", "service", "like", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films can now be restricted to the Scotiabank Theatre Toronto - one of the main venues for the festival - and can be shown elsewhere (such as at TIFF Bell Lightbox and other local cinemas) if they are distributed by a service like Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 94], [95, 105], [106, 113], [114, 121], [122, 123], [124, 127], [128, 130], [131, 134], [135, 139], [140, 146], [147, 150], [151, 154], [155, 163], [164, 165], [166, 169], [170, 173], [174, 176], [177, 182], [183, 192], [193, 194], [194, 198], [199, 201], [202, 204], [205, 209], [210, 214], [215, 223], [224, 227], [228, 233], [234, 239], [240, 247], [247, 248], [249, 251], [252, 256], [257, 260], [261, 272], [273, 275], [276, 277], [278, 285], [286, 290], [291, 298], [298, 299]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 2, "researcher"], [5, 8, "organisation"], [26, 31, "product"], [40, 42, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 6], "relations": [[0, 0, 5, 8, "related-to", "purchases", false, false], [5, 8, 2, 2, "origin", "founded_by", false, false], [26, 31, 0, 0, "artifact", "", false, false]], "relations_mapping_to_source": [0, 3, 4], "sentence": ["Unimation", "acquired", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "manufacturing", "a", "new", "model", "of", "the", "Programmable", "Universal", "Machine", "for", "Assembly", "robotic", "arm", "using", "Scheinman", "'s", "cutting", "-", "edge", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation acquired Victor Scheinman's Vicarm Inc. in 1977, and with Scheinman's help, the company created and began manufacturing a new model of the Programmable Universal Machine for Assembly robotic arm using Scheinman's cutting-edge VAL programming language.", "token2charspan": [[0, 9], [10, 18], [19, 25], [26, 35], [35, 37], [38, 44], [45, 49], [50, 52], [53, 57], [57, 58], [59, 62], [63, 67], [68, 77], [77, 79], [80, 84], [84, 85], [86, 89], [90, 97], [98, 105], [106, 109], [110, 115], [116, 129], [130, 131], [132, 135], [136, 141], [142, 144], [145, 148], [149, 161], [162, 171], [172, 179], [180, 183], [184, 192], [193, 200], [201, 204], [205, 210], [211, 220], [220, 222], [223, 230], [230, 231], [231, 235], [236, 239], [240, 251], [252, 260], [260, 261]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [9, 11, "algorithm"], [12, 17, "product"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 1, 9, 11, "origin", "implementation_of", false, false], [0, 1, 12, 17, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[12, 16, "product"], [19, 27, "misc"]], "ner_mapping_to_source": [1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20", "000", "times", "according", "to", "Google", "Scholar", "and", "was", "also", "awarded", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", "in", "2016", ",", "demonstrating", "that", "the", "paper", "has", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "its", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20 000 times according to Google Scholar and was also awarded the IEEE Signal Processing Society Sustained Impact Award in 2016, demonstrating that the paper has an unusually high impact for at least 10 years after its publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 47], [48, 51], [52, 57], [58, 67], [68, 70], [71, 77], [78, 85], [86, 89], [90, 93], [94, 98], [99, 106], [107, 110], [111, 115], [116, 122], [123, 133], [134, 141], [142, 151], [152, 158], [159, 164], [165, 167], [168, 172], [172, 173], [174, 187], [188, 192], [193, 196], [197, 202], [203, 206], [207, 209], [210, 219], [220, 224], [225, 231], [232, 235], [236, 238], [239, 244], [245, 247], [248, 253], [254, 259], [260, 263], [264, 275], [275, 276]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [19, 31, "product"], [36, 38, "product"], [41, 41, "organisation"], [42, 42, "product"], [47, 47, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 41, 41, "artifact", "", false, false], [19, 31, 0, 1, "related-to", "performs", false, false], [19, 31, 36, 38, "part-of", "", false, false], [41, 41, 47, 47, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "starting", "to", "become", "completely", "indistinguishable", "from", "real", "human", "voice", ",", "with", "the", "introduction", "in", "2016", "of", "Adobe", "Voco", "voice", "editing", "and", "production", "software", ",", "a", "prototype", "of", "which", "is", "to", "be", "part", "of", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", "."], "sentence-detokenized": "Speech synthesis is starting to become completely indistinguishable from real human voice, with the introduction in 2016 of Adobe Voco voice editing and production software, a prototype of which is to be part of Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 28], [29, 31], [32, 38], [39, 49], [50, 67], [68, 72], [73, 77], [78, 83], [84, 89], [89, 90], [91, 95], [96, 99], [100, 112], [113, 115], [116, 120], [121, 123], [124, 129], [130, 134], [135, 140], [141, 148], [149, 152], [153, 163], [164, 172], [172, 173], [174, 175], [176, 185], [186, 188], [189, 194], [195, 197], [198, 200], [201, 203], [204, 208], [209, 211], [212, 217], [218, 226], [227, 232], [232, 233], [234, 237], [238, 246], [247, 254], [254, 255], [256, 257], [258, 267], [268, 272], [273, 279], [279, 280]]}
{"doc_key": "ai-train-94", "ner": [[0, 2, "researcher"], [6, 9, "organisation"], [15, 20, "organisation"], [27, 27, "conference"], [33, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 2, 6, 9, "role", "", false, false], [0, 2, 15, 20, "role", "", false, false], [0, 2, 27, 27, "role", "", false, false], [0, 2, 33, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "Honorary", "Fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "member", "of", "the", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an Honorary Fellow of the Neuroscience Research Program, a member of the American Academy of Arts and Sciences, a founding member of the AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 123], [124, 132], [133, 139], [140, 142], [143, 146], [147, 151], [152, 155], [156, 157], [158, 166], [167, 173], [174, 176], [177, 180], [181, 189], [190, 199], [200, 203], [204, 209], [210, 218], [218, 219]]}
{"doc_key": "ai-train-95", "ner": [[5, 6, "task"], [8, 11, "task"], [19, 20, "task"], [23, 23, "misc"], [24, 25, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 19, 20, "cause-effect", "", false, false], [8, 11, 19, 20, "cause-effect", "", false, false], [24, 25, 19, 20, "topic", "", false, false], [24, 25, 23, 23, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Encouraged", "by", "the", "success", "of", "speech", "recognition", "and", "speech", "synthesis", ",", "the", "1990s", "saw", "the", "start", "of", "research", "into", "speech", "translation", "with", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "Encouraged by the success of speech recognition and speech synthesis, the 1990s saw the start of research into speech translation with the German Verbmobil project.", "token2charspan": [[0, 10], [11, 13], [14, 17], [18, 25], [26, 28], [29, 35], [36, 47], [48, 51], [52, 58], [59, 68], [68, 69], [70, 73], [74, 79], [80, 83], [84, 87], [88, 93], [94, 96], [97, 105], [106, 110], [111, 117], [118, 129], [130, 134], [135, 138], [139, 145], [146, 155], [156, 163], [163, 164]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [14, 16, "algorithm"], [20, 21, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 4, 8, 9, "role", "", false, false], [14, 16, 3, 4, "origin", "", false, false], [14, 16, 8, 9, "origin", "", false, false], [14, 16, 11, 12, "origin", "", false, false], [20, 21, 14, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisors", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forget", "gate", "(", "also", "called", "keep", "gate", ")", "in", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisors J\u00fcrgen Schmidhuber and Fred Cummins introduced the forget gate (also called keep gate) in the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 87], [88, 94], [95, 99], [100, 101], [101, 105], [106, 112], [113, 117], [118, 122], [122, 123], [124, 126], [127, 130], [131, 135], [136, 148], [148, 149]]}
{"doc_key": "ai-train-97", "ner": [[0, 3, "field"], [5, 8, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 12, 0, 3, "part-of", "", false, false], [9, 12, 5, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sine", "function", "is", "generally", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sine function is generally defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 94], [95, 102], [103, 105], [106, 113]]}
{"doc_key": "ai-train-98", "ner": [[3, 4, "field"], [10, 12, "researcher"], [19, 23, "conference"], [27, 30, "organisation"], [32, 32, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 4, 10, 12, "origin", "coined_term", false, false], [10, 12, 19, 23, "role", "", false, false], [10, 12, 27, 30, "role", "", false, false], [32, 32, 27, 30, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "\"", "computational", "linguistics", "\"", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term \"computational linguistics\" itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 10], [10, 23], [24, 35], [35, 36], [37, 43], [44, 47], [48, 53], [54, 60], [61, 63], [64, 69], [70, 74], [74, 75], [76, 77], [78, 86], [87, 93], [94, 96], [97, 101], [102, 105], [106, 117], [118, 121], [122, 135], [136, 147], [148, 151], [152, 155], [156, 169], [170, 179], [180, 182], [183, 196], [197, 208], [209, 210], [210, 214], [214, 215], [215, 216]]}
{"doc_key": "ai-train-99", "ner": [[7, 13, "misc"], [18, 18, "misc"], [33, 33, "metrics"], [32, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[32, 36, 33, 33, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one", "-dimensional", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPD", ",", "to", "solve", "the", "coefficients", "of", "the", "digital", "pre-designer", "polynomials", "and", "minimize", "the", "mean", "square", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "supersampled", "at", "a", "rate", "that", "allows", "the", "nonlinear", "products", "of", "the", "digital", "pre-designer", "order", "to", "be", "stored", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional polynomial-based memory (or memoryless) DPD, to solve the coefficients of the digital pre-designer polynomials and minimize the mean square error (MSE), the distorted output of the nonlinear system must be supersampled at a rate that allows the nonlinear products of the digital pre-designer order to be stored.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 35], [35, 47], [48, 58], [58, 59], [59, 64], [65, 71], [72, 73], [73, 75], [76, 86], [86, 87], [88, 91], [91, 92], [93, 95], [96, 101], [102, 105], [106, 118], [119, 121], [122, 125], [126, 133], [134, 146], [147, 158], [159, 162], [163, 171], [172, 175], [176, 180], [181, 187], [188, 193], [194, 195], [195, 198], [198, 199], [199, 200], [201, 204], [205, 214], [215, 221], [222, 224], [225, 228], [229, 238], [239, 245], [246, 250], [251, 253], [254, 266], [267, 269], [270, 271], [272, 276], [277, 281], [282, 288], [289, 292], [293, 302], [303, 311], [312, 314], [315, 318], [319, 326], [327, 339], [340, 345], [346, 348], [349, 351], [352, 358], [358, 359]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [10, 10, "location"], [12, 12, "location"], [13, 16, "country"], [20, 20, "location"], [22, 22, "country"], [33, 43, "organisation"], [44, 49, "organisation"], [50, 51, "location"], [55, 61, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 10, 10, "physical", "", false, false], [0, 1, 44, 49, "physical", "", false, false], [0, 1, 55, 61, "role", "", false, false], [10, 10, 12, 12, "physical", "", false, false], [12, 12, 13, 16, "physical", "", false, false], [33, 43, 44, 49, "part-of", "", false, false], [44, 49, 50, 51, "physical", "", false, false], [55, 61, 33, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "October", "5", ",", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldovan", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "an", "American", "senior", "research", "scientist", "(", "computer", "scientist", ")", "at", "MIT", "'s", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", ",", "Cambridge", ",", "and", "the", "director", "of", "the", "Laboratory", "'s", "InfoLab", "group", "."], "sentence-detokenized": "Boris Katz, (born October 5, 1947 in Chi\u0219in\u0103u, Moldovan SSR, Soviet Union, (now Chi\u0219in\u0103u, Moldova)) is an American senior research scientist (computer scientist) at MIT's Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, Cambridge, and the director of the Laboratory's InfoLab group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 25], [26, 27], [27, 28], [29, 33], [34, 36], [37, 45], [45, 46], [47, 55], [56, 59], [59, 60], [61, 67], [68, 73], [73, 74], [75, 76], [76, 79], [80, 88], [88, 89], [90, 97], [97, 98], [98, 99], [100, 102], [103, 105], [106, 114], [115, 121], [122, 130], [131, 140], [141, 142], [142, 150], [151, 160], [160, 161], [162, 164], [165, 168], [168, 170], [171, 179], [180, 187], [188, 191], [192, 202], [203, 215], [216, 226], [227, 229], [230, 233], [234, 247], [248, 257], [258, 260], [261, 271], [271, 272], [273, 282], [282, 283], [284, 287], [288, 291], [292, 300], [301, 303], [304, 307], [308, 318], [318, 320], [321, 328], [329, 334], [334, 335]]}
