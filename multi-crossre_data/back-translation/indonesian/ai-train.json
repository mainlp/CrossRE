{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [14, 15, "field"], [17, 18, "task"], [20, 21, "task"], [25, 27, "task"], [30, 31, "field"], [32, 34, "researcher"], [36, 38, "researcher"], [40, 41, "researcher"], [43, 44, "researcher"], [46, 48, "researcher"], [50, 51, "researcher"], [53, 54, "researcher"], [56, 57, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 14, 15, "part-of", "", false, false], [3, 7, 14, 15, "usage", "", false, false], [3, 7, 17, 18, "part-of", "", false, false], [3, 7, 17, 18, "usage", "", false, false], [3, 7, 20, 21, "part-of", "", false, false], [3, 7, 20, 21, "usage", "", false, false], [3, 7, 30, 31, "part-of", "", false, false], [3, 7, 30, 31, "usage", "", false, false], [25, 27, 20, 21, "part-of", "", false, false], [25, 27, 20, 21, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "of", "opinion", "-", "based", "recommender", "systems", "utilise", "a", "variety", "of", "techniques", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C..", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches of opinion-based recommender systems utilise a variety of techniques including text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C.. Liang, R.C. Guan, D. Xu, (2019), , 21(5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 55], [56, 63], [64, 65], [66, 73], [74, 76], [77, 87], [88, 97], [98, 102], [103, 109], [109, 110], [111, 122], [123, 132], [132, 133], [134, 143], [144, 152], [153, 154], [154, 157], [158, 162], [163, 173], [174, 183], [184, 192], [192, 193], [194, 197], [198, 202], [203, 211], [212, 215], [215, 216], [217, 221], [221, 222], [223, 224], [224, 225], [226, 231], [231, 232], [233, 237], [238, 241], [241, 242], [243, 247], [248, 253], [253, 254], [255, 256], [256, 257], [258, 261], [261, 262], [263, 268], [269, 274], [274, 275], [276, 280], [281, 285], [285, 286], [287, 289], [290, 292], [292, 293], [294, 295], [295, 299], [299, 300], [300, 301], [302, 303], [304, 306], [306, 307], [307, 308], [308, 309], [309, 310], [311, 317], [317, 318]]}
{"doc_key": "ai-train-2", "ner": [[8, 8, "university"], [14, 15, "researcher"], [17, 18, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[14, 15, 8, 8, "physical", "", false, false], [14, 15, 8, 8, "role", "", false, false], [17, 18, 8, 8, "physical", "", false, false], [17, 18, 8, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Proponents", "of", "procedural", "representation", "are", "mainly", "centred", "at", "MIT", ",", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Proponents of procedural representation are mainly centred at MIT, under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 10], [11, 13], [14, 24], [25, 39], [40, 43], [44, 50], [51, 58], [59, 61], [62, 65], [65, 66], [67, 72], [73, 76], [77, 87], [88, 90], [91, 97], [98, 104], [105, 108], [109, 116], [117, 123], [123, 124]]}
{"doc_key": "ai-train-3", "ner": [[9, 9, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 37], [38, 47], [48, 51], [52, 59], [60, 62], [63, 67], [67, 68]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [18, 18, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 18, 18, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "in", "solving", "linear", "and", "nonlinear", "problems", "numerically", ",", "and", "to", "perform", "other", "numerical", "experiments", "using", "mostly", "MATLAB", "compatible", "."], "sentence-detokenized": "Octave helps in solving linear and nonlinear problems numerically, and to perform other numerical experiments using mostly MATLAB compatible.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 23], [24, 30], [31, 34], [35, 44], [45, 53], [54, 65], [65, 66], [67, 70], [71, 73], [74, 81], [82, 87], [88, 97], [98, 109], [110, 115], [116, 122], [123, 129], [130, 140], [140, 141]]}
{"doc_key": "ai-train-5", "ner": [[3, 6, "algorithm"], [10, 11, "misc"], [13, 14, "researcher"], [20, 22, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 6, 13, 14, "origin", "", false, false], [10, 11, 13, 14, "origin", "", false, false], [13, 14, 20, 22, "physical", "", false, false], [13, 14, 20, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", "as", "well", "as", "unsupervised", "methods", "by", "Geoff", "Hinton", "and", "his", "colleagues", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "train", "deep", "and", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and his colleagues at the University of Toronto can be used to train deep and highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [43, 45], [46, 50], [51, 53], [54, 66], [67, 74], [75, 77], [78, 83], [84, 90], [91, 94], [95, 98], [99, 109], [110, 112], [113, 116], [117, 127], [128, 130], [131, 138], [139, 142], [143, 145], [146, 150], [151, 153], [154, 159], [160, 164], [165, 168], [169, 175], [176, 185], [186, 192], [193, 206], [206, 207], [208, 209], [209, 210], [210, 214], [215, 222]]}
{"doc_key": "ai-train-6", "ner": [[4, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalently", "using", "the", "DCG", "notation", ":"], "sentence-detokenized": "or equivalently using the DCG notation:", "token2charspan": [[0, 2], [3, 15], [16, 21], [22, 25], [26, 29], [30, 38], [38, 39]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 10, "algorithm"], [14, 15, "algorithm"], [19, 21, "algorithm"], [24, 24, "algorithm"], [26, 27, "algorithm"], [42, 45, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 10, "type-of", "", false, false], [0, 3, 14, 15, "usage", "part-of?", true, false], [14, 15, 19, 21, "compare", "", false, false], [24, 24, 19, 21, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "apply", "competitive", "learning", "as", "opposed", "to", "error", "correction", "learning", "such", "as", "backpropagation", "with", "gradient", "descent", ")", ",", "and", "in", "the", "sense", "that", "they", "use", "neighbourhood", "functions", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks in that they apply competitive learning as opposed to error correction learning such as backpropagation with gradient descent), and in the sense that they use neighbourhood functions to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 84], [85, 96], [97, 105], [106, 108], [109, 116], [117, 119], [120, 125], [126, 136], [137, 145], [146, 150], [151, 153], [154, 169], [170, 174], [175, 183], [184, 191], [191, 192], [192, 193], [194, 197], [198, 200], [201, 204], [205, 210], [211, 215], [216, 220], [221, 224], [225, 238], [239, 248], [249, 251], [252, 260], [261, 264], [265, 276], [277, 287], [288, 290], [291, 294], [295, 300], [301, 306], [306, 307]]}
{"doc_key": "ai-train-8", "ner": [[15, 17, "organisation"], [30, 31, "misc"], [39, 41, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "it", "has", "been", "recommended", "by", "several", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "that", "dynamic", "range", "measurements", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", ",", "which", "is", "then", "filtered", "in", "the", "noise", "floor", "measurement", "used", "in", "determining", "dynamic", "range", ".", "This", "is", "to", "avoid", "dubious", "measurements", "based", "on", "the", "use", "of", "blank", "media", ",", "or", "noiseless", "circuits", "."], "sentence-detokenized": "Since the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that dynamic range measurements be made in the presence of an audio signal, which is then filtered in the noise floor measurement used in determining dynamic range. This is to avoid dubious measurements based on the use of blank media, or noiseless circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 25], [26, 29], [30, 34], [35, 46], [47, 49], [50, 57], [58, 69], [69, 70], [71, 80], [81, 84], [85, 90], [91, 102], [103, 110], [110, 111], [112, 116], [117, 124], [125, 130], [131, 143], [144, 146], [147, 151], [152, 154], [155, 158], [159, 167], [168, 170], [171, 173], [174, 179], [180, 186], [186, 187], [188, 193], [194, 196], [197, 201], [202, 210], [211, 213], [214, 217], [218, 223], [224, 229], [230, 241], [242, 246], [247, 249], [250, 261], [262, 269], [270, 275], [275, 276], [277, 281], [282, 284], [285, 287], [288, 293], [294, 301], [302, 314], [315, 320], [321, 323], [324, 327], [328, 331], [332, 334], [335, 340], [341, 346], [346, 347], [348, 350], [351, 360], [361, 369], [369, 370]]}
{"doc_key": "ai-train-9", "ner": [[5, 5, "misc"], [16, 17, "task"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [29, 33, "task"], [35, 37, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[5, 5, 16, 17, "part-of", "concept_used_in", true, false], [5, 5, 19, 20, "part-of", "concept_used_in", false, false], [5, 5, 22, 23, "part-of", "concept_used_in", false, false], [5, 5, 25, 26, "part-of", "concept_used_in", false, false], [5, 5, 29, 33, "part-of", "concept_used_in", false, false], [5, 5, 35, 37, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["The", "techniques", "used", "in", "creating", "eigenfaces", "and", "using", "them", "for", "recognition", "are", "also", "used", "outside", "of", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "speech", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "imaging", "analysis", "."], "sentence-detokenized": "The techniques used in creating eigenfaces and using them for recognition are also used outside of face recognition: handwriting recognition, lip reading, speech recognition, sign language/hand gesture interpretation and medical imaging analysis.", "token2charspan": [[0, 3], [4, 14], [15, 19], [20, 22], [23, 31], [32, 42], [43, 46], [47, 52], [53, 57], [58, 61], [62, 73], [74, 77], [78, 82], [83, 87], [88, 95], [96, 98], [99, 103], [104, 115], [115, 116], [117, 128], [129, 140], [140, 141], [142, 145], [146, 153], [153, 154], [155, 161], [162, 173], [173, 174], [175, 179], [180, 188], [188, 189], [189, 193], [194, 201], [202, 216], [217, 220], [221, 228], [229, 236], [237, 245], [245, 246]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [10, 14, "organisation"], [16, 16, "organisation"], [20, 23, "organisation"], [29, 30, "organisation"], [26, 38, "organisation"], [39, 43, "organisation"], [45, 45, "organisation"], [50, 53, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 14, 0, 3, "part-of", "", false, false], [16, 16, 10, 14, "named", "", false, false], [20, 23, 0, 3, "part-of", "", false, false], [29, 30, 0, 3, "part-of", "", false, false], [26, 38, 0, 3, "part-of", "", false, false], [39, 43, 0, 3, "part-of", "", false, false], [45, 45, 39, 43, "named", "", false, false], [50, 53, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "is", "the", "umbrella", "under", "which", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defence", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", ",", "and", "the", "Office", "of", "Naval", "Research", "coordinate", "studies", "to", "inform", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation is the umbrella under which the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defence, the Defense Advanced Research Projects Agency (DARPA), and the Office of Naval Research coordinate studies to inform strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 34], [35, 38], [39, 47], [48, 53], [54, 59], [60, 63], [64, 72], [73, 84], [85, 88], [89, 94], [95, 109], [110, 111], [111, 115], [115, 116], [116, 117], [118, 121], [122, 124], [125, 135], [136, 138], [139, 145], [145, 146], [147, 150], [151, 153], [154, 164], [165, 167], [168, 176], [177, 181], [181, 182], [183, 186], [187, 189], [190, 200], [201, 203], [204, 211], [211, 212], [213, 216], [217, 224], [225, 233], [234, 242], [243, 251], [252, 258], [259, 260], [260, 265], [265, 266], [266, 267], [268, 271], [272, 275], [276, 282], [283, 285], [286, 291], [292, 300], [301, 311], [312, 319], [320, 322], [323, 329], [330, 339], [340, 348], [349, 351], [352, 357], [358, 371], [371, 372]]}
{"doc_key": "ai-train-11", "ner": [[5, 6, "metrics"], [9, 11, "algorithm"], [14, 15, "researcher"], [20, 20, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 9, 11, "part-of", "", false, false], [14, 15, 20, 20, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "fast", "method", "to", "calculate", "maximum", "likelihood", "estimates", "for", "probit", "models", "was", "proposed", "by", "Ronald", "Fisher", "as", "an", "appendix", "to", "Bliss", "'", "work", "in", "1935", "."], "sentence-detokenized": "A fast method to calculate maximum likelihood estimates for probit models was proposed by Ronald Fisher as an appendix to Bliss' work in 1935.", "token2charspan": [[0, 1], [2, 6], [7, 13], [14, 16], [17, 26], [27, 34], [35, 45], [46, 55], [56, 59], [60, 66], [67, 73], [74, 77], [78, 86], [87, 89], [90, 96], [97, 103], [104, 106], [107, 109], [110, 118], [119, 121], [122, 127], [127, 128], [129, 133], [134, 136], [137, 141], [141, 142]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 15, "product"], [19, 19, "organisation"], [18, 18, "product"], [22, 22, "organisation"], [23, 25, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[18, 18, 14, 15, "usage", "uses_software", false, false], [18, 18, 19, 19, "artifact", "", false, false], [18, 18, 23, 25, "named", "", false, false], [23, 25, 22, 22, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Some", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "supports", "BabelFish", "AltaVista", "(", "now", "Yahoo", "Babelfish", "as", "of", "9", "May", "2008", ")", "."], "sentence-detokenized": "Some of these programs are available online, such as Google Translate and the SYSTRAN system that supports BabelFish AltaVista (now Yahoo Babelfish as of 9 May 2008).", "token2charspan": [[0, 4], [5, 7], [8, 13], [14, 22], [23, 26], [27, 36], [37, 43], [43, 44], [45, 49], [50, 52], [53, 59], [60, 69], [70, 73], [74, 77], [78, 85], [86, 92], [93, 97], [98, 106], [107, 116], [117, 126], [127, 128], [128, 131], [132, 137], [138, 147], [148, 150], [151, 153], [154, 155], [156, 159], [160, 164], [164, 165], [165, 166]]}
{"doc_key": "ai-train-13", "ner": [[2, 2, "researcher"], [6, 7, "researcher"], [9, 10, "researcher"], [19, 21, "field"], [25, 26, "misc"], [31, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[2, 2, 19, 21, "related-to", "", true, false], [2, 2, 25, 26, "related-to", "", true, false], [2, 2, 31, 32, "related-to", "", true, false], [6, 7, 19, 21, "related-to", "", true, false], [6, 7, 25, 26, "related-to", "", true, false], [6, 7, 31, 32, "related-to", "", true, false], [9, 10, 19, 21, "related-to", "", true, false], [9, 10, 25, 26, "related-to", "", true, false], [9, 10, 31, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-", "motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002 Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [8, 14], [14, 15], [16, 24], [25, 29], [30, 36], [37, 48], [49, 52], [53, 58], [59, 63], [63, 64], [65, 74], [75, 78], [79, 88], [89, 90], [91, 103], [104, 110], [111, 113], [114, 124], [125, 132], [133, 145], [146, 151], [152, 154], [155, 164], [165, 176], [177, 183], [184, 187], [188, 194], [194, 195], [195, 204], [205, 218], [219, 227], [227, 228]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 19, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 19, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measure", "."], "sentence-detokenized": "The most common way is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 25], [26, 29], [30, 33], [34, 36], [36, 37], [37, 43], [44, 49], [50, 51], [51, 57], [57, 58], [58, 66], [67, 77], [78, 81], [82, 89], [90, 100], [100, 101], [102, 109], [109, 110]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [15, 16, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [0, 0, 15, 16, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "learning", "schemes", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides learning schemes, models and algorithms and can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 85], [86, 87], [88, 91], [92, 98], [99, 106], [106, 107], [108, 113], [114, 120], [120, 121], [122, 127], [128, 136], [136, 137], [138, 140], [141, 149], [150, 154], [154, 155]]}
{"doc_key": "ai-train-16", "ner": [[10, 11, "field"], [13, 14, "task"], [19, 21, "misc"], [33, 35, "programlang"], [0, 39, "product"]], "ner_mapping_to_source": [1, 2, 3, 4, 5], "relations": [[0, 39, 33, 35, "general-affiliation", "", true, false]], "relations_mapping_to_source": [4], "sentence": ["Weka", "contains", "a", "collection", "of", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modelling", ",", "along", "with", "a", "graphical", "user", "interface", "for", "easy", "access", "to", "these", "functions", ".", "But", "the", "latest", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "whose", "development", "began", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "particularly", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "Weka contains a collection of visualisation tools and algorithms for data analysis and predictive modelling, along with a graphical user interface for easy access to these functions. But the latest fully Java-based version (Weka 3), whose development began in 1997, is now used in many different application areas, particularly for educational and research purposes.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 107], [107, 108], [109, 114], [115, 119], [120, 121], [122, 131], [132, 136], [137, 146], [147, 150], [151, 155], [156, 162], [163, 165], [166, 171], [172, 181], [181, 182], [183, 186], [187, 190], [191, 197], [198, 203], [204, 208], [208, 209], [209, 214], [215, 222], [223, 224], [224, 228], [229, 230], [230, 231], [231, 232], [233, 238], [239, 250], [251, 256], [257, 259], [260, 264], [264, 265], [266, 268], [269, 272], [273, 277], [278, 280], [281, 285], [286, 295], [296, 307], [308, 313], [313, 314], [315, 327], [328, 331], [332, 343], [344, 347], [348, 356], [357, 365], [365, 366]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [13, 20, "misc"], [23, 25, "misc"], [28, 35, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 20, 0, 0, "topic", "", false, false], [13, 20, 23, 25, "win-defeat", "", false, false], [23, 25, 28, 35, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "enjoyed", "significant", "acclaim", ",", "with", "his", "paper", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "winning", "the", "Best", "Paper", "award", "at", "the", "1982", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and enjoyed significant acclaim, with his paper Heuretics: Theoretical and Study of Heuristic Rules winning the Best Paper award at the 1982 Association for the Advancement of Artificial Intelligence.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 53], [54, 65], [66, 73], [73, 74], [75, 79], [80, 83], [84, 89], [90, 99], [99, 100], [101, 112], [113, 116], [117, 122], [123, 125], [126, 135], [136, 141], [142, 149], [150, 153], [154, 158], [159, 164], [165, 170], [171, 173], [174, 177], [178, 182], [183, 194], [195, 198], [199, 202], [203, 214], [215, 217], [218, 228], [229, 241], [241, 242]]}
{"doc_key": "ai-train-18", "ner": [[8, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "allow", "for", "multiple", "entities", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To allow for multiple entities, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 8], [9, 12], [13, 21], [22, 30], [30, 31], [32, 33], [34, 42], [43, 48], [49, 53], [54, 56], [57, 67], [68, 71], [72, 76], [77, 84], [84, 85]]}
{"doc_key": "ai-train-19", "ner": [[8, 9, "product"], [11, 12, "product"], [14, 15, "product"], [17, 18, "product"], [21, 22, "product"], [24, 26, "product"], [33, 37, "product"], [40, 41, "product"], [43, 44, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 9, 24, 26, "type-of", "", false, false], [11, 12, 24, 26, "type-of", "", false, false], [14, 15, 24, 26, "type-of", "", false, false], [17, 18, 24, 26, "type-of", "", false, false], [21, 22, 24, 26, "type-of", "", false, false], [40, 41, 33, 37, "type-of", "", false, false], [43, 44, 33, 37, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "conversational", "assistants", "such", "as", "Apple", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", ",", "and", "Samsung", "Bixby", ",", "Voice", "Portal", "is", "now", "accessible", "through", "mobile", "devices", "and", "Far", "Field", "voice", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of conversational assistants such as Apple Siri, Amazon Alexa, Google Assistant, Microsoft Cortana, and Samsung Bixby, Voice Portal is now accessible through mobile devices and Far Field voice smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 33], [34, 44], [45, 49], [50, 52], [53, 58], [59, 63], [63, 64], [65, 71], [72, 77], [77, 78], [79, 85], [86, 95], [95, 96], [97, 106], [107, 114], [114, 115], [116, 119], [120, 127], [128, 133], [133, 134], [135, 140], [141, 147], [148, 150], [151, 154], [155, 165], [166, 173], [174, 180], [181, 188], [189, 192], [193, 196], [197, 202], [203, 208], [209, 214], [215, 223], [224, 228], [229, 231], [232, 238], [239, 243], [244, 247], [248, 254], [255, 259], [259, 260]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 15, "algorithm"], [18, 18, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "type-of", "", false, false], [9, 11, 2, 3, "type-of", "", false, false], [13, 15, 2, 3, "type-of", "", false, false], [18, 18, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "Naive", "Bayes", "classifier", ",", "Support", "vector", "machine", ",", "mixture", "of", "Gaussians", ",", "and", "networks", "."], "sentence-detokenized": "Examples of supervised learning are Naive Bayes classifier, Support vector machine, mixture of Gaussians, and networks.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 41], [42, 47], [48, 58], [58, 59], [60, 67], [68, 74], [75, 82], [82, 83], [84, 91], [92, 94], [95, 104], [104, 105], [106, 109], [110, 118], [118, 119]]}
{"doc_key": "ai-train-21", "ner": [[4, 5, "algorithm"], [26, 28, "algorithm"], [30, 30, "task"], [35, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 5, 26, 28, "part-of", "", true, false], [35, 36, 30, 30, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "derive", "math", "O", "(", "\\", "sqrt", "{", "T}", ")", "/", "math", "regret", "bounds", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "for", "classification", ",", "which", "uses", "math", "hinge", "loss", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to derive math O(\\ sqrt {T})/math regret bounds for the online version of the support vector machine for classification, which uses math hinge loss v _t (w) =\\ max\\{0, 1 - y _t (w\\ cdot x _t)\\}/math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 39], [40, 44], [45, 46], [46, 47], [47, 48], [49, 53], [54, 55], [55, 57], [57, 58], [58, 59], [59, 63], [64, 70], [71, 77], [78, 81], [82, 85], [86, 92], [93, 100], [101, 103], [104, 107], [108, 115], [116, 122], [123, 130], [131, 134], [135, 149], [149, 150], [151, 156], [157, 161], [162, 166], [167, 172], [173, 177], [178, 179], [180, 182], [183, 184], [184, 185], [185, 186], [187, 189], [190, 193], [193, 194], [194, 195], [195, 196], [196, 197], [198, 199], [200, 201], [202, 203], [204, 206], [207, 208], [208, 209], [209, 210], [211, 215], [216, 217], [218, 220], [220, 221], [221, 223], [223, 224], [224, 228]]}
{"doc_key": "ai-train-22", "ner": [[3, 4, "task"], [6, 7, "task"], [9, 9, "task"], [11, 12, "task"], [14, 15, "task"], [17, 18, "task"], [20, 21, "task"], [23, 25, "task"], [27, 28, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Its", "applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "fusion", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "wildlife", "individual", "identification", "and", "match", "removal", "."], "sentence-detokenized": "Its applications include object recognition, robotic mapping and navigation, image fusion, 3D modelling, gesture recognition, video tracking, wildlife individual identification and match removal.", "token2charspan": [[0, 3], [4, 16], [17, 24], [25, 31], [32, 43], [43, 44], [45, 52], [53, 60], [61, 64], [65, 75], [75, 76], [77, 82], [83, 89], [89, 90], [91, 93], [94, 103], [103, 104], [105, 112], [113, 124], [124, 125], [126, 131], [132, 140], [140, 141], [142, 150], [151, 161], [162, 176], [177, 180], [181, 186], [187, 194], [194, 195]]}
{"doc_key": "ai-train-23", "ner": [[8, 9, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [29, 32, "university"], [34, 36, "university"], [38, 40, "university"], [42, 43, "university"], [45, 50, "university"], [52, 52, "university"], [56, 60, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[8, 9, 14, 15, "related-to", "", true, false], [8, 9, 17, 19, "related-to", "", true, false], [8, 9, 21, 22, "related-to", "", true, false], [8, 9, 24, 25, "related-to", "", true, false], [8, 9, 29, 32, "related-to", "", true, false], [8, 9, 34, 36, "related-to", "", true, false], [8, 9, 38, 40, "related-to", "", true, false], [8, 9, 42, 43, "related-to", "", true, false], [8, 9, 45, 50, "related-to", "", true, false], [8, 9, 52, 52, "related-to", "", true, false], [8, 9, 56, 60, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "researching", "pose", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", ",", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST), and University of California, Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 48], [49, 53], [54, 64], [64, 65], [66, 75], [76, 82], [83, 85], [86, 91], [92, 102], [102, 103], [104, 112], [113, 119], [120, 130], [130, 131], [132, 135], [136, 148], [148, 149], [150, 158], [159, 169], [169, 170], [171, 181], [182, 184], [185, 195], [195, 196], [197, 200], [201, 206], [206, 207], [208, 218], [219, 221], [222, 229], [229, 230], [231, 236], [237, 245], [246, 251], [251, 252], [253, 256], [257, 263], [263, 264], [265, 273], [274, 284], [285, 287], [288, 296], [297, 300], [301, 311], [312, 313], [313, 317], [317, 318], [318, 319], [320, 323], [324, 334], [335, 337], [338, 348], [348, 349], [350, 356], [356, 357]]}
{"doc_key": "ai-train-24", "ner": [[0, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Sigmoid", "Cross", "entropy", "loss", "function", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "0.1", "/", "math", "."], "sentence-detokenized": "Sigmoid Cross entropy loss function is used to predict K independent probability values in 0.1/math.", "token2charspan": [[0, 7], [8, 13], [14, 21], [22, 26], [27, 35], [36, 38], [39, 43], [44, 46], [47, 54], [55, 56], [57, 68], [69, 80], [81, 87], [88, 90], [91, 94], [94, 95], [95, 99], [99, 100]]}
{"doc_key": "ai-train-25", "ner": [[3, 5, "misc"], [7, 7, "field"], [9, 9, "field"], [12, 14, "university"], [17, 17, "country"], [21, 23, "misc"], [25, 28, "university"], [30, 30, "country"], [35, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 5, 7, 7, "topic", "", false, false], [3, 5, 9, 9, "topic", "", false, false], [3, 5, 12, 14, "physical", "", true, false], [12, 14, 17, 17, "physical", "", false, false], [21, 23, 25, 28, "physical", "", true, false], [25, 28, 30, 30, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Chair", "of", "Mathematics", "and", "Informatics", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", ",", "and", "the", "Toshiba", "Endowed", "Chair", "at", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "Professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Chair of Mathematics and Informatics at the University of Groningen in the Netherlands, and the Toshiba Endowed Chair at Tokyo Institute of Technology in Japan before becoming Professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 34], [35, 37], [38, 49], [50, 53], [54, 65], [66, 68], [69, 72], [73, 83], [84, 86], [87, 96], [97, 99], [100, 103], [104, 115], [115, 116], [117, 120], [121, 124], [125, 132], [133, 140], [141, 146], [147, 149], [150, 155], [156, 165], [166, 168], [169, 179], [180, 182], [183, 188], [189, 195], [196, 204], [205, 214], [215, 217], [218, 227], [227, 228]]}
{"doc_key": "ai-train-26", "ner": [[5, 6, "algorithm"], [11, 15, "algorithm"], [17, 17, "algorithm"], [21, 22, "researcher"], [24, 25, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 11, 15, "usage", "", true, false], [11, 15, 21, 22, "origin", "", false, false], [11, 15, 24, 25, "origin", "", false, false], [17, 17, 11, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "specifically", "used", "for", "recurrent", "neural", "networks", "is", "the", "1997", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "network", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "."], "sentence-detokenized": "Another technique specifically used for recurrent neural networks is the 1997 long short-term memory (LSTM) network by Sepp Hochreiter & J\u00fcrgen Schmidhuber.", "token2charspan": [[0, 7], [8, 17], [18, 30], [31, 35], [36, 39], [40, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 77], [78, 82], [83, 88], [88, 89], [89, 93], [94, 100], [101, 102], [102, 106], [106, 107], [108, 115], [116, 118], [119, 123], [124, 134], [135, 136], [137, 143], [144, 155], [155, 156]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 9, "product"], [14, 14, "product"], [45, 45, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 4, 5, "general-affiliation", "", false, false], [8, 9, 14, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "until", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "versatile", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "modes", "in", "a", "similar", "way", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT until version 5.34, Cling from version 6) makes this package very versatile as it can be used in interactive, scripted and compiled modes in a similar way to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [81, 82], [83, 88], [89, 93], [94, 101], [102, 106], [107, 116], [117, 119], [120, 122], [123, 126], [127, 129], [130, 134], [135, 137], [138, 149], [149, 150], [151, 159], [160, 163], [164, 172], [173, 178], [179, 181], [182, 183], [184, 191], [192, 195], [196, 198], [199, 209], [210, 218], [219, 223], [224, 226], [227, 233], [233, 234]]}
{"doc_key": "ai-train-28", "ner": [[0, 2, "product"], [21, 23, "field"], [27, 28, "task"], [30, 33, "task"], [35, 36, "task"], [39, 40, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 2, 21, 23, "related-to", "", false, false], [27, 28, 21, 23, "part-of", "", false, false], [30, 33, 21, 23, "part-of", "", false, false], [35, 36, 21, 23, "part-of", "", false, false], [39, 40, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Voice", "user", "interfaces", "that", "interpret", "and", "manage", "conversational", "states", "are", "challenging", "to", "design", "due", "to", "the", "inherent", "difficulty", "in", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "name", "-", "entity", "recognition", ",", "information", "retrieval", ",", "and", "dialogue", "management", "."], "sentence-detokenized": "Voice user interfaces that interpret and manage conversational states are challenging to design due to the inherent difficulty in integrating complex natural language processing tasks such as coreference resolution, name-entity recognition, information retrieval, and dialogue management.", "token2charspan": [[0, 5], [6, 10], [11, 21], [22, 26], [27, 36], [37, 40], [41, 47], [48, 62], [63, 69], [70, 73], [74, 85], [86, 88], [89, 95], [96, 99], [100, 102], [103, 106], [107, 115], [116, 126], [127, 129], [130, 141], [142, 149], [150, 157], [158, 166], [167, 177], [178, 183], [184, 188], [189, 191], [192, 203], [204, 214], [214, 215], [216, 220], [220, 221], [221, 227], [228, 239], [239, 240], [241, 252], [253, 262], [262, 263], [264, 267], [268, 276], [277, 287], [287, 288]]}
{"doc_key": "ai-train-29", "ner": [[5, 6, "algorithm"], [9, 11, "algorithm"], [15, 17, "researcher"], [21, 25, "organisation"], [32, 33, "field"], [35, 36, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 15, 17, "origin", "", false, false], [5, 6, 32, 33, "part-of", "", false, false], [5, 6, 35, 36, "part-of", "", false, false], [9, 11, 15, 17, "origin", "", false, false], [9, 11, 32, 33, "part-of", "", false, false], [9, 11, 35, 36, "part-of", "", false, false], [15, 17, 21, 25, "physical", "", false, false], [15, 17, 21, 25, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "IDSIA", "'s", "Swiss", "AI", "Lab", "have", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in J\u00fcrgen Schmidhuber's research group at IDSIA's Swiss AI Lab have won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 69], [70, 76], [77, 85], [86, 95], [96, 98], [99, 105], [106, 117], [117, 119], [120, 128], [129, 134], [135, 137], [138, 143], [143, 145], [146, 151], [152, 154], [155, 158], [159, 163], [164, 167], [168, 173], [174, 187], [188, 200], [201, 203], [204, 211], [212, 223], [224, 227], [228, 235], [236, 244], [244, 245]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [14, 17, "task"], [16, 16, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 14, 17, "usage", "", true, false], [1, 3, 16, 16, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "and", "speech", "synthesis", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech and speech synthesis.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 89], [90, 96], [97, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 12, "misc"], [14, 14, "field"], [17, 20, "university"], [26, 29, "field"], [31, 34, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 12, 14, 14, "topic", "topic_of_award", false, false], [7, 12, 17, 20, "origin", "", true, false], [26, 29, 31, 34, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "an", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "Psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "doctorate", "in", "Design", "and", "Industrial", "Engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, an S. V. della laurea ad honorem in Psychology from the University of Padua in 1995 and a doctorate in Design and Industrial Engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 36], [37, 39], [40, 42], [43, 48], [49, 55], [56, 58], [59, 66], [67, 69], [70, 80], [81, 85], [86, 89], [90, 100], [101, 103], [104, 109], [110, 112], [113, 117], [118, 121], [122, 123], [124, 133], [134, 136], [137, 143], [144, 147], [148, 158], [159, 170], [171, 175], [176, 181], [182, 192], [193, 195], [196, 206], [206, 207]]}
{"doc_key": "ai-train-32", "ner": [[5, 6, "researcher"], [12, 15, "organisation"], [17, 17, "location"], [19, 19, "researcher"], [30, 31, "misc"], [45, 47, "misc"], [64, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[5, 6, 12, 15, "physical", "", false, false], [5, 6, 12, 15, "role", "", false, false], [12, 15, 17, 17, "physical", "", false, false], [19, 19, 30, 31, "related-to", "works_with", true, false], [19, 19, 45, 47, "related-to", "works_with", true, false], [19, 19, 64, 65, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["With", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "with", "impaired", "multiplication", ",", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobule", ")", "and", "others", "with", "impaired", "subtraction", ",", "but", "preserved", "multiplication", "(", "associated", "with", "lesions", "of", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "With long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication, but preserved subtraction (associated with lesions of the inferior parietal lobule) and others with impaired subtraction, but preserved multiplication (associated with lesions of the intraparietal sulcus).", "token2charspan": [[0, 4], [5, 9], [9, 10], [10, 14], [15, 27], [28, 35], [36, 41], [41, 42], [43, 44], [45, 56], [57, 59], [60, 63], [64, 69], [69, 70], [70, 81], [82, 90], [91, 93], [94, 99], [99, 100], [101, 108], [109, 113], [114, 124], [125, 133], [134, 138], [139, 146], [147, 149], [150, 159], [160, 167], [168, 170], [171, 174], [175, 183], [184, 188], [189, 193], [194, 202], [203, 217], [217, 218], [219, 222], [223, 232], [233, 244], [245, 246], [246, 256], [257, 261], [262, 269], [270, 272], [273, 276], [277, 285], [286, 294], [295, 301], [301, 302], [303, 306], [307, 313], [314, 318], [319, 327], [328, 339], [339, 340], [341, 344], [345, 354], [355, 369], [370, 371], [371, 381], [382, 386], [387, 394], [395, 397], [398, 401], [402, 415], [416, 422], [422, 423], [423, 424]]}
{"doc_key": "ai-train-33", "ner": [[6, 9, "product"], [14, 17, "misc"], [19, 20, "misc"], [27, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 17, 6, 9, "topic", "", false, false], [19, 20, 6, 9, "topic", "", false, false], [27, 27, 6, 9, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "robots", "with", "artificial", "intelligence", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", "and", "the", "2016", "TV", "adaptation", "of", "Westworld", "have", "engaged", "audience", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of robots with artificial intelligence in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have engaged audience sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 50], [51, 55], [56, 66], [67, 79], [80, 82], [83, 88], [89, 93], [94, 96], [97, 100], [100, 101], [102, 112], [113, 125], [126, 129], [130, 132], [133, 140], [141, 144], [145, 148], [149, 153], [154, 156], [157, 167], [168, 170], [171, 180], [181, 185], [186, 193], [194, 202], [203, 211], [212, 215], [216, 219], [220, 226], [227, 237], [237, 238]]}
{"doc_key": "ai-train-34", "ner": [[6, 7, "field"], [9, 11, "algorithm"], [13, 14, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 6, 7, "part-of", "", false, false], [13, 14, 6, 7, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "two", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "The two main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 20], [21, 25], [26, 28], [29, 41], [42, 50], [51, 54], [55, 64], [65, 74], [75, 83], [84, 87], [88, 95], [96, 104], [104, 105]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [19, 20, "misc"], [25, 26, "misc"], [28, 30, "person"], [35, 36, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[19, 20, 0, 3, "artifact", "", false, false], [25, 26, 0, 3, "artifact", "", false, false], [25, 26, 28, 30, "role", "director_of", false, false], [25, 26, 35, 36, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "more", "prominently", "in", "special", "places", "to", "impress", "audiences", "with", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "being", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films more prominently in special places to impress audiences with Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) being notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 55], [56, 67], [68, 70], [71, 78], [79, 85], [86, 88], [89, 96], [97, 106], [107, 111], [112, 117], [118, 126], [127, 128], [128, 132], [132, 133], [134, 137], [138, 145], [146, 148], [149, 150], [150, 157], [158, 162], [163, 170], [170, 171], [172, 176], [176, 177], [178, 186], [187, 194], [195, 202], [202, 203], [204, 209], [210, 217], [218, 226], [226, 227]]}
{"doc_key": "ai-train-36", "ner": [[12, 14, "field"], [19, 24, "task"], [26, 27, "task"], [29, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 14, "part-of", "", false, false], [26, 27, 12, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing for tasks such as part-of-speech tagging and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [95, 98], [99, 104], [105, 109], [110, 112], [113, 117], [117, 118], [118, 120], [120, 121], [121, 127], [128, 135], [136, 139], [140, 149], [150, 157], [158, 159], [159, 166], [166, 167], [168, 172], [172, 173], [173, 174]]}
{"doc_key": "ai-train-37", "ner": [[2, 4, "product"], [9, 13, "organisation"], [15, 16, "organisation"], [18, 18, "country"], [22, 27, "product"], [30, 31, "researcher"], [41, 41, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 13, 2, 4, "role", "introduces_to_market", true, false], [15, 16, 2, 4, "role", "introduces_to_market", true, false], [15, 16, 18, 18, "physical", "", false, false], [22, 27, 41, 41, "related-to", "sold_to", true, false], [30, 31, 22, 27, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", "Company", ".", "by", "KUKA", "robotics", "in", "Germany", ",", "and", "the", "Programmable", "Universal", "Machine", "for", "Assembly", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", ",", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced in 1963 by Fuji Yusoki Kogyo Company. by KUKA robotics in Germany, and the Programmable Universal Machine for Assembly was invented by Victor Scheinman in 1976, and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [72, 79], [79, 80], [81, 83], [84, 88], [89, 97], [98, 100], [101, 108], [108, 109], [110, 113], [114, 117], [118, 130], [131, 140], [141, 148], [149, 152], [153, 161], [162, 165], [166, 174], [175, 177], [178, 184], [185, 194], [195, 197], [198, 202], [202, 203], [204, 207], [208, 211], [212, 218], [219, 222], [223, 227], [228, 230], [231, 240], [240, 241]]}
{"doc_key": "ai-train-38", "ner": [[10, 10, "conference"], [12, 12, "researcher"], [19, 19, "field"], [35, 36, "researcher"], [43, 44, "researcher"], [57, 57, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[12, 12, 10, 10, "role", "president_of", false, false], [12, 12, 35, 36, "role", "colleagues", false, false], [19, 19, 57, 57, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "while", "serving", "as", "president", "of", "the", "AAAI", ",", "Hayes", "began", "a", "series", "of", "attacks", "on", "AI", "critics", ",", "mostly", "phrased", "in", "an", "ironic", "tone", ",", "and", "(", "along", "with", "his", "colleague", "Kenneth", "Ford", ")", "created", "an", "award", "named", "after", "Simon", "Newcomb", "to", "be", "given", "for", "the", "most", "ridiculous", "argument", "denying", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, while serving as president of the AAAI, Hayes began a series of attacks on AI critics, mostly phrased in an ironic tone, and (along with his colleague Kenneth Ford) created an award named after Simon Newcomb to be given for the most ridiculous argument denying the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 31], [32, 34], [35, 44], [45, 47], [48, 51], [52, 56], [56, 57], [58, 63], [64, 69], [70, 71], [72, 78], [79, 81], [82, 89], [90, 92], [93, 95], [96, 103], [103, 104], [105, 111], [112, 119], [120, 122], [123, 125], [126, 132], [133, 137], [137, 138], [139, 142], [143, 144], [144, 149], [150, 154], [155, 158], [159, 168], [169, 176], [177, 181], [181, 182], [183, 190], [191, 193], [194, 199], [200, 205], [206, 211], [212, 217], [218, 225], [226, 228], [229, 231], [232, 237], [238, 241], [242, 245], [246, 250], [251, 261], [262, 270], [271, 278], [279, 282], [283, 294], [295, 297], [298, 300], [300, 301]]}
{"doc_key": "ai-train-39", "ner": [[13, 15, "algorithm"], [37, 38, "algorithm"], [49, 51, "algorithm"], [54, 56, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 15, 37, 38, "named", "same", false, false], [49, 51, 13, 15, "type-of", "", false, false], [54, 56, 13, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "for", "math\\alpha", "/", "math", "can", "be", "found", "by", "using", "a", "line", "search", "algorithm", ",", "i.e.", ",", "the", "magnitude", "of", "math\\alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimises", "S", ",", "typically", "using", "line", "search", "in", "the", "interval", "math", "0\\", "alpha", "1", "/", "math", "or", "backtracking", "line", "search", "such", "as", "Armijo", "line", "search", "."], "sentence-detokenized": "The optimal value for math\\alpha/ math can be found by using a line search algorithm, i.e., the magnitude of math\\alpha/ math is determined by finding the value that minimises S, typically using line search in the interval math0\\alpha 1/ math or backtracking line search such as Armijo line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 21], [22, 32], [32, 33], [34, 38], [39, 42], [43, 45], [46, 51], [52, 54], [55, 60], [61, 62], [63, 67], [68, 74], [75, 84], [84, 85], [86, 90], [90, 91], [92, 95], [96, 105], [106, 108], [109, 119], [119, 120], [121, 125], [126, 128], [129, 139], [140, 142], [143, 150], [151, 154], [155, 160], [161, 165], [166, 175], [176, 177], [177, 178], [179, 188], [189, 194], [195, 199], [200, 206], [207, 209], [210, 213], [214, 222], [223, 227], [227, 229], [229, 234], [235, 236], [236, 237], [238, 242], [243, 245], [246, 258], [259, 263], [264, 270], [271, 275], [276, 278], [279, 285], [286, 290], [291, 297], [297, 298]]}
{"doc_key": "ai-train-40", "ner": [[3, 6, "algorithm"], [8, 11, "algorithm"], [21, 21, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "both", "Breadth", "-", "first", "search", "and", "Depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "his", "results", "represent", "expert", "systems", "that", "incarnate", "a", "lot", "of", "technical", "knowledge", ",", "but", "do", "little", "to", "highlight", "the", "mental", "processes", "that", "humans", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses both Breadth-first search and Depth-first search techniques, but ultimately concludes that his results represent expert systems that incarnate a lot of technical knowledge, but do little to highlight the mental processes that humans use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 17], [18, 25], [25, 26], [26, 31], [32, 38], [39, 42], [43, 48], [48, 49], [49, 54], [55, 61], [62, 72], [72, 73], [74, 77], [78, 88], [89, 98], [99, 103], [104, 107], [108, 115], [116, 125], [126, 132], [133, 140], [141, 145], [146, 155], [156, 157], [158, 161], [162, 164], [165, 174], [175, 184], [184, 185], [186, 189], [190, 192], [193, 199], [200, 202], [203, 212], [213, 216], [217, 223], [224, 233], [234, 238], [239, 245], [246, 249], [250, 252], [253, 258], [259, 263], [264, 271], [271, 272]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [4, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deal", "with", "how", "spoken", "language", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 44], [45, 49], [50, 53], [54, 60], [61, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 98], [99, 104], [105, 114], [114, 115]]}
{"doc_key": "ai-train-42", "ner": [[12, 13, "algorithm"], [27, 29, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^{", "*}", "/", "math", "is", "usually", "estimated", "using", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^{", "*}", "=\\", "theta^{", "ML}", "/", "math", ")", "or", "Maximum", "A", "Posteriori", "(", "math\\", "theta", "^{", "*}", "=\\", "theta", "^{", "MAP", "}", "/", "math", ")", "procedures", "."], "sentence-detokenized": "This math\\ theta^{*} / math is usually estimated using Maximum Likelihood (math\\ theta^{*} =\\ theta^{ML} / math) or Maximum A Posteriori (math\\ theta^{*} =\\ theta^{MAP} / math) procedures.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [16, 18], [18, 20], [21, 22], [23, 27], [28, 30], [31, 38], [39, 48], [49, 54], [55, 62], [63, 73], [74, 75], [75, 79], [79, 80], [81, 86], [86, 88], [88, 90], [91, 93], [94, 101], [101, 104], [105, 106], [107, 111], [111, 112], [113, 115], [116, 123], [124, 125], [126, 136], [137, 138], [138, 143], [144, 149], [149, 151], [151, 153], [154, 156], [157, 162], [162, 164], [164, 167], [167, 168], [169, 170], [171, 175], [175, 176], [177, 187], [187, 188]]}
{"doc_key": "ai-train-43", "ner": [[10, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "widely", "spoken", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesizer", "for", "their", "speech", ";", "resulting", "in", "robotic", "and", "awkward", "sounds", "that", "may", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less widely spoken languages use the open-source eSpeak synthesizer for their speech; resulting in robotic and awkward sounds that may be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 23], [24, 33], [34, 37], [38, 41], [42, 46], [46, 47], [47, 53], [54, 60], [61, 72], [73, 76], [77, 82], [83, 89], [89, 90], [91, 100], [101, 103], [104, 111], [112, 115], [116, 123], [124, 130], [131, 135], [136, 139], [140, 142], [143, 152], [153, 155], [156, 166], [166, 167]]}
{"doc_key": "ai-train-44", "ner": [[19, 19, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[19, 19, 35, 36, "compare", "", false, false], [19, 19, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "used", "primarily", "by", "statisticians", "and", "other", "practitioners", "who", "need", "an", "environment", "for", "statistical", "calculations", "and", "software", "development", ",", "R", "can", "also", "operate", "as", "a", "general", "matrix", "calculation", "toolbox", "-", "with", "performance", "benchmarks", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although used primarily by statisticians and other practitioners who need an environment for statistical calculations and software development, R can also operate as a general matrix calculation toolbox - with performance benchmarks comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 13], [14, 23], [24, 26], [27, 40], [41, 44], [45, 50], [51, 64], [65, 68], [69, 73], [74, 76], [77, 88], [89, 92], [93, 104], [105, 117], [118, 121], [122, 130], [131, 142], [142, 143], [144, 145], [146, 149], [150, 154], [155, 162], [163, 165], [166, 167], [168, 175], [176, 182], [183, 194], [195, 202], [203, 204], [205, 209], [210, 221], [222, 232], [233, 243], [244, 246], [247, 250], [251, 257], [258, 260], [261, 267], [267, 268]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 4, "field"], [6, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 4, "part-of", "", false, false], [0, 0, 12, 13, "origin", "", false, false], [6, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "-", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "combining", "the", "mixing", "of", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden that creates new frequencies by combining the mixing of two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [75, 76], [76, 84], [85, 93], [94, 103], [104, 108], [109, 116], [117, 120], [121, 132], [133, 135], [136, 145], [146, 149], [150, 156], [157, 159], [160, 163], [164, 175], [175, 176]]}
{"doc_key": "ai-train-46", "ner": [[14, 16, "person"], [17, 17, "misc"], [21, 23, "organisation"], [26, 26, "organisation"], [28, 30, "misc"], [32, 33, "person"], [36, 36, "organisation"], [38, 40, "misc"], [42, 43, "person"], [45, 46, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[14, 16, 17, 17, "role", "actor_in", false, false], [17, 17, 21, 23, "artifact", "", false, false], [28, 30, 26, 26, "artifact", "", false, false], [32, 33, 28, 30, "role", "actor_in", false, false], [38, 40, 36, 36, "artifact", "", false, false], [42, 43, 38, 40, "role", "actor_in", false, false], [45, 46, 38, 40, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Some", "other", "features", "that", "helped", "put", "3D", "back", "on", "the", "map", "that", "month", "were", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", ",", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Some other features that helped put 3D back on the map that month were John Wayne's Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth, and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 4], [5, 10], [11, 19], [20, 24], [25, 31], [32, 35], [36, 38], [39, 43], [44, 46], [47, 50], [51, 54], [55, 59], [60, 65], [66, 70], [71, 75], [76, 81], [81, 83], [84, 89], [90, 91], [91, 102], [103, 105], [106, 112], [113, 117], [117, 118], [118, 119], [119, 120], [121, 129], [129, 131], [132, 136], [137, 142], [143, 151], [152, 156], [157, 161], [162, 170], [170, 171], [172, 175], [176, 185], [185, 187], [188, 193], [194, 198], [199, 203], [204, 208], [209, 213], [214, 220], [221, 224], [225, 230], [231, 236], [236, 237]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 4, "field"], [5, 6, "task"], [14, 14, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 6, "general-affiliation", "", false, false], [0, 0, 14, 14, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "research", "group", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a research group at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 81], [82, 84], [85, 93], [93, 94]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [12, 13, "field"], [24, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 12, 13, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [24, 27, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "premier", "computer", "graphics", "academic", "conference", ",", "and", "the", "main", "topic", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the premier computer graphics academic conference, and the main topic of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 89], [90, 98], [99, 109], [109, 110], [111, 114], [115, 118], [119, 123], [124, 129], [130, 132], [133, 136], [137, 143], [144, 153], [154, 156], [157, 165], [166, 176], [176, 177]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 5, "task"], [12, 14, "algorithm"], [16, 16, "algorithm"], [19, 21, "algorithm"], [23, 23, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [36, 36, "misc"], [43, 45, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[12, 14, 36, 36, "general-affiliation", "", false, false], [16, 16, 12, 14, "named", "", false, false], [19, 21, 36, 36, "general-affiliation", "", false, false], [23, 23, 19, 21, "named", "", false, false], [27, 29, 36, 36, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "one", "step", "using", "Principal", "Component", "Analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "by", "k", "-", "NN", "on", "the", "feature", "vectors", "in", "the", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in one step using Principal Component Analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on the feature vectors in the reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [152, 153], [154, 156], [157, 166], [167, 178], [179, 187], [188, 189], [189, 192], [192, 193], [194, 204], [205, 207], [208, 209], [210, 224], [225, 229], [229, 230], [231, 239], [240, 242], [243, 253], [254, 256], [257, 258], [258, 259], [259, 261], [262, 264], [265, 268], [269, 276], [277, 284], [285, 287], [288, 291], [292, 299], [300, 311], [312, 317], [317, 318]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[0, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [36, 39, "algorithm"], [40, 41, "researcher"], [43, 45, "researcher"], [47, 53, "misc"], [55, 64, "conference"], [66, 66, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 0, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [47, 53, 36, 39, "topic", "", false, false], [47, 53, 40, 41, "artifact", "", false, false], [47, 53, 43, 45, "artifact", "", false, false], [47, 53, 55, 64, "temporal", "", false, false], [66, 66, 55, 64, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["C", ".", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histograms", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": "C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as histograms of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [1, 2], [3, 15], [16, 19], [20, 22], [23, 29], [29, 30], [31, 32], [33, 42], [43, 53], [54, 63], [64, 70], [70, 71], [72, 85], [86, 93], [94, 96], [97, 105], [106, 112], [113, 114], [114, 118], [118, 119], [119, 120], [121, 126], [127, 128], [128, 129], [130, 132], [132, 133], [133, 135], [135, 136], [137, 141], [142, 148], [149, 152], [153, 158], [159, 167], [168, 172], [173, 175], [176, 186], [187, 189], [190, 198], [199, 208], [209, 211], [212, 217], [217, 218], [219, 220], [220, 221], [222, 228], [228, 229], [230, 240], [241, 243], [244, 252], [253, 262], [263, 266], [267, 272], [273, 282], [282, 283], [284, 288], [289, 297], [298, 305], [306, 316], [317, 319], [320, 328], [329, 335], [336, 339], [340, 347], [348, 359], [360, 361], [361, 365], [365, 366], [366, 367], [368, 373], [374, 375], [375, 376], [377, 384], [384, 385], [386, 390], [391, 402], [402, 403]]}
{"doc_key": "ai-train-52", "ner": [[0, 0, "algorithm"], [5, 7, "algorithm"], [10, 12, "task"], [15, 17, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 7, "type-of", "", false, false], [10, 12, 0, 0, "usage", "", true, false], [10, 12, 15, 17, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "Feature", "learning", "in", "an", "unsupervised", "learning", "manner", "."], "sentence-detokenized": "Autoencoder is a type of artificial neural network used to learn Feature learning in an unsupervised learning manner.", "token2charspan": [[0, 11], [12, 14], [15, 16], [17, 21], [22, 24], [25, 35], [36, 42], [43, 50], [51, 55], [56, 58], [59, 64], [65, 72], [73, 81], [82, 84], [85, 87], [88, 100], [101, 109], [110, 116], [116, 117]]}
{"doc_key": "ai-train-53", "ner": [[0, 0, "researcher"], [5, 5, "organisation"], [10, 11, "field"], [13, 14, "field"], [20, 24, "organisation"], [26, 26, "organisation"], [32, 33, "field"], [35, 36, "field"], [42, 42, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 0, 5, 5, "role", "fellow_of", false, false], [0, 0, 10, 11, "related-to", "contributes_to", false, false], [0, 0, 13, 14, "related-to", "contributes_to", false, false], [0, 0, 20, 24, "role", "fellow_of", false, false], [0, 0, 32, 33, "related-to", "contributes_to", false, false], [0, 0, 35, 36, "related-to", "contributes_to", false, false], [26, 26, 20, 24, "named", "", false, false], [42, 42, 20, 24, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "IEEE", "for", "his", "contributions", "in", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "in", "pattern", "recognition", ",", "image", "processing", ",", "and", "for", "services", "to", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition, image processing, and for services to IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 28], [29, 32], [33, 36], [37, 50], [51, 53], [54, 62], [63, 69], [70, 73], [74, 79], [80, 90], [91, 94], [95, 96], [97, 103], [104, 106], [107, 110], [111, 124], [125, 136], [137, 140], [141, 148], [149, 160], [161, 162], [162, 166], [166, 167], [168, 171], [172, 175], [176, 189], [190, 192], [193, 200], [201, 212], [212, 213], [214, 219], [220, 230], [230, 231], [232, 235], [236, 239], [240, 248], [249, 251], [252, 256], [256, 257]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [23, 24, "researcher"], [26, 27, "organisation"], [29, 30, "researcher"], [33, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 9, 13, 15, "usage", "", false, false], [13, 15, 23, 24, "origin", "", true, false], [13, 15, 29, 30, "origin", "", true, false], [17, 17, 13, 15, "named", "", false, false], [23, 24, 26, 27, "physical", "", false, false], [23, 24, 26, 27, "role", "", false, false], [29, 30, 33, 35, "physical", "", false, false], [29, 30, 33, 35, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "with", "the", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "based", "system", "introduced", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was with the Connectionist Temporal Classification (CTC) based system introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 44], [45, 48], [49, 62], [63, 71], [72, 86], [87, 88], [88, 91], [91, 92], [93, 98], [99, 105], [106, 116], [117, 119], [120, 124], [125, 131], [132, 134], [135, 141], [142, 150], [151, 154], [155, 162], [163, 169], [170, 172], [173, 176], [177, 187], [188, 190], [191, 198], [199, 201], [202, 206], [206, 207]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [11, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [11, 11, 0, 2, "type-of", "", false, false], [13, 13, 11, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear-", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-fractional programming (LFP) is a generalisation of linear programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [7, 12, "misc"], [15, 22, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 7, 12, "win-defeat", "", false, false], [7, 12, 15, 22, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "received", "many", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "2011", "&", "2012", ","], "sentence-detokenized": "Lafferty received many awards, including two Test-of-Time awards at the International Conference on Machine Learning 2011 & 2012,", "token2charspan": [[0, 8], [9, 17], [18, 22], [23, 29], [29, 30], [31, 40], [41, 44], [45, 49], [49, 50], [50, 52], [52, 53], [53, 57], [58, 64], [65, 67], [68, 71], [72, 85], [86, 96], [97, 99], [100, 107], [108, 116], [117, 121], [122, 123], [124, 128], [128, 129]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [23, 24, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "deploy", "neural", "networks", "developed", "into", "these", "frameworks", "as", "inheritable", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to deploy neural networks developed into these frameworks as inheritable components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 128], [129, 135], [136, 144], [145, 154], [155, 159], [160, 165], [166, 176], [177, 179], [180, 191], [192, 202], [202, 203]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "illustration", ")", "between", "two", "sentences", ",", "a", "candidate", "translation", "string", "and", "a", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see illustration) between two sentences, a candidate translation string and a reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 73], [74, 79], [80, 87], [88, 90], [91, 100], [101, 102], [102, 105], [106, 118], [118, 119], [120, 127], [128, 131], [132, 141], [141, 142], [143, 144], [145, 154], [155, 166], [167, 173], [174, 177], [178, 179], [180, 189], [190, 201], [202, 208], [208, 209]]}
{"doc_key": "ai-train-59", "ner": [[7, 11, "conference"], [20, 20, "task"], [22, 23, "task"], [27, 28, "metrics"], [30, 36, "metrics"], [41, 44, "conference"], [46, 46, "conference"], [49, 49, "location"], [51, 51, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[7, 11, 20, 20, "related-to", "subject_at", false, false], [7, 11, 22, 23, "related-to", "subject_at", false, false], [27, 28, 7, 11, "temporal", "", false, false], [30, 36, 27, 28, "named", "", true, false], [46, 46, 41, 44, "named", "", false, false], [49, 49, 51, 51, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "in", "the", "annual", "NIST", "Document", "Understanding", "Conference", ",", "where", "research", "groups", "submit", "their", "systems", "for", "summarisation", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Progress", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used in the annual NIST Document Understanding Conference, where research groups submit their systems for summarisation and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Progress of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 30], [31, 37], [38, 42], [43, 51], [52, 65], [66, 76], [76, 77], [78, 83], [84, 92], [93, 99], [100, 106], [107, 112], [113, 120], [121, 124], [125, 138], [139, 142], [143, 154], [155, 160], [160, 161], [162, 164], [165, 168], [169, 174], [175, 181], [182, 183], [183, 189], [189, 190], [190, 198], [199, 209], [210, 213], [214, 221], [222, 232], [232, 233], [234, 236], [237, 245], [246, 248], [249, 255], [256, 267], [268, 278], [279, 286], [287, 288], [288, 292], [292, 293], [293, 294], [295, 303], [303, 304], [305, 311], [311, 312], [313, 321], [322, 323], [324, 328], [328, 329]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [11, 12, "programlang"], [15, 15, "product"], [21, 21, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 11, 12, "type-of", "", false, false], [6, 6, 21, 21, "named", "", false, false], [8, 8, 11, 12, "part-of", "", false, false], [8, 8, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "to", "run", "in", "Java", "with", "JShell", "(", "minimum", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, to run in Java with JShell (minimum Java 9): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 23], [24, 27], [28, 30], [31, 35], [36, 40], [41, 47], [48, 49], [49, 56], [57, 61], [62, 63], [63, 64], [64, 65], [66, 76], [77, 87], [88, 89], [90, 109], [110, 114], [115, 116], [117, 121]]}
{"doc_key": "ai-train-61", "ner": [[0, 3, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 3, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLEU", "metric", ",", "but", "with", "some", "changes", "."], "sentence-detokenized": "The NIST metric is based on the BLEU metric, but with some changes.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 66], [66, 67]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", "but", "with", "the", "additional", "constraint", "that", "edges", "are", "restricted", "to", "come", "from", "a", "limited", "set", "of", "possible", "relationships", ",", "to", "facilitate", "algebra", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called Knowledge Graphs, which are semantic networks but with the additional constraint that edges are restricted to come from a limited set of possible relationships, to facilitate algebra on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 116], [117, 118], [119, 126], [127, 133], [134, 143], [144, 150], [150, 151], [152, 157], [158, 161], [162, 170], [171, 179], [180, 183], [184, 188], [189, 192], [193, 203], [204, 214], [215, 219], [220, 225], [226, 229], [230, 240], [241, 243], [244, 248], [249, 253], [254, 255], [256, 263], [264, 267], [268, 270], [271, 279], [280, 293], [293, 294], [295, 297], [298, 308], [309, 316], [317, 319], [320, 323], [324, 329], [329, 330]]}
{"doc_key": "ai-train-63", "ner": [[0, 2, "product"], [15, 16, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 15, 16, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "larger", "programmes", ",", "such", "as", "word", "processors", ",", "but", "are", "also", "available", "as", "stand", "-", "alone", "applications", "that", "can", "be", "activated", "from", "within", "programmes", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of larger programmes, such as word processors, but are also available as stand-alone applications that can be activated from within programmes that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 66], [67, 77], [77, 78], [79, 83], [84, 86], [87, 91], [92, 102], [102, 103], [104, 107], [108, 111], [112, 116], [117, 126], [127, 129], [130, 135], [135, 136], [136, 141], [142, 154], [155, 159], [160, 163], [164, 166], [167, 176], [177, 181], [182, 188], [189, 199], [200, 204], [205, 209], [210, 214], [215, 223], [224, 228], [228, 229]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [14, 20, "conference"], [23, 25, "organisation"], [30, 32, "conference"], [34, 36, "conference"], [40, 41, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "Fellow", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "Cognitive", "Science", "Society", ",", "and", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a Fellow of the American Association for the Advancement of Science, Association for the Advancement of Artificial Intelligence, and Cognitive Science Society, and editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 86], [87, 90], [91, 94], [95, 106], [107, 109], [110, 120], [121, 133], [133, 134], [135, 138], [139, 148], [149, 156], [157, 164], [164, 165], [166, 169], [170, 176], [177, 179], [180, 182], [183, 192], [193, 202], [202, 203], [204, 206], [207, 215], [216, 224], [224, 225], [226, 229], [230, 232], [233, 240], [241, 249], [249, 250]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "task"], [21, 22, "researcher"], [24, 25, "university"], [27, 28, "researcher"], [30, 33, "organisation"], [35, 35, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 21, 22, "origin", "", false, false], [0, 2, 27, 28, "origin", "", false, false], [4, 4, 0, 2, "named", "", false, false], [21, 22, 24, 25, "physical", "", false, false], [21, 22, 24, 25, "role", "", false, false], [27, 28, 30, 33, "role", "", false, false], [35, 35, 30, 33, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "be", "developed", "with", "the", "work", "of", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear predictive coding (LPC), a form of speech coding, began to be developed with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 68], [69, 78], [79, 83], [84, 87], [88, 92], [93, 95], [96, 104], [105, 112], [113, 115], [116, 122], [123, 133], [134, 137], [138, 143], [144, 149], [150, 152], [153, 159], [160, 169], [170, 173], [174, 183], [184, 185], [185, 188], [188, 189], [190, 192], [193, 197], [197, 198]]}
{"doc_key": "ai-train-66", "ner": [[48, 49, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "further", "ergodic", ",", "all", "sample", "paths", "exhibit", "the", "same", "time", "average", "and", "thus", "mathR_x", "^{", "n", "/T_0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^{", "n", "/T_0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is further ergodic, all sample paths exhibit the same time average and thus mathR_x^{n/T_0}(\\ tau) =\\ widehat {R} _ x^{n/T_0}(\\ tau)/ math in the sense of mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 24], [25, 32], [32, 33], [34, 37], [38, 44], [45, 50], [51, 58], [59, 62], [63, 67], [68, 72], [73, 80], [81, 84], [85, 89], [90, 97], [97, 99], [99, 100], [100, 104], [104, 105], [105, 106], [106, 107], [108, 111], [111, 112], [113, 115], [116, 123], [124, 125], [125, 126], [126, 127], [128, 129], [130, 131], [131, 133], [133, 134], [134, 138], [138, 139], [139, 140], [140, 141], [142, 145], [145, 146], [146, 147], [148, 152], [153, 155], [156, 159], [160, 165], [166, 168], [169, 173], [174, 180], [181, 186], [186, 187]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [12, 14, "algorithm"], [16, 16, "algorithm"], [19, 21, "algorithm"], [23, 23, "algorithm"], [26, 28, "algorithm"], [30, 30, "algorithm"], [34, 36, "algorithm"], [38, 38, "algorithm"], [43, 44, "misc"], [49, 51, "algorithm"], [54, 55, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[12, 14, 43, 44, "related-to", "", false, false], [16, 16, 12, 14, "named", "", false, false], [19, 21, 43, 44, "related-to", "", false, false], [23, 23, 19, 21, "named", "", false, false], [26, 28, 43, 44, "related-to", "", false, false], [30, 30, 26, 28, "named", "", false, false], [34, 36, 43, 44, "related-to", "", false, false], [38, 38, 34, 36, "named", "", false, false], [49, 51, 54, 55, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "one", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", ",", "or", "non-negative", "matrix", "factorisation", "(", "NMF", ")", "techniques", "as", "a", "pre-processing", "step", "followed", "by", "clustering", "by", "K", "-", "NN", "on", "the", "feature", "vectors", "in", "the", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorisation (NMF) techniques as a pre-processing step followed by clustering by K-NN on the feature vectors in the reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [152, 153], [154, 163], [164, 175], [176, 184], [185, 186], [186, 189], [189, 190], [190, 191], [192, 194], [195, 207], [208, 214], [215, 228], [229, 230], [230, 233], [233, 234], [235, 245], [246, 248], [249, 250], [251, 265], [266, 270], [271, 279], [280, 282], [283, 293], [294, 296], [297, 298], [298, 299], [299, 301], [302, 304], [305, 308], [309, 316], [317, 324], [325, 327], [328, 331], [332, 339], [340, 351], [352, 357], [357, 358]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "directly", "called", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 64], [65, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [10, 12, "task"], [30, 31, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 10, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognising", "named", "entities", "in", "the", "text", "is", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognising named entities in the text is Named Entity Recognition, while the task of determining the identity of named entities mentioned in the text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 59], [60, 66], [67, 78], [78, 79], [80, 85], [86, 89], [90, 94], [95, 97], [98, 109], [110, 113], [114, 122], [123, 125], [126, 131], [132, 140], [141, 150], [151, 153], [154, 157], [158, 162], [163, 165], [166, 172], [173, 179], [180, 187], [187, 188]]}
{"doc_key": "ai-train-70", "ner": [[0, 1, "algorithm"], [28, 28, "programlang"], [30, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 30, 30, "part-of", "", true, false], [30, 30, 28, 28, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "function", "and", "its", "derivatives", "used", "in", "this", "package", "were", "originally", "included", "in", "the", "package", ",", "from", "version", "0.8.0", "onwards", ",", "these", "were", "released", "in", "a", "separate", "R", "package", "sigmoid", ",", "with", "the", "aim", "of", "allowing", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid function and its derivatives used in this package were originally included in the package, from version 0.8.0 onwards, these were released in a separate R package sigmoid, with the aim of allowing more general use.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 24], [25, 28], [29, 40], [41, 45], [46, 48], [49, 53], [54, 61], [62, 66], [67, 77], [78, 86], [87, 89], [90, 93], [94, 101], [101, 102], [103, 107], [108, 115], [116, 121], [122, 129], [129, 130], [131, 136], [137, 141], [142, 150], [151, 153], [154, 155], [156, 164], [165, 166], [167, 174], [175, 182], [182, 183], [184, 188], [189, 192], [193, 196], [197, 199], [200, 208], [209, 213], [214, 221], [222, 225], [225, 226]]}
{"doc_key": "ai-train-71", "ner": [[0, 2, "programlang"], [7, 11, "organisation"], [13, 13, "organisation"], [17, 17, "location"], [19, 19, "location"], [24, 25, "researcher"], [27, 28, "researcher"], [31, 32, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 24, 25, "artifact", "", true, false], [0, 2, 27, 28, "artifact", "", true, false], [0, 2, 31, 32, "artifact", "", true, false], [13, 13, 7, 11, "named", "", false, false], [13, 13, 17, 17, "physical", "", false, false], [17, 17, 19, 19, "physical", "", false, false], [24, 25, 7, 11, "role", "", false, false], [27, 28, 7, 11, "role", "", false, false], [31, 32, 7, 11, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "Cambridge", ",", "Massachusetts", "research", "firm", ",", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", ",", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The logo was created in 1967 at Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts research firm, by Wally Feurzeig, Cynthia Solomon, and Seymour Papert.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 36], [36, 37], [38, 45], [46, 49], [50, 56], [57, 58], [58, 61], [61, 62], [62, 63], [64, 65], [66, 75], [75, 76], [77, 90], [91, 99], [100, 104], [104, 105], [106, 108], [109, 114], [115, 123], [123, 124], [125, 132], [133, 140], [140, 141], [142, 145], [146, 153], [154, 160], [160, 161]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [18, 19, "field"], [23, 24, "algorithm"], [26, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 18, 19, "compare", "", false, false], [23, 24, 18, 19, "part-of", "", false, false], [26, 27, 18, 19, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", ",", "and", "can", "be", "contrasted", "with", "conventional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "on", "neural", "networks", "with", "fixed", "topologies", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm, and can be contrasted with conventional deep learning techniques that use gradient descent on neural networks with fixed topologies.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [78, 79], [80, 83], [84, 87], [88, 90], [91, 101], [102, 106], [107, 119], [120, 124], [125, 133], [134, 144], [145, 149], [150, 153], [154, 162], [163, 170], [171, 173], [174, 180], [181, 189], [190, 194], [195, 200], [201, 211], [211, 212]]}
{"doc_key": "ai-train-73", "ner": [[3, 4, "algorithm"], [55, 57, "metrics"], [59, 59, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[59, 59, 55, 57, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "least", "squares", "to", "fit", "a", "function", "in", "the", "form", "of", "a", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "we", "can", "assess", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use least squares to fit a function in the form of a hyperplane \u0177 = a + \u03b2 supT/sup x to the data (x sub i/sub, y sub i/sub) sub 1 \u2264 i \u2264n/sub, we can assess the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 15], [16, 23], [24, 26], [27, 30], [31, 32], [33, 41], [42, 44], [45, 48], [49, 53], [54, 56], [57, 58], [59, 69], [70, 71], [72, 73], [74, 75], [76, 77], [78, 79], [80, 84], [84, 85], [85, 88], [89, 90], [91, 93], [94, 97], [98, 102], [103, 104], [104, 105], [106, 109], [110, 111], [111, 112], [112, 115], [115, 116], [117, 118], [119, 122], [123, 124], [124, 125], [125, 128], [128, 129], [130, 133], [134, 135], [136, 137], [138, 139], [140, 142], [142, 143], [143, 146], [146, 147], [148, 150], [151, 154], [155, 161], [162, 165], [166, 169], [170, 175], [176, 179], [180, 184], [185, 192], [193, 198], [199, 200], [200, 203], [203, 204], [204, 205]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [13, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [48, 48, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "locations", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "UK", "."], "sentence-detokenized": "The company has international locations in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the UK.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 39], [40, 42], [43, 52], [52, 53], [54, 60], [60, 61], [62, 68], [68, 69], [70, 75], [75, 76], [77, 84], [84, 85], [86, 91], [91, 92], [93, 98], [98, 99], [100, 105], [105, 106], [107, 112], [112, 113], [114, 123], [123, 124], [125, 131], [131, 132], [133, 141], [141, 142], [143, 154], [154, 155], [156, 162], [162, 163], [164, 173], [173, 174], [175, 180], [181, 187], [187, 188], [189, 194], [194, 195], [196, 202], [202, 203], [204, 212], [212, 213], [214, 220], [221, 224], [225, 228], [229, 231], [231, 232]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [6, 9, "field"], [14, 14, "organisation"], [17, 21, "university"], [29, 31, "organisation"], [33, 36, "university"], [43, 44, "university"], [46, 47, "university"], [50, 52, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 6, 9, "topic", "", false, false], [3, 3, 14, 14, "origin", "", false, false], [3, 3, 17, 21, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "D.Sc", ".", "in", "electrical", "and", "computer", "engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", ",", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", "as", "well", "as", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a D.Sc. in electrical and computer engineering (2000) from Inria and the University of Nice Sophia Antipolis, and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech as well as visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 15], [15, 16], [17, 19], [20, 30], [31, 34], [35, 43], [44, 55], [56, 57], [57, 61], [61, 62], [63, 67], [68, 73], [74, 77], [78, 81], [82, 92], [93, 95], [96, 100], [101, 107], [108, 117], [117, 118], [119, 122], [123, 126], [127, 131], [132, 141], [142, 151], [152, 154], [155, 162], [163, 172], [173, 183], [183, 184], [185, 190], [191, 194], [195, 200], [201, 210], [211, 213], [214, 218], [219, 221], [222, 230], [231, 240], [241, 243], [244, 251], [252, 262], [262, 263], [264, 268], [269, 279], [280, 283], [284, 287], [288, 298], [299, 301], [302, 309], [309, 310]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [10, 10, "researcher"], [14, 15, "product"], [18, 19, "country"], [21, 21, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 7, 8, "role", "licensing_patent_to", false, false], [10, 10, 18, 19, "physical", "", false, false], [21, 21, 10, 10, "artifact", "", false, false], [21, 21, 14, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Licensing", "the", "original", "patent", "granted", "to", "inventor", "George", "Devol", ",", "Engelberger", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Licensing the original patent granted to inventor George Devol, Engelberger developed the first industrial robot in the United States, Unimate, in the 1950s.", "token2charspan": [[0, 9], [10, 13], [14, 22], [23, 29], [30, 37], [38, 40], [41, 49], [50, 56], [57, 62], [62, 63], [64, 75], [76, 85], [86, 89], [90, 95], [96, 106], [107, 112], [113, 115], [116, 119], [120, 126], [127, 133], [133, 134], [135, 142], [142, 143], [144, 146], [147, 150], [151, 156], [156, 157]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[4, 4, "programlang"], [7, 7, "programlang"], [15, 17, "programlang"], [20, 20, "programlang"], [30, 31, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[4, 4, 15, 17, "named", "", false, false], [7, 7, 4, 4, "origin", "descendant_of", false, false], [7, 7, 20, 20, "general-affiliation", "", false, false], [7, 7, 30, 31, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["A", "descendant", "of", "the", "CLIPS", "language", "including", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "that", "was", "rewritten", "in", "Java", ",", "then", "grew", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by"], "sentence-detokenized": "A descendant of the CLIPS language including Jess (the rule-based part of CLIPS that was rewritten in Java, then grew in a different direction), JESS was originally inspired by", "token2charspan": [[0, 1], [2, 12], [13, 15], [16, 19], [20, 25], [26, 34], [35, 44], [45, 49], [50, 51], [51, 54], [55, 59], [59, 60], [60, 65], [66, 70], [71, 73], [74, 79], [80, 84], [85, 88], [89, 98], [99, 101], [102, 106], [106, 107], [108, 112], [113, 117], [118, 120], [121, 122], [123, 132], [133, 142], [142, 143], [143, 144], [145, 149], [150, 153], [154, 164], [165, 173], [174, 176]]}
{"doc_key": "ai-train-79", "ner": [[6, 6, "product"], [11, 13, "product"], [16, 17, "organisation"], [21, 22, "product"], [40, 41, "product"], [43, 45, "product"], [63, 64, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[11, 13, 6, 6, "type-of", "", false, false], [16, 17, 11, 13, "usage", "", false, false], [21, 22, 16, 17, "artifact", "", false, false], [40, 41, 16, 17, "origin", "", true, false], [40, 41, 63, 64, "related-to", "", true, false], [43, 45, 16, 17, "origin", "", true, false], [43, 45, 63, 64, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "company", "also", "created", "flexible", "intelligent", "AGV", "applications", ",", "designing", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", ",", "used", "for", "complex", "pick", "and", "place", "operations", ",", "along", "with", "gantry", "systems", "and", "industrial", "robotic", "arms", ",", "used", "in", "first", "-", "tier", "automobile", "supply", "plants", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "The company also created flexible intelligent AGV applications, designing the Motivity control system used by RMT Robotics to develop the ADAM iAGV (Self-Guided Vehicle), used for complex pick and place operations, along with gantry systems and industrial robotic arms, used in first-tier automobile supply plants to move products from process to process in non-linear layouts.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 24], [25, 33], [34, 45], [46, 49], [50, 62], [62, 63], [64, 73], [74, 77], [78, 86], [87, 94], [95, 101], [102, 106], [107, 109], [110, 113], [114, 122], [123, 125], [126, 133], [134, 137], [138, 142], [143, 147], [148, 149], [149, 153], [153, 154], [154, 160], [161, 168], [168, 169], [169, 170], [171, 175], [176, 179], [180, 187], [188, 192], [193, 196], [197, 202], [203, 213], [213, 214], [215, 220], [221, 225], [226, 232], [233, 240], [241, 244], [245, 255], [256, 263], [264, 268], [268, 269], [270, 274], [275, 277], [278, 283], [283, 284], [284, 288], [289, 299], [300, 306], [307, 313], [314, 316], [317, 321], [322, 330], [331, 335], [336, 343], [344, 346], [347, 354], [355, 357], [358, 368], [369, 376], [376, 377]]}
{"doc_key": "ai-train-80", "ner": [[7, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameter", "\u03b2", "is", "usually", "estimated", "by", "maximum", "likelihood", "."], "sentence-detokenized": "The parameter \u03b2 is usually estimated by maximum likelihood.", "token2charspan": [[0, 3], [4, 13], [14, 15], [16, 18], [19, 26], [27, 36], [37, 39], [40, 47], [48, 58], [58, 59]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [5, 5, "metrics"], [7, 7, "metrics"], [9, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 0, 1, "part-of", "", false, false], [7, 7, 0, 1, "part-of", "", false, false], [9, 10, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", "such", "as", "precision", "and", "recall", "or", "DCG", "are", "useful", "for", "assessing", "the", "quality", "of", "recommendation", "methods", "."], "sentence-detokenized": "Information retrieval metrics such as precision and recall or DCG are useful for assessing the quality of recommendation methods.", "token2charspan": [[0, 11], [12, 21], [22, 29], [30, 34], [35, 37], [38, 47], [48, 51], [52, 58], [59, 61], [62, 65], [66, 69], [70, 76], [77, 80], [81, 90], [91, 94], [95, 102], [103, 105], [106, 120], [121, 128], [128, 129]]}
{"doc_key": "ai-train-82", "ner": [[6, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "contains", "hundreds", "of", "industrial", "robots", "working", "on", "a", "fully", "automated", "production", "line", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "A typical factory contains hundreds of industrial robots working on a fully automated production line, with one robot for every ten human workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 26], [27, 35], [36, 38], [39, 49], [50, 56], [57, 64], [65, 67], [68, 69], [70, 75], [76, 85], [86, 96], [97, 101], [101, 102], [103, 107], [108, 111], [112, 117], [118, 121], [122, 127], [128, 131], [132, 137], [138, 145], [145, 146]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [11, 12, "field"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [29, 30, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[11, 12, 5, 5, "usage", "", false, true], [17, 18, 11, 12, "part-of", "", false, false], [20, 21, 11, 12, "part-of", "", false, false], [23, 24, 11, 12, "part-of", "", false, false], [26, 27, 11, 12, "part-of", "", false, false], [29, 30, 11, 12, "part-of", "", false, false], [33, 34, 11, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "various", "image", "processing", "applications", ",", "including", ":", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growth", ",", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in various image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growth, and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 53], [54, 59], [60, 70], [71, 83], [83, 84], [85, 94], [94, 95], [96, 101], [102, 114], [114, 115], [116, 123], [124, 134], [134, 135], [136, 140], [141, 151], [151, 152], [153, 159], [160, 169], [169, 170], [171, 177], [178, 184], [184, 185], [186, 189], [190, 195], [196, 205], [205, 206]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 18, "field"], [21, 23, "misc"], [26, 32, "conference"], [34, 34, "conference"], [39, 41, "misc"], [44, 50, "conference"], [51, 52, "conference"], [54, 58, "conference"], [60, 60, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 16, 18, "related-to", "contributes_to", false, false], [0, 0, 21, 23, "win-defeat", "", false, false], [0, 0, 39, 41, "win-defeat", "", false, false], [21, 23, 26, 32, "temporal", "", false, false], [34, 34, 26, 32, "named", "", false, false], [39, 41, 44, 50, "temporal", "", false, false], [39, 41, 54, 58, "temporal", "", false, false], [51, 52, 44, 50, "named", "", false, false], [60, 60, 54, 58, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "international", "conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "and", "the", "Best", "Reviewer", "Award", "at", "the", "international", "conferences", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision and won the Best Paper Award at the international conference on Non-Photorealistic Rendering and Animation (NPAR) 2012 and the Best Reviewer Award at the international conferences Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 160], [161, 171], [172, 174], [175, 193], [194, 203], [204, 207], [208, 217], [218, 219], [219, 223], [223, 224], [225, 229], [230, 233], [234, 237], [238, 242], [243, 251], [252, 257], [258, 260], [261, 264], [265, 278], [279, 290], [291, 296], [297, 307], [308, 310], [311, 319], [320, 326], [327, 331], [332, 336], [337, 340], [341, 354], [355, 365], [366, 368], [369, 377], [378, 384], [385, 386], [386, 390], [390, 391], [392, 396], [396, 397]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 15, "researcher"], [16, 17, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 2, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [16, 17, 0, 0, "usage", "", false, false], [16, 17, 13, 15, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "the", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "CycL", "project", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is the ontology language used by Doug Lenat's CycL project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 59], [60, 68], [69, 77], [78, 82], [83, 85], [86, 90], [91, 96], [96, 98], [99, 103], [104, 111], [111, 112]]}
{"doc_key": "ai-train-86", "ner": [[2, 3, "task"], [6, 8, "metrics"], [17, 18, "metrics"], [20, 27, "metrics"], [37, 39, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 3, "part-of", "", false, false], [17, 18, 6, 8, "named", "", false, false], [20, 27, 6, 8, "named", "", false, false], [37, 39, 6, 8, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "the", "mean", "square", "error", ",", "often", "referred", "to", "as", "the", "mean", "square", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "square", "error", ",", "can", "refer", "to", "the", "average", "value", "of", "the", "squared", "deviation", "of", "the", "prediction", "from", "the", "TRUE", "value", ",", "over", "the", "out", "-", "of", "-", "sample", "test", "space", ",", "produced", "by", "the", "estimated", "model", "on", "a", "given", "sample", "space", "."], "sentence-detokenized": "Also in regression analysis, the mean square error, often referred to as the mean square prediction error or out-of-sample mean square error, can refer to the average value of the squared deviation of the prediction from the TRUE value, over the out-of-sample test space, produced by the estimated model on a given sample space.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 32], [33, 37], [38, 44], [45, 50], [50, 51], [52, 57], [58, 66], [67, 69], [70, 72], [73, 76], [77, 81], [82, 88], [89, 99], [100, 105], [106, 108], [109, 112], [112, 113], [113, 115], [115, 116], [116, 122], [123, 127], [128, 134], [135, 140], [140, 141], [142, 145], [146, 151], [152, 154], [155, 158], [159, 166], [167, 172], [173, 175], [176, 179], [180, 187], [188, 197], [198, 200], [201, 204], [205, 215], [216, 220], [221, 224], [225, 229], [230, 235], [235, 236], [237, 241], [242, 245], [246, 249], [249, 250], [250, 252], [252, 253], [253, 259], [260, 264], [265, 270], [270, 271], [272, 280], [281, 283], [284, 287], [288, 297], [298, 303], [304, 306], [307, 308], [309, 314], [315, 321], [322, 327], [327, 328]]}
{"doc_key": "ai-train-87", "ner": [[6, 8, "algorithm"], [10, 11, "algorithm"], [19, 22, "algorithm"], [34, 35, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 8, 10, 11, "compare", "", false, false], [6, 8, 19, 22, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "for", "the", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "performed", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptor", "maintaining", "a", "slight", "edge", "in", "detection", "loss", "rate", "at", "a", "fixed", "FALSE", "positive", "rate", "across", "both", "data", "sets", "."], "sentence-detokenized": "As for the results, the C-HOG and R-HOG block descriptors performed comparably, with the C-HOG descriptor maintaining a slight edge in detection loss rate at a fixed FALSE positive rate across both data sets.", "token2charspan": [[0, 2], [3, 6], [7, 10], [11, 18], [18, 19], [20, 23], [24, 25], [25, 26], [26, 29], [30, 33], [34, 36], [36, 39], [40, 45], [46, 57], [58, 67], [68, 78], [78, 79], [80, 84], [85, 88], [89, 90], [90, 91], [91, 94], [95, 105], [106, 117], [118, 119], [120, 126], [127, 131], [132, 134], [135, 144], [145, 149], [150, 154], [155, 157], [158, 159], [160, 165], [166, 171], [172, 180], [181, 185], [186, 192], [193, 197], [198, 202], [203, 207], [207, 208]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [17, 18, "algorithm"], [20, 22, "algorithm"], [24, 26, "algorithm"], [28, 29, "misc"], [34, 36, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 28, 29, "usage", "", false, false], [14, 15, 28, 29, "usage", "", false, false], [17, 18, 28, 29, "usage", "", false, false], [20, 22, 28, 29, "usage", "", false, false], [24, 26, 28, 29, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "Fisherface", "algorithm", ",", "hidden", "Markov", "models", ",", "multilinear", "subspace", "learning", "using", "tensor", "representations", ",", "and", "neuronally", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using Fisherface algorithm, hidden Markov models, multilinear subspace learning using tensor representations, and neuronally motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 149], [150, 159], [159, 160], [161, 167], [168, 174], [175, 181], [181, 182], [183, 194], [195, 203], [204, 212], [213, 218], [219, 225], [226, 241], [241, 242], [243, 246], [247, 257], [258, 267], [268, 275], [276, 280], [281, 289], [289, 290]]}
{"doc_key": "ai-train-89", "ner": [[3, 7, "misc"], [17, 20, "location"], [37, 39, "location"], [50, 50, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 20, 3, 7, "temporal", "", false, false], [37, 39, 3, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "of", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "can", "now", "be", "restricted", "from", "screening", "at", "Toronto", "'s", "Scotiabank", "Theatre", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "screened", "elsewhere", "(", "such", "as", "the", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "distributed", "by", "services", "like", "Netflix", "."], "sentence-detokenized": "As of the 2019 Toronto International Film Festival, films can now be restricted from screening at Toronto's Scotiabank Theatre - one of the festival's main venues - and screened elsewhere (such as the TIFF Bell Lightbox and other local cinemas) if distributed by services like Netflix.", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 14], [15, 22], [23, 36], [37, 41], [42, 50], [50, 51], [52, 57], [58, 61], [62, 65], [66, 68], [69, 79], [80, 84], [85, 94], [95, 97], [98, 105], [105, 107], [108, 118], [119, 126], [127, 128], [129, 132], [133, 135], [136, 139], [140, 148], [148, 150], [151, 155], [156, 162], [163, 164], [165, 168], [169, 177], [178, 187], [188, 189], [189, 193], [194, 196], [197, 200], [201, 205], [206, 210], [211, 219], [220, 223], [224, 229], [230, 235], [236, 243], [243, 244], [245, 247], [248, 259], [260, 262], [263, 271], [272, 276], [277, 284], [284, 285]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 3, "researcher"], [5, 6, "organisation"], [12, 13, "researcher"], [23, 27, "product"], [38, 39, "researcher"], [43, 45, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 0, 5, 6, "related-to", "purchases", false, false], [2, 3, 12, 13, "named", "same", false, false], [2, 3, 38, 39, "named", "same", false, false], [5, 6, 2, 3, "origin", "founded_by", false, false], [23, 27, 0, 0, "artifact", "", false, false], [43, 45, 38, 39, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Unimation", "purchased", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "manufacturing", "the", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "a", "new", "model", "of", "robotic", "arm", ",", "and", "used", "Scheinman", "'s", "cutting", "-", "edge", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation purchased Victor Scheinman's Vicarm Inc. in 1977, and with Scheinman's help, the company created and began manufacturing the Programmable Universal Machine for Assembly, a new model of robotic arm, and used Scheinman's cutting-edge VAL programming language.", "token2charspan": [[0, 9], [10, 19], [20, 26], [27, 36], [36, 38], [39, 45], [46, 50], [51, 53], [54, 58], [58, 59], [60, 63], [64, 68], [69, 78], [78, 80], [81, 85], [85, 86], [87, 90], [91, 98], [99, 106], [107, 110], [111, 116], [117, 130], [131, 134], [135, 147], [148, 157], [158, 165], [166, 169], [170, 178], [178, 179], [180, 181], [182, 185], [186, 191], [192, 194], [195, 202], [203, 206], [206, 207], [208, 211], [212, 216], [217, 226], [226, 228], [229, 236], [236, 237], [237, 241], [242, 245], [246, 257], [258, 266], [266, 267]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [10, 11, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 10, 11, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[2, 2, "metrics"], [13, 14, "product"], [20, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[2, 2, 13, 14, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", "according", "to", "Google", "Scholar", ",", "it", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", "for", "2016", ",", "which", "denotes", "a", "paper", "that", "has", "had", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "its", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20,000 times according to Google Scholar, it also received the IEEE Signal Processing Society Sustained Impact Award for 2016, which denotes a paper that has had an unusually high impact for at least 10 years after its publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 51], [52, 57], [58, 67], [68, 70], [71, 77], [78, 85], [85, 86], [87, 89], [90, 94], [95, 103], [104, 107], [108, 112], [113, 119], [120, 130], [131, 138], [139, 148], [149, 155], [156, 161], [162, 165], [166, 170], [170, 171], [172, 177], [178, 185], [186, 187], [188, 193], [194, 198], [199, 202], [203, 206], [207, 209], [210, 219], [220, 224], [225, 231], [232, 235], [236, 238], [239, 244], [245, 247], [248, 253], [254, 259], [260, 263], [264, 275], [275, 276]]}
{"doc_key": "ai-train-93", "ner": [[0, 2, "task"], [15, 22, "product"], [33, 35, "product"], [37, 37, "organisation"], [38, 38, "product"], [43, 43, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 2, 37, 37, "artifact", "", false, false], [15, 22, 0, 2, "related-to", "performs", false, false], [15, 22, 33, 35, "part-of", "", false, false], [37, 37, 43, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "almost", "completely", "indistinguishable", "from", "a", "real", "human", "voice", "with", "the", "introduction", "of", "Adobe", "Voco", "voice", "editing", "and", "creation", "software", "in", "2016", ",", "a", "prototype", "slated", "to", "become", "part", "of", "the", "Adobe", "Creative", "Suite", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", "."], "sentence-detokenized": "Speech synthesis is almost completely indistinguishable from a real human voice with the introduction of Adobe Voco voice editing and creation software in 2016, a prototype slated to become part of the Adobe Creative Suite and DeepMind WaveNet, a prototype from Google.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 26], [27, 37], [38, 55], [56, 60], [61, 62], [63, 67], [68, 73], [74, 79], [80, 84], [85, 88], [89, 101], [102, 104], [105, 110], [111, 115], [116, 121], [122, 129], [130, 133], [134, 142], [143, 151], [152, 154], [155, 159], [159, 160], [161, 162], [163, 172], [173, 179], [180, 182], [183, 189], [190, 194], [195, 197], [198, 201], [202, 207], [208, 216], [217, 222], [223, 226], [227, 235], [236, 243], [243, 244], [245, 246], [247, 256], [257, 261], [262, 268], [268, 269]]}
{"doc_key": "ai-train-94", "ner": [[0, 0, "researcher"], [7, 9, "organisation"], [15, 22, "organisation"], [26, 26, "conference"], [33, 37, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 9, "role", "", false, false], [0, 0, 15, 22, "role", "", false, false], [0, 0, 26, 26, "role", "", false, false], [0, 0, 33, 37, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "member", "of", "the", "Neuroscience", "Research", "Programme", ",", "a", "member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "and", "a", "founding", "fellow", "of", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary member of the Neuroscience Research Programme, a member of the American Academy of Arts and Sciences and a founding fellow of AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 67], [67, 68], [69, 70], [71, 77], [78, 80], [81, 84], [85, 93], [94, 101], [102, 104], [105, 109], [110, 113], [114, 122], [123, 126], [127, 128], [129, 137], [138, 144], [145, 147], [148, 152], [153, 156], [157, 158], [159, 167], [168, 174], [175, 177], [178, 181], [182, 190], [191, 200], [201, 204], [205, 210], [211, 219], [219, 220]]}
{"doc_key": "ai-train-95", "ner": [[8, 9, "task"], [11, 12, "task"], [17, 18, "task"], [24, 24, "misc"], [25, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 9, 17, 18, "cause-effect", "", false, false], [11, 12, 17, 18, "cause-effect", "", false, false], [25, 26, 17, 18, "topic", "", false, false], [25, 26, 24, 24, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["During", "the", "1990s", ",", "fuelled", "by", "successes", "in", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "began", "in", "speech", "translation", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "During the 1990s, fuelled by successes in speech recognition and speech synthesis, research began in speech translation with the development of the German Verbmobil project.", "token2charspan": [[0, 6], [7, 10], [11, 16], [16, 17], [18, 25], [26, 28], [29, 38], [39, 41], [42, 48], [49, 60], [61, 64], [65, 71], [72, 81], [81, 82], [83, 91], [92, 97], [98, 100], [101, 107], [108, 119], [120, 124], [125, 128], [129, 140], [141, 143], [144, 147], [148, 154], [155, 164], [165, 172], [172, 173]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 16, "algorithm"], [20, 21, "algorithm"], [25, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 16, 3, 4, "origin", "", false, false], [15, 16, 8, 9, "origin", "", false, false], [15, 16, 11, 12, "origin", "", false, false], [15, 16, 25, 26, "part-of", "", false, false], [20, 21, 15, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisors", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forget", "gate", "(", "also", "called", "keep", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisors J\u00fcrgen Schmidhuber and Fred Cummins introduced the forget gate (also called keep gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 87], [88, 94], [95, 99], [100, 101], [101, 105], [106, 112], [113, 117], [118, 122], [122, 123], [124, 128], [129, 132], [133, 137], [138, 150], [150, 151]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 12, 1, 3, "part-of", "", false, false], [9, 12, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalised", "sinc", "function", "is", "usually", "defined", "by"], "sentence-detokenized": "In digital signal processing and information theory, the normalised sinc function is usually defined by", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [17, 20, "conference"], [23, 27, "organisation"], [29, 29, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 17, 20, "role", "", false, false], [9, 10, 23, 27, "role", "", false, false], [29, 29, 23, 27, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hays, a founding member of the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 72], [72, 73], [74, 75], [76, 84], [85, 91], [92, 94], [95, 98], [99, 110], [111, 114], [115, 128], [129, 140], [141, 144], [145, 148], [149, 162], [163, 172], [173, 175], [176, 189], [190, 201], [202, 203], [203, 207], [207, 208], [208, 209]]}
{"doc_key": "ai-train-99", "ner": [[8, 13, "misc"], [18, 18, "misc"], [33, 35, "metrics"], [37, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[37, 37, 33, 35, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one", "-dimensional", "polynomial", "memory", "-", "based", "(", "or", "memoryless", ")", "DPD", ",", "in", "order", "to", "solve", "for", "the", "digital", "pre-distortion", "polynomial", "coefficients", "and", "minimise", "the", "mean", "square", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "over", "-sampled", "at", "a", "rate", "that", "allows", "the", "capture", "of", "the", "nonlinear", "product", "of", "the", "digital", "pre-distortion", "sequence", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional polynomial memory-based (or memoryless) DPD, in order to solve for the digital pre-distortion polynomial coefficients and minimise the mean square error (MSE), the distorted output of the nonlinear system must be over-sampled at a rate that allows the capture of the nonlinear product of the digital pre-distortion sequence.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 35], [35, 47], [48, 58], [59, 65], [65, 66], [66, 71], [72, 73], [73, 75], [76, 86], [86, 87], [88, 91], [91, 92], [93, 95], [96, 101], [102, 104], [105, 110], [111, 114], [115, 118], [119, 126], [127, 141], [142, 152], [153, 165], [166, 169], [170, 178], [179, 182], [183, 187], [188, 194], [195, 200], [201, 202], [202, 205], [205, 206], [206, 207], [208, 211], [212, 221], [222, 228], [229, 231], [232, 235], [236, 245], [246, 252], [253, 257], [258, 260], [261, 265], [265, 273], [274, 276], [277, 278], [279, 283], [284, 288], [289, 295], [296, 299], [300, 307], [308, 310], [311, 314], [315, 324], [325, 332], [333, 335], [336, 339], [340, 347], [348, 362], [363, 371], [371, 372]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 9, "location"], [11, 12, "location"], [14, 15, "country"], [19, 19, "location"], [21, 21, "country"], [36, 42, "organisation"], [45, 48, "organisation"], [50, 50, "location"], [57, 58, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 9, 9, "physical", "", false, false], [0, 1, 45, 48, "physical", "", false, false], [0, 1, 57, 58, "role", "", false, false], [9, 9, 11, 12, "physical", "", false, false], [11, 12, 14, 15, "physical", "", false, false], [36, 42, 45, 48, "part-of", "", false, false], [45, 48, 50, 50, "physical", "", false, false], [57, 58, 36, 42, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "5", "October", "1947", ",", "Chi\u0219in\u0103u", ",", "Moldavian", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "an", "American", "principal", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "and", "head", "of", "the", "Laboratory", "'s", "InfoLab", "Group", "."], "sentence-detokenized": "Boris Katz, (born 5 October 1947, Chi\u0219in\u0103u, Moldavian SSR, Soviet Union, (now Chi\u0219in\u0103u, Moldova)) is an American principal research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and head of the Laboratory's InfoLab Group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 19], [20, 27], [28, 32], [32, 33], [34, 42], [42, 43], [44, 53], [54, 57], [57, 58], [59, 65], [66, 71], [71, 72], [73, 74], [74, 77], [78, 86], [86, 87], [88, 95], [95, 96], [96, 97], [98, 100], [101, 103], [104, 112], [113, 122], [123, 131], [132, 141], [142, 143], [143, 151], [152, 161], [161, 162], [163, 165], [166, 169], [170, 173], [174, 182], [183, 190], [191, 194], [195, 205], [206, 218], [219, 229], [230, 232], [233, 236], [237, 250], [251, 260], [261, 263], [264, 274], [275, 277], [278, 287], [288, 291], [292, 296], [297, 299], [300, 303], [304, 314], [314, 316], [317, 324], [325, 330], [330, 331]]}
