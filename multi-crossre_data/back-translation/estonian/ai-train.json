{"doc_key": "ai-train-1", "ner": [[3, 8, "product"], [16, 17, "field"], [19, 20, "task"], [22, 23, "task"], [27, 29, "task"], [32, 33, "field"], [34, 36, "researcher"], [38, 40, "researcher"], [42, 43, "researcher"], [45, 46, "researcher"], [48, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"], [58, 59, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 8, 16, 17, "part-of", "", false, false], [3, 8, 16, 17, "usage", "", false, false], [3, 8, 19, 20, "part-of", "", false, false], [3, 8, 19, 20, "usage", "", false, false], [3, 8, 22, 23, "part-of", "", false, false], [3, 8, 22, 23, "usage", "", false, false], [3, 8, 32, 33, "part-of", "", false, false], [3, 8, 32, 33, "usage", "", false, false], [27, 29, 22, 23, "part-of", "", false, false], [27, 29, 22, 23, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "an", "opinion", "-", "based", "recommender", "system", "use", "a", "variety", "of", "techniques", ",", "including", "text", "mining", ",", "information", "mining", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to an opinion-based recommender system use a variety of techniques, including text mining, information mining, sentiment analysis (see also multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 24], [25, 32], [32, 33], [33, 38], [39, 50], [51, 57], [58, 61], [62, 63], [64, 71], [72, 74], [75, 85], [85, 86], [87, 96], [97, 101], [102, 108], [108, 109], [110, 121], [122, 128], [128, 129], [130, 139], [140, 148], [149, 150], [150, 153], [154, 158], [159, 169], [170, 179], [180, 188], [188, 189], [190, 193], [194, 198], [199, 207], [208, 211], [211, 212], [213, 217], [217, 218], [219, 220], [220, 221], [222, 227], [227, 228], [229, 233], [234, 237], [237, 238], [239, 243], [244, 249], [249, 250], [251, 252], [252, 253], [254, 257], [257, 258], [259, 263], [264, 269], [269, 270], [271, 275], [276, 280], [280, 281], [282, 284], [285, 287], [287, 288], [289, 290], [290, 294], [294, 295], [295, 296], [296, 297], [298, 300], [301, 302], [302, 303], [303, 304], [304, 305], [306, 312], [312, 313]]}
{"doc_key": "ai-train-2", "ner": [[7, 8, "university"], [13, 14, "researcher"], [16, 17, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[13, 14, 7, 8, "physical", "", false, false], [13, 14, 7, 8, "role", "", false, false], [16, 17, 7, 8, "physical", "", false, false], [16, 17, 7, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Proponents", "of", "procedural", "representations", "were", "mainly", "concentrated", "at", "MIT", "under", "the", "leadership", "of", "Marvin", "Minski", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Proponents of procedural representations were mainly concentrated at MIT under the leadership of Marvin Minski and Seymour Papert.", "token2charspan": [[0, 10], [11, 13], [14, 24], [25, 40], [41, 45], [46, 52], [53, 65], [66, 68], [69, 72], [73, 78], [79, 82], [83, 93], [94, 96], [97, 103], [104, 110], [111, 114], [115, 122], [123, 129], [129, 130]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Octave", "helps", "you", "solve", "linear", "and", "non-linear", "problems", "numerically", ",", "and", "perform", "other", "numerical", "experiments", ",", "mostly", "using", "a", "MATLAB", "-", "compatible", "program", "."], "sentence-detokenized": "Octave helps you solve linear and non-linear problems numerically, and perform other numerical experiments, mostly using a MATLAB-compatible program.", "token2charspan": [[0, 6], [7, 12], [13, 16], [17, 22], [23, 29], [30, 33], [34, 44], [45, 53], [54, 65], [65, 66], [67, 70], [71, 78], [79, 84], [85, 94], [95, 106], [106, 107], [108, 114], [115, 120], [121, 122], [123, 129], [129, 130], [130, 140], [141, 148], [148, 149]]}
{"doc_key": "ai-train-5", "ner": [[3, 7, "algorithm"], [9, 10, "misc"], [13, 14, "researcher"], [11, 21, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 7, 13, 14, "origin", "", false, false], [9, 10, 13, 14, "origin", "", false, false], [13, 14, 11, 21, "physical", "", false, false], [13, 14, 11, 21, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Both", "variants", "of", "the", "back", "-", "propagation", "algorithm", "and", "unsupervised", "methods", "used", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", ",", "{{", "cite", "journal"], "sentence-detokenized": "Both variants of the back-propagation algorithm and unsupervised methods used by Geoff Hinton and colleagues at the University of Toronto, {{cite journal", "token2charspan": [[0, 4], [5, 13], [14, 16], [17, 20], [21, 25], [25, 26], [26, 37], [38, 47], [48, 51], [52, 64], [65, 72], [73, 77], [78, 80], [81, 86], [87, 93], [94, 97], [98, 108], [109, 111], [112, 115], [116, 126], [127, 129], [130, 137], [137, 138], [139, 141], [141, 145], [146, 153]]}
{"doc_key": "ai-train-6", "ner": [[3, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalently", "using", "DCG", "notation", ":"], "sentence-detokenized": "or equivalently using DCG notation:", "token2charspan": [[0, 2], [3, 15], [16, 21], [22, 25], [26, 34], [34, 35]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 11, "algorithm"], [14, 15, "algorithm"], [20, 22, "algorithm"], [25, 25, "algorithm"], [27, 28, "algorithm"], [41, 44, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 11, "type-of", "", false, false], [0, 3, 14, 15, "usage", "part-of?", true, false], [14, 15, 20, 22, "compare", "", false, false], [25, 25, 20, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "implement", "competitive", "learning", ",", "as", "opposed", "to", "error", "-correction", "learning", "(", "e.g.", "backpropagation", "with", "gradient", "descent", ")", ",", "and", "in", "that", "they", "use", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps differ from other artificial neural networks in that they implement competitive learning, as opposed to error-correction learning (e.g. backpropagation with gradient descent), and in that they use a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 88], [89, 100], [101, 109], [109, 110], [111, 113], [114, 121], [122, 124], [125, 130], [130, 141], [142, 150], [151, 152], [152, 156], [157, 172], [173, 177], [178, 186], [187, 194], [194, 195], [195, 196], [197, 200], [201, 203], [204, 208], [209, 213], [214, 217], [218, 219], [220, 232], [233, 241], [242, 244], [245, 253], [254, 257], [258, 269], [270, 280], [281, 283], [284, 287], [288, 293], [294, 299], [299, 300]]}
{"doc_key": "ai-train-8", "ner": [[12, 16, "organisation"], [29, 33, "misc"], [38, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "a", "number", "of", "bodies", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "should", "be", "made", "in", "conjunction", "with", "the", "sound", "signal", ",", "which", "is", "filtered", "out", "in", "the", "noise", "floor", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "damping", "circuits", "."], "sentence-detokenized": "Since the early 1990s, a number of bodies, including the Audio Engineering Society, have recommended that dynamic range measurements should be made in conjunction with the sound signal, which is filtered out in the noise floor measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or damping circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 24], [25, 31], [32, 34], [35, 41], [41, 42], [43, 52], [53, 56], [57, 62], [63, 74], [75, 82], [82, 83], [84, 88], [89, 100], [101, 105], [106, 113], [114, 119], [120, 132], [133, 139], [140, 142], [143, 147], [148, 150], [151, 162], [163, 167], [168, 171], [172, 177], [178, 184], [184, 185], [186, 191], [192, 194], [195, 203], [204, 207], [208, 210], [211, 214], [215, 220], [221, 226], [227, 238], [239, 243], [244, 246], [247, 256], [257, 264], [265, 270], [270, 271], [272, 276], [277, 283], [284, 296], [297, 309], [310, 315], [316, 318], [319, 322], [323, 326], [327, 329], [330, 335], [336, 341], [342, 344], [345, 352], [353, 361], [361, 362]]}
{"doc_key": "ai-train-9", "ner": [[6, 7, "misc"], [14, 15, "task"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 31, "task"], [33, 35, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[6, 7, 14, 15, "part-of", "concept_used_in", true, false], [6, 7, 17, 18, "part-of", "concept_used_in", false, false], [6, 7, 20, 21, "part-of", "concept_used_in", false, false], [6, 7, 23, 24, "part-of", "concept_used_in", false, false], [6, 7, 26, 31, "part-of", "concept_used_in", false, false], [6, 7, 33, 35, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["Techniques", "used", "to", "create", "and", "use", "facial", "images", "for", "recognition", "are", "also", "used", "beyond", "facial", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "Techniques used to create and use facial images for recognition are also used beyond facial recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation and medical image analysis.", "token2charspan": [[0, 10], [11, 15], [16, 18], [19, 25], [26, 29], [30, 33], [34, 40], [41, 47], [48, 51], [52, 63], [64, 67], [68, 72], [73, 77], [78, 84], [85, 91], [92, 103], [103, 104], [105, 116], [117, 128], [128, 129], [130, 133], [134, 141], [141, 142], [143, 148], [149, 160], [160, 161], [162, 166], [167, 175], [175, 176], [176, 180], [181, 188], [189, 203], [204, 207], [208, 215], [216, 221], [222, 230], [230, 231]]}
{"doc_key": "ai-train-10", "ner": [[0, 5, "organisation"], [11, 14, "organisation"], [10, 16, "organisation"], [20, 23, "organisation"], [26, 31, "organisation"], [35, 38, "organisation"], [45, 45, "organisation"], [41, 47, "organisation"], [51, 56, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[11, 14, 0, 5, "part-of", "", false, false], [10, 16, 11, 14, "named", "", false, false], [20, 23, 0, 5, "part-of", "", false, false], [26, 31, 0, 5, "part-of", "", false, false], [35, 38, 0, 5, "part-of", "", false, false], [45, 45, 0, 5, "part-of", "", false, false], [41, 47, 45, 45, "named", "", false, false], [51, 56, 0, 5, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "served", "as", "an", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "(", "NIST", ")", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", ",", "which", "coordinated", "studies", "to", "inform", "strategic", "planners", "during", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation served as an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce (NIST), the US Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research, which coordinated studies to inform strategic planners during their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 38], [39, 41], [42, 44], [45, 53], [54, 57], [58, 61], [62, 70], [71, 82], [83, 86], [87, 92], [93, 107], [108, 109], [109, 113], [113, 114], [114, 115], [116, 119], [120, 122], [123, 133], [134, 136], [137, 143], [143, 144], [145, 148], [149, 151], [152, 162], [163, 165], [166, 174], [175, 176], [176, 180], [180, 181], [181, 182], [183, 186], [187, 189], [190, 200], [201, 203], [204, 211], [211, 212], [213, 216], [217, 224], [225, 233], [234, 242], [243, 251], [252, 258], [259, 260], [260, 265], [265, 266], [267, 270], [271, 274], [275, 281], [282, 284], [285, 290], [291, 299], [299, 300], [301, 306], [307, 318], [319, 326], [327, 329], [330, 336], [337, 346], [347, 355], [356, 362], [363, 368], [369, 382], [382, 383]]}
{"doc_key": "ai-train-11", "ner": [[19, 20, "metrics"], [23, 25, "algorithm"], [0, 1, "researcher"], [10, 11, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 20, 23, 25, "part-of", "", false, false], [0, 1, 10, 11, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Ronald", "Fisher", "proposed", "in", "1935", ",", "as", "an", "addendum", "to", "Bliss", "'s", "work", ",", "a", "quick", "method", "for", "calculating", "maximum", "likelihood", "estimates", "from", "a", "probability", "model", "."], "sentence-detokenized": "Ronald Fisher proposed in 1935, as an addendum to Bliss's work, a quick method for calculating maximum likelihood estimates from a probability model.", "token2charspan": [[0, 6], [7, 13], [14, 22], [23, 25], [26, 30], [30, 31], [32, 34], [35, 37], [38, 46], [47, 49], [50, 55], [55, 57], [58, 62], [62, 63], [64, 65], [66, 71], [72, 78], [79, 82], [83, 94], [95, 102], [103, 113], [114, 123], [124, 128], [129, 130], [131, 142], [143, 148], [148, 149]]}
{"doc_key": "ai-train-12", "ner": [[12, 13, "product"], [16, 17, "product"], [24, 25, "organisation"], [26, 26, "product"], [34, 34, "organisation"], [35, 35, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[26, 26, 16, 17, "usage", "uses_software", false, false], [26, 26, 24, 25, "artifact", "", false, false], [26, 26, 35, 35, "named", "", false, false], [35, 35, 34, 34, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "of", "these", "programs", "are", "available", "on", "the", "Internet", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", ",", "which", "is", "the", "basis", "for", "AltaVista", "'s", "BabelFish", "(", "since", "9", "May", "2008", "it", "is", "Yahoo", "Babelfish", ")", "."], "sentence-detokenized": "Several of these programs are available on the Internet, such as Google Translate and the SYSTRAN system, which is the basis for AltaVista's BabelFish (since 9 May 2008 it is Yahoo Babelfish).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 39], [40, 42], [43, 46], [47, 55], [55, 56], [57, 61], [62, 64], [65, 71], [72, 81], [82, 85], [86, 89], [90, 97], [98, 104], [104, 105], [106, 111], [112, 114], [115, 118], [119, 124], [125, 128], [129, 138], [138, 140], [141, 150], [151, 152], [152, 157], [158, 159], [160, 163], [164, 168], [169, 171], [172, 174], [175, 180], [181, 190], [190, 191], [191, 192]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 13, "researcher"], [20, 22, "field"], [26, 27, "misc"], [32, 33, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 32, 33, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 32, 33, "related-to", "", true, false], [10, 13, 20, 22, "related-to", "", true, false], [10, 13, 26, 27, "related-to", "", true, false], [10, 13, 32, 33, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "general", "artificial", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-", "based", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of general artificial intelligence based on idealised intelligent agents and reward-based reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 122], [123, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 196], [196, 201], [202, 215], [216, 224], [224, 225]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 19, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 19, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "method", "."], "sentence-detokenized": "The most common way is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) method.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 25], [26, 29], [30, 33], [34, 36], [36, 37], [37, 43], [44, 49], [50, 51], [51, 57], [57, 58], [58, 66], [67, 77], [78, 81], [82, 89], [90, 100], [100, 101], [102, 108], [108, 109]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [12, 12, "programlang"], [17, 18, "researcher"], [20, 21, "organisation"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[0, 0, 12, 12, "related-to", "", false, false], [17, 18, 20, 21, "role", "", false, false]], "relations_mapping_to_source": [0, 2], "sentence": ["RapidMiner", "provides", "tutorials", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "with", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides tutorials, models and algorithms and can be extended with R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 29], [29, 30], [31, 37], [38, 41], [42, 52], [53, 56], [57, 60], [61, 63], [64, 72], [73, 77], [78, 79], [80, 83], [84, 90], [91, 98], [98, 99], [100, 105], [106, 112], [112, 113], [114, 119], [120, 128], [128, 129], [130, 132], [133, 141], [142, 146], [146, 147]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [9, 14, "task"], [18, 21, "misc"], [39, 44, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 9, 14, "related-to", "", false, false], [0, 0, 39, 44, "related-to", "", true, false], [18, 21, 0, 0, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["tity", "includes", "a", "set", "of", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "forecasting", "modelling", ",", "together", "with", "graphical", "user", "interfaces", "that", "allow", "easy", "access", "to", "these", "functions", ".", "but", "the", "more", "recent", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "which", "was", "first", "developed", "in", "1997", ",", "is", "now", "used", "in", "a", "wide", "range", "of", "applications", ",", "particularly", "in", "education", "and", "research", "."], "sentence-detokenized": "tity includes a set of visualisation tools and algorithms for data analysis and forecasting modelling, together with graphical user interfaces that allow easy access to these functions. but the more recent fully Java-based version (Weka 3), which was first developed in 1997, is now used in a wide range of applications, particularly in education and research.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 19], [20, 22], [23, 36], [37, 42], [43, 46], [47, 57], [58, 61], [62, 66], [67, 75], [76, 79], [80, 91], [92, 101], [101, 102], [103, 111], [112, 116], [117, 126], [127, 131], [132, 142], [143, 147], [148, 153], [154, 158], [159, 165], [166, 168], [169, 174], [175, 184], [184, 185], [186, 189], [190, 193], [194, 198], [199, 205], [206, 211], [212, 216], [216, 217], [217, 222], [223, 230], [231, 232], [232, 236], [237, 238], [238, 239], [239, 240], [241, 246], [247, 250], [251, 256], [257, 266], [267, 269], [270, 274], [274, 275], [276, 278], [279, 282], [283, 287], [288, 290], [291, 292], [293, 297], [298, 303], [304, 306], [307, 319], [319, 320], [321, 333], [334, 336], [337, 346], [347, 350], [351, 359], [359, 360]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [13, 21, "misc"], [24, 26, "misc"], [29, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 21, 0, 0, "topic", "", false, false], [13, 21, 24, 26, "win-defeat", "", false, false], [24, 26, 29, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "enjoyed", "considerable", "acclaim", ",", "with", "his", "work", "Heuretics", ":", "The", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "winning", "the", "best", "paper", "award", "at", "the", "1982", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "conference", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and enjoyed considerable acclaim, with his work Heuretics: The Theoretical and Study of Heuristic Rules winning the best paper award at the 1982 Association for the Advancement of Artificial Intelligence conference.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 53], [54, 66], [67, 74], [74, 75], [76, 80], [81, 84], [85, 89], [90, 99], [99, 100], [101, 104], [105, 116], [117, 120], [121, 126], [127, 129], [130, 139], [140, 145], [146, 153], [154, 157], [158, 162], [163, 168], [169, 174], [175, 177], [178, 181], [182, 186], [187, 198], [199, 202], [203, 206], [207, 218], [219, 221], [222, 232], [233, 245], [246, 256], [256, 257]]}
{"doc_key": "ai-train-18", "ner": [[8, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "account", "for", "multiple", "units", ",", "a", "separate", "Hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To account for multiple units, a separate Hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 10], [11, 14], [15, 23], [24, 29], [29, 30], [31, 32], [33, 41], [42, 47], [48, 52], [53, 55], [56, 66], [67, 70], [71, 75], [76, 83], [83, 84]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 14, "product"], [16, 17, "product"], [19, 21, "product"], [23, 25, "product"], [27, 28, "product"], [37, 39, "product"], [42, 43, "product"], [33, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 27, 28, "type-of", "", false, false], [12, 14, 27, 28, "type-of", "", false, false], [16, 17, 27, 28, "type-of", "", false, false], [19, 21, 27, 28, "type-of", "", false, false], [23, 25, 27, 28, "type-of", "", false, false], [42, 43, 37, 39, "type-of", "", false, false], [33, 46, 37, 39, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "emergence", "of", "conversational", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "'s", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "'s", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "remote", "voice", "assistants", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the emergence of conversational assistants such as Apple's Siri, Amazon's Alexa, Google Assistant, Microsoft's Cortana and Samsung's Bixby, voice portals can now be accessed via mobile devices and remote voice assistants such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 36], [37, 47], [48, 52], [53, 55], [56, 61], [61, 63], [64, 68], [68, 69], [70, 76], [76, 78], [79, 84], [84, 85], [86, 92], [93, 102], [102, 103], [104, 113], [113, 115], [116, 123], [124, 127], [128, 135], [135, 137], [138, 143], [143, 144], [145, 150], [151, 158], [159, 162], [163, 166], [167, 169], [170, 178], [179, 182], [183, 189], [190, 197], [198, 201], [202, 208], [209, 214], [215, 225], [226, 230], [231, 233], [234, 240], [241, 245], [246, 249], [250, 256], [257, 261], [261, 262]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [6, 8, "algorithm"], [11, 13, "algorithm"], [15, 16, "algorithm"], [19, 19, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 3, "type-of", "", false, false], [11, 13, 2, 3, "type-of", "", false, false], [15, 16, 2, 3, "type-of", "", false, false], [19, 19, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "the", "Naive", "Bayes", "classifier", ",", "the", "support", "vector", "machine", ",", "Gaussian", "mixtures", "and", "the", "network", "."], "sentence-detokenized": "Examples of supervised learning are the Naive Bayes classifier, the support vector machine, Gaussian mixtures and the network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 39], [40, 45], [46, 51], [52, 62], [62, 63], [64, 67], [68, 75], [76, 82], [83, 90], [90, 91], [92, 100], [101, 109], [110, 113], [114, 117], [118, 125], [125, 126]]}
{"doc_key": "ai-train-21", "ner": [[0, 6, "algorithm"], [25, 26, "task"], [32, 34, "metrics"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[32, 34, 25, 26, "usage", "", true, false]], "relations_mapping_to_source": [1], "sentence": ["The", "OSD", "algorithm", "can", "be", "used", "to", "derive", "the", "mathematical", "O", "(", "\\sqrt", "{", "T", "})", "/mathematical", "regret", "bounds", "for", "a", "web", "version", "of", "the", "classification", "of", "support", "vector", "machines", "that", "uses", "the", "hinge", "loss", "mathematical", "v", "_t", "(", "w", ")=\\max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w\\cdot", "x", "_t", ")", "\\}.", "/", "math"], "sentence-detokenized": "The OSD algorithm can be used to derive the mathematical O(\\sqrt {T})/mathematical regret bounds for a web version of the classification of support vector machines that uses the hinge loss mathematical v _t(w)=\\max\\{0, 1 - y _t(w\\cdot x _t)\\}./math", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 24], [25, 29], [30, 32], [33, 39], [40, 43], [44, 56], [57, 58], [58, 59], [59, 64], [65, 66], [66, 67], [67, 69], [69, 82], [83, 89], [90, 96], [97, 100], [101, 102], [103, 106], [107, 114], [115, 117], [118, 121], [122, 136], [137, 139], [140, 147], [148, 154], [155, 163], [164, 168], [169, 173], [174, 177], [178, 183], [184, 188], [189, 201], [202, 203], [204, 206], [206, 207], [207, 208], [208, 214], [214, 215], [215, 216], [216, 217], [217, 218], [219, 220], [221, 222], [223, 224], [225, 227], [227, 228], [228, 234], [235, 236], [237, 239], [239, 240], [240, 243], [243, 244], [244, 248]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"], [26, 27, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robot", "mapping", "and", "navigation", ",", "image", "merging", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "recognition", "and", "game", "motion", "."], "sentence-detokenized": "Applications include object recognition, robot mapping and navigation, image merging, 3D modelling, gesture recognition, video tracking, individual wildlife recognition and game motion.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 46], [47, 54], [55, 58], [59, 69], [69, 70], [71, 76], [77, 84], [84, 85], [86, 88], [89, 98], [98, 99], [100, 107], [108, 119], [119, 120], [121, 126], [127, 135], [135, 136], [137, 147], [148, 156], [157, 168], [169, 172], [173, 177], [178, 184], [184, 185]]}
{"doc_key": "ai-train-23", "ner": [[0, 1, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [28, 31, "university"], [33, 35, "university"], [37, 39, "university"], [41, 42, "university"], [47, 49, "university"], [46, 51, "university"], [54, 58, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[0, 1, 14, 15, "related-to", "", true, false], [0, 1, 17, 19, "related-to", "", true, false], [0, 1, 21, 22, "related-to", "", true, false], [0, 1, 24, 25, "related-to", "", true, false], [0, 1, 28, 31, "related-to", "", true, false], [0, 1, 33, 35, "related-to", "", true, false], [0, 1, 37, 39, "related-to", "", true, false], [0, 1, 41, 42, "related-to", "", true, false], [0, 1, 47, 49, "related-to", "", true, false], [0, 1, 46, 51, "related-to", "", true, false], [0, 1, 54, 58, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Attitude", "assessment", "is", "being", "studied", "by", "several", "groups", "and", "companies", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "Attitude assessment is being studied by several groups and companies, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 28], [29, 36], [37, 39], [40, 47], [48, 54], [55, 58], [59, 68], [68, 69], [70, 79], [80, 86], [87, 89], [90, 95], [96, 106], [106, 107], [108, 116], [117, 123], [124, 134], [134, 135], [136, 139], [140, 152], [152, 153], [154, 162], [163, 173], [173, 174], [175, 185], [186, 188], [189, 199], [200, 203], [204, 209], [209, 210], [211, 221], [222, 224], [225, 232], [232, 233], [234, 239], [240, 248], [249, 254], [254, 255], [256, 259], [260, 266], [266, 267], [268, 276], [277, 287], [288, 290], [291, 299], [300, 303], [304, 314], [315, 316], [316, 320], [320, 321], [322, 325], [326, 336], [337, 339], [340, 350], [350, 351], [352, 358], [358, 359]]}
{"doc_key": "ai-train-24", "ner": [[0, 7, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "mathematics", "0,1", "/", "mathematics", "."], "sentence-detokenized": "The sigmoid function Cross entropy loss is used to predict K independent probability values in mathematics 0,1 / mathematics.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 26], [27, 34], [35, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 106], [107, 110], [111, 112], [113, 124], [124, 125]]}
{"doc_key": "ai-train-25", "ner": [[10, 11, "misc"], [14, 14, "field"], [16, 18, "field"], [19, 21, "university"], [22, 24, "country"], [27, 30, "misc"], [31, 36, "university"], [37, 37, "country"], [5, 5, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 11, 14, 14, "topic", "", false, false], [10, 11, 16, 18, "topic", "", false, false], [10, 11, 19, 21, "physical", "", true, false], [19, 21, 22, 24, "physical", "", false, false], [27, 30, 31, 36, "physical", "", true, false], [31, 36, 37, 37, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Prior", "to", "becoming", "Professor", "at", "Cambridge", ",", "he", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Informatics", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "."], "sentence-detokenized": "Prior to becoming Professor at Cambridge, he held the Johann Bernoulli Chair in Mathematics and Informatics at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan.", "token2charspan": [[0, 5], [6, 8], [9, 17], [18, 27], [28, 30], [31, 40], [40, 41], [42, 44], [45, 49], [50, 53], [54, 60], [61, 70], [71, 76], [77, 79], [80, 91], [92, 95], [96, 107], [108, 110], [111, 114], [115, 125], [126, 128], [129, 138], [139, 141], [142, 145], [146, 157], [158, 161], [162, 165], [166, 173], [174, 181], [182, 187], [188, 190], [191, 194], [195, 200], [201, 210], [211, 213], [214, 224], [225, 227], [228, 233], [233, 234]]}
{"doc_key": "ai-train-26", "ner": [[8, 8, "algorithm"], [18, 18, "algorithm"], [14, 20, "algorithm"], [25, 26, "researcher"], [28, 30, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 8, 18, 18, "usage", "", true, false], [18, 18, 25, 26, "origin", "", false, false], [18, 18, 28, 30, "origin", "", false, false], [14, 20, 18, 18, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", ",", "which", "is", "particularly", "used", "for", "recursive", "neural", "networks", ",", "is", "the", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "network", "developed", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another technique, which is particularly used for recursive neural networks, is the long short-term memory (LSTM) network developed by Sepp Hochreiter & J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 17], [17, 18], [19, 24], [25, 27], [28, 40], [41, 45], [46, 49], [50, 59], [60, 66], [67, 75], [75, 76], [77, 79], [80, 83], [84, 88], [89, 94], [94, 95], [95, 99], [100, 106], [107, 108], [108, 112], [112, 113], [114, 121], [122, 131], [132, 134], [135, 139], [140, 150], [151, 152], [153, 159], [160, 171], [172, 174], [175, 179], [179, 180]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 9, "product"], [15, 15, "product"], [44, 44, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 4, 5, "general-affiliation", "", false, false], [8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "addition", "of", "the", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "versatile", ",", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "modes", "similar", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The addition of the C++ interpreter (CINT up to version 5.34, Cling from version 6) makes this package very versatile, as it can be used in interactive, scripted and compiled modes similar to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 12], [13, 15], [16, 19], [20, 21], [21, 23], [24, 35], [36, 37], [37, 39], [39, 41], [42, 44], [45, 47], [48, 55], [56, 60], [60, 61], [62, 67], [68, 72], [73, 80], [81, 82], [82, 83], [84, 89], [90, 94], [95, 102], [103, 107], [108, 117], [117, 118], [119, 121], [122, 124], [125, 128], [129, 131], [132, 136], [137, 139], [140, 151], [151, 152], [153, 161], [162, 165], [166, 174], [175, 180], [181, 188], [189, 191], [192, 202], [203, 211], [212, 216], [217, 219], [220, 226], [226, 227]]}
{"doc_key": "ai-train-28", "ner": [[0, 1, "product"], [19, 21, "field"], [25, 27, "task"], [29, 30, "task"], [32, 33, "task"], [35, 36, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 19, 21, "related-to", "", false, false], [25, 27, 19, 21, "part-of", "", false, false], [29, 30, 19, 21, "part-of", "", false, false], [32, 33, 19, 21, "part-of", "", false, false], [35, 36, 19, 21, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Speech", "interfaces", "that", "interpret", "and", "manage", "conversational", "context", "are", "difficult", "to", "design", "because", "of", "the", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "kernel", "query", "resolution", ",", "name", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Speech interfaces that interpret and manage conversational context are difficult to design because of the difficulty of integrating complex natural language processing tasks such as kernel query resolution, name recognition, information retrieval and dialogue management.", "token2charspan": [[0, 6], [7, 17], [18, 22], [23, 32], [33, 36], [37, 43], [44, 58], [59, 66], [67, 70], [71, 80], [81, 83], [84, 90], [91, 98], [99, 101], [102, 105], [106, 116], [117, 119], [120, 131], [132, 139], [140, 147], [148, 156], [157, 167], [168, 173], [174, 178], [179, 181], [182, 188], [189, 194], [195, 205], [205, 206], [207, 211], [212, 223], [223, 224], [225, 236], [237, 246], [247, 250], [251, 259], [260, 270], [270, 271]]}
{"doc_key": "ai-train-29", "ner": [[5, 5, "algorithm"], [7, 10, "algorithm"], [15, 17, "researcher"], [20, 26, "organisation"], [36, 37, "field"], [39, 40, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 5, 15, 17, "origin", "", false, false], [5, 5, 36, 37, "part-of", "", false, false], [5, 5, 39, 40, "part-of", "", false, false], [7, 10, 15, 17, "origin", "", false, false], [7, 10, 36, 37, "part-of", "", false, false], [7, 10, 39, 40, "part-of", "", false, false], [15, 17, 20, 26, "physical", "", false, false], [15, 17, 20, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recursive", "and", "deep", "feed", "-", "forward", "neural", "networks", "developed", "by", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "artificial", "intelligence", "laboratory", "IDSIA", "have", "won", "eight", "international", "competitions", "in", "the", "field", "of", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recursive and deep feed-forward neural networks developed by J\u00fcrgen Schmidhuber's research group at the Swiss artificial intelligence laboratory IDSIA have won eight international competitions in the field of pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 36], [37, 41], [42, 46], [46, 47], [47, 54], [55, 61], [62, 70], [71, 80], [81, 83], [84, 90], [91, 102], [102, 104], [105, 113], [114, 119], [120, 122], [123, 126], [127, 132], [133, 143], [144, 156], [157, 167], [168, 173], [174, 178], [179, 182], [183, 188], [189, 202], [203, 215], [216, 218], [219, 222], [223, 228], [229, 231], [232, 239], [240, 251], [252, 255], [256, 263], [264, 272], [272, 273]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 12, "misc"], [14, 14, "field"], [15, 20, "university"], [26, 29, "field"], [30, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 12, 14, 14, "topic", "topic_of_award", false, false], [7, 12, 15, 20, "origin", "", true, false], [26, 29, 30, 35, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "two", "honorary", "degrees", ",", "one", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "one", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "the", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He holds two honorary degrees, one S. V. della laurea ad honorem in psychology from the University of Padua in 1995 and one doctorate in industrial design and engineering from the Delft University of Technology.", "token2charspan": [[0, 2], [3, 8], [9, 12], [13, 21], [22, 29], [29, 30], [31, 34], [35, 37], [38, 40], [41, 46], [47, 53], [54, 56], [57, 64], [65, 67], [68, 78], [79, 83], [84, 87], [88, 98], [99, 101], [102, 107], [108, 110], [111, 115], [116, 119], [120, 123], [124, 133], [134, 136], [137, 147], [148, 154], [155, 158], [159, 170], [171, 175], [176, 179], [180, 185], [186, 196], [197, 199], [200, 210], [210, 211]]}
{"doc_key": "ai-train-32", "ner": [[6, 7, "researcher"], [11, 16, "organisation"], [17, 18, "location"], [20, 20, "researcher"], [31, 33, "misc"], [46, 48, "misc"], [64, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[6, 7, 11, 16, "physical", "", false, false], [6, 7, 11, 16, "role", "", false, false], [11, 16, 17, 18, "physical", "", false, false], [20, 20, 31, 33, "related-to", "works_with", true, false], [20, 20, 46, 48, "related-to", "works_with", true, false], [20, 20, 64, 65, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Along", "with", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "that", "had", "reduced", "subtraction", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobe", ")", "and", "others", "with", "reduced", "subtraction", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "of", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Along with long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe that had reduced subtraction but preserved subtraction (associated with lesions of the inferior parietal lobe) and others with reduced subtraction but preserved subtraction (associated with lesions of the intraparietal sulcus).", "token2charspan": [[0, 5], [6, 10], [11, 15], [15, 16], [16, 20], [21, 33], [34, 41], [42, 47], [47, 48], [49, 50], [51, 62], [63, 65], [66, 69], [70, 75], [75, 76], [76, 87], [88, 96], [97, 99], [100, 105], [105, 106], [107, 114], [115, 119], [120, 130], [131, 139], [140, 144], [145, 152], [153, 155], [156, 165], [166, 173], [174, 176], [177, 180], [181, 189], [190, 194], [195, 199], [200, 203], [204, 211], [212, 223], [224, 227], [228, 237], [238, 249], [250, 251], [251, 261], [262, 266], [267, 274], [275, 277], [278, 281], [282, 290], [291, 299], [300, 304], [304, 305], [306, 309], [310, 316], [317, 321], [322, 329], [330, 341], [342, 345], [346, 355], [356, 367], [368, 369], [369, 379], [380, 384], [385, 392], [393, 395], [396, 399], [400, 413], [414, 420], [420, 421], [421, 422]]}
{"doc_key": "ai-train-33", "ner": [[6, 9, "product"], [13, 16, "misc"], [18, 19, "misc"], [26, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 9, "topic", "", false, false], [18, 19, 6, 9, "topic", "", false, false], [26, 26, 6, 9, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "depictions", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "and", "the", "2016", "TV", "adaptation", "Westworld", ",", "have", "awakened", "audience", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional depictions of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina, and the 2016 TV adaptation Westworld, have awakened audience sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 35], [36, 38], [39, 51], [52, 63], [64, 70], [71, 73], [74, 79], [80, 84], [85, 87], [88, 91], [91, 92], [93, 103], [104, 116], [117, 120], [121, 123], [124, 131], [131, 132], [133, 136], [137, 140], [141, 145], [146, 148], [149, 159], [160, 169], [169, 170], [171, 175], [176, 184], [185, 193], [194, 202], [203, 206], [207, 210], [211, 217], [218, 228], [228, 229]]}
{"doc_key": "ai-train-34", "ner": [[3, 4, "field"], [7, 9, "algorithm"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[7, 9, 3, 4, "part-of", "", false, false], [11, 12, 3, 4, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "two", "main", "unsupervised", "learning", "methods", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "The two main unsupervised learning methods are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 25], [26, 34], [35, 42], [43, 46], [47, 56], [57, 66], [67, 75], [76, 79], [80, 87], [88, 96], [96, 97]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [22, 23, "misc"], [28, 29, "misc"], [31, 33, "person"], [38, 41, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[22, 23, 0, 3, "artifact", "", false, false], [28, 29, 0, 3, "artifact", "", false, false], [28, 29, 31, 33, "role", "director_of", false, false], [28, 29, 38, 41, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "make", "more", "prominent", "use", "of", "3D", "films", "in", "special", "locations", "to", "impress", "audiences", ",", "with", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "being", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to make more prominent use of 3D films in special locations to impress audiences, with Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) being notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 42], [43, 47], [48, 57], [58, 61], [62, 64], [65, 67], [68, 73], [74, 76], [77, 84], [85, 94], [95, 97], [98, 105], [106, 115], [115, 116], [117, 121], [122, 127], [128, 136], [137, 138], [138, 142], [142, 143], [144, 147], [148, 155], [156, 158], [159, 160], [160, 167], [168, 172], [173, 180], [180, 181], [182, 186], [186, 187], [188, 196], [197, 204], [205, 212], [212, 213], [214, 219], [220, 227], [228, 236], [236, 237]]}
{"doc_key": "ai-train-36", "ner": [[9, 11, "field"], [16, 21, "task"], [23, 24, "task"], [26, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[16, 21, 9, 11, "part-of", "", false, false], [23, 24, 9, 11, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "labelling", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in natural language processing for tasks such as part-of-speech labelling and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 61], [62, 70], [71, 81], [82, 85], [86, 91], [92, 96], [97, 99], [100, 104], [104, 105], [105, 107], [107, 108], [108, 114], [115, 124], [125, 128], [129, 138], [139, 146], [147, 148], [148, 155], [155, 156], [157, 161], [161, 162], [162, 163]]}
{"doc_key": "ai-train-37", "ner": [[2, 4, "product"], [11, 15, "organisation"], [16, 18, "organisation"], [19, 19, "country"], [21, 25, "product"], [30, 33, "researcher"], [37, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[11, 15, 2, 4, "role", "introduces_to_market", true, false], [16, 18, 2, 4, "role", "introduces_to_market", true, false], [16, 18, 19, 19, "physical", "", false, false], [21, 25, 37, 38, "related-to", "sold_to", true, false], [30, 33, 21, 25, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "pallet", "picking", "robot", "was", "launched", "in", "1963", "by", "the", "Fuji", "Yusoki", "Kogyo", "Company", ".", "KUKA", "robotics", "in", "Germany", "and", "a", "programmable", "universal", "assembly", "machine", "invented", "in", "1976", "by", "Victor", "Scheinman", ",", "whose", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first pallet picking robot was launched in 1963 by the Fuji Yusoki Kogyo Company. KUKA robotics in Germany and a programmable universal assembly machine invented in 1976 by Victor Scheinman, whose design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 16], [17, 24], [25, 30], [31, 34], [35, 43], [44, 46], [47, 51], [52, 54], [55, 58], [59, 63], [64, 70], [71, 76], [77, 84], [84, 85], [86, 90], [91, 99], [100, 102], [103, 110], [111, 114], [115, 116], [117, 129], [130, 139], [140, 148], [149, 156], [157, 165], [166, 168], [169, 173], [174, 176], [177, 183], [184, 193], [193, 194], [195, 200], [201, 207], [208, 211], [212, 216], [217, 219], [220, 229], [229, 230]]}
{"doc_key": "ai-train-38", "ner": [[10, 10, "conference"], [5, 6, "researcher"], [21, 22, "field"], [35, 36, "researcher"], [40, 41, "researcher"], [54, 55, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 10, 10, "role", "president_of", false, false], [5, 6, 35, 36, "role", "colleagues", false, false], [21, 22, 54, 55, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "when", "Hayes", "was", "president", "of", "the", "AAAI", ",", "he", "launched", "a", "series", "of", "attacks", "on", "critics", "of", "artificial", "intelligence", ",", "mostly", "in", "an", "ironic", "light", ",", "and", "(", "with", "his", "colleague", "Kenneth", "Ford", ")", "invented", "the", "Simon", "Newcomb", "Prize", ",", "awarded", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "artificial", "intelligence", "."], "sentence-detokenized": "In the mid-1990s, when Hayes was president of the AAAI, he launched a series of attacks on critics of artificial intelligence, mostly in an ironic light, and (with his colleague Kenneth Ford) invented the Simon Newcomb Prize, awarded for the most ridiculous argument refuting the possibility of artificial intelligence.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 22], [23, 28], [29, 32], [33, 42], [43, 45], [46, 49], [50, 54], [54, 55], [56, 58], [59, 67], [68, 69], [70, 76], [77, 79], [80, 87], [88, 90], [91, 98], [99, 101], [102, 112], [113, 125], [125, 126], [127, 133], [134, 136], [137, 139], [140, 146], [147, 152], [152, 153], [154, 157], [158, 159], [159, 163], [164, 167], [168, 177], [178, 185], [186, 190], [190, 191], [192, 200], [201, 204], [205, 210], [211, 218], [219, 224], [224, 225], [226, 233], [234, 237], [238, 241], [242, 246], [247, 257], [258, 266], [267, 275], [276, 279], [280, 291], [292, 294], [295, 305], [306, 318], [318, 319]]}
{"doc_key": "ai-train-39", "ner": [[10, 12, "algorithm"], [33, 33, "algorithm"], [43, 45, "algorithm"], [48, 51, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[10, 12, 33, 33, "named", "same", false, false], [43, 45, 10, 12, "type-of", "", false, false], [48, 51, 10, 12, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "of", "math\\alpha/", "math", "can", "be", "found", "using", "a", "linefinding", "algorithm", ",", "i.e.", "the", "size", "of", "math\\alpha/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "typically", "using", "a", "linefinding", "interval", "math", "0\\", "alpha", "1", "/", "math", "or", "a", "backward", "linefinding", "interval", "such", "as", "the", "Armijo", "linefinding", "interval", "."], "sentence-detokenized": "The optimal value of math\\alpha/math can be found using a linefinding algorithm, i.e. the size of math\\alpha/math is determined by finding the value that minimizes S, typically using a linefinding interval math0\\alpha 1/math or a backward linefinding interval such as the Armijo linefinding interval.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 20], [21, 32], [32, 36], [37, 40], [41, 43], [44, 49], [50, 55], [56, 57], [58, 69], [70, 79], [79, 80], [81, 85], [86, 89], [90, 94], [95, 97], [98, 109], [109, 113], [114, 116], [117, 127], [128, 130], [131, 138], [139, 142], [143, 148], [149, 153], [154, 163], [164, 165], [165, 166], [167, 176], [177, 182], [183, 184], [185, 196], [197, 205], [206, 210], [210, 212], [212, 217], [218, 219], [219, 220], [220, 224], [225, 227], [228, 229], [230, 238], [239, 250], [251, 259], [260, 264], [265, 267], [268, 271], [272, 278], [279, 290], [291, 299], [299, 300]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 10, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "Breadth", "-", "first", "search", "and", "Depth", "-", "first", "search", "methods", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "embody", "a", "lot", "of", "technical", "knowledge", ",", "but", "do", "not", "shed", "much", "light", "on", "the", "mental", "processes", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses Breadth-first search and Depth-first search methods, but ultimately concludes that the results represent expert systems that embody a lot of technical knowledge, but do not shed much light on the mental processes people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 64], [64, 65], [66, 69], [70, 80], [81, 90], [91, 95], [96, 99], [100, 107], [108, 117], [118, 124], [125, 132], [133, 137], [138, 144], [145, 146], [147, 150], [151, 153], [154, 163], [164, 173], [173, 174], [175, 178], [179, 181], [182, 185], [186, 190], [191, 195], [196, 201], [202, 204], [205, 208], [209, 215], [216, 225], [226, 232], [233, 236], [237, 239], [240, 245], [246, 250], [251, 258], [258, 259]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deal", "with", "how", "speech", "language", "can", "be", "understood", "or", "created", "by", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis deal with how speech language can be understood or created by computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 44], [45, 49], [50, 53], [54, 60], [61, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 98], [99, 101], [102, 111], [111, 112]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"], [34, 36, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^", "{", "*}", "/", "math", "is", "usually", "estimated", "using", "the", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "the", "Maximum", "A", "Posteriori", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "MAP", "}", "/", "math", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta ^ {*} / math is usually estimated using the Maximum Likelihood (math\\ theta ^ {*} =\\ theta ^ {ML} / math) or the Maximum A Posteriori (math\\ theta ^ {*} =\\ theta ^ {MAP} / math) procedure.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [17, 18], [19, 20], [20, 22], [23, 24], [25, 29], [30, 32], [33, 40], [41, 50], [51, 56], [57, 60], [61, 68], [69, 79], [80, 81], [81, 85], [85, 86], [87, 92], [93, 94], [95, 96], [96, 98], [99, 101], [102, 107], [108, 109], [110, 111], [111, 113], [113, 114], [115, 116], [117, 121], [121, 122], [123, 125], [126, 129], [130, 137], [138, 139], [140, 150], [151, 152], [152, 156], [156, 157], [158, 163], [164, 165], [166, 167], [167, 169], [170, 172], [173, 178], [179, 180], [181, 182], [182, 185], [185, 186], [187, 188], [189, 193], [193, 194], [195, 204], [204, 205]]}
{"doc_key": "ai-train-43", "ner": [[5, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "common", "languages", "use", "an", "open", "-", "source", "eSpeak", "synthesizer", "for", "their", "speech", ",", "which", "produces", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less common languages use an open-source eSpeak synthesizer for their speech, which produces a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 26], [27, 30], [31, 33], [34, 38], [38, 39], [39, 45], [46, 52], [53, 64], [65, 68], [69, 74], [75, 81], [81, 82], [83, 88], [89, 97], [98, 99], [100, 107], [107, 108], [109, 116], [117, 122], [123, 127], [128, 131], [132, 134], [135, 144], [145, 147], [148, 158], [158, 159]]}
{"doc_key": "ai-train-44", "ner": [[1, 1, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 35, 36, "compare", "", false, false], [1, 1, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "mainly", "used", "by", "statisticians", "and", "other", "practitioners", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "it", "can", "also", "act", "as", "a", "general", "matrix", "computing", "toolkit", "with", "performance", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is mainly used by statisticians and other practitioners who need an environment for statistical computing and software development, it can also act as a general matrix computing toolkit with performance comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 20], [21, 25], [26, 28], [29, 42], [43, 46], [47, 52], [53, 66], [67, 70], [71, 75], [76, 78], [79, 90], [91, 94], [95, 106], [107, 116], [117, 120], [121, 129], [130, 141], [141, 142], [143, 145], [146, 149], [150, 154], [155, 158], [159, 161], [162, 163], [164, 171], [172, 178], [179, 188], [189, 196], [197, 201], [202, 213], [214, 224], [225, 227], [228, 231], [232, 238], [239, 241], [242, 248], [248, 249]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [8, 11, "misc"], [12, 14, "researcher"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 0, 12, 14, "origin", "", false, false], [8, 11, 12, 14, "named", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["Heterodynamics", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "mixing", "two", "frequencies", "."], "sentence-detokenized": "Heterodynamics is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden that creates new frequencies by mixing two frequencies.", "token2charspan": [[0, 14], [15, 17], [18, 19], [20, 26], [27, 37], [38, 47], [48, 56], [57, 59], [60, 68], [69, 77], [78, 81], [82, 90], [91, 99], [100, 109], [110, 114], [115, 122], [123, 126], [127, 138], [139, 141], [142, 148], [149, 152], [153, 164], [164, 165]]}
{"doc_key": "ai-train-46", "ner": [[14, 16, "person"], [18, 18, "misc"], [22, 24, "organisation"], [27, 27, "organisation"], [29, 31, "misc"], [32, 34, "person"], [36, 36, "organisation"], [38, 40, "misc"], [42, 43, "person"], [45, 46, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[14, 16, 18, 18, "role", "actor_in", false, false], [18, 18, 22, 24, "artifact", "", false, false], [29, 31, 27, 27, "artifact", "", false, false], [32, 34, 29, 31, "role", "actor_in", false, false], [38, 40, 36, 36, "artifact", "", false, false], [42, 43, 38, 40, "role", "actor_in", false, false], [45, 46, 38, 40, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Several", "other", "films", "that", "helped", "put", "3D", "back", "on", "the", "map", "this", "month", "were", "John", "Wayne", "'s", "feature", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Several other films that helped put 3D back on the map this month were John Wayne's feature Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 7], [8, 13], [14, 19], [20, 24], [25, 31], [32, 35], [36, 38], [39, 43], [44, 46], [47, 50], [51, 54], [55, 59], [60, 65], [66, 70], [71, 75], [76, 81], [81, 83], [84, 91], [92, 97], [98, 99], [99, 110], [111, 113], [114, 120], [121, 125], [125, 126], [126, 127], [127, 128], [129, 137], [137, 139], [140, 144], [145, 150], [151, 159], [160, 164], [165, 169], [170, 178], [179, 182], [183, 192], [192, 194], [195, 200], [201, 205], [206, 210], [211, 215], [216, 220], [221, 227], [228, 231], [232, 237], [238, 243], [243, 244]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [9, 9, "organisation"]], "ner_mapping_to_source": [0, 3], "relations": [[0, 0, 9, 9, "artifact", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["DeepFace", "is", "a", "facial", "recognition", "system", "created", "by", "the", "Facebook", "research", "team", "."], "sentence-detokenized": "DeepFace is a facial recognition system created by the Facebook research team.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 32], [33, 39], [40, 47], [48, 50], [51, 54], [55, 63], [64, 72], [73, 77], [77, 78]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [15, 16, "field"], [25, 28, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [25, 28, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "Processing", "is", "a", "joint", "research", "theme", "of", "SIGGRAPH", ",", "the", "premier", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "theme", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometry Processing is a joint research theme of SIGGRAPH, the premier academic conference on computer graphics, and the main theme of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 30], [31, 39], [40, 45], [46, 48], [49, 57], [57, 58], [59, 62], [63, 70], [71, 79], [80, 90], [91, 93], [94, 102], [103, 111], [111, 112], [113, 116], [117, 120], [121, 125], [126, 131], [132, 134], [135, 138], [139, 145], [146, 155], [156, 158], [159, 167], [168, 178], [178, 179]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [18, 22, "algorithm"], [26, 26, "algorithm"], [25, 29, "algorithm"], [32, 34, "algorithm"], [36, 36, "algorithm"], [14, 14, "misc"], [41, 43, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 8, 9], "relations": [[26, 26, 14, 14, "general-affiliation", "", false, false], [25, 29, 26, 26, "named", "", false, false], [32, 34, 14, 14, "general-affiliation", "", false, false], [36, 36, 32, 34, "named", "", false, false]], "relations_mapping_to_source": [2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", ",", "using", "pre-processing", "methods", "such", "as", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", ",", "followed", "by", "k", "-", "NN", "clustering", "on", "feature", "vectors", "in", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step, using pre-processing methods such as principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA), followed by k -NN clustering on feature vectors in reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [80, 81], [82, 87], [88, 102], [103, 110], [111, 115], [116, 118], [119, 128], [129, 138], [139, 147], [148, 149], [149, 152], [152, 153], [153, 154], [155, 161], [162, 174], [175, 183], [184, 185], [185, 188], [188, 189], [190, 192], [193, 202], [203, 214], [215, 223], [224, 225], [225, 228], [228, 229], [229, 230], [231, 239], [240, 242], [243, 244], [245, 246], [246, 248], [249, 259], [260, 262], [263, 270], [271, 278], [279, 281], [282, 289], [290, 301], [302, 307], [307, 308]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "at", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel at machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [36, 38, "algorithm"], [39, 40, "researcher"], [42, 44, "researcher"], [46, 52, "misc"], [54, 63, "conference"], [59, 65, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [46, 52, 36, 38, "topic", "", false, false], [46, 52, 39, 40, "artifact", "", false, false], [46, 52, 42, 44, "artifact", "", false, false], [46, 52, 54, 63, "temporal", "", false, false], [59, 65, 54, 63, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pp", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histogram", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pp", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pp 1: 15-33, 2000 others use local features such as histogram oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pp 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 125], [126, 127], [127, 128], [129, 131], [131, 132], [132, 134], [134, 135], [136, 140], [141, 147], [148, 151], [152, 157], [158, 166], [167, 171], [172, 174], [175, 184], [185, 193], [194, 203], [204, 206], [207, 212], [212, 213], [214, 215], [215, 216], [217, 223], [223, 224], [225, 235], [236, 238], [239, 247], [248, 257], [258, 261], [262, 267], [268, 277], [277, 278], [279, 283], [284, 292], [293, 300], [301, 311], [312, 314], [315, 323], [324, 330], [331, 334], [335, 342], [343, 354], [355, 356], [356, 360], [360, 361], [361, 362], [363, 365], [366, 367], [367, 368], [369, 376], [376, 377], [378, 382], [383, 394], [394, 395]]}
{"doc_key": "ai-train-52", "ner": [[0, 2, "algorithm"], [7, 9, "algorithm"], [13, 13, "task"], [15, 16, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 7, 9, "type-of", "", false, false], [13, 13, 0, 2, "usage", "", true, false], [13, 13, 15, 16, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "auto", "-encoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "functions", "through", "unsupervised", "learning", "."], "sentence-detokenized": "An auto-encoder is a type of artificial neural network used to learn functions through unsupervised learning.", "token2charspan": [[0, 2], [3, 7], [7, 15], [16, 18], [19, 20], [21, 25], [26, 28], [29, 39], [40, 46], [47, 54], [55, 59], [60, 62], [63, 68], [69, 78], [79, 86], [87, 99], [100, 108], [108, 109]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [3, 3, "organisation"], [9, 10, "field"], [12, 13, "field"], [23, 24, "organisation"], [20, 28, "organisation"], [32, 33, "field"], [35, 36, "field"], [41, 41, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 2, 3, 3, "role", "fellow_of", false, false], [0, 2, 9, 10, "related-to", "contributes_to", false, false], [0, 2, 12, 13, "related-to", "contributes_to", false, false], [0, 2, 23, 24, "role", "fellow_of", false, false], [0, 2, 32, 33, "related-to", "contributes_to", false, false], [0, 2, 35, 36, "related-to", "contributes_to", false, false], [20, 28, 23, 24, "named", "", false, false], [41, 41, 23, 24, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "an", "IEEE", "Fellow", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", ",", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "pattern", "recognition", ",", "image", "processing", "and", "service", "to", "the", "IAPR", "."], "sentence-detokenized": "Haralick is an IEEE Fellow for his contributions to computer vision and image processing, and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions to pattern recognition, image processing and service to the IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 14], [15, 19], [20, 26], [27, 30], [31, 34], [35, 48], [49, 51], [52, 60], [61, 67], [68, 71], [72, 77], [78, 88], [88, 89], [90, 93], [94, 95], [96, 102], [103, 105], [106, 109], [110, 123], [124, 135], [136, 139], [140, 147], [148, 159], [160, 161], [161, 165], [165, 166], [167, 170], [171, 174], [175, 188], [189, 191], [192, 199], [200, 211], [211, 212], [213, 218], [219, 229], [230, 233], [234, 241], [242, 244], [245, 248], [249, 253], [253, 254]]}
{"doc_key": "ai-train-54", "ner": [[4, 5, "task"], [29, 31, "algorithm"], [10, 11, "researcher"], [12, 14, "organisation"], [16, 17, "researcher"], [18, 22, "university"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6], "relations": [[4, 5, 29, 31, "usage", "", false, false], [29, 31, 10, 11, "origin", "", true, false], [29, 31, 16, 17, "origin", "", true, false], [10, 11, 12, 14, "physical", "", false, false], [10, 11, 12, 14, "role", "", false, false], [16, 17, 18, 22, "physical", "", false, false], [16, 17, 18, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 4, 5, 6, 7], "sentence": ["The", "first", "attempts", "at", "direct", "ASRs", "were", "in", "2014", "when", "Alex", "Graves", "from", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "from", "the", "University", "of", "Toronto", "introduced", "CTC", "-", "based", "systems", "(", "Connectionist", "Temporal", "Classification", ")", "."], "sentence-detokenized": "The first attempts at direct ASRs were in 2014 when Alex Graves from Google DeepMind and Navdeep Jaitly from the University of Toronto introduced CTC-based systems (Connectionist Temporal Classification).", "token2charspan": [[0, 3], [4, 9], [10, 18], [19, 21], [22, 28], [29, 33], [34, 38], [39, 41], [42, 46], [47, 51], [52, 56], [57, 63], [64, 68], [69, 75], [76, 84], [85, 88], [89, 96], [97, 103], [104, 108], [109, 112], [113, 123], [124, 126], [127, 134], [135, 145], [146, 149], [149, 150], [150, 155], [156, 163], [164, 165], [165, 178], [179, 187], [188, 202], [202, 203], [203, 204]]}
{"doc_key": "ai-train-55", "ner": [[0, 8, "algorithm"], [12, 12, "algorithm"], [11, 14, "algorithm"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[11, 14, 12, 12, "named", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["Linear", "-", "Fragmented", "Programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "Linear", "Programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-Fragmented Programming (LFP) is a generalisation of Linear Programming (LP).", "token2charspan": [[0, 6], [6, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 14, "misc"], [15, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 14, "win-defeat", "", false, false], [8, 14, 15, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "won", "several", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Machine", "Learning", "Conference", "in", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has won several awards, including two Test-of-Time awards at the International Machine Learning Conference in 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 16], [17, 24], [25, 31], [31, 32], [33, 42], [43, 46], [47, 51], [51, 52], [52, 54], [54, 55], [55, 59], [60, 66], [67, 69], [70, 73], [74, 87], [88, 95], [96, 104], [105, 115], [116, 118], [119, 123], [124, 127], [128, 132], [132, 133]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [24, 25, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "deploy", "the", "neural", "network", "under", "development", "as", "inherent", "components", "of", "these", "frameworks", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to deploy the neural network under development as inherent components of these frameworks.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 128], [129, 132], [133, 139], [140, 147], [148, 153], [154, 165], [166, 168], [169, 177], [178, 188], [189, 191], [192, 197], [198, 208], [208, 209]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "figures", ")", "between", "the", "two", "sentences", ",", "the", "translation", "candidate", "and", "the", "reference", "translation", "path", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see figures) between the two sentences, the translation candidate and the reference translation path.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 73], [74, 79], [80, 87], [88, 90], [91, 100], [101, 102], [102, 105], [106, 113], [113, 114], [115, 122], [123, 126], [127, 130], [131, 140], [140, 141], [142, 145], [146, 157], [158, 167], [168, 171], [172, 175], [176, 185], [186, 197], [198, 202], [202, 203]]}
{"doc_key": "ai-train-59", "ner": [[7, 11, "conference"], [21, 21, "task"], [23, 24, "task"], [27, 27, "metrics"], [29, 35, "metrics"], [42, 43, "conference"], [40, 45, "conference"], [48, 48, "location"], [50, 50, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[7, 11, 21, 21, "related-to", "subject_at", false, false], [7, 11, 23, 24, "related-to", "subject_at", false, false], [27, 27, 7, 11, "temporal", "", false, false], [29, 35, 27, 27, "named", "", true, false], [40, 45, 42, 43, "named", "", false, false], [48, 48, 50, 50, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "the", "annual", "NIST", "Document", "Understanding", "Conferences", ",", "where", "research", "teams", "present", "their", "systems", "for", "both", "summarization", "and", "translation", "tasks", ",", "is", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used at the annual NIST Document Understanding Conferences, where research teams present their systems for both summarization and translation tasks, is ROUGE (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 30], [31, 37], [38, 42], [43, 51], [52, 65], [66, 77], [77, 78], [79, 84], [85, 93], [94, 99], [100, 107], [108, 113], [114, 121], [122, 125], [126, 130], [131, 144], [145, 148], [149, 160], [161, 166], [166, 167], [168, 170], [171, 176], [177, 178], [178, 184], [184, 185], [185, 193], [194, 204], [205, 208], [209, 216], [217, 227], [227, 228], [229, 231], [232, 240], [241, 243], [244, 250], [251, 262], [263, 273], [274, 281], [282, 283], [283, 287], [287, 288], [288, 289], [290, 298], [298, 299], [300, 306], [306, 307], [308, 316], [317, 318], [319, 323], [323, 324]]}
{"doc_key": "ai-train-60", "ner": [[5, 6, "programlang"], [4, 4, "product"], [10, 11, "programlang"], [14, 14, "product"], [20, 20, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 10, 11, "type-of", "", false, false], [5, 6, 20, 20, "named", "", false, false], [4, 4, 10, 11, "part-of", "", false, false], [4, 4, 14, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", "to", "run", "JShell", "in", "Java", "(", "at", "least", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation to run JShell in Java (at least Java 9): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [20, 22], [23, 26], [27, 33], [34, 36], [37, 41], [42, 43], [43, 45], [46, 51], [52, 56], [57, 58], [58, 59], [59, 60], [61, 71], [72, 82], [83, 84], [85, 104], [105, 109], [110, 111], [112, 116]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "scale", "is", "based", "on", "the", "BLEU", "scale", "but", "has", "been", "modified", "."], "sentence-detokenized": "The NIST scale is based on the BLEU scale but has been modified.", "token2charspan": [[0, 3], [4, 8], [9, 14], [15, 17], [18, 23], [24, 26], [27, 30], [31, 35], [36, 41], [42, 45], [46, 49], [50, 54], [55, 63], [63, 64]]}
{"doc_key": "ai-train-62", "ner": [[8, 8, "country"], [14, 14, "university"], [12, 19, "university"], [27, 28, "product"], [33, 38, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[14, 14, 8, 8, "physical", "", false, false], [12, 19, 8, 8, "physical", "", false, false], [27, 28, 14, 14, "origin", "", false, false], [27, 28, 12, 19, "origin", "", false, false], [27, 28, 33, 38, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["At", "the", "end", "of", "the", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "\"", "Knowledge", "Graphs", "\"", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "added", "constraint", "that", "edges", "must", "be", "restricted", "to", "a", "finite", "set", "of", "possible", "relations", "to", "facilitate", "the", "use", "of", "algebra", "."], "sentence-detokenized": "At the end of the 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called \"Knowledge Graphs\", which are semantic networks, but with the added constraint that edges must be restricted to a finite set of possible relations to facilitate the use of algebra.", "token2charspan": [[0, 2], [3, 6], [7, 10], [11, 13], [14, 17], [18, 23], [23, 24], [25, 28], [29, 34], [35, 47], [47, 48], [49, 52], [53, 63], [64, 66], [67, 76], [77, 80], [81, 84], [85, 95], [96, 98], [99, 105], [105, 106], [107, 114], [115, 122], [123, 124], [125, 132], [133, 139], [140, 141], [141, 150], [151, 157], [157, 158], [158, 159], [160, 165], [166, 169], [170, 178], [179, 187], [187, 188], [189, 192], [193, 197], [198, 201], [202, 207], [208, 218], [219, 223], [224, 229], [230, 234], [235, 237], [238, 248], [249, 251], [252, 253], [254, 260], [261, 264], [265, 267], [268, 276], [277, 286], [287, 289], [290, 300], [301, 304], [305, 308], [309, 311], [312, 319], [319, 320]]}
{"doc_key": "ai-train-63", "ner": [[0, 1, "product"], [16, 19, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 16, 19, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "word", "processing", "program", ",", "but", "they", "are", "also", "available", "as", "a", "standalone", "application", "that", "can", "be", "activated", "from", "within", "programs", "working", "with", "the", "text", "to", "be", "edited", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program, such as a word processing program, but they are also available as a standalone application that can be activated from within programs working with the text to be edited.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [76, 77], [78, 82], [83, 85], [86, 87], [88, 92], [93, 103], [104, 111], [111, 112], [113, 116], [117, 121], [122, 125], [126, 130], [131, 140], [141, 143], [144, 145], [146, 156], [157, 168], [169, 173], [174, 177], [178, 180], [181, 190], [191, 195], [196, 202], [203, 211], [212, 219], [220, 224], [225, 228], [229, 233], [234, 236], [237, 239], [240, 246], [246, 247]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 21, "conference"], [25, 27, "organisation"], [33, 35, "conference"], [37, 39, "conference"], [42, 44, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "an", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence, and the Cognitive Science Society, and an editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [137, 138], [139, 142], [143, 146], [147, 156], [157, 164], [165, 172], [172, 173], [174, 177], [178, 180], [181, 187], [188, 190], [191, 193], [194, 203], [204, 213], [213, 214], [215, 217], [218, 226], [227, 235], [235, 236], [237, 240], [241, 243], [244, 251], [252, 260], [260, 261]]}
{"doc_key": "ai-train-65", "ner": [[1, 2, "algorithm"], [0, 4, "algorithm"], [10, 11, "task"], [17, 18, "researcher"], [19, 21, "university"], [23, 24, "researcher"], [25, 29, "organisation"], [31, 33, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 2, 10, 11, "type-of", "", false, false], [1, 2, 17, 18, "origin", "", false, false], [1, 2, 23, 24, "origin", "", false, false], [0, 4, 1, 2, "named", "", false, false], [17, 18, 19, 21, "physical", "", false, false], [17, 18, 19, 21, "role", "", false, false], [23, 24, 25, 29, "role", "", false, false], [31, 33, 25, 29, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "was", "first", "developed", "by", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear Predictive Coding (LPC), a form of speech coding, was first developed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 60], [61, 66], [67, 76], [77, 79], [80, 88], [89, 96], [97, 99], [100, 106], [107, 117], [118, 121], [122, 127], [128, 133], [134, 136], [137, 143], [144, 153], [154, 157], [158, 167], [168, 169], [169, 172], [172, 173], [174, 176], [177, 181], [181, 182]]}
{"doc_key": "ai-train-66", "ner": [[61, 63, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "additionally", "ergodic", ",", "then", "all", "sample", "paths", "will", "show", "the", "same", "time", "-", "average", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=", "\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "terms", "of", "the", "mean", "squared", "error", "."], "sentence-detokenized": "If the signal is additionally ergodic, then all sample paths will show the same time-average and thus mathR _ x ^ {n / T _ 0} (\\ tau) = \\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in terms of the mean squared error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 29], [30, 37], [37, 38], [39, 43], [44, 47], [48, 54], [55, 60], [61, 65], [66, 70], [71, 74], [75, 79], [80, 84], [84, 85], [85, 92], [93, 96], [97, 101], [102, 107], [108, 109], [110, 111], [112, 113], [114, 115], [115, 116], [117, 118], [119, 120], [121, 122], [123, 124], [124, 125], [126, 127], [127, 128], [129, 132], [132, 133], [134, 135], [136, 137], [138, 145], [146, 147], [147, 148], [148, 149], [150, 151], [152, 153], [154, 155], [156, 157], [157, 158], [159, 160], [161, 162], [163, 164], [165, 166], [166, 167], [168, 169], [169, 170], [171, 174], [174, 175], [176, 177], [178, 182], [183, 185], [186, 191], [192, 194], [195, 198], [199, 203], [204, 211], [212, 217], [217, 218]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [18, 22, "algorithm"], [26, 26, "algorithm"], [25, 29, "algorithm"], [32, 32, "algorithm"], [33, 36, "algorithm"], [39, 41, "algorithm"], [39, 43, "algorithm"], [14, 14, "misc"], [57, 59, "algorithm"], [49, 52, "misc"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[26, 26, 14, 14, "related-to", "", false, false], [25, 29, 26, 26, "named", "", false, false], [32, 32, 14, 14, "related-to", "", false, false], [33, 36, 32, 32, "named", "", false, false], [39, 41, 14, 14, "related-to", "", false, false], [39, 43, 39, 41, "named", "", false, false], [57, 59, 49, 52, "related-to", "", true, false]], "relations_mapping_to_source": [2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", ",", "using", "pre-processing", "methods", "such", "as", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorization", "(", "NMF", ")", ",", "followed", "by", "clustering", "in", "feature", "vectors", "in", "dimensionality", "reduced", "space", "using", "K", "-", "NN", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step, using pre-processing methods such as principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorization (NMF), followed by clustering in feature vectors in dimensionality reduced space using K-NN.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [80, 81], [82, 87], [88, 102], [103, 110], [111, 115], [116, 118], [119, 128], [129, 138], [139, 147], [148, 149], [149, 152], [152, 153], [153, 154], [155, 161], [162, 174], [175, 183], [184, 185], [185, 188], [188, 189], [189, 190], [191, 200], [201, 212], [213, 221], [222, 223], [223, 226], [226, 227], [228, 230], [231, 243], [244, 250], [251, 264], [265, 266], [266, 269], [269, 270], [270, 271], [272, 280], [281, 283], [284, 294], [295, 297], [298, 305], [306, 313], [314, 316], [317, 331], [332, 339], [340, 345], [346, 351], [352, 353], [353, 354], [354, 356], [356, 357]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [14, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[14, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [14, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [14, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [14, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [10, 12, "task"], [29, 30, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 10, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognizing", "named", "entities", "in", "the", "text", "is", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "the", "text", "is", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognizing named entities in the text is Named Entity Recognition, while the task of determining the identity of named entities mentioned in the text is Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 59], [60, 66], [67, 78], [78, 79], [80, 85], [86, 89], [90, 94], [95, 97], [98, 109], [110, 113], [114, 122], [123, 125], [126, 131], [132, 140], [141, 150], [151, 153], [154, 157], [158, 162], [163, 165], [166, 172], [173, 180], [180, 181]]}
{"doc_key": "ai-train-70", "ner": [[3, 3, "algorithm"], [27, 28, "algorithm"]], "ner_mapping_to_source": [0, 2], "relations": [[3, 3, 27, 28, "part-of", "", true, false]], "relations_mapping_to_source": [0], "sentence": ["Initially", ",", "the", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "included", ",", "but", "since", "version", "0.8.0", "they", "have", "been", "published", "separately", "in", "the", "R", "package", "sigmoid", "to", "allow", "more", "general", "use", "."], "sentence-detokenized": "Initially, the sigmoid functions and derivatives used in the package were included, but since version 0.8.0 they have been published separately in the R package sigmoid to allow more general use.", "token2charspan": [[0, 9], [9, 10], [11, 14], [15, 22], [23, 32], [33, 36], [37, 48], [49, 53], [54, 56], [57, 60], [61, 68], [69, 73], [74, 82], [82, 83], [84, 87], [88, 93], [94, 101], [102, 107], [108, 112], [113, 117], [118, 122], [123, 132], [133, 143], [144, 146], [147, 150], [151, 152], [153, 160], [161, 168], [169, 171], [172, 177], [178, 182], [183, 190], [191, 194], [194, 195]]}
{"doc_key": "ai-train-71", "ner": [[1, 1, "programlang"], [19, 23, "organisation"], [25, 27, "organisation"], [28, 28, "location"], [30, 30, "location"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 15, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 1, 7, 8, "artifact", "", true, false], [1, 1, 10, 11, "artifact", "", true, false], [1, 1, 13, 15, "artifact", "", true, false], [25, 27, 19, 23, "named", "", false, false], [25, 27, 28, 28, "physical", "", false, false], [28, 28, 30, 30, "physical", "", false, false], [7, 8, 19, 23, "role", "", false, false], [10, 11, 19, 23, "role", "", false, false], [13, 15, 19, 23, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "at", "the", "research", "firm", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", "in", "Cambridge", ",", "Massachusetts", "."], "sentence-detokenized": "The logo was created in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert at the research firm Bolt, Beranek and Newman (BBN) in Cambridge, Massachusetts.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 37], [38, 46], [46, 47], [48, 55], [56, 63], [64, 67], [68, 75], [76, 82], [83, 85], [86, 89], [90, 98], [99, 103], [104, 108], [108, 109], [110, 117], [118, 121], [122, 128], [129, 130], [130, 133], [133, 134], [135, 137], [138, 147], [147, 148], [149, 162], [162, 163]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [17, 18, "field"], [23, 26, "algorithm"], [27, 28, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 17, 18, "compare", "", false, false], [23, 26, 17, 18, "part-of", "", false, false], [27, 28, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "a", "reinforcement", "learning", "paradigm", "and", "can", "be", "contrasted", "with", "conventional", "deep", "learning", "techniques", "that", "use", "a", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of a reinforcement learning paradigm and can be contrasted with conventional deep learning techniques that use a gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 44], [45, 58], [59, 67], [68, 76], [77, 80], [81, 84], [85, 87], [88, 98], [99, 103], [104, 116], [117, 121], [122, 130], [131, 141], [142, 146], [147, 150], [151, 152], [153, 161], [162, 169], [170, 172], [173, 174], [175, 181], [182, 189], [190, 194], [195, 196], [197, 202], [203, 211], [211, 212]]}
{"doc_key": "ai-train-73", "ner": [[3, 4, "algorithm"], [51, 55, "metrics"]], "ner_mapping_to_source": [0, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "we", "use", "least", "squares", "to", "fit", "the", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "shape", "function", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264", "n/sub", ")", ",", "then", "we", "can", "estimate", "the", "fit", "using", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use least squares to fit the hyperplane \u0177 = a + \u03b2 supT/sup x to the shape function data (x sub i/sub, y sub i/sub) sub 1 \u2264 i \u2264 n/sub), then we can estimate the fit using mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 15], [16, 23], [24, 26], [27, 30], [31, 34], [35, 45], [46, 47], [48, 49], [50, 51], [52, 53], [54, 55], [56, 60], [60, 61], [61, 64], [65, 66], [67, 69], [70, 73], [74, 79], [80, 88], [89, 93], [94, 95], [95, 96], [97, 100], [101, 102], [102, 103], [103, 106], [106, 107], [108, 109], [110, 113], [114, 115], [115, 116], [116, 119], [119, 120], [121, 124], [125, 126], [127, 128], [129, 130], [131, 132], [133, 138], [138, 139], [139, 140], [141, 145], [146, 148], [149, 152], [153, 161], [162, 165], [166, 169], [170, 175], [176, 180], [181, 188], [189, 194], [195, 196], [196, 199], [199, 200], [200, 201]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 48, "country"], [32, 50, "country"], [34, 34, "country"], [36, 53, "country"], [39, 55, "country"], [41, 57, "country"], [43, 43, "country"], [45, 47, "country"], [60, 61, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", ",", "the", "Philippines", ",", "Russia", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey, the Philippines, Russia, South Africa, Spain, Taiwan and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 152], [152, 153], [154, 160], [160, 161], [162, 171], [171, 172], [173, 178], [179, 185], [185, 186], [187, 192], [192, 193], [194, 200], [200, 201], [202, 210], [210, 211], [212, 218], [218, 219], [220, 223], [224, 235], [235, 236], [237, 243], [243, 244], [245, 250], [251, 257], [257, 258], [259, 264], [264, 265], [266, 272], [273, 276], [277, 280], [281, 287], [288, 295], [295, 296]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 8, "field"], [16, 16, "organisation"], [12, 25, "university"], [29, 31, "organisation"], [33, 38, "university"], [42, 43, "university"], [45, 46, "university"], [49, 51, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 8, "topic", "", false, false], [3, 3, 16, 16, "origin", "", false, false], [3, 3, 12, 25, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "Electrical", "and", "Computer", "Engineering", "(", "2000", ")", "from", "the", "Universities", "of", "Inria", "and", "Sophia", "Antipolis", ",", "Nice", ",", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in Electrical and Computer Engineering (2000) from the Universities of Inria and Sophia Antipolis, Nice, and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, and visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 69], [70, 82], [83, 85], [86, 91], [92, 95], [96, 102], [103, 112], [112, 113], [114, 118], [118, 119], [120, 123], [124, 127], [128, 132], [133, 142], [143, 152], [153, 155], [156, 163], [164, 173], [174, 184], [184, 185], [186, 191], [192, 195], [196, 201], [202, 211], [211, 212], [213, 216], [217, 225], [226, 235], [236, 238], [239, 246], [247, 257], [257, 258], [259, 263], [264, 274], [275, 278], [279, 282], [283, 293], [294, 296], [297, 304], [304, 305]]}
{"doc_key": "ai-train-76", "ner": [[6, 7, "researcher"], [9, 9, "researcher"], [13, 14, "product"], [15, 18, "country"], [20, 23, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 9, 6, 7, "role", "licensing_patent_to", false, false], [9, 9, 15, 18, "physical", "", false, false], [20, 23, 9, 9, "artifact", "", false, false], [20, 23, 13, 14, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Licensing", "the", "original", "patent", "to", "inventor", "George", "Devol", ",", "Engelberger", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Licensing the original patent to inventor George Devol, Engelberger developed the first industrial robot in the United States, Unimate, in the 1950s.", "token2charspan": [[0, 9], [10, 13], [14, 22], [23, 29], [30, 32], [33, 41], [42, 48], [49, 54], [54, 55], [56, 67], [68, 77], [78, 81], [82, 87], [88, 98], [99, 104], [105, 107], [108, 111], [112, 118], [119, 125], [125, 126], [127, 134], [134, 135], [136, 138], [139, 142], [143, 148], [148, 149]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [9, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[0, 1, "programlang"], [4, 4, "programlang"], [12, 12, "programlang"], [15, 15, "programlang"], [26, 26, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 12, 12, "named", "", false, false], [4, 4, 0, 1, "origin", "descendant_of", false, false], [4, 4, 15, 15, "general-affiliation", "", false, false], [4, 4, 26, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["CLIPS", "language", "descendants", "include", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "rewritten", "in", "Java", ",", "later", "grown", "up", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by", "the"], "sentence-detokenized": "CLIPS language descendants include Jess (the rule-based part of CLIPS rewritten in Java, later grown up in a different direction), JESS was originally inspired by the", "token2charspan": [[0, 5], [6, 14], [15, 26], [27, 34], [35, 39], [40, 41], [41, 44], [45, 49], [49, 50], [50, 55], [56, 60], [61, 63], [64, 69], [70, 79], [80, 82], [83, 87], [87, 88], [89, 94], [95, 100], [101, 103], [104, 106], [107, 108], [109, 118], [119, 128], [128, 129], [129, 130], [131, 135], [136, 139], [140, 150], [151, 159], [160, 162], [163, 166]]}
{"doc_key": "ai-train-79", "ner": [[5, 5, "product"], [10, 12, "product"], [14, 15, "organisation"], [20, 21, "product"], [41, 42, "product"], [44, 46, "product"], [62, 63, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[10, 12, 5, 5, "type-of", "", false, false], [14, 15, 10, 12, "usage", "", false, false], [20, 21, 14, 15, "artifact", "", false, false], [41, 42, 14, 15, "origin", "", true, false], [41, 42, 62, 63, "related-to", "", true, false], [44, 46, 14, 15, "origin", "", true, false], [44, 46, 62, 63, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["He", "also", "created", "flexible", "intelligent", "AGV", "applications", ",", "designing", "the", "Motivity", "control", "system", "that", "RMT", "Robotics", "used", "to", "develop", "its", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", ",", "used", "for", "complex", "pick", "-", "and", "-", "place", "operations", ",", "together", "with", "gantry", "systems", "and", "industrial", "robotic", "grippers", "used", "in", "tier", "-", "one", "automotive", "factories", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "He also created flexible intelligent AGV applications, designing the Motivity control system that RMT Robotics used to develop its ADAM iAGV (Self-Guided Vehicle), used for complex pick-and-place operations, together with gantry systems and industrial robotic grippers used in tier-one automotive factories to move products from process to process in non-linear layouts.", "token2charspan": [[0, 2], [3, 7], [8, 15], [16, 24], [25, 36], [37, 40], [41, 53], [53, 54], [55, 64], [65, 68], [69, 77], [78, 85], [86, 92], [93, 97], [98, 101], [102, 110], [111, 115], [116, 118], [119, 126], [127, 130], [131, 135], [136, 140], [141, 142], [142, 146], [146, 147], [147, 153], [154, 161], [161, 162], [162, 163], [164, 168], [169, 172], [173, 180], [181, 185], [185, 186], [186, 189], [189, 190], [190, 195], [196, 206], [206, 207], [208, 216], [217, 221], [222, 228], [229, 236], [237, 240], [241, 251], [252, 259], [260, 268], [269, 273], [274, 276], [277, 281], [281, 282], [282, 285], [286, 296], [297, 306], [307, 309], [310, 314], [315, 323], [324, 328], [329, 336], [337, 339], [340, 347], [348, 350], [351, 361], [362, 369], [369, 370]]}
{"doc_key": "ai-train-80", "ner": [[5, 7, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Parameters", "\u03b2", "are", "usually", "estimated", "with", "maximum", "likelihood", "."], "sentence-detokenized": "Parameters \u03b2 are usually estimated with maximum likelihood.", "token2charspan": [[0, 10], [11, 12], [13, 16], [17, 24], [25, 34], [35, 39], [40, 47], [48, 58], [58, 59]]}
{"doc_key": "ai-train-81", "ner": [[2, 3, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 11, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 2, 3, "part-of", "", false, false], [8, 8, 2, 3, "part-of", "", false, false], [10, 11, 2, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Measures", "of", "information", "retrieval", "such", "as", "accuracy", "and", "recall", "or", "DCG", "are", "useful", "to", "assess", "the", "quality", "of", "the", "recommendation", "method", "."], "sentence-detokenized": "Measures of information retrieval such as accuracy and recall or DCG are useful to assess the quality of the recommendation method.", "token2charspan": [[0, 8], [9, 11], [12, 23], [24, 33], [34, 38], [39, 41], [42, 50], [51, 54], [55, 61], [62, 64], [65, 68], [69, 72], [73, 79], [80, 82], [83, 89], [90, 93], [94, 101], [102, 104], [105, 108], [109, 123], [124, 130], [130, 131]]}
{"doc_key": "ai-train-82", "ner": [[9, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "there", "are", "hundreds", "of", "industrial", "robots", "running", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "In a typical factory, there are hundreds of industrial robots running on fully automated production lines, with one robot for every ten human workers.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 27], [28, 31], [32, 40], [41, 43], [44, 54], [55, 61], [62, 69], [70, 72], [73, 78], [79, 88], [89, 99], [100, 105], [105, 106], [107, 111], [112, 115], [116, 121], [122, 125], [126, 131], [132, 135], [136, 141], [142, 149], [149, 150]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [30, 31, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7], "relations": [], "relations_mapping_to_source": [], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", ",", "including", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growing", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications, including image segmentation, feature generation, face extraction, motion detection, region growing and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [88, 89], [90, 99], [100, 105], [106, 118], [118, 119], [120, 127], [128, 138], [138, 139], [140, 144], [145, 155], [155, 156], [157, 163], [164, 173], [173, 174], [175, 181], [182, 189], [190, 193], [194, 199], [200, 209], [209, 210]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [13, 17, "field"], [23, 27, "misc"], [28, 34, "conference"], [36, 36, "conference"], [41, 44, "misc"], [45, 50, "conference"], [51, 52, "conference"], [56, 59, "conference"], [55, 61, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 13, 17, "related-to", "contributes_to", false, false], [0, 0, 23, 27, "win-defeat", "", false, false], [0, 0, 41, 44, "win-defeat", "", false, false], [23, 27, 28, 34, "temporal", "", false, false], [36, 36, 28, 34, "named", "", false, false], [41, 44, 45, 50, "temporal", "", false, false], [41, 44, 56, 59, "temporal", "", false, false], [51, 52, 45, 50, "named", "", false, false], [55, 61, 56, 59, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", ",", "and", "has", "won", "the", "best", "paper", "award", "at", "the", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "and", "the", "best", "reviewer", "award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "the", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision, and has won the best paper award at the International Conference on Non-Photorealistic Rendering and Animation (NPAR) 2012 and the best reviewer award at the Asian Conference on Computer Vision ACCV 2012 and the International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [110, 111], [112, 115], [116, 119], [120, 123], [124, 127], [128, 132], [133, 138], [139, 144], [145, 147], [148, 151], [152, 165], [166, 176], [177, 179], [180, 198], [199, 208], [209, 212], [213, 222], [223, 224], [224, 228], [228, 229], [230, 234], [235, 238], [239, 242], [243, 247], [248, 256], [257, 262], [263, 265], [266, 269], [270, 275], [276, 286], [287, 289], [290, 298], [299, 305], [306, 310], [311, 315], [316, 319], [320, 323], [324, 337], [338, 348], [349, 351], [352, 360], [361, 367], [368, 369], [369, 373], [373, 374], [375, 379], [379, 380]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [5, 7, "field"], [9, 10, "field"], [3, 4, "misc"], [14, 14, "researcher"], [17, 19, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 5, 7, "part-of", "", false, false], [0, 0, 9, 10, "part-of", "", false, false], [0, 0, 3, 4, "type-of", "", false, false], [17, 19, 0, 0, "usage", "", false, false], [17, 19, 14, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "is", "an", "ontology", "language", "in", "computer", "science", "and", "artificial", "intelligence", ",", "used", "by", "Doug", "Lenat", "'s", "Cyc", "technology", "project", "."], "sentence-detokenized": "CycL is an ontology language in computer science and artificial intelligence, used by Doug Lenat's Cyc technology project.", "token2charspan": [[0, 4], [5, 7], [8, 10], [11, 19], [20, 28], [29, 31], [32, 40], [41, 48], [49, 52], [53, 63], [64, 76], [76, 77], [78, 82], [83, 85], [86, 90], [91, 96], [96, 98], [99, 102], [103, 113], [114, 121], [121, 122]]}
{"doc_key": "ai-train-86", "ner": [[1, 5, "task"], [6, 8, "metrics"], [15, 20, "metrics"], [23, 28, "metrics"], [54, 57, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 1, 5, "part-of", "", false, false], [15, 20, 6, 8, "named", "", false, false], [23, 28, 6, 8, "named", "", false, false], [54, 57, 6, 8, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "error", "of", "the", "estimates", "or", "the", "inter", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "inter", "-", "sample", "test", "space", "created", "by", "the", "model", "estimated", "from", "a", "particular", "sample", "design", "for", "the", "mean", "of", "the", "squared", "errors", "of", "the", "estimates", "."], "sentence-detokenized": "Also in regression analysis, the mean squared error, often referred to as the mean squared error of the estimates or the inter-sample mean squared error, can refer to the inter-sample test space created by the model estimated from a particular sample design for the mean of the squared errors of the estimates.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 32], [33, 37], [38, 45], [46, 51], [51, 52], [53, 58], [59, 67], [68, 70], [71, 73], [74, 77], [78, 82], [83, 90], [91, 96], [97, 99], [100, 103], [104, 113], [114, 116], [117, 120], [121, 126], [126, 127], [127, 133], [134, 138], [139, 146], [147, 152], [152, 153], [154, 157], [158, 163], [164, 166], [167, 170], [171, 176], [176, 177], [177, 183], [184, 188], [189, 194], [195, 202], [203, 205], [206, 209], [210, 215], [216, 225], [226, 230], [231, 232], [233, 243], [244, 250], [251, 257], [258, 261], [262, 265], [266, 270], [271, 273], [274, 277], [278, 285], [286, 292], [293, 295], [296, 299], [300, 309], [309, 310]]}
{"doc_key": "ai-train-87", "ner": [[9, 11, "algorithm"], [13, 15, "algorithm"], [22, 25, "algorithm"], [38, 39, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[9, 11, 13, 15, "compare", "", false, false], [9, 11, 22, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "terms", "of", "results", ",", "the", "descriptors", "of", "the", "C", "-", "HOG", "and", "R", "-", "HOG", "blocks", "are", "comparable", ",", "with", "the", "C", "-", "HOG", "descriptors", "retaining", "a", "slight", "advantage", "in", "terms", "of", "detection", "error", "rates", "for", "fixed", "FALSE", "positive", "rates", "for", "both", "datasets", "."], "sentence-detokenized": "In terms of results, the descriptors of the C-HOG and R-HOG blocks are comparable, with the C-HOG descriptors retaining a slight advantage in terms of detection error rates for fixed FALSE positive rates for both datasets.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 19], [19, 20], [21, 24], [25, 36], [37, 39], [40, 43], [44, 45], [45, 46], [46, 49], [50, 53], [54, 55], [55, 56], [56, 59], [60, 66], [67, 70], [71, 81], [81, 82], [83, 87], [88, 91], [92, 93], [93, 94], [94, 97], [98, 109], [110, 119], [120, 121], [122, 128], [129, 138], [139, 141], [142, 147], [148, 150], [151, 160], [161, 166], [167, 172], [173, 176], [177, 182], [183, 188], [189, 197], [198, 203], [204, 207], [208, 212], [213, 221], [221, 222]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [17, 17, "algorithm"], [19, 21, "algorithm"], [23, 25, "algorithm"], [27, 28, "misc"], [34, 36, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 27, 28, "usage", "", false, false], [14, 15, 27, 28, "usage", "", false, false], [17, 17, 27, 28, "usage", "", false, false], [19, 21, 27, 28, "usage", "", false, false], [23, 25, 27, 28, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "Fisherface", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neuron", "-", "motivated", "dynamic", "link", "fitting", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using Fisherface, hidden Markov model, multilinear subspace learning using tensor representation, and neuron-motivated dynamic link fitting.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 149], [149, 150], [151, 157], [158, 164], [165, 170], [170, 171], [172, 183], [184, 192], [193, 201], [202, 207], [208, 214], [215, 229], [229, 230], [231, 234], [235, 241], [241, 242], [242, 251], [252, 259], [260, 264], [265, 272], [272, 273]]}
{"doc_key": "ai-train-89", "ner": [[2, 7, "misc"], [18, 21, "location"], [39, 43, "location"], [58, 58, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[18, 21, 2, 7, "temporal", "", false, false], [39, 43, 2, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "of", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "can", "now", "be", "restricted", "from", "screening", "at", "the", "Scotiabank", "Theatre", "in", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "can", "be", "screened", "elsewhere", "(", "such", "as", "at", "the", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "by", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "As of the 2019 Toronto International Film Festival, films can now be restricted from screening at the Scotiabank Theatre in Toronto - one of the festival's main venues - and can be screened elsewhere (such as at the TIFF Bell Lightbox and other local cinemas) if they are distributed by a service such as Netflix.", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 14], [15, 22], [23, 36], [37, 41], [42, 50], [50, 51], [52, 57], [58, 61], [62, 65], [66, 68], [69, 79], [80, 84], [85, 94], [95, 97], [98, 101], [102, 112], [113, 120], [121, 123], [124, 131], [132, 133], [134, 137], [138, 140], [141, 144], [145, 153], [153, 155], [156, 160], [161, 167], [168, 169], [170, 173], [174, 177], [178, 180], [181, 189], [190, 199], [200, 201], [201, 205], [206, 208], [209, 211], [212, 215], [216, 220], [221, 225], [226, 234], [235, 238], [239, 244], [245, 250], [251, 258], [258, 259], [260, 262], [263, 267], [268, 271], [272, 283], [284, 286], [287, 288], [289, 296], [297, 301], [302, 304], [305, 312], [312, 313]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 2, "researcher"], [6, 11, "organisation"], [27, 31, "product"], [43, 45, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 6], "relations": [[0, 0, 6, 11, "related-to", "purchases", false, false], [6, 11, 2, 2, "origin", "founded_by", false, false], [27, 31, 0, 0, "artifact", "", false, false]], "relations_mapping_to_source": [0, 3, 4], "sentence": ["Unimation", "bought", "Victor", "Scheinman", "'s", "company", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "a", "new", "robotic", "arm", "model", ",", "the", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "and", "began", "manufacturing", "it", "using", "Scheinman", "'s", "cutting", "-", "edge", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation bought Victor Scheinman's company Vicarm Inc. in 1977, and with Scheinman's help, the company created a new robotic arm model, the Programmable Universal Machine for Assembly, and began manufacturing it using Scheinman's cutting-edge VAL programming language.", "token2charspan": [[0, 9], [10, 16], [17, 23], [24, 33], [33, 35], [36, 43], [44, 50], [51, 55], [56, 58], [59, 63], [63, 64], [65, 68], [69, 73], [74, 83], [83, 85], [86, 90], [90, 91], [92, 95], [96, 103], [104, 111], [112, 113], [114, 117], [118, 125], [126, 129], [130, 135], [135, 136], [137, 140], [141, 153], [154, 163], [164, 171], [172, 175], [176, 184], [184, 185], [186, 189], [190, 195], [196, 209], [210, 212], [213, 218], [219, 228], [228, 230], [231, 238], [238, 239], [239, 243], [244, 247], [248, 259], [260, 268], [268, 269]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [9, 11, "algorithm"], [12, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 9, 11, "origin", "implementation_of", false, false], [0, 1, 12, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[2, 2, "metrics"], [13, 14, "product"], [20, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[2, 2, 13, 14, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", "according", "to", "Google", "Scholar", ",", "and", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", "in", "2016", ",", "indicating", "that", "the", "paper", "has", "an", "unusually", "high", "impact", "at", "least", "10", "years", "after", "its", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20,000 times according to Google Scholar, and also received the IEEE Signal Processing Society Sustained Impact Award in 2016, indicating that the paper has an unusually high impact at least 10 years after its publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 51], [52, 57], [58, 67], [68, 70], [71, 77], [78, 85], [85, 86], [87, 90], [91, 95], [96, 104], [105, 108], [109, 113], [114, 120], [121, 131], [132, 139], [140, 149], [150, 156], [157, 162], [163, 165], [166, 170], [170, 171], [172, 182], [183, 187], [188, 191], [192, 197], [198, 201], [202, 204], [205, 214], [215, 219], [220, 226], [227, 229], [230, 235], [236, 238], [239, 244], [245, 250], [251, 254], [255, 266], [266, 267]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [26, 27, "product"], [36, 38, "product"], [41, 41, "organisation"], [42, 42, "product"], [46, 50, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 41, 41, "artifact", "", false, false], [26, 27, 0, 1, "related-to", "performs", false, false], [26, 27, 36, 38, "part-of", "", false, false], [41, 41, 46, 50, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "getting", "to", "the", "point", "where", "it", "is", "completely", "indistinguishable", "from", "the", "real", "human", "voice", ".", "In", "2016", ",", "voice", "editing", "and", "generation", "software", "Adobe", "Voco", ",", "a", "prototype", "designed", "as", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", ",", "were", "introduced", "."], "sentence-detokenized": "Speech synthesis is getting to the point where it is completely indistinguishable from the real human voice. In 2016, voice editing and generation software Adobe Voco, a prototype designed as part of the Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google, were introduced.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 27], [28, 30], [31, 34], [35, 40], [41, 46], [47, 49], [50, 52], [53, 63], [64, 81], [82, 86], [87, 90], [91, 95], [96, 101], [102, 107], [107, 108], [109, 111], [112, 116], [116, 117], [118, 123], [124, 131], [132, 135], [136, 146], [147, 155], [156, 161], [162, 166], [166, 167], [168, 169], [170, 179], [180, 188], [189, 191], [192, 196], [197, 199], [200, 203], [204, 209], [210, 218], [219, 224], [224, 225], [226, 229], [230, 238], [239, 246], [246, 247], [248, 249], [250, 259], [260, 264], [265, 271], [271, 272], [273, 277], [278, 288], [288, 289]]}
{"doc_key": "ai-train-94", "ner": [[0, 1, "researcher"], [7, 9, "organisation"], [15, 20, "organisation"], [26, 26, "conference"], [34, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 7, 9, "role", "", false, false], [0, 1, 15, 20, "role", "", false, false], [0, 1, 26, 26, "role", "", false, false], [0, 1, 34, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "Honorary", "Fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "Fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "and", "a", "founding", "member", "of", "AAAI", ",", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an Honorary Fellow of the Neuroscience Research Program, a Fellow of the American Academy of Arts and Sciences and a founding member of AAAI, and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [121, 124], [125, 126], [127, 135], [136, 142], [143, 145], [146, 150], [150, 151], [152, 155], [156, 157], [158, 166], [167, 173], [174, 176], [177, 180], [181, 189], [190, 199], [200, 203], [204, 209], [210, 218], [218, 219]]}
{"doc_key": "ai-train-95", "ner": [[8, 9, "task"], [11, 12, "task"], [16, 17, "task"], [26, 26, "misc"], [27, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 9, 16, 17, "cause-effect", "", false, false], [11, 12, 16, 17, "cause-effect", "", false, false], [27, 28, 16, 17, "topic", "", false, false], [27, 28, 26, 26, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "inspired", "by", "advances", "in", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "into", "speech", "interpretation", "began", ",", "leading", "to", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, inspired by advances in speech recognition and speech synthesis, research into speech interpretation began, leading to the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 22], [23, 25], [26, 34], [35, 37], [38, 44], [45, 56], [57, 60], [61, 67], [68, 77], [77, 78], [79, 87], [88, 92], [93, 99], [100, 114], [115, 120], [120, 121], [122, 129], [130, 132], [133, 136], [137, 148], [149, 151], [152, 155], [156, 162], [163, 172], [173, 180], [180, 181]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [14, 16, "algorithm"], [15, 15, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 4, 8, 9, "role", "", false, false], [14, 16, 3, 4, "origin", "", false, false], [14, 16, 8, 9, "origin", "", false, false], [14, 16, 11, 12, "origin", "", false, false], [15, 15, 14, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisor", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "a", "keep", "gate", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisor J\u00fcrgen Schmidhuber and Fred Cummins introduced a keep gate into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 35], [36, 42], [43, 54], [55, 58], [59, 63], [64, 71], [72, 82], [83, 84], [85, 89], [90, 94], [95, 99], [100, 103], [104, 108], [109, 121], [121, 122]]}
{"doc_key": "ai-train-97", "ner": [[0, 3, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 2], "relations": [[9, 12, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sine", "function", "is", "usually", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sine function is usually defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103], [104, 111]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [19, 24, "conference"], [28, 30, "organisation"], [26, 32, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 19, 24, "role", "", false, false], [9, 10, 28, 30, "role", "", false, false], [26, 32, 28, 30, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "who", "was", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "for", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hays, who was a founding member of both the Association for Computational Linguistics and the International Committee for Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 72], [72, 73], [74, 77], [78, 81], [82, 83], [84, 92], [93, 99], [100, 102], [103, 107], [108, 111], [112, 123], [124, 127], [128, 141], [142, 153], [154, 157], [158, 161], [162, 175], [176, 185], [186, 189], [190, 203], [204, 215], [216, 217], [217, 221], [221, 222], [222, 223]]}
{"doc_key": "ai-train-99", "ner": [[9, 14, "misc"], [7, 19, "misc"], [35, 35, "metrics"], [34, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[34, 38, 35, 35, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "For", "a", "one", "-dimensional", "polynomial", "memory", "-", "based", "(", "or", "memoryless", ")", "DPD", ",", "to", "solve", "for", "the", "coefficients", "of", "the", "digital", "prefetch", "polynomials", "and", "minimize", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "allows", "the", "nonlinear", "products", "of", "the", "digital", "prefetch", "sequence", "to", "be", "captured", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 For a one-dimensional polynomial memory-based (or memoryless) DPD, to solve for the coefficients of the digital prefetch polynomials and minimize the mean squared error (MSE), the distorted output of the nonlinear system must be oversampled at a rate that allows the nonlinear products of the digital prefetch sequence to be captured.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 32], [33, 34], [35, 38], [38, 50], [51, 61], [62, 68], [68, 69], [69, 74], [75, 76], [76, 78], [79, 89], [89, 90], [91, 94], [94, 95], [96, 98], [99, 104], [105, 108], [109, 112], [113, 125], [126, 128], [129, 132], [133, 140], [141, 149], [150, 161], [162, 165], [166, 174], [175, 178], [179, 183], [184, 191], [192, 197], [198, 199], [199, 202], [202, 203], [203, 204], [205, 208], [209, 218], [219, 225], [226, 228], [229, 232], [233, 242], [243, 249], [250, 254], [255, 257], [258, 269], [270, 272], [273, 274], [275, 279], [280, 284], [285, 291], [292, 295], [296, 305], [306, 314], [315, 317], [318, 321], [322, 329], [330, 338], [339, 347], [348, 350], [351, 353], [354, 362], [362, 363]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 9, "location"], [11, 12, "location"], [14, 15, "country"], [18, 18, "location"], [20, 24, "country"], [33, 38, "organisation"], [42, 46, "organisation"], [47, 49, "location"], [58, 59, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 9, 9, "physical", "", false, false], [0, 1, 42, 46, "physical", "", false, false], [0, 1, 58, 59, "role", "", false, false], [9, 9, 11, 12, "physical", "", false, false], [11, 12, 14, 15, "physical", "", false, false], [33, 38, 42, 46, "part-of", "", false, false], [42, 46, 47, 49, "physical", "", false, false], [58, 59, 33, 38, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", "(", "born", "October", "5", ",", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldavian", "SSR", ",", "Soviet", "Union", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "a", "Principal", "Investigator", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", ",", "USA", ",", "and", "the", "head", "of", "the", "Laboratory", "'s", "InfoLab", "group", "."], "sentence-detokenized": "Boris Katz (born October 5, 1947 in Chi\u0219in\u0103u, Moldavian SSR, Soviet Union (now Chi\u0219in\u0103u, Moldova)) is a Principal Investigator (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge, USA, and the head of the Laboratory's InfoLab group.", "token2charspan": [[0, 5], [6, 10], [11, 12], [12, 16], [17, 24], [25, 26], [26, 27], [28, 32], [33, 35], [36, 44], [44, 45], [46, 55], [56, 59], [59, 60], [61, 67], [68, 73], [74, 75], [75, 78], [79, 87], [87, 88], [89, 96], [96, 97], [97, 98], [99, 101], [102, 103], [104, 113], [114, 126], [127, 128], [128, 136], [137, 146], [146, 147], [148, 150], [151, 154], [155, 158], [159, 167], [168, 175], [176, 179], [180, 190], [191, 203], [204, 214], [215, 217], [218, 221], [222, 235], [236, 245], [246, 248], [249, 259], [260, 262], [263, 272], [272, 273], [274, 277], [277, 278], [279, 282], [283, 286], [287, 291], [292, 294], [295, 298], [299, 309], [309, 311], [312, 319], [320, 325], [325, 326]]}
