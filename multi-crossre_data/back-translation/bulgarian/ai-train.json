{"doc_key": "ai-train-1", "ner": [[6, 7, "product"], [15, 16, "field"], [18, 19, "task"], [21, 22, "task"], [26, 28, "task"], [32, 33, "field"], [34, 38, "researcher"], [40, 42, "researcher"], [44, 47, "researcher"], [49, 53, "researcher"], [55, 57, "researcher"], [59, 62, "researcher"], [64, 66, "researcher"], [68, 69, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[6, 7, 15, 16, "part-of", "", false, false], [6, 7, 15, 16, "usage", "", false, false], [6, 7, 18, 19, "part-of", "", false, false], [6, 7, 18, 19, "usage", "", false, false], [6, 7, 21, 22, "part-of", "", false, false], [6, 7, 21, 22, "usage", "", false, false], [6, 7, 32, 33, "part-of", "", false, false], [6, 7, 32, 33, "usage", "", false, false], [26, 28, 21, 22, "part-of", "", false, false], [26, 28, 21, 22, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "opinion", "-", "based", "recommender", "systems", "use", "a", "variety", "of", "techniques", ",", "including", "text", "mining", ",", "information", "extraction", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", ",", "and", "deep", "learning", "X", ".", "Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y", ".", "J.", "Ren", ",", "P", ".", "H", ".", "Shang", ",", "Y", ".", "Zhu", ",", "Y", ".", "C.", "Liang", ",", "R.", "C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to opinion-based recommender systems use a variety of techniques, including text mining, information extraction, sentiment analysis (see also multimodal sentiment analysis), and deep learning X. Y. Feng, H. Zhang, Y. J. Ren, P. H. Shang, Y. Zhu, Y. C. Liang, R. C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 55], [56, 59], [60, 61], [62, 69], [70, 72], [73, 83], [83, 84], [85, 94], [95, 99], [100, 106], [106, 107], [108, 119], [120, 130], [130, 131], [132, 141], [142, 150], [151, 152], [152, 155], [156, 160], [161, 171], [172, 181], [182, 190], [190, 191], [191, 192], [193, 196], [197, 201], [202, 210], [211, 212], [212, 213], [214, 215], [215, 216], [217, 221], [221, 222], [223, 224], [224, 225], [226, 231], [231, 232], [233, 234], [234, 235], [236, 238], [239, 242], [242, 243], [244, 245], [245, 246], [247, 248], [248, 249], [250, 255], [255, 256], [257, 258], [258, 259], [260, 263], [263, 264], [265, 266], [266, 267], [268, 270], [271, 276], [276, 277], [278, 280], [281, 283], [284, 288], [288, 289], [290, 292], [293, 295], [295, 296], [297, 298], [298, 302], [302, 303], [303, 304], [304, 305], [306, 308], [309, 310], [310, 311], [311, 312], [312, 313], [314, 320], [320, 321]]}
{"doc_key": "ai-train-2", "ner": [[8, 10, "university"], [13, 14, "researcher"], [16, 17, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[13, 14, 8, 10, "physical", "", false, false], [13, 14, 8, 10, "role", "", false, false], [16, 17, 8, 10, "physical", "", false, false], [16, 17, 8, 10, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Proponents", "of", "procedural", "presentations", "are", "mainly", "concentrated", "at", "MIT", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Pappert", "."], "sentence-detokenized": "Proponents of procedural presentations are mainly concentrated at MIT under the leadership of Marvin Minsky and Seymour Pappert.", "token2charspan": [[0, 10], [11, 13], [14, 24], [25, 38], [39, 42], [43, 49], [50, 62], [63, 65], [66, 69], [70, 75], [76, 79], [80, 90], [91, 93], [94, 100], [101, 107], [108, 111], [112, 119], [120, 127], [127, 128]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [28, 28, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 28, 28, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "to", "numerically", "solve", "linear", "and", "nonlinear", "problems", ",", "as", "well", "as", "to", "perform", "other", "numerical", "experiments", "using", "a", "program", "that", "in", "most", "cases", "is", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps to numerically solve linear and nonlinear problems, as well as to perform other numerical experiments using a program that in most cases is compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 27], [28, 33], [34, 40], [41, 44], [45, 54], [55, 63], [63, 64], [65, 67], [68, 72], [73, 75], [76, 78], [79, 86], [87, 92], [93, 102], [103, 114], [115, 120], [121, 122], [123, 130], [131, 135], [136, 138], [139, 143], [144, 149], [150, 152], [153, 163], [164, 168], [169, 175], [175, 176]]}
{"doc_key": "ai-train-5", "ner": [[3, 8, "algorithm"], [11, 13, "misc"], [14, 15, "researcher"], [21, 23, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 8, 14, 15, "origin", "", false, false], [11, 13, 14, 15, "origin", "", false, false], [14, 15, 21, 23, "physical", "", false, false], [14, 15, 21, 23, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", ",", "as", "well", "as", "unsupervised", "methods", "by", "Jeff", "Hinton", "and", "his", "colleagues", "at", "the", "University", "of", "Toronto", ",", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm, as well as unsupervised methods by Jeff Hinton and his colleagues at the University of Toronto, can be used to train deep, highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [42, 43], [44, 46], [47, 51], [52, 54], [55, 67], [68, 75], [76, 78], [79, 83], [84, 90], [91, 94], [95, 98], [99, 109], [110, 112], [113, 116], [117, 127], [128, 130], [131, 138], [138, 139], [140, 143], [144, 146], [147, 151], [152, 154], [155, 160], [161, 165], [165, 166], [167, 173], [174, 183], [184, 190], [191, 204], [204, 205], [206, 207], [207, 208], [208, 212], [213, 220]]}
{"doc_key": "ai-train-6", "ner": [[5, 6, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalent", ",", "using", "the", "DCG", "notation", ":"], "sentence-detokenized": "or equivalent, using the DCG notation:", "token2charspan": [[0, 2], [3, 13], [13, 14], [15, 20], [21, 24], [25, 28], [29, 37], [37, 38]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 11, "algorithm"], [14, 18, "algorithm"], [19, 21, "algorithm"], [25, 25, "algorithm"], [27, 28, "algorithm"], [42, 45, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 11, "type-of", "", false, false], [0, 3, 14, 18, "usage", "part-of?", true, false], [14, 18, 19, 21, "compare", "", false, false], [25, 25, 19, 21, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "apply", "competitive", "learning", "as", "opposed", "to", "error", "-correcting", "learning", ",", "such", "as", "backpropagation", "with", "gradient", "descent", ")", ",", "and", "in", "that", "they", "use", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps differ from other artificial neural networks in that they apply competitive learning as opposed to error-correcting learning, such as backpropagation with gradient descent), and in that they use a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 84], [85, 96], [97, 105], [106, 108], [109, 116], [117, 119], [120, 125], [125, 136], [137, 145], [145, 146], [147, 151], [152, 154], [155, 170], [171, 175], [176, 184], [185, 192], [192, 193], [193, 194], [195, 198], [199, 201], [202, 206], [207, 211], [212, 215], [216, 217], [218, 230], [231, 239], [240, 242], [243, 251], [252, 255], [256, 267], [268, 278], [279, 281], [282, 285], [286, 291], [292, 297], [297, 298]]}
{"doc_key": "ai-train-8", "ner": [[10, 12, "organisation"], [26, 33, "misc"], [38, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "several", "bodies", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", ",", "which", "is", "then", "filtered", "in", "the", "noise", "level", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "muting", "schemes", "."], "sentence-detokenized": "Since the early 1990s, several bodies, including the Audio Engineering Society, have recommended that dynamic range measurements be made in the presence of an audio signal, which is then filtered in the noise level measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or muting schemes.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 58], [59, 70], [71, 78], [78, 79], [80, 84], [85, 96], [97, 101], [102, 109], [110, 115], [116, 128], [129, 131], [132, 136], [137, 139], [140, 143], [144, 152], [153, 155], [156, 158], [159, 164], [165, 171], [171, 172], [173, 178], [179, 181], [182, 186], [187, 195], [196, 198], [199, 202], [203, 208], [209, 214], [215, 226], [227, 231], [232, 234], [235, 244], [245, 252], [253, 258], [258, 259], [260, 264], [265, 271], [272, 284], [285, 297], [298, 303], [304, 306], [307, 310], [311, 314], [315, 317], [318, 323], [324, 329], [330, 332], [333, 339], [340, 347], [347, 348]]}
{"doc_key": "ai-train-9", "ner": [[4, 5, "misc"], [15, 16, "task"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 32, "task"], [35, 37, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[4, 5, 15, 16, "part-of", "concept_used_in", true, false], [4, 5, 18, 19, "part-of", "concept_used_in", false, false], [4, 5, 21, 22, "part-of", "concept_used_in", false, false], [4, 5, 24, 25, "part-of", "concept_used_in", false, false], [4, 5, 27, 32, "part-of", "concept_used_in", false, false], [4, 5, 35, 37, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["Techniques", "used", "to", "create", "own", "faces", "and", "use", "them", "for", "recognition", "are", "also", "used", "beyond", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", ",", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "Techniques used to create own faces and use them for recognition are also used beyond face recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation, and medical image analysis.", "token2charspan": [[0, 10], [11, 15], [16, 18], [19, 25], [26, 29], [30, 35], [36, 39], [40, 43], [44, 48], [49, 52], [53, 64], [65, 68], [69, 73], [74, 78], [79, 85], [86, 90], [91, 102], [102, 103], [104, 115], [116, 127], [127, 128], [129, 132], [133, 140], [140, 141], [142, 147], [148, 159], [159, 160], [161, 165], [166, 174], [174, 175], [175, 179], [180, 187], [188, 202], [202, 203], [204, 207], [208, 215], [216, 221], [222, 230], [230, 231]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [9, 13, "organisation"], [15, 15, "organisation"], [19, 22, "organisation"], [25, 29, "organisation"], [32, 35, "organisation"], [38, 42, "organisation"], [44, 44, "organisation"], [49, 52, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[9, 13, 0, 3, "part-of", "", false, false], [15, 15, 9, 13, "named", "", false, false], [19, 22, 0, 3, "part-of", "", false, false], [25, 29, 0, 3, "part-of", "", false, false], [32, 35, 0, 3, "part-of", "", false, false], [38, 42, 0, 3, "part-of", "", false, false], [44, 44, 38, 42, "named", "", false, false], [49, 52, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "the", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "U.S.", "Department", "of", "Energy", ",", "the", "U.S.", "Department", "of", "Commerce", "NIST", ",", "the", "U.S.", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", ",", "and", "the", "Office", "of", "Naval", "Research", "coordinated", "the", "studies", "to", "provide", "input", "to", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was the umbrella for the National Aeronautics and Space Administration (NASA), the U.S. Department of Energy, the U.S. Department of Commerce NIST, the U.S. Department of Defense, the Defense Advanced Research Projects Agency (DARPA), and the Office of Naval Research coordinated the studies to provide input to strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 39], [40, 48], [49, 52], [53, 56], [57, 65], [66, 77], [78, 81], [82, 87], [88, 102], [103, 104], [104, 108], [108, 109], [109, 110], [111, 114], [115, 119], [120, 130], [131, 133], [134, 140], [140, 141], [142, 145], [146, 150], [151, 161], [162, 164], [165, 173], [174, 178], [178, 179], [180, 183], [184, 188], [189, 199], [200, 202], [203, 210], [210, 211], [212, 215], [216, 223], [224, 232], [233, 241], [242, 250], [251, 257], [258, 259], [259, 264], [264, 265], [265, 266], [267, 270], [271, 274], [275, 281], [282, 284], [285, 290], [291, 299], [300, 311], [312, 315], [316, 323], [324, 326], [327, 334], [335, 340], [341, 343], [344, 353], [354, 362], [363, 365], [366, 371], [372, 385], [385, 386]]}
{"doc_key": "ai-train-11", "ner": [[5, 6, "metrics"], [10, 11, "algorithm"], [15, 16, "researcher"], [21, 22, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 10, 11, "part-of", "", false, false], [15, 16, 21, 22, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "fast", "method", "for", "computing", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "as", "an", "application", "to", "Bliss", "'s", "work", "in", "1935", "."], "sentence-detokenized": "A fast method for computing maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an application to Bliss's work in 1935.", "token2charspan": [[0, 1], [2, 6], [7, 13], [14, 17], [18, 27], [28, 35], [36, 46], [47, 56], [57, 60], [61, 64], [65, 71], [72, 77], [78, 81], [82, 90], [91, 93], [94, 100], [101, 107], [108, 110], [111, 113], [114, 125], [126, 128], [129, 134], [134, 136], [137, 141], [142, 144], [145, 149], [149, 150]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [13, 17, "product"], [21, 21, "organisation"], [19, 19, "product"], [26, 31, "organisation"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[19, 19, 13, 17, "usage", "uses_software", false, false], [19, 19, 21, 21, "artifact", "", false, false], [19, 19, 24, 24, "named", "", false, false], [24, 24, 26, 31, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", ",", "which", "supports", "BabelFish", "on", "AltaVista", "(", "now", "Babelfish", "on", "Yahoo", "since", "May", "9", ",", "2008", ")", "."], "sentence-detokenized": "Several of these programs are available online, such as Google Translate and the SYSTRAN system, which supports BabelFish on AltaVista (now Babelfish on Yahoo since May 9, 2008).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 39], [40, 46], [46, 47], [48, 52], [53, 55], [56, 62], [63, 72], [73, 76], [77, 80], [81, 88], [89, 95], [95, 96], [97, 102], [103, 111], [112, 121], [122, 124], [125, 134], [135, 136], [136, 139], [140, 149], [150, 152], [153, 158], [159, 164], [165, 168], [169, 170], [170, 171], [172, 176], [176, 177], [177, 178]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 22, "field"], [26, 27, "misc"], [31, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 31, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 31, 32, "related-to", "", true, false], [10, 11, 20, 22, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 31, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "H\u00fctter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealized", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, H\u00fctter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealized intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 19, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 19, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measure", "."], "sentence-detokenized": "The most common way is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 25], [26, 29], [30, 33], [34, 36], [36, 37], [37, 43], [44, 49], [50, 51], [51, 57], [57, 58], [58, 66], [67, 77], [78, 81], [82, 89], [90, 100], [100, 101], [102, 109], [109, 110]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [15, 15, "programlang"], [17, 17, "programlang"], [19, 20, "researcher"], [22, 23, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 15, 15, "related-to", "", false, false], [0, 0, 17, 17, "related-to", "", false, false], [19, 20, 22, 23, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "training", "schemes", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "scripts", "in", "R", "and", "Python", ".", "David", "Norris", ",", "Bloor", "Research", ",", "November", "13", ",", "2013", "."], "sentence-detokenized": "RapidMiner provides training schemes, models and algorithms and can be extended using scripts in R and Python. David Norris, Bloor Research, November 13, 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 85], [86, 93], [94, 96], [97, 98], [99, 102], [103, 109], [109, 110], [111, 116], [117, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 149], [150, 152], [152, 153], [154, 158], [158, 159]]}
{"doc_key": "ai-train-16", "ner": [[5, 7, "programlang"], [10, 11, "product"]], "ner_mapping_to_source": [4, 5], "relations": [[10, 11, 5, 7, "general-affiliation", "", true, false]], "relations_mapping_to_source": [4], "sentence": ["But", "the", "newer", ",", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "whose", "development", "began", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "particularly", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "But the newer, fully Java-based version (Weka 3), whose development began in 1997, is now used in many different application areas, particularly for educational and research purposes.", "token2charspan": [[0, 3], [4, 7], [8, 13], [13, 14], [15, 20], [21, 25], [25, 26], [26, 31], [32, 39], [40, 41], [41, 45], [46, 47], [47, 48], [48, 49], [50, 55], [56, 67], [68, 73], [74, 76], [77, 81], [81, 82], [83, 85], [86, 89], [90, 94], [95, 97], [98, 102], [103, 112], [113, 124], [125, 130], [130, 131], [132, 144], [145, 148], [149, 160], [161, 164], [165, 173], [174, 182], [182, 183]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [13, 22, "misc"], [25, 29, "misc"], [30, 38, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 22, 0, 0, "topic", "", false, false], [13, 22, 25, 29, "win-defeat", "", false, false], [25, 29, 30, 38, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eversco", "made", "many", "interesting", "discoveries", "and", "enjoyed", "considerable", "recognition", ",", "with", "his", "paper", "Heuretics", ":", "A", "Theoretical", "and", "Exploratory", "Study", "of", "Heuristic", "Rules", "winning", "the", "best", "paper", "award", "of", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "in", "1982", "."], "sentence-detokenized": "Eversco made many interesting discoveries and enjoyed considerable recognition, with his paper Heuretics: A Theoretical and Exploratory Study of Heuristic Rules winning the best paper award of the Association for the Advancement of Artificial Intelligence in 1982.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 53], [54, 66], [67, 78], [78, 79], [80, 84], [85, 88], [89, 94], [95, 104], [104, 105], [106, 107], [108, 119], [120, 123], [124, 135], [136, 141], [142, 144], [145, 154], [155, 160], [161, 168], [169, 172], [173, 177], [178, 183], [184, 189], [190, 192], [193, 196], [197, 208], [209, 212], [213, 216], [217, 228], [229, 231], [232, 242], [243, 255], [256, 258], [259, 263], [263, 264]]}
{"doc_key": "ai-train-18", "ner": [[8, 14, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "allow", "for", "multiple", "units", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To allow for multiple units, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 8], [9, 12], [13, 21], [22, 27], [27, 28], [29, 30], [31, 39], [40, 45], [46, 50], [51, 53], [54, 64], [65, 68], [69, 73], [74, 81], [81, 82]]}
{"doc_key": "ai-train-19", "ner": [[9, 11, "product"], [13, 14, "product"], [16, 17, "product"], [19, 20, "product"], [22, 24, "product"], [28, 29, "product"], [37, 43, "product"], [47, 48, "product"], [50, 51, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[9, 11, 28, 29, "type-of", "", false, false], [13, 14, 28, 29, "type-of", "", false, false], [16, 17, 28, 29, "type-of", "", false, false], [19, 20, 28, 29, "type-of", "", false, false], [22, 24, 28, 29, "type-of", "", false, false], [47, 48, 37, 43, "type-of", "", false, false], [50, 51, 37, 43, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "conversational", "assistants", ",", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "access", "to", "voice", "portals", "is", "now", "possible", "through", "mobile", "devices", "and", "Far", "Field", "voice", "-", "enabled", "smart", "speakers", ",", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of conversational assistants, such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, access to voice portals is now possible through mobile devices and Far Field voice-enabled smart speakers, such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 33], [34, 44], [44, 45], [46, 50], [51, 53], [54, 59], [59, 61], [62, 66], [66, 67], [68, 74], [75, 80], [80, 81], [82, 88], [89, 98], [98, 99], [100, 109], [110, 117], [118, 121], [122, 129], [129, 131], [132, 137], [137, 138], [139, 145], [146, 148], [149, 154], [155, 162], [163, 165], [166, 169], [170, 178], [179, 186], [187, 193], [194, 201], [202, 205], [206, 209], [210, 215], [216, 221], [221, 222], [222, 229], [230, 235], [236, 244], [244, 245], [246, 250], [251, 253], [254, 260], [261, 265], [266, 269], [270, 276], [277, 281], [281, 282]]}
{"doc_key": "ai-train-20", "ner": [[2, 5, "field"], [6, 8, "algorithm"], [10, 13, "algorithm"], [15, 16, "algorithm"], [20, 20, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 5, "type-of", "", false, false], [10, 13, 2, 5, "type-of", "", false, false], [15, 16, 2, 5, "type-of", "", false, false], [20, 20, 2, 5, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "the", "Naive", "Bayes", "classifier", ",", "the", "support", "vector", "machine", ",", "Gaussian", "mixtures", ",", "and", "the", "network", "."], "sentence-detokenized": "Examples of supervised learning are the Naive Bayes classifier, the support vector machine, Gaussian mixtures, and the network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 39], [40, 45], [46, 51], [52, 62], [62, 63], [64, 67], [68, 75], [76, 82], [83, 90], [90, 91], [92, 100], [101, 109], [109, 110], [111, 114], [115, 118], [119, 126], [126, 127]]}
{"doc_key": "ai-train-21", "ner": [[0, 2, "algorithm"], [24, 26, "algorithm"], [28, 30, "task"], [33, 34, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 24, 26, "part-of", "", true, false], [33, 34, 28, 30, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "OSD", "algorithm", "can", "be", "used", "to", "derive", "mathematical", "O", "(", "\\", "sqrt", "{", "T", "})", "/mathematical", "regret", "bounds", "for", "the", "online", "version", "of", "Support", "vector", "machine", "for", "classification", ",", "which", "uses", "the", "pivot", "loss", "mathematical", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "The OSD algorithm can be used to derive mathematical O(\\ sqrt {T})/mathematical regret bounds for the online version of Support vector machine for classification, which uses the pivot loss mathematical v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\}/math", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 24], [25, 29], [30, 32], [33, 39], [40, 52], [53, 54], [54, 55], [55, 56], [57, 61], [62, 63], [63, 64], [64, 66], [66, 79], [80, 86], [87, 93], [94, 97], [98, 101], [102, 108], [109, 116], [117, 119], [120, 127], [128, 134], [135, 142], [143, 146], [147, 161], [161, 162], [163, 168], [169, 173], [174, 177], [178, 183], [184, 188], [189, 201], [202, 203], [204, 206], [207, 208], [208, 209], [209, 210], [211, 213], [214, 217], [217, 218], [219, 220], [220, 221], [221, 222], [223, 224], [225, 226], [227, 228], [229, 231], [232, 233], [233, 234], [234, 235], [236, 240], [241, 242], [243, 245], [245, 246], [246, 248], [248, 249], [249, 253]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"], [27, 28, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modeling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", ",", "and", "motion", "matching", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modeling, gesture recognition, video tracking, individual wildlife identification, and motion matching.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 101], [101, 102], [103, 110], [111, 122], [122, 123], [124, 129], [130, 138], [138, 139], [140, 150], [151, 159], [160, 174], [174, 175], [176, 179], [180, 186], [187, 195], [195, 196]]}
{"doc_key": "ai-train-23", "ner": [[8, 9, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [27, 31, "university"], [33, 35, "university"], [37, 40, "university"], [42, 43, "university"], [45, 50, "university"], [52, 52, "university"], [56, 59, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[8, 9, 14, 15, "related-to", "", true, false], [8, 9, 17, 19, "related-to", "", true, false], [8, 9, 21, 22, "related-to", "", true, false], [8, 9, 24, 25, "related-to", "", true, false], [8, 9, 27, 31, "related-to", "", true, false], [8, 9, 33, 35, "related-to", "", true, false], [8, 9, 37, 40, "related-to", "", true, false], [8, 9, 42, 43, "related-to", "", true, false], [8, 9, 45, 50, "related-to", "", true, false], [8, 9, 52, 52, "related-to", "", true, false], [8, 9, 56, 59, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "researching", "pose", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", "San", "Diego", ",", "University", "of", "Toronto", ",", "Central", "School", "of", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", ",", "and", "University", "of", "California", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California San Diego, University of Toronto, Central School of Paris, ETH Zurich, National University of Science and Technology (NUST), and University of California Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 48], [49, 53], [54, 64], [64, 65], [66, 75], [76, 82], [83, 85], [86, 91], [92, 102], [102, 103], [104, 112], [113, 119], [120, 130], [130, 131], [132, 135], [136, 148], [148, 149], [150, 158], [159, 169], [169, 170], [171, 181], [182, 184], [185, 195], [196, 199], [200, 205], [205, 206], [207, 217], [218, 220], [221, 228], [228, 229], [230, 237], [238, 244], [245, 247], [248, 253], [253, 254], [255, 258], [259, 265], [265, 266], [267, 275], [276, 286], [287, 289], [290, 297], [298, 301], [302, 312], [313, 314], [314, 318], [318, 319], [319, 320], [321, 324], [325, 335], [336, 338], [339, 349], [350, 356], [356, 357]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "Sigmoid", "Cross", "Entropy", "Loss", "function", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "0,1", "/", "mathematics", "."], "sentence-detokenized": "The Sigmoid Cross Entropy Loss function is used to predict K independent probability values in 0,1/mathematics.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 25], [26, 30], [31, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 98], [98, 99], [99, 110], [110, 111]]}
{"doc_key": "ai-train-25", "ner": [[10, 13, "misc"], [14, 14, "field"], [16, 17, "field"], [20, 22, "university"], [24, 25, "country"], [28, 30, "misc"], [33, 36, "university"], [38, 38, "country"], [5, 5, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 13, 14, 14, "topic", "", false, false], [10, 13, 16, 17, "topic", "", false, false], [10, 13, 20, 22, "physical", "", true, false], [20, 22, 24, 25, "physical", "", false, false], [28, 30, 33, 36, "physical", "", true, false], [33, 36, 38, 38, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Before", "becoming", "a", "professor", "at", "Cambridge", ",", "he", "held", "the", "Johan", "Bernoulli", "Chair", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", ",", "the", "Netherlands", ",", "and", "the", "Toshiba", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", ",", "Japan", "."], "sentence-detokenized": "Before becoming a professor at Cambridge, he held the Johan Bernoulli Chair in Mathematics and Computer Science at the University of Groningen, the Netherlands, and the Toshiba Chair at the Tokyo Institute of Technology, Japan.", "token2charspan": [[0, 6], [7, 15], [16, 17], [18, 27], [28, 30], [31, 40], [40, 41], [42, 44], [45, 49], [50, 53], [54, 59], [60, 69], [70, 75], [76, 78], [79, 90], [91, 94], [95, 103], [104, 111], [112, 114], [115, 118], [119, 129], [130, 132], [133, 142], [142, 143], [144, 147], [148, 159], [159, 160], [161, 164], [165, 168], [169, 176], [177, 182], [183, 185], [186, 189], [190, 195], [196, 205], [206, 208], [209, 219], [219, 220], [221, 226], [226, 227]]}
{"doc_key": "ai-train-26", "ner": [[5, 6, "algorithm"], [11, 15, "algorithm"], [10, 17, "algorithm"], [21, 22, "researcher"], [24, 25, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 11, 15, "usage", "", true, false], [11, 15, 21, 22, "origin", "", false, false], [11, 15, 24, 25, "origin", "", false, false], [10, 17, 11, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "particularly", "used", "for", "recurrent", "neural", "networks", "is", "the", "1997", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "network", "of", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "."], "sentence-detokenized": "Another technique particularly used for recurrent neural networks is the 1997 long short-term memory (LSTM) network of Sepp Hochreiter and J\u00fcrgen Schmidhuber.", "token2charspan": [[0, 7], [8, 17], [18, 30], [31, 35], [36, 39], [40, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 77], [78, 82], [83, 88], [88, 89], [89, 93], [94, 100], [101, 102], [102, 106], [106, 107], [108, 115], [116, 118], [119, 123], [124, 134], [135, 138], [139, 145], [146, 157], [157, 158]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 9, "product"], [15, 15, "product"], [46, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 4, 5, "general-affiliation", "", false, false], [8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "flexible", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "mode", "in", "a", "similar", "way", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT up to version 5.34, Cling from version 6) makes this package very flexible as it can be used in interactive, scripted and compiled mode in a similar way to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 43], [44, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [81, 82], [83, 88], [89, 93], [94, 101], [102, 106], [107, 115], [116, 118], [119, 121], [122, 125], [126, 128], [129, 133], [134, 136], [137, 148], [148, 149], [150, 158], [159, 162], [163, 171], [172, 176], [177, 179], [180, 181], [182, 189], [190, 193], [194, 196], [197, 207], [208, 216], [217, 221], [222, 224], [225, 231], [231, 232]]}
{"doc_key": "ai-train-28", "ner": [[0, 2, "product"], [21, 22, "field"], [27, 28, "task"], [30, 32, "task"], [34, 35, "task"], [38, 39, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 2, 21, 22, "related-to", "", false, false], [27, 28, 21, 22, "part-of", "", false, false], [30, 32, 21, 22, "part-of", "", false, false], [34, 35, 21, 22, "part-of", "", false, false], [38, 39, 21, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Voice", "user", "interfaces", "that", "interpret", "and", "manage", "conversation", "state", "are", "challenging", "to", "design", "due", "to", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "named", "object", "recognition", ",", "information", "retrieval", ",", "and", "dialogue", "management", "."], "sentence-detokenized": "Voice user interfaces that interpret and manage conversation state are challenging to design due to the inherent difficulty of integrating complex natural language processing tasks such as coreference resolution, named object recognition, information retrieval, and dialogue management.", "token2charspan": [[0, 5], [6, 10], [11, 21], [22, 26], [27, 36], [37, 40], [41, 47], [48, 60], [61, 66], [67, 70], [71, 82], [83, 85], [86, 92], [93, 96], [97, 99], [100, 103], [104, 112], [113, 123], [124, 126], [127, 138], [139, 146], [147, 154], [155, 163], [164, 174], [175, 180], [181, 185], [186, 188], [189, 200], [201, 211], [211, 212], [213, 218], [219, 225], [226, 237], [237, 238], [239, 250], [251, 260], [260, 261], [262, 265], [266, 274], [275, 285], [285, 286]]}
{"doc_key": "ai-train-29", "ner": [[5, 7, "algorithm"], [9, 11, "algorithm"], [14, 16, "researcher"], [21, 25, "organisation"], [34, 35, "field"], [37, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 7, 14, 16, "origin", "", false, false], [5, 7, 34, 35, "part-of", "", false, false], [5, 7, 37, 38, "part-of", "", false, false], [9, 11, 14, 16, "origin", "", false, false], [9, 11, 34, 35, "part-of", "", false, false], [9, 11, 37, 38, "part-of", "", false, false], [14, 16, 21, 25, "physical", "", false, false], [14, 16, 21, 25, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "artificial", "intelligence", "laboratory", "IDSIA", "won", "eight", "international", "competitions", "in", "the", "field", "of", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep neural networks developed in J\u00fcrgen Schmidhuber's research group at the Swiss artificial intelligence laboratory IDSIA won eight international competitions in the field of pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 64], [65, 73], [74, 83], [84, 86], [87, 93], [94, 105], [105, 107], [108, 116], [117, 122], [123, 125], [126, 129], [130, 135], [136, 146], [147, 159], [160, 170], [171, 176], [177, 180], [181, 186], [187, 200], [201, 213], [214, 216], [217, 220], [221, 226], [227, 229], [230, 237], [238, 249], [250, 253], [254, 261], [262, 270], [270, 271]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 12, "product"], [14, 17, "task"], [16, 16, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 12, "usage", "", false, false], [1, 3, 14, 17, "usage", "", true, false], [1, 3, 16, 16, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "and", "speech", "synthesis", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech and speech synthesis.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 89], [90, 96], [97, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[6, 11, "misc"], [13, 13, "field"], [15, 18, "university"], [25, 28, "field"], [30, 33, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 11, 13, 13, "topic", "topic_of_award", false, false], [6, 11, 15, 18, "origin", "", true, false], [25, 28, 30, 33, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", "-", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "Psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "PhD", "in", "Industrial", "Design", "and", "Engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees - S. V. della laurea ad honorem in Psychology from the University of Padua in 1995 and a PhD in Industrial Design and Engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [33, 34], [35, 37], [38, 40], [41, 46], [47, 53], [54, 56], [57, 64], [65, 67], [68, 78], [79, 83], [84, 87], [88, 98], [99, 101], [102, 107], [108, 110], [111, 115], [116, 119], [120, 121], [122, 125], [126, 128], [129, 139], [140, 146], [147, 150], [151, 162], [163, 167], [168, 173], [174, 184], [185, 187], [188, 198], [198, 199]]}
{"doc_key": "ai-train-32", "ner": [[5, 8, "researcher"], [12, 15, "organisation"], [17, 17, "location"], [19, 19, "researcher"], [30, 31, "misc"], [44, 46, "misc"], [62, 63, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[5, 8, 12, 15, "physical", "", false, false], [5, 8, 12, 15, "role", "", false, false], [12, 15, 17, 17, "physical", "", false, false], [19, 19, 30, 31, "related-to", "works_with", true, false], [19, 19, 44, 46, "related-to", "works_with", true, false], [19, 19, 62, 63, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "longtime", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "H\u00f4pital", "Pitier", "-", "Salpetriere", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "with", "impaired", "multiplication", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobe", ")", "and", "others", "with", "impaired", "subtraction", "but", "preserved", "multiplication", "(", "associated", "with", "lesions", "of", "the", "intraparietal", "groove", ")", "."], "sentence-detokenized": "Together with his longtime collaborator Laurent Cohen, a neurologist at the H\u00f4pital Pitier-Salpetriere in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication but preserved subtraction (associated with lesions of the inferior parietal lobe) and others with impaired subtraction but preserved multiplication (associated with lesions of the intraparietal groove).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 26], [27, 39], [40, 47], [48, 53], [53, 54], [55, 56], [57, 68], [69, 71], [72, 75], [76, 83], [84, 90], [90, 91], [91, 102], [103, 105], [106, 111], [111, 112], [113, 120], [121, 125], [126, 136], [137, 145], [146, 150], [151, 158], [159, 161], [162, 171], [172, 179], [180, 182], [183, 186], [187, 195], [196, 200], [201, 205], [206, 214], [215, 229], [230, 233], [234, 243], [244, 255], [256, 257], [257, 267], [268, 272], [273, 280], [281, 283], [284, 287], [288, 296], [297, 305], [306, 310], [310, 311], [312, 315], [316, 322], [323, 327], [328, 336], [337, 348], [349, 352], [353, 362], [363, 377], [378, 379], [379, 389], [390, 394], [395, 402], [403, 405], [406, 409], [410, 423], [424, 430], [430, 431], [431, 432]]}
{"doc_key": "ai-train-33", "ner": [[5, 8, "product"], [13, 13, "misc"], [15, 16, "misc"], [23, 23, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 13, 5, 8, "topic", "", false, false], [15, 16, 5, 8, "topic", "", false, false], [23, 23, 5, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Recently", ",", "fictional", "representations", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "Artificial", "Intelligence", "and", "Ex", "Machina", "and", "the", "2016", "television", "adaptation", "of", "Westworld", "have", "evoked", "audience", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "Recently, fictional representations of artificially intelligent robots in films such as Artificial Intelligence and Ex Machina and the 2016 television adaptation of Westworld have evoked audience sympathy for the robots themselves.", "token2charspan": [[0, 8], [8, 9], [10, 19], [20, 35], [36, 38], [39, 51], [52, 63], [64, 70], [71, 73], [74, 79], [80, 84], [85, 87], [88, 98], [99, 111], [112, 115], [116, 118], [119, 126], [127, 130], [131, 134], [135, 139], [140, 150], [151, 161], [162, 164], [165, 174], [175, 179], [180, 186], [187, 195], [196, 204], [205, 208], [209, 212], [213, 219], [220, 230], [230, 231]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [10, 12, "algorithm"], [14, 15, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 12, 7, 8, "part-of", "", false, false], [14, 15, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [20, 21, "misc"], [26, 27, "misc"], [29, 31, "person"], [36, 37, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[20, 21, 0, 3, "artifact", "", false, false], [26, 27, 0, 3, "artifact", "", false, false], [26, 27, 29, 31, "role", "director_of", false, false], [26, 27, 36, 37, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "in", "special", "locations", "to", "impress", "audiences", ",", "notable", "examples", "being", "Magical", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films in special locations to impress audiences, notable examples being Magical Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 53], [54, 61], [62, 71], [72, 74], [75, 82], [83, 92], [92, 93], [94, 101], [102, 110], [111, 116], [117, 124], [125, 133], [134, 135], [135, 139], [139, 140], [141, 144], [145, 152], [153, 155], [156, 157], [157, 164], [165, 169], [170, 177], [177, 178], [179, 183], [183, 184], [185, 193], [194, 201], [202, 209], [209, 210], [210, 211]]}
{"doc_key": "ai-train-36", "ner": [[12, 15, "field"], [19, 24, "task"], [26, 27, "task"], [29, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 15, "part-of", "", false, false], [26, 27, 12, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "such", "tasks", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [95, 98], [99, 103], [104, 109], [110, 112], [113, 117], [117, 118], [118, 120], [120, 121], [121, 127], [128, 135], [136, 139], [140, 149], [150, 157], [158, 159], [159, 166], [166, 167], [168, 172], [172, 173], [173, 174]]}
{"doc_key": "ai-train-37", "ner": [[2, 4, "product"], [9, 12, "organisation"], [14, 15, "organisation"], [17, 19, "country"], [21, 25, "product"], [28, 29, "researcher"], [38, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 12, 2, 4, "role", "introduces_to_market", true, false], [14, 15, 2, 4, "role", "introduces_to_market", true, false], [14, 15, 17, 19, "physical", "", false, false], [21, 25, 38, 38, "related-to", "sold_to", true, false], [28, 29, 21, 25, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletizing", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", ".", "of", "KUKA", "robotics", "in", "Germany", ",", "and", "the", "programmable", "universal", "assembly", "machine", "was", "invented", "by", "Victor", "Sheinman", "in", "1976", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletizing robot was introduced in 1963 by Fuji Yusoki Kogyo. of KUKA robotics in Germany, and the programmable universal assembly machine was invented by Victor Sheinman in 1976 and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [71, 72], [73, 75], [76, 80], [81, 89], [90, 92], [93, 100], [100, 101], [102, 105], [106, 109], [110, 122], [123, 132], [133, 141], [142, 149], [150, 153], [154, 162], [163, 165], [166, 172], [173, 181], [182, 184], [185, 189], [190, 193], [194, 197], [198, 204], [205, 208], [209, 213], [214, 216], [217, 226], [226, 227]]}
{"doc_key": "ai-train-38", "ner": [[8, 8, "conference"], [10, 10, "researcher"], [19, 19, "field"], [32, 33, "researcher"], [40, 44, "researcher"], [54, 54, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[10, 10, 8, 8, "role", "president_of", false, false], [10, 10, 32, 33, "role", "colleagues", false, false], [19, 19, 54, 54, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ".", "While", "president", "of", "the", "AAAI", ",", "Hayes", "launched", "a", "series", "of", "attacks", "on", "critics", "of", "AI", ",", "mostly", "couched", "in", "ironic", "terms", ",", "and", "(", "with", "his", "colleague", "Kenneth", "Ford", ")", "invented", "a", "prize", "named", "after", "Simon", "Newcomb", "to", "be", "awarded", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s. While president of the AAAI, Hayes launched a series of attacks on critics of AI, mostly couched in ironic terms, and (with his colleague Kenneth Ford) invented a prize named after Simon Newcomb to be awarded for the most ridiculous argument refuting the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 33], [34, 36], [37, 40], [41, 45], [45, 46], [47, 52], [53, 61], [62, 63], [64, 70], [71, 73], [74, 81], [82, 84], [85, 92], [93, 95], [96, 98], [98, 99], [100, 106], [107, 114], [115, 117], [118, 124], [125, 130], [130, 131], [132, 135], [136, 137], [137, 141], [142, 145], [146, 155], [156, 163], [164, 168], [168, 169], [170, 178], [179, 180], [181, 186], [187, 192], [193, 198], [199, 204], [205, 212], [213, 215], [216, 218], [219, 226], [227, 230], [231, 234], [235, 239], [240, 250], [251, 259], [260, 268], [269, 272], [273, 284], [285, 287], [288, 290], [290, 291]]}
{"doc_key": "ai-train-39", "ner": [[15, 19, "algorithm"], [42, 43, "algorithm"], [54, 59, "algorithm"], [61, 63, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[15, 19, 42, 43, "named", "same", false, false], [54, 59, 15, 19, "type-of", "", false, false], [61, 63, 15, 19, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "for", "math", "/", "alpha", "/", "math", "can", "be", "found", "by", "using", "a", "line", "search", "algorithm", ",", "i.e.", ",", "the", "magnitude", "of", "math", "/", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "typically", "by", "a", "line", "search", "in", "the", "interval", "math0", "/", "alpha", "1", "/", "math", "or", "a", "backtracking", "line", "search", "such", "as", "an", "Armijo", "line", "search", "."], "sentence-detokenized": "The optimal value for math/alpha/math can be found by using a line search algorithm, i.e., the magnitude of math/alpha/math is determined by finding the value that minimizes S, typically by a line search in the interval math0/alpha 1/math or a backtracking line search such as an Armijo line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 21], [22, 26], [26, 27], [27, 32], [32, 33], [33, 37], [38, 41], [42, 44], [45, 50], [51, 53], [54, 59], [60, 61], [62, 66], [67, 73], [74, 83], [83, 84], [85, 89], [89, 90], [91, 94], [95, 104], [105, 107], [108, 112], [112, 113], [113, 118], [118, 119], [119, 123], [124, 126], [127, 137], [138, 140], [141, 148], [149, 152], [153, 158], [159, 163], [164, 173], [174, 175], [175, 176], [177, 186], [187, 189], [190, 191], [192, 196], [197, 203], [204, 206], [207, 210], [211, 219], [220, 225], [225, 226], [226, 231], [232, 233], [233, 234], [234, 238], [239, 241], [242, 243], [244, 256], [257, 261], [262, 268], [269, 273], [274, 276], [277, 279], [280, 286], [287, 291], [292, 298], [298, 299]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 14, "algorithm"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "breadth", "-", "first", "search", "and", "depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "contain", "a", "lot", "of", "technical", "knowledge", "but", "do", "n't", "shed", "much", "light", "on", "the", "mental", "processes", "that", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses breadth-first search and depth-first search techniques, but ultimately concludes that the results represent expert systems that contain a lot of technical knowledge but don't shed much light on the mental processes that people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 67], [67, 68], [69, 72], [73, 83], [84, 93], [94, 98], [99, 102], [103, 110], [111, 120], [121, 127], [128, 135], [136, 140], [141, 148], [149, 150], [151, 154], [155, 157], [158, 167], [168, 177], [178, 181], [182, 184], [184, 187], [188, 192], [193, 197], [198, 203], [204, 206], [207, 210], [211, 217], [218, 227], [228, 232], [233, 239], [240, 243], [244, 246], [247, 252], [253, 257], [258, 265], [265, 266]]}
{"doc_key": "ai-train-41", "ner": [[1, 1, "task"], [0, 6, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deal", "with", "how", "spoken", "language", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 44], [45, 49], [50, 53], [54, 60], [61, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 98], [99, 104], [105, 114], [114, 115]]}
{"doc_key": "ai-train-42", "ner": [[13, 14, "algorithm"], [28, 31, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^{", "*}", "/", "math", "is", "usually", "estimated", "using", "a", "maximum", "likelihood", "procedure", "(", "math\\", "theta", "^{", "*}", "=\\", "theta^{", "ML}", "/", "math", ")", "or", "maximum", "a", "posteriori", "estimation", "(", "math", "\\", "theta", "^{", "*}", "=\\", "theta", "^{", "MAP", "}", "/", "math", ")", "."], "sentence-detokenized": "This math\\ theta^{*} / math is usually estimated using a maximum likelihood procedure (math\\ theta^{*} =\\ theta^{ML} / math) or maximum a posteriori estimation (math\\ theta^{*} =\\ theta^{MAP} / math).", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [16, 18], [18, 20], [21, 22], [23, 27], [28, 30], [31, 38], [39, 48], [49, 54], [55, 56], [57, 64], [65, 75], [76, 85], [86, 87], [87, 92], [93, 98], [98, 100], [100, 102], [103, 105], [106, 113], [113, 116], [117, 118], [119, 123], [123, 124], [125, 127], [128, 135], [136, 137], [138, 148], [149, 159], [160, 161], [161, 165], [165, 166], [167, 172], [172, 174], [174, 176], [177, 179], [180, 185], [185, 187], [187, 190], [190, 191], [192, 193], [194, 198], [198, 199], [199, 200]]}
{"doc_key": "ai-train-43", "ner": [[9, 12, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "common", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesizer", "for", "their", "speech", ";", "it", "creates", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less common languages use the open-source eSpeak synthesizer for their speech; it creates a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 26], [27, 30], [31, 34], [35, 39], [39, 40], [40, 46], [47, 53], [54, 65], [66, 69], [70, 75], [76, 82], [82, 83], [84, 86], [87, 94], [95, 96], [97, 104], [104, 105], [106, 113], [114, 119], [120, 124], [125, 128], [129, 131], [132, 141], [142, 144], [145, 155], [155, 156]]}
{"doc_key": "ai-train-44", "ner": [[19, 19, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[19, 19, 35, 36, "compare", "", false, false], [19, 19, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "primarily", "used", "by", "statisticians", "and", "other", "professionals", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "R", "can", "also", "work", "as", "a", "general", "matrix", "computing", "toolkit", "-", "with", "performance", "metrics", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although primarily used by statisticians and other professionals who need an environment for statistical computing and software development, R can also work as a general matrix computing toolkit-with performance metrics comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 18], [19, 23], [24, 26], [27, 40], [41, 44], [45, 50], [51, 64], [65, 68], [69, 73], [74, 76], [77, 88], [89, 92], [93, 104], [105, 114], [115, 118], [119, 127], [128, 139], [139, 140], [141, 142], [143, 146], [147, 151], [152, 156], [157, 159], [160, 161], [162, 169], [170, 176], [177, 186], [187, 194], [194, 195], [195, 199], [200, 211], [212, 219], [220, 230], [231, 233], [234, 237], [238, 244], [245, 247], [248, 254], [254, 255]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 3, "field"], [8, 11, "misc"], [12, 14, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 3, "part-of", "", false, false], [0, 0, 12, 14, "origin", "", false, false], [8, 11, 12, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyne", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "-", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "combining", "two", "frequencies", "."], "sentence-detokenized": "Heterodyne is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden that creates new frequencies by combining two frequencies.", "token2charspan": [[0, 10], [11, 13], [14, 15], [16, 22], [23, 33], [34, 43], [44, 52], [53, 55], [56, 64], [65, 73], [73, 74], [74, 82], [83, 91], [92, 101], [102, 106], [107, 114], [115, 118], [119, 130], [131, 133], [134, 143], [144, 147], [148, 159], [159, 160]]}
{"doc_key": "ai-train-46", "ner": [[15, 17, "person"], [19, 19, "misc"], [24, 26, "organisation"], [29, 36, "organisation"], [32, 34, "misc"], [37, 38, "person"], [40, 40, "organisation"], [43, 44, "misc"], [47, 48, "person"], [50, 51, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[15, 17, 19, 19, "role", "actor_in", false, false], [19, 19, 24, 26, "artifact", "", false, false], [32, 34, 29, 36, "artifact", "", false, false], [37, 38, 32, 34, "role", "actor_in", false, false], [43, 44, 40, 40, "artifact", "", false, false], [47, 48, 43, 44, "role", "actor_in", false, false], [50, 51, 43, 44, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["A", "few", "other", "films", "helping", "to", "put", "3D", "back", "on", "the", "map", "this", "month", "are", "John", "Wayne", "'s", "\"", "Hondo", "\"", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "\"", "Miss", "Sadie", "Thompson", "\"", "with", "Rita", "Hayworth", "and", "Paramount", "'s", "\"", "Home", "Money", "\"", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "A few other films helping to put 3D back on the map this month are John Wayne's \"Hondo\" (distributed by Warner Bros. ), Columbia's \"Miss Sadie Thompson\" with Rita Hayworth and Paramount's \"Home Money\" with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 1], [2, 5], [6, 11], [12, 17], [18, 25], [26, 28], [29, 32], [33, 35], [36, 40], [41, 43], [44, 47], [48, 51], [52, 56], [57, 62], [63, 66], [67, 71], [72, 77], [77, 79], [80, 81], [81, 86], [86, 87], [88, 89], [89, 100], [101, 103], [104, 110], [111, 115], [115, 116], [117, 118], [118, 119], [120, 128], [128, 130], [131, 132], [132, 136], [137, 142], [143, 151], [151, 152], [153, 157], [158, 162], [163, 171], [172, 175], [176, 185], [185, 187], [188, 189], [189, 193], [194, 199], [199, 200], [201, 205], [206, 210], [211, 217], [218, 221], [222, 227], [228, 233], [233, 234]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 4, "field"], [5, 6, "task"], [14, 14, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 6, "general-affiliation", "", false, false], [0, 0, 14, 14, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "face", "recognition", "system", "created", "by", "a", "research", "group", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep learning face recognition system created by a research group at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 32], [33, 44], [45, 51], [52, 59], [60, 62], [63, 64], [65, 73], [74, 79], [80, 82], [83, 91], [91, 92]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 10, "conference"], [15, 16, "field"], [25, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [8, 10, 0, 1, "topic", "", false, false], [25, 27, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "premier", "academic", "conference", "on", "computer", "graphics", ",", "and", "a", "major", "theme", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the premier academic conference on computer graphics, and a major theme of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 91], [92, 94], [95, 103], [104, 112], [112, 113], [114, 117], [118, 119], [120, 125], [126, 131], [132, 134], [135, 138], [139, 145], [146, 154], [155, 165], [166, 175], [175, 176]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 9, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [36, 40, "misc"], [43, 46, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 15, 36, 40, "general-affiliation", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 36, 40, "general-affiliation", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 36, 40, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "preprocessing", "step", ",", "followed", "by", "clustering", "by", "k", "-", "NN", "of", "feature", "vectors", "in", "the", "dimensionality", "reduced", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) techniques as a preprocessing step, followed by clustering by k -NN of feature vectors in the dimensionality reduced space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [158, 160], [161, 170], [171, 182], [183, 191], [192, 193], [193, 196], [196, 197], [198, 208], [209, 211], [212, 213], [214, 227], [228, 232], [232, 233], [234, 242], [243, 245], [246, 256], [257, 259], [260, 261], [262, 263], [263, 265], [266, 268], [269, 276], [277, 284], [285, 287], [288, 291], [292, 306], [307, 314], [315, 320], [320, 321]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [37, 40, "algorithm"], [41, 42, "researcher"], [44, 46, "researcher"], [48, 54, "misc"], [56, 65, "conference"], [67, 67, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [48, 54, 37, 40, "topic", "", false, false], [48, 54, 41, 42, "artifact", "", false, false], [48, 54, 44, 46, "artifact", "", false, false], [48, 54, 56, 65, "temporal", "", false, false], [67, 67, 56, 65, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", ".", "Others", "use", "local", "features", "such", "as", "histogram", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", ",", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000. Others use local features such as histogram of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005, descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 143], [143, 144], [145, 151], [152, 155], [156, 161], [162, 170], [171, 175], [176, 178], [179, 188], [189, 191], [192, 200], [201, 210], [211, 213], [214, 219], [219, 220], [221, 222], [222, 223], [224, 230], [230, 231], [232, 242], [243, 245], [246, 254], [255, 264], [265, 268], [269, 274], [275, 284], [284, 285], [286, 290], [291, 299], [300, 307], [308, 318], [319, 321], [322, 330], [331, 337], [338, 341], [342, 349], [350, 361], [362, 363], [363, 367], [367, 368], [368, 369], [370, 375], [376, 377], [377, 378], [379, 386], [386, 387], [388, 392], [392, 393], [394, 405], [405, 406]]}
{"doc_key": "ai-train-52", "ner": [[0, 1, "algorithm"], [6, 10, "algorithm"], [14, 14, "task"], [16, 17, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 10, "type-of", "", false, false], [14, 14, 0, 1, "usage", "", true, false], [14, 14, 16, 17, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "that", "is", "used", "to", "learn", "functions", "in", "an", "unsupervised", "manner", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network that is used to learn functions in an unsupervised manner.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 61], [62, 66], [67, 69], [70, 75], [76, 85], [86, 88], [89, 91], [92, 104], [105, 111], [111, 112]]}
{"doc_key": "ai-train-53", "ner": [[0, 0, "researcher"], [6, 8, "organisation"], [11, 12, "field"], [14, 15, "field"], [21, 25, "organisation"], [27, 29, "organisation"], [33, 34, "field"], [36, 37, "field"], [44, 44, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 0, 6, 8, "role", "fellow_of", false, false], [0, 0, 11, 12, "related-to", "contributes_to", false, false], [0, 0, 14, 15, "related-to", "contributes_to", false, false], [0, 0, 21, 25, "role", "fellow_of", false, false], [0, 0, 33, 34, "related-to", "contributes_to", false, false], [0, 0, 36, 37, "related-to", "contributes_to", false, false], [27, 29, 21, 25, "named", "", false, false], [44, 44, 21, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralik", "is", "a", "Fellow", "of", "the", "IEEE", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "pattern", "recognition", ",", "image", "processing", ",", "and", "for", "his", "work", "in", "IAPR", "."], "sentence-detokenized": "Haralik is a Fellow of the IEEE for his contributions to computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions to pattern recognition, image processing, and for his work in IAPR.", "token2charspan": [[0, 7], [8, 10], [11, 12], [13, 19], [20, 22], [23, 26], [27, 31], [32, 35], [36, 39], [40, 53], [54, 56], [57, 65], [66, 72], [73, 76], [77, 82], [83, 93], [94, 97], [98, 99], [100, 106], [107, 109], [110, 113], [114, 127], [128, 139], [140, 143], [144, 151], [152, 163], [164, 165], [165, 169], [169, 170], [171, 174], [175, 178], [179, 192], [193, 195], [196, 203], [204, 215], [215, 216], [217, 222], [223, 233], [233, 234], [235, 238], [239, 242], [243, 246], [247, 251], [252, 254], [255, 259], [259, 260]]}
{"doc_key": "ai-train-54", "ner": [[4, 6, "task"], [11, 13, "algorithm"], [15, 15, "algorithm"], [20, 21, "researcher"], [23, 24, "organisation"], [26, 27, "researcher"], [30, 32, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 6, 11, 13, "usage", "", false, false], [11, 13, 20, 21, "origin", "", true, false], [11, 13, 26, 27, "origin", "", true, false], [15, 15, 11, 13, "named", "", false, false], [20, 21, 23, 24, "physical", "", false, false], [20, 21, 23, 24, "role", "", false, false], [26, 27, 30, 32, "physical", "", false, false], [26, 27, 30, 32, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "comprehensive", "ASR", "was", "with", "systems", "based", "on", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", ",", "presented", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitley", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at comprehensive ASR was with systems based on Connectionist Temporal Classification (CTC), presented by Alex Graves of Google DeepMind and Navdeep Jaitley of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 34], [35, 38], [39, 42], [43, 47], [48, 55], [56, 61], [62, 64], [65, 78], [79, 87], [88, 102], [103, 104], [104, 107], [107, 108], [108, 109], [110, 119], [120, 122], [123, 127], [128, 134], [135, 137], [138, 144], [145, 153], [154, 157], [158, 165], [166, 173], [174, 176], [177, 180], [181, 191], [192, 194], [195, 202], [203, 205], [206, 210], [210, 211]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalization", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear fractional programming (LFP) is a generalization of linear programming (LP).", "token2charspan": [[0, 6], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 15, "misc"], [16, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 15, "win-defeat", "", false, false], [8, 15, 16, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Machine", "Learning", "Conference", "in", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the International Machine Learning Conference in 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 101], [102, 110], [111, 121], [122, 124], [125, 129], [130, 133], [134, 138], [138, 139]]}
{"doc_key": "ai-train-57", "ner": [[11, 11, "product"], [13, 13, "programlang"], [26, 28, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", ",", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "deploy", "the", "developed", "neural", "network", "in", "these", "frameworks", "as", "legacy", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks, such as .NET and Java, component-based development environments are able to deploy the developed neural network in these frameworks as legacy components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [45, 46], [47, 51], [52, 54], [55, 59], [60, 63], [64, 68], [68, 69], [70, 79], [79, 80], [80, 85], [86, 97], [98, 110], [111, 114], [115, 119], [120, 122], [123, 129], [130, 133], [134, 143], [144, 150], [151, 158], [159, 161], [162, 167], [168, 178], [179, 181], [182, 188], [189, 199], [199, 200]]}
{"doc_key": "ai-train-58", "ner": [[2, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "with", "the", "algorithm", "first", "creating", "an", "alignment", "(", "see", "illustrations", ")", "between", "two", "sentences", ",", "a", "candidate", "translation", "string", "and", "a", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, with the algorithm first creating an alignment (see illustrations) between two sentences, a candidate translation string and a reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 64], [65, 68], [69, 78], [79, 84], [85, 93], [94, 96], [97, 106], [107, 108], [108, 111], [112, 125], [125, 126], [127, 134], [135, 138], [139, 148], [148, 149], [150, 151], [152, 161], [162, 173], [174, 180], [181, 184], [185, 186], [187, 196], [197, 208], [209, 215], [215, 216]]}
{"doc_key": "ai-train-59", "ner": [[6, 12, "conference"], [19, 19, "task"], [21, 23, "task"], [25, 35, "metrics"], [27, 35, "metrics"], [40, 43, "conference"], [45, 45, "conference"], [48, 48, "location"], [50, 50, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 12, 19, 19, "related-to", "subject_at", false, false], [6, 12, 21, 23, "related-to", "subject_at", false, false], [25, 35, 6, 12, "temporal", "", false, false], [27, 35, 25, 35, "named", "", true, false], [45, 45, 40, 43, "named", "", false, false], [48, 48, 50, 50, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "in", "NIST", "'s", "annual", "document", "understanding", "conferences", "where", "research", "groups", "present", "their", "systems", "for", "summarization", "and", "translation", "tasks", "is", "the", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "metric", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used in NIST's annual document understanding conferences where research groups present their systems for summarization and translation tasks is the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 31], [31, 33], [34, 40], [41, 49], [50, 63], [64, 75], [76, 81], [82, 90], [91, 97], [98, 105], [106, 111], [112, 119], [120, 123], [124, 137], [138, 141], [142, 153], [154, 159], [160, 162], [163, 166], [167, 172], [173, 174], [174, 180], [180, 181], [181, 189], [190, 200], [201, 204], [205, 212], [213, 223], [223, 224], [225, 231], [231, 232], [233, 235], [236, 244], [245, 247], [248, 254], [255, 266], [267, 277], [278, 285], [286, 287], [287, 291], [291, 292], [292, 293], [294, 302], [302, 303], [304, 310], [310, 311], [312, 320], [321, 322], [323, 327], [327, 328]]}
{"doc_key": "ai-train-60", "ner": [[5, 5, "programlang"], [7, 7, "product"], [10, 11, "programlang"], [14, 14, "product"], [18, 20, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 5, 10, 11, "type-of", "", false, false], [5, 5, 18, 20, "named", "", false, false], [7, 7, 10, 11, "part-of", "", false, false], [7, 7, 14, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", "to", "run", "in", "Java", "with", "JShell", "(", "minimum", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation to run in Java with JShell (minimum Java 9): codejshell scriptfile/codesyntaxhighlight lang=java", "token2charspan": [[0, 4], [5, 19], [20, 22], [23, 26], [27, 29], [30, 34], [35, 39], [40, 46], [47, 48], [48, 55], [56, 60], [61, 62], [62, 63], [63, 64], [65, 75], [76, 86], [86, 87], [87, 106], [107, 111], [111, 112], [112, 116]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLEU", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLEU metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 27, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 27, 10, 12, "origin", "", false, false], [24, 27, 15, 17, "origin", "", false, false], [24, 27, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "started", "a", "joint", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "additional", "constraint", "that", "the", "edges", "are", "restricted", "to", "a", "limited", "set", "of", "possible", "connections", "to", "facilitate", "algebra", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, started a joint project called Knowledge Graphs, which are semantic networks, but with the additional constraint that the edges are restricted to a limited set of possible connections to facilitate algebra on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 110], [111, 116], [117, 124], [125, 131], [132, 141], [142, 148], [148, 149], [150, 155], [156, 159], [160, 168], [169, 177], [177, 178], [179, 182], [183, 187], [188, 191], [192, 202], [203, 213], [214, 218], [219, 222], [223, 228], [229, 232], [233, 243], [244, 246], [247, 248], [249, 256], [257, 260], [261, 263], [264, 272], [273, 284], [285, 287], [288, 298], [299, 306], [307, 309], [310, 313], [314, 319], [319, 320]]}
{"doc_key": "ai-train-63", "ner": [[0, 1, "product"], [17, 20, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 17, 20, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "word", "processor", ",", "but", "are", "also", "available", "as", "a", "standalone", "application", "that", "can", "be", "activated", "by", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but are also available as a standalone application that can be activated by programs that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [76, 77], [78, 82], [83, 85], [86, 87], [88, 92], [93, 102], [102, 103], [104, 107], [108, 111], [112, 116], [117, 126], [127, 129], [130, 131], [132, 142], [143, 154], [155, 159], [160, 163], [164, 166], [167, 176], [177, 179], [180, 188], [189, 193], [194, 198], [199, 203], [204, 212], [213, 217], [217, 218]]}
{"doc_key": "ai-train-64", "ner": [[5, 12, "organisation"], [15, 21, "conference"], [25, 31, "organisation"], [34, 36, "conference"], [38, 40, "conference"], [43, 45, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "Fellow", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "the", "Society", "for", "Cognitive", "Science", ",", "and", "an", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a Fellow of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence, and the Society for Cognitive Science, and an editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [137, 138], [139, 142], [143, 146], [147, 154], [155, 158], [159, 168], [169, 176], [176, 177], [178, 181], [182, 184], [185, 191], [192, 194], [195, 197], [198, 207], [208, 217], [217, 218], [219, 221], [222, 230], [231, 239], [239, 240], [241, 244], [245, 247], [248, 255], [256, 264], [264, 265]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 7, "algorithm"], [11, 11, "task"], [21, 22, "researcher"], [24, 25, "university"], [27, 28, "researcher"], [30, 33, "organisation"], [35, 35, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 11, 11, "type-of", "", false, false], [0, 2, 21, 22, "origin", "", false, false], [0, 2, 27, 28, "origin", "", false, false], [4, 7, 0, 2, "named", "", false, false], [21, 22, 24, 25, "physical", "", false, false], [21, 22, 24, 25, "role", "", false, false], [27, 28, 30, 33, "role", "", false, false], [35, 35, 30, 33, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "be", "developed", "with", "the", "work", "of", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear Predictive Coding (LPC), a form of speech coding, began to be developed with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 68], [69, 78], [79, 83], [84, 87], [88, 92], [93, 95], [96, 104], [105, 112], [113, 115], [116, 122], [123, 133], [134, 137], [138, 143], [144, 149], [150, 152], [153, 159], [160, 169], [170, 173], [174, 183], [184, 185], [185, 188], [188, 189], [190, 192], [193, 197], [197, 198]]}
{"doc_key": "ai-train-66", "ner": [[58, 60, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "additionally", "ergodic", ",", "all", "sample", "paths", "show", "the", "same", "mean", "over", "time", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "mean", "squared", "error", "."], "sentence-detokenized": "If the signal is additionally ergodic, all sample paths show the same mean over time and thus mathR _ x^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x^ {n / T _ 0} (\\ tau) / math in the sense of mean squared error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 29], [30, 37], [37, 38], [39, 42], [43, 49], [50, 55], [56, 60], [61, 64], [65, 69], [70, 74], [75, 79], [80, 84], [85, 88], [89, 93], [94, 99], [100, 101], [102, 103], [103, 104], [105, 106], [106, 107], [108, 109], [110, 111], [112, 113], [114, 115], [115, 116], [117, 118], [118, 119], [120, 123], [123, 124], [125, 127], [128, 135], [136, 137], [137, 138], [138, 139], [140, 141], [142, 143], [143, 144], [145, 146], [146, 147], [148, 149], [150, 151], [152, 153], [154, 155], [155, 156], [157, 158], [158, 159], [160, 163], [163, 164], [165, 166], [167, 171], [172, 174], [175, 178], [179, 184], [185, 187], [188, 192], [193, 200], [201, 206], [206, 207]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 9, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [35, 37, "algorithm"], [39, 39, "algorithm"], [44, 48, "misc"], [49, 51, "algorithm"], [54, 55, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[13, 15, 44, 48, "related-to", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 44, 48, "related-to", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 44, 48, "related-to", "", false, false], [31, 31, 27, 29, "named", "", false, false], [35, 37, 44, 48, "related-to", "", false, false], [39, 39, 35, 37, "named", "", false, false], [49, 51, 54, 55, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", ",", "or", "nonnegative", "matrix", "factorization", "(", "NMF", ")", "techniques", "as", "a", "preprocessing", "step", ",", "followed", "by", "K", "-", "NN", "clustering", "on", "feature", "vectors", "in", "a", "dimensionality", "-", "reduced", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or nonnegative matrix factorization (NMF) techniques as a preprocessing step, followed by K-NN clustering on feature vectors in a dimensionality-reduced space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [157, 158], [159, 168], [169, 180], [181, 189], [190, 191], [191, 194], [194, 195], [195, 196], [197, 199], [200, 211], [212, 218], [219, 232], [233, 234], [234, 237], [237, 238], [239, 249], [250, 252], [253, 254], [255, 268], [269, 273], [273, 274], [275, 283], [284, 286], [287, 288], [288, 289], [289, 291], [292, 302], [303, 305], [306, 313], [314, 321], [322, 324], [325, 326], [327, 341], [341, 342], [342, 349], [350, 355], [355, 356]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 10, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 10, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [11, 15, "task"], [31, 32, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 11, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognizing", "named", "entities", "in", "the", "text", "is", "called", "Named", "Entity", "Recognition", ",", "and", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "Entity", "Matching", "."], "sentence-detokenized": "The task of recognizing named entities in the text is called Named Entity Recognition, and the task of determining the identity of named entities mentioned in the text is called Entity Matching.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 60], [61, 66], [67, 73], [74, 85], [85, 86], [87, 90], [91, 94], [95, 99], [100, 102], [103, 114], [115, 118], [119, 127], [128, 130], [131, 136], [137, 145], [146, 155], [156, 158], [159, 162], [163, 167], [168, 170], [171, 177], [178, 184], [185, 193], [193, 194]]}
{"doc_key": "ai-train-70", "ner": [[0, 1, "algorithm"], [27, 27, "programlang"], [29, 31, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 29, 31, "part-of", "", true, false], [29, 31, 27, 27, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "and", "from", "version", "0.8.0", "onwards", "they", "were", "released", "in", "a", "separate", "R", "package", "sigmoid", "to", "allow", "wider", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, and from version 0.8.0 onwards they were released in a separate R package sigmoid to allow wider use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 102], [103, 107], [108, 115], [116, 121], [122, 129], [130, 134], [135, 139], [140, 148], [149, 151], [152, 153], [154, 162], [163, 164], [165, 172], [173, 180], [181, 183], [184, 189], [190, 195], [196, 199], [199, 200]]}
{"doc_key": "ai-train-71", "ner": [[0, 1, "programlang"], [10, 14, "organisation"], [16, 16, "organisation"], [19, 19, "location"], [21, 23, "location"], [24, 25, "researcher"], [27, 28, "researcher"], [30, 31, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 1, 24, 25, "artifact", "", true, false], [0, 1, 27, 28, "artifact", "", true, false], [0, 1, 30, 31, "artifact", "", true, false], [16, 16, 10, 14, "named", "", false, false], [16, 16, 19, 19, "physical", "", false, false], [19, 19, 21, 23, "physical", "", false, false], [24, 25, 10, 14, "role", "", false, false], [27, 28, 10, 14, "role", "", false, false], [30, 31, 10, 14, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "at", "the", "research", "firm", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", "in", "Cambridge", ",", "Massachusetts", ",", "by", "Wally", "Voerzig", ",", "Cynthia", "Solomon", "and", "Seymour", "Pappert", "."], "sentence-detokenized": "The logo was created in 1967 at the research firm Bolt, Beranek and Newman (BBN) in Cambridge, Massachusetts, by Wally Voerzig, Cynthia Solomon and Seymour Pappert.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 35], [36, 44], [45, 49], [50, 54], [54, 55], [56, 63], [64, 67], [68, 74], [75, 76], [76, 79], [79, 80], [81, 83], [84, 93], [93, 94], [95, 108], [108, 109], [110, 112], [113, 118], [119, 126], [126, 127], [128, 135], [136, 143], [144, 147], [148, 155], [156, 163], [163, 164]]}
{"doc_key": "ai-train-72", "ner": [[0, 0, "misc"], [8, 9, "field"], [17, 18, "field"], [22, 23, "algorithm"], [26, 28, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 8, 9, "part-of", "", false, false], [0, 0, 17, 18, "compare", "", false, false], [22, 23, 17, 18, "part-of", "", false, false], [26, 28, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "contrasted", "with", "conventional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and can be contrasted with conventional deep learning techniques that use gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 86], [87, 89], [90, 100], [101, 105], [106, 118], [119, 123], [124, 132], [133, 143], [144, 148], [149, 152], [153, 161], [162, 169], [170, 172], [173, 174], [175, 181], [182, 189], [190, 194], [195, 196], [197, 202], [203, 211], [211, 212]]}
{"doc_key": "ai-train-73", "ner": [[4, 5, "algorithm"], [58, 60, "metrics"], [62, 62, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[62, 62, 58, 60, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "the", "least", "squares", "method", "to", "fit", "a", "function", "in", "the", "form", "of", "a", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264", "n", "/", "sub", ",", "we", "can", "estimate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use the least squares method to fit a function in the form of a hyperplane \u0177 = a + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264 n / sub, we can estimate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 13], [14, 19], [20, 27], [28, 34], [35, 37], [38, 41], [42, 43], [44, 52], [53, 55], [56, 59], [60, 64], [65, 67], [68, 69], [70, 80], [81, 82], [83, 84], [85, 86], [87, 88], [89, 90], [91, 95], [96, 97], [98, 101], [102, 103], [104, 106], [107, 110], [111, 115], [116, 117], [117, 118], [119, 122], [123, 124], [125, 126], [127, 130], [130, 131], [132, 133], [134, 137], [138, 139], [140, 141], [142, 145], [145, 146], [147, 150], [151, 152], [153, 154], [155, 156], [157, 158], [159, 160], [161, 162], [163, 166], [166, 167], [168, 170], [171, 174], [175, 183], [184, 187], [188, 191], [192, 197], [198, 201], [202, 206], [207, 214], [215, 220], [221, 222], [222, 225], [225, 226], [226, 227]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [47, 49, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 152], [152, 153], [154, 160], [160, 161], [162, 171], [171, 172], [173, 178], [179, 185], [185, 186], [187, 192], [192, 193], [194, 200], [200, 201], [202, 210], [210, 211], [212, 218], [219, 222], [223, 226], [227, 233], [234, 241], [241, 242]]}
{"doc_key": "ai-train-75", "ner": [[2, 2, "misc"], [5, 8, "field"], [13, 13, "organisation"], [16, 20, "university"], [27, 29, "organisation"], [31, 38, "university"], [42, 43, "university"], [45, 46, "university"], [49, 51, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[2, 2, 5, 8, "topic", "", false, false], [2, 2, 13, 13, "origin", "", false, false], [2, 2, 16, 20, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "Electrical", "and", "Computer", "Engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "as", "well", "as", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in Electrical and Computer Engineering (2000) from Inria and the University of Nice Sophia Antipolis and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, as well as visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 90], [91, 93], [94, 98], [99, 105], [106, 115], [116, 119], [120, 123], [124, 128], [129, 138], [139, 148], [149, 151], [152, 159], [160, 169], [170, 180], [180, 181], [182, 187], [188, 191], [192, 197], [198, 207], [207, 208], [209, 211], [212, 216], [217, 219], [220, 228], [229, 238], [239, 241], [242, 249], [250, 260], [260, 261], [262, 266], [267, 277], [278, 281], [282, 285], [286, 296], [297, 299], [300, 307], [307, 308]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [10, 10, "researcher"], [14, 15, "product"], [18, 19, "country"], [21, 25, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 7, 8, "role", "licensing_patent_to", false, false], [10, 10, 18, 19, "physical", "", false, false], [21, 25, 10, 10, "artifact", "", false, false], [21, 25, 14, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Licensing", "the", "original", "patent", "issued", "to", "inventor", "George", "Devoll", ",", "Engelberger", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Licensing the original patent issued to inventor George Devoll, Engelberger developed the first industrial robot in the United States, Unimate, in the 1950s.", "token2charspan": [[0, 9], [10, 13], [14, 22], [23, 29], [30, 36], [37, 39], [40, 48], [49, 55], [56, 62], [62, 63], [64, 75], [76, 85], [86, 89], [90, 95], [96, 106], [107, 112], [113, 115], [116, 119], [120, 126], [127, 133], [133, 134], [135, 142], [142, 143], [144, 146], [147, 150], [151, 156], [156, 157]]}
{"doc_key": "ai-train-77", "ner": [[4, 6, "task"], [12, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[3, 3, "programlang"], [6, 6, "programlang"], [14, 15, "programlang"], [17, 19, "programlang"], [27, 27, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 3, 14, 15, "named", "", false, false], [6, 6, 3, 3, "origin", "descendant_of", false, false], [6, 6, 17, 19, "general-affiliation", "", false, false], [6, 6, 27, 27, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Descendants", "of", "the", "CLIPS", "language", "include", "Jess", "(", "a", "rules", "-", "based", "part", "of", "CLIPS", "rewritten", "in", "Java", "that", "later", "evolved", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by", "the"], "sentence-detokenized": "Descendants of the CLIPS language include Jess (a rules-based part of CLIPS rewritten in Java that later evolved in a different direction), JESS was originally inspired by the", "token2charspan": [[0, 11], [12, 14], [15, 18], [19, 24], [25, 33], [34, 41], [42, 46], [47, 48], [48, 49], [50, 55], [55, 56], [56, 61], [62, 66], [67, 69], [70, 75], [76, 85], [86, 88], [89, 93], [94, 98], [99, 104], [105, 112], [113, 115], [116, 117], [118, 127], [128, 137], [137, 138], [138, 139], [140, 144], [145, 148], [149, 159], [160, 168], [169, 171], [172, 175]]}
{"doc_key": "ai-train-79", "ner": [[8, 8, "product"], [11, 14, "product"], [17, 19, "organisation"], [22, 23, "product"], [41, 42, "product"], [44, 48, "product"], [63, 64, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[11, 14, 8, 8, "type-of", "", false, false], [17, 19, 11, 14, "usage", "", false, false], [22, 23, 17, 19, "artifact", "", false, false], [41, 42, 17, 19, "origin", "", true, false], [41, 42, 63, 64, "related-to", "", true, false], [44, 48, 17, 19, "origin", "", true, false], [44, 48, 63, 64, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["It", "has", "also", "created", "flexible", "intelligent", "applications", "for", "AGVs", ",", "designing", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", "used", "for", "complex", "pick", "and", "place", "operations", ",", "in", "combination", "with", "gantry", "systems", "and", "industrial", "robotic", "arms", "used", "in", "tier", "-", "one", "vehicle", "delivery", "plants", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "circuits", "."], "sentence-detokenized": "It has also created flexible intelligent applications for AGVs, designing the Motivity control system used by RMT Robotics to develop the ADAM iAGV (Self-Guided Vehicle) used for complex pick and place operations, in combination with gantry systems and industrial robotic arms used in tier-one vehicle delivery plants to move products from process to process in non-linear circuits.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 19], [20, 28], [29, 40], [41, 53], [54, 57], [58, 62], [62, 63], [64, 73], [74, 77], [78, 86], [87, 94], [95, 101], [102, 106], [107, 109], [110, 113], [114, 122], [123, 125], [126, 133], [134, 137], [138, 142], [143, 147], [148, 149], [149, 153], [153, 154], [154, 160], [161, 168], [168, 169], [170, 174], [175, 178], [179, 186], [187, 191], [192, 195], [196, 201], [202, 212], [212, 213], [214, 216], [217, 228], [229, 233], [234, 240], [241, 248], [249, 252], [253, 263], [264, 271], [272, 276], [277, 281], [282, 284], [285, 289], [289, 290], [290, 293], [294, 301], [302, 310], [311, 317], [318, 320], [321, 325], [326, 334], [335, 339], [340, 347], [348, 350], [351, 358], [359, 361], [362, 372], [373, 381], [381, 382]]}
{"doc_key": "ai-train-80", "ner": [[8, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "\u03b2", "parameters", "are", "usually", "estimated", "using", "the", "maximum", "likelihood", "method", "."], "sentence-detokenized": "The \u03b2 parameters are usually estimated using the maximum likelihood method.", "token2charspan": [[0, 3], [4, 5], [6, 16], [17, 20], [21, 28], [29, 38], [39, 44], [45, 48], [49, 56], [57, 67], [68, 74], [74, 75]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 0, 1, "part-of", "", false, false], [8, 8, 0, 1, "part-of", "", false, false], [10, 10, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", ",", "such", "as", "precision", "and", "recall", "or", "DCG", ",", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics, such as precision and recall or DCG, are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [29, 30], [31, 35], [36, 38], [39, 48], [49, 52], [53, 59], [60, 62], [63, 66], [66, 67], [68, 71], [72, 78], [79, 82], [83, 92], [93, 96], [97, 104], [105, 107], [108, 109], [110, 124], [125, 131], [131, 132]]}
{"doc_key": "ai-train-82", "ner": [[9, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "there", "are", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "workers", "."], "sentence-detokenized": "In a typical factory, there are hundreds of industrial robots working on fully automated production lines, with one robot for every ten workers.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 27], [28, 31], [32, 40], [41, 43], [44, 54], [55, 61], [62, 69], [70, 72], [73, 78], [79, 88], [89, 99], [100, 105], [105, 106], [107, 111], [112, 115], [116, 121], [122, 125], [126, 131], [132, 135], [136, 143], [143, 144]]}
{"doc_key": "ai-train-83", "ner": [[5, 7, "product"], [13, 14, "field"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [31, 31, "task"], [34, 35, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[13, 14, 5, 7, "usage", "", false, true], [18, 19, 13, 14, "part-of", "", false, false], [21, 22, 13, 14, "part-of", "", false, false], [24, 25, 13, 14, "part-of", "", false, false], [27, 28, 13, 14, "part-of", "", false, false], [31, 31, 13, 14, "part-of", "", false, false], [34, 35, 13, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", "including", ":", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "enhancement", ",", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications including: image segmentation, feature generation, face extraction, motion detection, region enhancement, and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [89, 98], [98, 99], [100, 105], [106, 118], [118, 119], [120, 127], [128, 138], [138, 139], [140, 144], [145, 155], [155, 156], [157, 163], [164, 173], [173, 174], [175, 181], [182, 193], [193, 194], [195, 198], [199, 204], [205, 214], [214, 215]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 17, "field"], [22, 24, "misc"], [28, 34, "conference"], [36, 36, "conference"], [40, 45, "misc"], [46, 50, "conference"], [51, 51, "conference"], [55, 59, "conference"], [61, 61, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 16, 17, "related-to", "contributes_to", false, false], [0, 0, 22, 24, "win-defeat", "", false, false], [0, 0, 40, 45, "win-defeat", "", false, false], [22, 24, 28, 34, "temporal", "", false, false], [36, 36, 28, 34, "named", "", false, false], [40, 45, 46, 50, "temporal", "", false, false], [40, 45, 55, 59, "temporal", "", false, false], [51, 51, 46, 50, "named", "", false, false], [61, 61, 55, 59, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", ",", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "2012", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "and", "the", "Best", "Reviewer", "Award", "at", "the", "2012", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "and", "the", "2015", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision, and won the Best Paper Award at the 2012 International Conference on Non-Photorealistic Rendering and Animation (NPAR) and the Best Reviewer Award at the 2012 Asian Conference on Computer Vision ACCV and the 2015 International Conference on Computer Vision (ICCV).", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [110, 111], [112, 115], [116, 119], [120, 123], [124, 128], [129, 134], [135, 140], [141, 143], [144, 147], [148, 152], [153, 166], [167, 177], [178, 180], [181, 199], [200, 209], [210, 213], [214, 223], [224, 225], [225, 229], [229, 230], [231, 234], [235, 238], [239, 243], [244, 252], [253, 258], [259, 261], [262, 265], [266, 270], [271, 276], [277, 287], [288, 290], [291, 299], [300, 306], [307, 311], [312, 315], [316, 319], [320, 324], [325, 338], [339, 349], [350, 352], [353, 361], [362, 368], [369, 370], [370, 374], [374, 375], [375, 376]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 14, "researcher"], [15, 18, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 2, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [15, 18, 0, 0, "usage", "", false, false], [15, 18, 13, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "an", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is an ontology language used by Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 67], [68, 76], [77, 81], [82, 84], [85, 89], [90, 95], [95, 97], [98, 101], [102, 112], [113, 120], [120, 121]]}
{"doc_key": "ai-train-86", "ner": [[3, 4, "task"], [6, 9, "metrics"], [16, 21, "metrics"], [24, 35, "metrics"], [40, 43, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 9, 3, 4, "part-of", "", false, false], [16, 21, 6, 9, "named", "", false, false], [24, 35, 6, 9, "named", "", false, false], [40, 43, 6, 9, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", ",", "in", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "error", "of", "the", "prediction", "or", "the", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "of", "the", "squared", "deviations", "of", "the", "predictions", "from", "the", "TRUE", "values", "in", "the", "out", "-", "of", "-", "sample", "test", "space", "generated", "by", "a", "model", "estimated", "in", "a", "particular", "sample", "space", "."], "sentence-detokenized": "Also, in regression analysis, the mean squared error, often referred to as the mean squared error of the prediction or the out-of-sample mean squared error, can refer to the average of the squared deviations of the predictions from the TRUE values in the out-of-sample test space generated by a model estimated in a particular sample space.", "token2charspan": [[0, 4], [4, 5], [6, 8], [9, 19], [20, 28], [28, 29], [30, 33], [34, 38], [39, 46], [47, 52], [52, 53], [54, 59], [60, 68], [69, 71], [72, 74], [75, 78], [79, 83], [84, 91], [92, 97], [98, 100], [101, 104], [105, 115], [116, 118], [119, 122], [123, 126], [126, 127], [127, 129], [129, 130], [130, 136], [137, 141], [142, 149], [150, 155], [155, 156], [157, 160], [161, 166], [167, 169], [170, 173], [174, 181], [182, 184], [185, 188], [189, 196], [197, 207], [208, 210], [211, 214], [215, 226], [227, 231], [232, 235], [236, 240], [241, 247], [248, 250], [251, 254], [255, 258], [258, 259], [259, 261], [261, 262], [262, 268], [269, 273], [274, 279], [280, 289], [290, 292], [293, 294], [295, 300], [301, 310], [311, 313], [314, 315], [316, 326], [327, 333], [334, 339], [339, 340]]}
{"doc_key": "ai-train-87", "ner": [[6, 8, "algorithm"], [10, 11, "algorithm"], [20, 23, "algorithm"], [36, 39, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 8, 10, 11, "compare", "", false, false], [6, 8, 20, 23, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "terms", "of", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "reasonably", "well", ",", "with", "the", "C", "-", "HOG", "descriptors", "retaining", "a", "slight", "advantage", "in", "terms", "of", "missed", "detection", "rates", "for", "fixed", "false", "positive", "rates", "in", "both", "datasets", "."], "sentence-detokenized": "In terms of results, the C-HOG and R-HOG block descriptors perform reasonably well, with the C-HOG descriptors retaining a slight advantage in terms of missed detection rates for fixed false positive rates in both datasets.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 19], [19, 20], [21, 24], [25, 26], [26, 27], [27, 30], [31, 34], [35, 37], [37, 40], [41, 46], [47, 58], [59, 66], [67, 77], [78, 82], [82, 83], [84, 88], [89, 92], [93, 94], [94, 95], [95, 98], [99, 110], [111, 120], [121, 122], [123, 129], [130, 139], [140, 142], [143, 148], [149, 151], [152, 158], [159, 168], [169, 174], [175, 178], [179, 184], [185, 190], [191, 199], [200, 205], [206, 208], [209, 213], [214, 222], [222, 223]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [17, 19, "algorithm"], [21, 23, "algorithm"], [25, 27, "algorithm"], [29, 30, "misc"], [36, 38, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 29, 30, "usage", "", false, false], [14, 15, 29, 30, "usage", "", false, false], [17, 19, 29, 30, "usage", "", false, false], [21, 23, 29, 30, "usage", "", false, false], [25, 27, 29, 30, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "Fisher", "'s", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neural", "-", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using Fisher's algorithm, hidden Markov model, multilinear subspace learning using tensor representation, and neural-motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 145], [145, 147], [148, 157], [157, 158], [159, 165], [166, 172], [173, 178], [178, 179], [180, 191], [192, 200], [201, 209], [210, 215], [216, 222], [223, 237], [237, 238], [239, 242], [243, 249], [249, 250], [250, 259], [260, 267], [268, 272], [273, 281], [281, 282]]}
{"doc_key": "ai-train-89", "ner": [[2, 7, "misc"], [17, 19, "location"], [39, 41, "location"], [55, 55, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 19, 2, 7, "temporal", "", false, false], [39, 41, 2, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "can", "no", "longer", "be", "screened", "at", "the", "Scotiabank", "Theatre", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "can", "be", "screened", "at", "other", "venues", "(", "such", "as", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "by", "a", "service", "like", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films can no longer be screened at the Scotiabank Theatre Toronto - one of the festival's main venues - and can be screened at other venues (such as TIFF Bell Lightbox and other local cinemas) if they are distributed by a service like Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 69], [70, 72], [73, 79], [80, 82], [83, 91], [92, 94], [95, 98], [99, 109], [110, 117], [118, 125], [126, 127], [128, 131], [132, 134], [135, 138], [139, 147], [147, 149], [150, 154], [155, 161], [162, 163], [164, 167], [168, 171], [172, 174], [175, 183], [184, 186], [187, 192], [193, 199], [200, 201], [201, 205], [206, 208], [209, 213], [214, 218], [219, 227], [228, 231], [232, 237], [238, 243], [244, 251], [251, 252], [253, 255], [256, 260], [261, 264], [265, 276], [277, 279], [280, 281], [282, 289], [290, 294], [295, 302], [302, 303]]}
{"doc_key": "ai-train-90", "ner": [[3, 3, "organisation"], [5, 11, "researcher"], [8, 9, "organisation"], [12, 12, "researcher"], [18, 22, "product"], [32, 32, "researcher"], [33, 39, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[3, 3, 8, 9, "related-to", "purchases", false, false], [5, 11, 12, 12, "named", "same", false, false], [5, 11, 32, 32, "named", "same", false, false], [8, 9, 5, 11, "origin", "founded_by", false, false], [18, 22, 3, 3, "artifact", "", false, false], [33, 39, 32, 32, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1977", ",", "Unimation", "bought", "Victor", "Shainman", "'s", "Vicarm", "Inc.", "and", "with", "his", "help", "created", "and", "began", "manufacturing", "the", "Programmable", "Universal", "Assembly", "Machine", ",", "a", "new", "model", "of", "robotic", "arm", ",", "using", "Shainman", "'s", "cutting", "-", "edge", "VAL", "programming", "language", "."], "sentence-detokenized": "In 1977, Unimation bought Victor Shainman's Vicarm Inc. and with his help created and began manufacturing the Programmable Universal Assembly Machine, a new model of robotic arm, using Shainman's cutting-edge VAL programming language.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 18], [19, 25], [26, 32], [33, 41], [41, 43], [44, 50], [51, 55], [56, 59], [60, 64], [65, 68], [69, 73], [74, 81], [82, 85], [86, 91], [92, 105], [106, 109], [110, 122], [123, 132], [133, 141], [142, 149], [149, 150], [151, 152], [153, 156], [157, 162], [163, 165], [166, 173], [174, 177], [177, 178], [179, 184], [185, 193], [193, 195], [196, 203], [203, 204], [204, 208], [209, 212], [213, 224], [225, 233], [233, 234]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [5, 6, "programlang"], [10, 11, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 5, 6, "general-affiliation", "", false, false], [0, 1, 10, 11, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[0, 2, "metrics"], [13, 14, "product"], [20, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 13, 14, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", "according", "to", "Google", "Scholar", ",", "It", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "'s", "2016", "Sustained", "Impact", "Award", ",", "which", "is", "indicative", "of", "a", "paper", "with", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited more than 20,000 times according to Google Scholar, It also received the IEEE Signal Processing Society's 2016 Sustained Impact Award, which is indicative of a paper with unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 44], [45, 51], [52, 57], [58, 67], [68, 70], [71, 77], [78, 85], [85, 86], [87, 89], [90, 94], [95, 103], [104, 107], [108, 112], [113, 119], [120, 130], [131, 138], [138, 140], [141, 145], [146, 155], [156, 162], [163, 168], [168, 169], [170, 175], [176, 178], [179, 189], [190, 192], [193, 194], [195, 200], [201, 205], [206, 215], [216, 220], [221, 227], [228, 231], [232, 234], [235, 240], [241, 243], [244, 249], [250, 255], [256, 267], [267, 268]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [18, 26, "product"], [35, 37, "product"], [40, 40, "organisation"], [41, 43, "product"], [44, 44, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 40, 40, "artifact", "", false, false], [18, 26, 0, 1, "related-to", "performs", false, false], [18, 26, 35, 37, "part-of", "", false, false], [40, 40, 44, 44, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "poised", "to", "become", "completely", "indistinguishable", "from", "the", "real", "human", "voice", "with", "the", "2016", "launch", "of", "Adobe", "Voco", "voice", "editing", "and", "generation", "software", ",", "a", "prototype", "that", "is", "planned", "to", "be", "part", "of", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "Google", "prototype", "."], "sentence-detokenized": "Speech synthesis is poised to become completely indistinguishable from the real human voice with the 2016 launch of Adobe Voco voice editing and generation software, a prototype that is planned to be part of Adobe Creative Suite, and DeepMind WaveNet, a Google prototype.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 26], [27, 29], [30, 36], [37, 47], [48, 65], [66, 70], [71, 74], [75, 79], [80, 85], [86, 91], [92, 96], [97, 100], [101, 105], [106, 112], [113, 115], [116, 121], [122, 126], [127, 132], [133, 140], [141, 144], [145, 155], [156, 164], [164, 165], [166, 167], [168, 177], [178, 182], [183, 185], [186, 193], [194, 196], [197, 199], [200, 204], [205, 207], [208, 213], [214, 222], [223, 228], [228, 229], [230, 233], [234, 242], [243, 250], [250, 251], [252, 253], [254, 260], [261, 270], [270, 271]]}
{"doc_key": "ai-train-94", "ner": [[0, 0, "researcher"], [7, 11, "organisation"], [15, 22, "organisation"], [26, 26, "conference"], [34, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 11, "role", "", false, false], [0, 0, 15, 22, "role", "", false, false], [0, 0, 26, 26, "role", "", false, false], [0, 0, 34, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "Honorary", "Fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "Fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "member", "of", "AAAI", ",", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an Honorary Fellow of the Neuroscience Research Program, a Fellow of the American Academy of Arts and Sciences, a founding member of AAAI, and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 123], [124, 132], [133, 139], [140, 142], [143, 147], [147, 148], [149, 152], [153, 154], [155, 163], [164, 170], [171, 173], [174, 177], [178, 186], [187, 196], [197, 200], [201, 206], [207, 215], [215, 216]]}
{"doc_key": "ai-train-95", "ner": [[8, 11, "task"], [11, 11, "task"], [15, 19, "task"], [23, 23, "misc"], [24, 25, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 11, 15, 19, "cause-effect", "", false, false], [11, 11, 15, 19, "cause-effect", "", false, false], [24, 25, 15, 19, "topic", "", false, false], [24, 25, 23, 23, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "encouraged", "by", "successes", "in", "speech", "recognition", "and", "synthesis", ",", "research", "in", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, encouraged by successes in speech recognition and synthesis, research in speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 37], [38, 40], [41, 47], [48, 59], [60, 63], [64, 73], [73, 74], [75, 83], [84, 86], [87, 93], [94, 105], [106, 111], [112, 116], [117, 120], [121, 132], [133, 135], [136, 139], [140, 146], [147, 156], [157, 164], [164, 165]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 16, "algorithm"], [20, 21, "algorithm"], [25, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 16, 3, 4, "origin", "", false, false], [15, 16, 8, 9, "origin", "", false, false], [15, 16, 11, 12, "origin", "", false, false], [15, 16, 25, 26, "part-of", "", false, false], [20, 21, 15, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisor", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forget", "gate", "(", "also", "called", "keep", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisor J\u00fcrgen Schmidhuber and Fred Cummins introduced the forget gate (also called keep gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 35], [36, 42], [43, 54], [55, 58], [59, 63], [64, 71], [72, 82], [83, 86], [87, 93], [94, 98], [99, 100], [100, 104], [105, 111], [112, 116], [117, 121], [121, 122], [123, 127], [128, 131], [132, 136], [137, 149], [149, 150]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [8, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[8, 11, 1, 3, "part-of", "", false, false], [8, 11, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "function", "sinc", "is", "usually", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalized function sinc is usually defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 76], [77, 81], [82, 84], [85, 92], [93, 100], [101, 103], [104, 111]]}
{"doc_key": "ai-train-98", "ner": [[3, 4, "field"], [11, 14, "researcher"], [19, 22, "conference"], [25, 29, "organisation"], [31, 31, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 4, 11, 14, "origin", "coined_term", false, false], [11, 14, 19, 22, "role", "", false, false], [11, 14, 25, 29, "role", "", false, false], [31, 31, 25, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "\"", "computational", "linguistics", "\"", "itself", "was", "first", "coined", "by", "David", "Hayes", ",", "a", "founding", "member", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "for", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term \"computational linguistics\" itself was first coined by David Hayes, a founding member of the Association for Computational Linguistics and the International Committee for Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 10], [10, 23], [24, 35], [35, 36], [37, 43], [44, 47], [48, 53], [54, 60], [61, 63], [64, 69], [70, 75], [75, 76], [77, 78], [79, 87], [88, 94], [95, 97], [98, 101], [102, 113], [114, 117], [118, 131], [132, 143], [144, 147], [148, 151], [152, 165], [166, 175], [176, 179], [180, 193], [194, 205], [206, 207], [207, 211], [211, 212], [212, 213]]}
{"doc_key": "ai-train-99", "ner": [[9, 13, "misc"], [18, 18, "misc"], [34, 36, "metrics"], [38, 41, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[38, 41, 34, 36, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "October", "2011", ".", "In", "one", "-dimensional", "memory", "-", "based", "(", "or", "memoryless", ")", "DPD", ",", "in", "order", "to", "solve", "the", "coefficients", "of", "the", "digital", "pre-decoder", "polynomials", "and", "minimize", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "sampled", "at", "a", "rate", "that", "permits", "the", "capture", "of", "the", "nonlinear", "products", "of", "the", "order", "of", "the", "digital", "pre-decoder", "."], "sentence-detokenized": "59, pp. 2547-2553, October 2011. In one-dimensional memory-based (or memoryless) DPD, in order to solve the coefficients of the digital pre-decoder polynomials and minimize the mean squared error (MSE), the distorted output of the nonlinear system must be sampled at a rate that permits the capture of the nonlinear products of the order of the digital pre-decoder.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 26], [27, 31], [31, 32], [33, 35], [36, 39], [39, 51], [52, 58], [58, 59], [59, 64], [65, 66], [66, 68], [69, 79], [79, 80], [81, 84], [84, 85], [86, 88], [89, 94], [95, 97], [98, 103], [104, 107], [108, 120], [121, 123], [124, 127], [128, 135], [136, 147], [148, 159], [160, 163], [164, 172], [173, 176], [177, 181], [182, 189], [190, 195], [196, 197], [197, 200], [200, 201], [201, 202], [203, 206], [207, 216], [217, 223], [224, 226], [227, 230], [231, 240], [241, 247], [248, 252], [253, 255], [256, 263], [264, 266], [267, 268], [269, 273], [274, 278], [279, 286], [287, 290], [291, 298], [299, 301], [302, 305], [306, 315], [316, 324], [325, 327], [328, 331], [332, 337], [338, 340], [341, 344], [345, 352], [353, 364], [364, 365]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 9, "location"], [11, 12, "location"], [14, 15, "country"], [19, 19, "location"], [21, 21, "country"], [35, 42, "organisation"], [43, 46, "organisation"], [48, 48, "location"], [56, 57, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 9, 9, "physical", "", false, false], [0, 1, 43, 46, "physical", "", false, false], [0, 1, 56, 57, "role", "", false, false], [9, 9, 11, 12, "physical", "", false, false], [11, 12, 14, 15, "physical", "", false, false], [35, 42, 43, 46, "part-of", "", false, false], [43, 46, 48, 48, "physical", "", false, false], [56, 57, 35, 42, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", "(", "born", "October", "5", ",", "1947", "in", "Chisinau", ",", "Moldavian", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chisinau", ",", "Moldova", ")", ")", "is", "an", "American", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "and", "the", "leader", "of", "the", "lab", "'s", "InfoLab", "group", "."], "sentence-detokenized": "Boris Katz (born October 5, 1947 in Chisinau, Moldavian SSR, Soviet Union, (now Chisinau, Moldova)) is an American research scientist (computer scientist) at the Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and the leader of the lab's InfoLab group.", "token2charspan": [[0, 5], [6, 10], [11, 12], [12, 16], [17, 24], [25, 26], [26, 27], [28, 32], [33, 35], [36, 44], [44, 45], [46, 55], [56, 59], [59, 60], [61, 67], [68, 73], [73, 74], [75, 76], [76, 79], [80, 88], [88, 89], [90, 97], [97, 98], [98, 99], [100, 102], [103, 105], [106, 114], [115, 123], [124, 133], [134, 135], [135, 143], [144, 153], [153, 154], [155, 157], [158, 161], [162, 170], [171, 178], [179, 182], [183, 193], [194, 206], [207, 217], [218, 220], [221, 224], [225, 238], [239, 248], [249, 251], [252, 262], [263, 265], [266, 275], [276, 279], [280, 283], [284, 290], [291, 293], [294, 297], [298, 301], [301, 303], [304, 311], [312, 317], [317, 318]]}
