{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [29, 30, "field"], [31, 33, "researcher"], [35, 37, "researcher"], [39, 40, "researcher"], [42, 43, "researcher"], [45, 47, "researcher"], [49, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 20, "part-of", "", false, false], [3, 7, 19, 20, "usage", "", false, false], [3, 7, 29, 30, "part-of", "", false, false], [3, 7, 29, 30, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "meaning", "-", "based", "recommendation", "systems", "employ", "various", "techniques", ",", "including", "text", "mining", ",", "information", "retrieval", ",", "emotion", "analysis", "(", "see", "also", "Multimodal", "Emotion", "Analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to meaning-based recommendation systems employ various techniques, including text mining, information retrieval, emotion analysis (see also Multimodal Emotion Analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 50], [51, 58], [59, 65], [66, 73], [74, 84], [84, 85], [86, 95], [96, 100], [101, 107], [107, 108], [109, 120], [121, 130], [130, 131], [132, 139], [140, 148], [149, 150], [150, 153], [154, 158], [159, 169], [170, 177], [178, 186], [186, 187], [188, 191], [192, 196], [197, 205], [206, 209], [209, 210], [211, 215], [215, 216], [217, 218], [218, 219], [220, 225], [225, 226], [227, 231], [232, 235], [235, 236], [237, 241], [242, 247], [247, 248], [249, 250], [250, 251], [252, 255], [255, 256], [257, 261], [262, 267], [267, 268], [269, 273], [274, 278], [278, 279], [280, 282], [283, 285], [285, 286], [287, 288], [288, 292], [292, 293], [293, 294], [294, 295], [296, 298], [299, 300], [300, 301], [301, 302], [302, 303], [304, 310], [310, 311]]}
{"doc_key": "ai-train-2", "ner": [[9, 9, "university"], [14, 15, "researcher"], [17, 18, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[14, 15, 9, 9, "physical", "", false, false], [14, 15, 9, 9, "role", "", false, false], [17, 18, 9, 9, "physical", "", false, false], [17, 18, 9, 9, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "advocates", "for", "procedural", "representations", "were", "mainly", "centered", "at", "MIT", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The advocates for procedural representations were mainly centered at MIT under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 3], [4, 13], [14, 17], [18, 28], [29, 44], [45, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 78], [79, 82], [83, 93], [94, 96], [97, 103], [104, 110], [111, 114], [115, 122], [123, 129], [129, 130]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "default", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The default interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 29], [30, 40], [41, 50], [51, 54], [55, 62], [63, 65], [66, 70], [70, 71]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [23, 23, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 23, 23, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "to", "solve", "linear", "and", "non-linear", "problems", "numerically", "and", "to", "perform", "other", "numerical", "experiments", "using", "a", "program", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps to solve linear and non-linear problems numerically and to perform other numerical experiments using a program that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 21], [22, 28], [29, 32], [33, 43], [44, 52], [53, 64], [65, 68], [69, 71], [72, 79], [80, 85], [86, 95], [96, 107], [108, 113], [114, 115], [116, 123], [124, 128], [129, 131], [132, 138], [139, 149], [150, 154], [155, 161], [161, 162]]}
{"doc_key": "ai-train-5", "ner": [[3, 6, "algorithm"], [10, 11, "misc"], [13, 14, "researcher"], [19, 21, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 6, 13, 14, "origin", "", false, false], [10, 11, 13, 14, "origin", "", false, false], [13, 14, 19, 21, "physical", "", false, false], [13, 14, 19, 21, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", "as", "well", "as", "unsupervised", "methods", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [43, 45], [46, 50], [51, 53], [54, 66], [67, 74], [75, 77], [78, 83], [84, 90], [91, 94], [95, 105], [106, 108], [109, 112], [113, 123], [124, 126], [127, 134], [135, 138], [139, 141], [142, 146], [147, 149], [150, 155], [156, 160], [160, 161], [162, 168], [169, 178], [179, 185], [186, 199], [199, 200], [201, 202], [202, 203], [203, 207], [208, 215]]}
{"doc_key": "ai-train-6", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalent", "using", "DCG", "notation", ":"], "sentence-detokenized": "or equivalent using DCG notation:", "token2charspan": [[0, 2], [3, 13], [14, 19], [20, 23], [24, 32], [32, 33]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 9, "algorithm"], [12, 14, "algorithm"], [17, 19, "algorithm"], [22, 22, "algorithm"], [24, 25, "algorithm"], [36, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 9, "type-of", "", false, false], [0, 3, 12, 14, "usage", "part-of?", true, false], [12, 14, 17, 19, "compare", "", false, false], [22, 22, 17, 19, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "differ", "from", "other", "artificial", "neural", "networks", "by", "using", "competitive", "learning", "as", "opposed", "to", "error", "correction", "learning", "such", "as", "backpropagation", "with", "gradient", "descent", ")", "and", "by", "using", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps differ from other artificial neural networks by using competitive learning as opposed to error correction learning such as backpropagation with gradient descent) and by using a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 74], [75, 86], [87, 95], [96, 98], [99, 106], [107, 109], [110, 115], [116, 126], [127, 135], [136, 140], [141, 143], [144, 159], [160, 164], [165, 173], [174, 181], [181, 182], [183, 186], [187, 189], [190, 195], [196, 197], [198, 210], [211, 219], [220, 222], [223, 231], [232, 235], [236, 247], [248, 258], [259, 261], [262, 265], [266, 271], [272, 277], [277, 278]]}
{"doc_key": "ai-train-8", "ner": [[15, 17, "organisation"], [27, 28, "misc"], [38, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "it", "has", "been", "recommended", "by", "several", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "that", "dynamic", "range", "measurements", "be", "made", "with", "an", "audio", "signal", "present", ",", "which", "is", "then", "filtered", "out", "in", "the", "noise", "floor", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "muting", "circuits", "."], "sentence-detokenized": "Since the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that dynamic range measurements be made with an audio signal present, which is then filtered out in the noise floor measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or muting circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 25], [26, 29], [30, 34], [35, 46], [47, 49], [50, 57], [58, 69], [69, 70], [71, 80], [81, 84], [85, 90], [91, 102], [103, 110], [110, 111], [112, 116], [117, 124], [125, 130], [131, 143], [144, 146], [147, 151], [152, 156], [157, 159], [160, 165], [166, 172], [173, 180], [180, 181], [182, 187], [188, 190], [191, 195], [196, 204], [205, 208], [209, 211], [212, 215], [216, 221], [222, 227], [228, 239], [240, 244], [245, 247], [248, 257], [258, 265], [266, 271], [271, 272], [273, 277], [278, 284], [285, 297], [298, 310], [311, 316], [317, 319], [320, 323], [324, 327], [328, 330], [331, 336], [337, 342], [343, 345], [346, 352], [353, 361], [361, 362]]}
{"doc_key": "ai-train-9", "ner": [[5, 5, "misc"], [15, 16, "task"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 32, "task"], [34, 36, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[5, 5, 15, 16, "part-of", "concept_used_in", true, false], [5, 5, 18, 19, "part-of", "concept_used_in", false, false], [5, 5, 21, 22, "part-of", "concept_used_in", false, false], [5, 5, 24, 25, "part-of", "concept_used_in", false, false], [5, 5, 27, 32, "part-of", "concept_used_in", false, false], [5, 5, 34, 36, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["The", "techniques", "used", "to", "create", "eigenfaces", "and", "use", "them", "for", "recognition", "are", "also", "used", "outside", "facial", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "imaging", "analysis", "."], "sentence-detokenized": "The techniques used to create eigenfaces and use them for recognition are also used outside facial recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation and medical imaging analysis.", "token2charspan": [[0, 3], [4, 14], [15, 19], [20, 22], [23, 29], [30, 40], [41, 44], [45, 48], [49, 53], [54, 57], [58, 69], [70, 73], [74, 78], [79, 83], [84, 91], [92, 98], [99, 110], [110, 111], [112, 123], [124, 135], [135, 136], [137, 140], [141, 148], [148, 149], [150, 155], [156, 167], [167, 168], [169, 173], [174, 182], [182, 183], [183, 187], [188, 195], [196, 210], [211, 214], [215, 222], [223, 230], [231, 239], [239, 240]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [9, 13, "organisation"], [15, 15, "organisation"], [19, 22, "organisation"], [25, 29, "organisation"], [32, 35, "organisation"], [38, 42, "organisation"], [44, 44, "organisation"], [48, 51, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[9, 13, 0, 3, "part-of", "", false, false], [15, 15, 9, 13, "named", "", false, false], [19, 22, 0, 3, "part-of", "", false, false], [25, 29, 0, 3, "part-of", "", false, false], [32, 35, 0, 3, "part-of", "", false, false], [38, 42, 0, 3, "part-of", "", false, false], [44, 44, 38, 42, "named", "", false, false], [48, 51, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "an", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "coordinated", "studies", "to", "inform", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 38], [39, 47], [48, 51], [52, 55], [56, 64], [65, 76], [77, 80], [81, 86], [87, 101], [102, 103], [103, 107], [107, 108], [108, 109], [110, 113], [114, 116], [117, 127], [128, 130], [131, 137], [137, 138], [139, 142], [143, 145], [146, 156], [157, 159], [160, 168], [169, 173], [173, 174], [175, 178], [179, 181], [182, 192], [193, 195], [196, 203], [203, 204], [205, 208], [209, 216], [217, 225], [226, 234], [235, 243], [244, 250], [251, 252], [252, 257], [257, 258], [259, 262], [263, 266], [267, 273], [274, 276], [277, 282], [283, 291], [292, 303], [304, 311], [312, 314], [315, 321], [322, 331], [332, 340], [341, 343], [344, 349], [350, 363], [363, 364]]}
{"doc_key": "ai-train-11", "ner": [[5, 5, "metrics"], [10, 11, "algorithm"], [15, 16, "researcher"], [21, 21, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 10, 11, "part-of", "", false, false], [15, 16, 21, 21, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "fast", "method", "for", "calculating", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "as", "an", "addendum", "to", "Bliss", "'", "work", "in", "1935", "."], "sentence-detokenized": "A fast method for calculating maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an addendum to Bliss' work in 1935.", "token2charspan": [[0, 1], [2, 6], [7, 13], [14, 17], [18, 29], [30, 37], [38, 48], [49, 58], [59, 62], [63, 66], [67, 73], [74, 79], [80, 83], [84, 92], [93, 95], [96, 102], [103, 109], [110, 112], [113, 115], [116, 124], [125, 127], [128, 133], [133, 134], [135, 139], [140, 142], [143, 147], [147, 148]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 17, "product"], [19, 20, "organisation"], [21, 21, "product"], [26, 28, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[21, 21, 14, 17, "usage", "uses_software", false, false], [21, 21, 19, 20, "artifact", "", false, false], [21, 21, 26, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Several", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", ",", "which", "powers", "AltaVista", "'s", "BabelFish", "(", "now", "Yahoo", "'s", "Babelfish", "as", "of", "May", "9", ",", "2008", ")", "."], "sentence-detokenized": "Several of these programs are available online, such as Google Translate and the SYSTRAN system, which powers AltaVista's BabelFish (now Yahoo's Babelfish as of May 9, 2008).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 39], [40, 46], [46, 47], [48, 52], [53, 55], [56, 62], [63, 72], [73, 76], [77, 80], [81, 88], [89, 95], [95, 96], [97, 102], [103, 109], [110, 119], [119, 121], [122, 131], [132, 133], [133, 136], [137, 142], [142, 144], [145, 154], [155, 157], [158, 160], [161, 164], [165, 166], [166, 167], [168, 172], [172, 173], [173, 174]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 22, "field"], [26, 27, "misc"], [31, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 31, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 31, 32, "related-to", "", true, false], [10, 11, 20, 22, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 31, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[13, 19, "metrics"]], "ner_mapping_to_source": [1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "most", "common", "way", "is", "using", "the", "so", "-", "called", "ROUGE", "measurement", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "."], "sentence-detokenized": "The most common way is using the so-called ROUGE measurement (Recall-Oriented Understudy for Gisting Evaluation).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 28], [29, 32], [33, 35], [35, 36], [36, 42], [43, 48], [49, 60], [61, 62], [62, 68], [68, 69], [69, 77], [78, 88], [89, 92], [93, 100], [101, 111], [111, 112], [112, 113]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 2], "sentence": ["RapidMiner", "contains", "learning", "schemes", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner contains learning schemes, models and algorithms and can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 85], [86, 87], [88, 91], [92, 98], [99, 106], [106, 107], [108, 113], [114, 120], [120, 121], [122, 127], [128, 136], [136, 137], [138, 140], [141, 149], [150, 154], [154, 155]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [13, 14, "task"], [19, 21, "misc"], [39, 40, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 13, 14, "related-to", "", false, false], [0, 0, 39, 40, "related-to", "", true, false], [19, 21, 0, 0, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["tity", "provides", "a", "collection", "of", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modelling", ",", "as", "well", "as", "graphical", "user", "interfaces", "for", "easy", "access", "to", "these", "functions", ".", "but", "the", "more", "recent", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "whose", "development", "started", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "applications", ",", "particularly", "for", "educational", "purposes", "and", "research", "."], "sentence-detokenized": "tity provides a collection of visualisation tools and algorithms for data analysis and predictive modelling, as well as graphical user interfaces for easy access to these functions. but the more recent fully Java-based version (Weka 3), whose development started in 1997, is now used in many different applications, particularly for educational purposes and research.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 107], [107, 108], [109, 111], [112, 116], [117, 119], [120, 129], [130, 134], [135, 145], [146, 149], [150, 154], [155, 161], [162, 164], [165, 170], [171, 180], [180, 181], [182, 185], [186, 189], [190, 194], [195, 201], [202, 207], [208, 212], [212, 213], [213, 218], [219, 226], [227, 228], [228, 232], [233, 234], [234, 235], [235, 236], [237, 242], [243, 254], [255, 262], [263, 265], [266, 270], [270, 271], [272, 274], [275, 278], [279, 283], [284, 286], [287, 291], [292, 301], [302, 314], [314, 315], [316, 328], [329, 332], [333, 344], [345, 353], [354, 357], [358, 366], [366, 367]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [12, 19, "misc"], [22, 25, "misc"], [28, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 19, 0, 0, "topic", "", false, false], [12, 19, 22, 25, "win-defeat", "", false, false], [22, 25, 28, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "gained", "wide", "recognition", "with", "his", "paper", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "winning", "the", "prize", "for", "best", "paper", "at", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "in", "1982", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and gained wide recognition with his paper Heuretics: Theoretical and Study of Heuristic Rules winning the prize for best paper at the Association for the Advancement of Artificial Intelligence in 1982.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 52], [53, 57], [58, 69], [70, 74], [75, 78], [79, 84], [85, 94], [94, 95], [96, 107], [108, 111], [112, 117], [118, 120], [121, 130], [131, 136], [137, 144], [145, 148], [149, 154], [155, 158], [159, 163], [164, 169], [170, 172], [173, 176], [177, 188], [189, 192], [193, 196], [197, 208], [209, 211], [212, 222], [223, 235], [236, 238], [239, 243], [243, 244]]}
{"doc_key": "ai-train-18", "ner": [[8, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "account", "for", "multiple", "units", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To account for multiple units, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 10], [11, 14], [15, 23], [24, 29], [29, 30], [31, 32], [33, 41], [42, 47], [48, 52], [53, 55], [56, 66], [67, 70], [71, 75], [76, 83], [83, 84]]}
{"doc_key": "ai-train-19", "ner": [[7, 9, "product"], [11, 13, "product"], [15, 16, "product"], [18, 19, "product"], [21, 23, "product"], [25, 26, "product"], [35, 36, "product"], [38, 39, "product"], [41, 42, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[7, 9, 25, 26, "type-of", "", false, false], [11, 13, 25, 26, "type-of", "", false, false], [15, 16, 25, 26, "type-of", "", false, false], [18, 19, 25, 26, "type-of", "", false, false], [21, 23, 25, 26, "type-of", "", false, false], [38, 39, 35, 36, "type-of", "", false, false], [41, 42, 35, 36, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "talking", "assistants", "like", "Apple", "'s", "Siri", ",", "Amazon", "'s", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "smart", "speakers", "like", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of talking assistants like Apple's Siri, Amazon's Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, voice portals can now be accessed via mobile devices and smart speakers like Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 26], [27, 37], [38, 42], [43, 48], [48, 50], [51, 55], [55, 56], [57, 63], [63, 65], [66, 71], [71, 72], [73, 79], [80, 89], [89, 90], [91, 100], [101, 108], [109, 112], [113, 120], [120, 122], [123, 128], [128, 129], [130, 135], [136, 143], [144, 147], [148, 151], [152, 154], [155, 163], [164, 167], [168, 174], [175, 182], [183, 186], [187, 192], [193, 201], [202, 206], [207, 213], [214, 218], [219, 222], [223, 229], [230, 234], [234, 235]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 14, "algorithm"], [16, 16, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "type-of", "", false, false], [9, 11, 2, 3, "type-of", "", false, false], [13, 14, 2, 3, "type-of", "", false, false], [16, 16, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "Naive", "Bayes", "classifier", ",", "support", "vector", "machine", ",", "Gaussian", "mixtures", "and", "networks", "."], "sentence-detokenized": "Examples of supervised learning are Naive Bayes classifier, support vector machine, Gaussian mixtures and networks.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 41], [42, 47], [48, 58], [58, 59], [60, 67], [68, 74], [75, 82], [82, 83], [84, 92], [93, 101], [102, 105], [106, 114], [114, 115]]}
{"doc_key": "ai-train-21", "ner": [[3, 5, "algorithm"], [27, 29, "algorithm"], [31, 31, "task"], [35, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 5, 27, 29, "part-of", "", true, false], [35, 37, 31, 31, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "derive", "math", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "math", "regret", "bounds", "for", "the", "online", "version", "of", "the", "Support", "vector", "machine", "for", "classification", ",", "which", "uses", "the", "hinge", "loss", "math", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to derive math O (\\ sqrt {T}) / math regret bounds for the online version of the Support vector machine for classification, which uses the hinge loss math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 39], [40, 44], [45, 46], [47, 48], [48, 49], [50, 54], [55, 56], [56, 57], [57, 58], [58, 59], [60, 61], [62, 66], [67, 73], [74, 80], [81, 84], [85, 88], [89, 95], [96, 103], [104, 106], [107, 110], [111, 118], [119, 125], [126, 133], [134, 137], [138, 152], [152, 153], [154, 159], [160, 164], [165, 168], [169, 174], [175, 179], [180, 184], [185, 186], [187, 189], [190, 191], [191, 192], [192, 193], [194, 196], [197, 200], [200, 201], [202, 203], [203, 204], [204, 205], [206, 207], [208, 209], [210, 211], [212, 214], [215, 216], [216, 217], [217, 218], [219, 223], [224, 225], [226, 228], [228, 229], [229, 231], [232, 233], [234, 238]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"], [26, 27, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robot", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", "and", "match", "moving", "."], "sentence-detokenized": "Applications include object recognition, robot mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, individual wildlife identification and match moving.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 46], [47, 54], [55, 58], [59, 69], [69, 70], [71, 76], [77, 86], [86, 87], [88, 90], [91, 100], [100, 101], [102, 109], [110, 121], [121, 122], [123, 128], [129, 137], [137, 138], [139, 149], [150, 158], [159, 173], [174, 177], [178, 183], [184, 190], [190, 191]]}
{"doc_key": "ai-train-23", "ner": [[8, 9, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [27, 32, "university"], [34, 36, "university"], [38, 40, "university"], [42, 43, "university"], [45, 50, "university"], [52, 52, "university"], [55, 59, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[8, 9, 14, 15, "related-to", "", true, false], [8, 9, 17, 19, "related-to", "", true, false], [8, 9, 21, 22, "related-to", "", true, false], [8, 9, 24, 25, "related-to", "", true, false], [8, 9, 27, 32, "related-to", "", true, false], [8, 9, 34, 36, "related-to", "", true, false], [8, 9, 38, 40, "related-to", "", true, false], [8, 9, 42, 43, "related-to", "", true, false], [8, 9, 45, 50, "related-to", "", true, false], [8, 9, 52, 52, "related-to", "", true, false], [8, 9, 55, 59, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "researching", "pose", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbr\u00fccken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbr\u00fccken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 48], [49, 53], [54, 64], [64, 65], [66, 75], [76, 82], [83, 85], [86, 91], [92, 102], [102, 103], [104, 112], [113, 119], [120, 130], [130, 131], [132, 135], [136, 147], [147, 148], [149, 157], [158, 168], [168, 169], [170, 180], [181, 183], [184, 194], [194, 195], [196, 199], [200, 205], [205, 206], [207, 217], [218, 220], [221, 228], [228, 229], [230, 235], [236, 244], [245, 250], [250, 251], [252, 255], [256, 262], [262, 263], [264, 272], [273, 283], [284, 286], [287, 295], [296, 299], [300, 310], [311, 312], [312, 316], [316, 317], [318, 321], [322, 332], [333, 335], [336, 346], [346, 347], [348, 354], [354, 355]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "math", "0.1", "/", "math", "."], "sentence-detokenized": "The sigmoid function Cross entropy loss is used to predict K independent probability values in math 0.1/math.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 26], [27, 34], [35, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 99], [100, 103], [103, 104], [104, 108], [108, 109]]}
{"doc_key": "ai-train-25", "ner": [[3, 5, "misc"], [7, 7, "field"], [9, 10, "field"], [13, 15, "university"], [18, 18, "country"], [21, 22, "misc"], [25, 28, "university"], [30, 30, "country"], [36, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 5, 7, 7, "topic", "", false, false], [3, 5, 9, 10, "topic", "", false, false], [3, 5, 13, 15, "physical", "", true, false], [13, 15, 18, 18, "physical", "", false, false], [21, 22, 25, 28, "physical", "", true, false], [25, 28, 30, 30, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "a", "professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Chair in Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Chair at the Tokyo Institute of Technology in Japan before becoming a professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 34], [35, 37], [38, 49], [50, 53], [54, 62], [63, 70], [71, 73], [74, 77], [78, 88], [89, 91], [92, 101], [102, 104], [105, 108], [109, 120], [121, 124], [125, 128], [129, 136], [137, 142], [143, 145], [146, 149], [150, 155], [156, 165], [166, 168], [169, 179], [180, 182], [183, 188], [189, 195], [196, 204], [205, 206], [207, 216], [217, 219], [220, 229], [229, 230]]}
{"doc_key": "ai-train-26", "ner": [[6, 7, "algorithm"], [11, 14, "algorithm"], [22, 23, "researcher"], [25, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[6, 7, 11, 14, "usage", "", true, false], [11, 14, 22, 23, "origin", "", false, false], [11, 14, 25, 26, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Another", "technique", "used", "in", "particular", "for", "recurrent", "neural", "networks", "is", "the", "Long", "Short", "Term", "Memory", "(", "LSTM", ")", "network", "of", "1997", "by", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "."], "sentence-detokenized": "Another technique used in particular for recurrent neural networks is the Long Short Term Memory (LSTM) network of 1997 by Sepp Hochreiter and J\u00fcrgen Schmidhuber.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 25], [26, 36], [37, 40], [41, 50], [51, 57], [58, 66], [67, 69], [70, 73], [74, 78], [79, 84], [85, 89], [90, 96], [97, 98], [98, 102], [102, 103], [104, 111], [112, 114], [115, 119], [120, 122], [123, 127], [128, 138], [139, 142], [143, 149], [150, 161], [161, 162]]}
{"doc_key": "ai-train-27", "ner": [[8, 9, "product"], [14, 14, "product"], [46, 46, "product"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[8, 9, 14, 14, "named", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "until", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "versatile", ",", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "modes", "in", "the", "same", "way", "as", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT until version 5.34, Cling from version 6) makes this package very versatile, as it can be used in interactive, scripted and compiled modes in the same way as commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [81, 82], [83, 88], [89, 93], [94, 101], [102, 106], [107, 116], [116, 117], [118, 120], [121, 123], [124, 127], [128, 130], [131, 135], [136, 138], [139, 150], [150, 151], [152, 160], [161, 164], [165, 173], [174, 179], [180, 182], [183, 186], [187, 191], [192, 195], [196, 198], [199, 209], [210, 218], [219, 223], [224, 226], [227, 233], [233, 234]]}
{"doc_key": "ai-train-28", "ner": [[1, 3, "product"], [20, 20, "field"], [26, 28, "task"], [30, 32, "task"], [34, 35, "task"], [38, 39, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[1, 3, 20, 20, "related-to", "", false, false], [26, 28, 20, 20, "part-of", "", false, false], [30, 32, 20, 20, "part-of", "", false, false], [34, 35, 20, 20, "part-of", "", false, false], [38, 39, 20, 20, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Designing", "voice", "user", "interfaces", "that", "interpret", "and", "handle", "conversation", "status", "is", "challenging", "due", "to", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "kernel", "reference", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", ",", "and", "dialogue", "management", "."], "sentence-detokenized": "Designing voice user interfaces that interpret and handle conversation status is challenging due to the inherent difficulty of integrating complex natural language processing tasks such as kernel reference resolution, named entity recognition, information retrieval, and dialogue management.", "token2charspan": [[0, 9], [10, 15], [16, 20], [21, 31], [32, 36], [37, 46], [47, 50], [51, 57], [58, 70], [71, 77], [78, 80], [81, 92], [93, 96], [97, 99], [100, 103], [104, 112], [113, 123], [124, 126], [127, 138], [139, 146], [147, 154], [155, 163], [164, 174], [175, 180], [181, 185], [186, 188], [189, 195], [196, 205], [206, 216], [216, 217], [218, 223], [224, 230], [231, 242], [242, 243], [244, 255], [256, 265], [265, 266], [267, 270], [271, 279], [280, 290], [290, 291]]}
{"doc_key": "ai-train-29", "ner": [[6, 7, "algorithm"], [10, 12, "algorithm"], [16, 17, "researcher"], [23, 26, "organisation"], [35, 36, "field"], [38, 39, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[6, 7, 16, 17, "origin", "", false, false], [6, 7, 35, 36, "part-of", "", false, false], [6, 7, 38, 39, "part-of", "", false, false], [10, 12, 16, 17, "origin", "", false, false], [10, 12, 35, 36, "part-of", "", false, false], [10, 12, 38, 39, "part-of", "", false, false], [16, 17, 23, 26, "physical", "", false, false], [16, 17, 23, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "the", "recurrent", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "AI", "Lab", "IDSIA", "won", "eight", "international", "competitions", "in", "the", "field", "of", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in J\u00fcrgen Schmidhuber's research group at the Swiss AI Lab IDSIA won eight international competitions in the field of pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 26], [27, 36], [37, 43], [44, 52], [53, 56], [57, 61], [62, 73], [74, 80], [81, 89], [90, 99], [100, 102], [103, 109], [110, 121], [121, 123], [124, 132], [133, 138], [139, 141], [142, 145], [146, 151], [152, 154], [155, 158], [159, 164], [165, 168], [169, 174], [175, 188], [189, 201], [202, 204], [205, 208], [209, 214], [215, 217], [218, 225], [226, 237], [238, 241], [242, 249], [250, 258], [258, 259]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 9, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 9, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 12, "misc"], [14, 14, "field"], [17, 19, "university"], [26, 29, "field"], [31, 34, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 12, 14, 14, "topic", "topic_of_award", false, false], [7, 12, 17, 19, "origin", "", true, false], [26, 29, 31, 34, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "a", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padova", "in", "1995", "and", "a", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, a S. V. della laurea ad honorem in psychology from the University of Padova in 1995 and a doctorate in industrial design and engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 35], [36, 38], [39, 41], [42, 47], [48, 54], [55, 57], [58, 65], [66, 68], [69, 79], [80, 84], [85, 88], [89, 99], [100, 102], [103, 109], [110, 112], [113, 117], [118, 121], [122, 123], [124, 133], [134, 136], [137, 147], [148, 154], [155, 158], [159, 170], [171, 175], [176, 181], [182, 192], [193, 195], [196, 206], [206, 207]]}
{"doc_key": "ai-train-32", "ner": [[7, 8, "researcher"], [14, 17, "organisation"], [19, 19, "location"], [21, 21, "researcher"], [32, 33, "misc"], [46, 48, "misc"], [64, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[7, 8, 14, 17, "physical", "", false, false], [7, 8, 14, 17, "role", "", false, false], [14, 17, 19, 19, "physical", "", false, false], [21, 21, 32, 33, "related-to", "works_with", true, false], [21, 21, 46, 48, "related-to", "works_with", true, false], [21, 21, 64, 65, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "with", "impaired", "multiplication", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "in", "the", "inferior", "parietal", "lobe", ")", "and", "others", "with", "impaired", "subtraction", "but", "preserved", "multiplication", "(", "associated", "with", "lesions", "in", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Together with his long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication but preserved subtraction (associated with lesions in the inferior parietal lobe) and others with impaired subtraction but preserved multiplication (associated with lesions in the intraparietal sulcus).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [41, 48], [49, 54], [54, 55], [56, 57], [58, 69], [70, 72], [73, 76], [77, 82], [82, 83], [83, 94], [95, 103], [104, 106], [107, 112], [112, 113], [114, 121], [122, 126], [127, 137], [138, 146], [147, 151], [152, 159], [160, 162], [163, 172], [173, 180], [181, 183], [184, 187], [188, 196], [197, 201], [202, 206], [207, 215], [216, 230], [231, 234], [235, 244], [245, 256], [257, 258], [258, 268], [269, 273], [274, 281], [282, 284], [285, 288], [289, 297], [298, 306], [307, 311], [311, 312], [313, 316], [317, 323], [324, 328], [329, 337], [338, 349], [350, 353], [354, 363], [364, 378], [379, 380], [380, 390], [391, 395], [396, 403], [404, 406], [407, 410], [411, 424], [425, 431], [431, 432], [432, 433]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [13, 16, "misc"], [18, 19, "misc"], [26, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 8, "topic", "", false, false], [18, 19, 6, 8, "topic", "", false, false], [26, 26, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "depictions", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "and", "the", "2016", "TV", "movie", "Westworld", ",", "have", "aroused", "audience", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional depictions of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina, and the 2016 TV movie Westworld, have aroused audience sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 35], [36, 38], [39, 51], [52, 63], [64, 70], [71, 73], [74, 79], [80, 84], [85, 87], [88, 91], [91, 92], [93, 103], [104, 116], [117, 120], [121, 123], [124, 131], [131, 132], [133, 136], [137, 140], [141, 145], [146, 148], [149, 154], [155, 164], [164, 165], [166, 170], [171, 178], [179, 187], [188, 196], [197, 200], [201, 204], [205, 211], [212, 222], [222, 223]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [10, 12, "algorithm"], [14, 15, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 12, 7, 8, "part-of", "", false, false], [14, 15, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [20, 21, "misc"], [26, 27, "misc"], [29, 31, "person"], [36, 37, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[20, 21, 0, 3, "artifact", "", false, false], [26, 27, 0, 3, "artifact", "", false, false], [26, 27, 29, 31, "role", "director_of", false, false], [26, 27, 36, 37, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "more", "prominently", "in", "specific", "locations", "to", "impress", "audiences", ",", "with", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "as", "notable", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films more prominently in specific locations to impress audiences, with Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) as notable examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 55], [56, 67], [68, 70], [71, 79], [80, 89], [90, 92], [93, 100], [101, 110], [110, 111], [112, 116], [117, 122], [123, 131], [132, 133], [133, 137], [137, 138], [139, 142], [143, 150], [151, 153], [154, 155], [155, 162], [163, 167], [168, 175], [175, 176], [177, 181], [181, 182], [183, 191], [192, 199], [200, 207], [207, 208], [209, 211], [212, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-36", "ner": [[9, 11, "field"], [16, 21, "task"], [23, 24, "task"], [26, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[16, 21, 9, 11, "part-of", "", false, false], [23, 24, 9, 11, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptual", "training", "has", "become", "popular", "in", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptual training has become popular in natural language processing for tasks such as part-of-speech tagging and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 61], [62, 70], [71, 81], [82, 85], [86, 91], [92, 96], [97, 99], [100, 104], [104, 105], [105, 107], [107, 108], [108, 114], [115, 122], [123, 126], [127, 136], [137, 144], [145, 146], [146, 153], [153, 154], [155, 159], [159, 160], [160, 161]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [9, 13, "organisation"], [15, 16, "organisation"], [18, 18, "country"], [22, 25, "product"], [29, 30, "researcher"], [40, 40, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 13, 2, 3, "role", "introduces_to_market", true, false], [15, 16, 2, 3, "role", "introduces_to_market", true, false], [15, 16, 18, 18, "physical", "", false, false], [22, 25, 40, 40, "related-to", "sold_to", true, false], [29, 30, 22, 25, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletizing", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", "Company", ".", "by", "KUKA", "robotics", "in", "Germany", ",", "and", "the", "programmable", "multi-purpose", "picking", "machine", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", ",", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletizing robot was introduced in 1963 by Fuji Yusoki Kogyo Company. by KUKA robotics in Germany, and the programmable multi-purpose picking machine was invented by Victor Scheinman in 1976, and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [72, 79], [79, 80], [81, 83], [84, 88], [89, 97], [98, 100], [101, 108], [108, 109], [110, 113], [114, 117], [118, 130], [131, 144], [145, 152], [153, 160], [161, 164], [165, 173], [174, 176], [177, 183], [184, 193], [194, 196], [197, 201], [201, 202], [203, 206], [207, 210], [211, 217], [218, 221], [222, 226], [227, 229], [230, 239], [239, 240]]}
{"doc_key": "ai-train-38", "ner": [[8, 8, "conference"], [10, 10, "researcher"], [19, 19, "field"], [33, 34, "researcher"], [41, 42, "researcher"], [55, 55, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[10, 10, 8, 8, "role", "president_of", false, false], [10, 10, 33, 34, "role", "colleagues", false, false], [19, 19, 55, 55, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "while", "president", "of", "the", "AAAI", ",", "Hayes", "launched", "a", "series", "of", "attacks", "on", "critics", "of", "AI", ",", "mostly", "tongue", "-", "in", "-", "cheek", ",", "and", "(", "with", "his", "colleague", "Kenneth", "Ford", ")", "invented", "a", "prize", "named", "after", "Simon", "Newcomb", "to", "be", "awarded", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, while president of the AAAI, Hayes launched a series of attacks on critics of AI, mostly tongue-in-cheek, and (with his colleague Kenneth Ford) invented a prize named after Simon Newcomb to be awarded for the most ridiculous argument refuting the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 33], [34, 36], [37, 40], [41, 45], [45, 46], [47, 52], [53, 61], [62, 63], [64, 70], [71, 73], [74, 81], [82, 84], [85, 92], [93, 95], [96, 98], [98, 99], [100, 106], [107, 113], [113, 114], [114, 116], [116, 117], [117, 122], [122, 123], [124, 127], [128, 129], [129, 133], [134, 137], [138, 147], [148, 155], [156, 160], [160, 161], [162, 170], [171, 172], [173, 178], [179, 184], [185, 190], [191, 196], [197, 204], [205, 207], [208, 210], [211, 218], [219, 222], [223, 226], [227, 231], [232, 242], [243, 251], [252, 260], [261, 264], [265, 276], [277, 279], [280, 282], [282, 283]]}
{"doc_key": "ai-train-39", "ner": [[14, 16, "algorithm"], [40, 41, "algorithm"], [53, 56, "algorithm"], [58, 60, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 16, 40, 41, "named", "same", false, false], [53, 56, 14, 16, "type-of", "", false, false], [58, 60, 14, 16, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "optimal", "value", "for", "math", "\\", "alpha", "/", "math", "can", "be", "found", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "usually", "using", "a", "line", "search", "in", "the", "interval", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "backtracking", "line", "search", "such", "as", "armijo", "line", "search", "."], "sentence-detokenized": "An optimal value for math\\ alpha/math can be found using a line search algorithm, i.e. the size of math\\ alpha/math is determined by finding the value that minimizes S, usually using a line search in the interval math0\\ alpha 1/math or a backtracking line search such as armijo line search.", "token2charspan": [[0, 2], [3, 10], [11, 16], [17, 20], [21, 25], [25, 26], [27, 32], [32, 33], [33, 37], [38, 41], [42, 44], [45, 50], [51, 56], [57, 58], [59, 63], [64, 70], [71, 80], [80, 81], [82, 86], [87, 90], [91, 95], [96, 98], [99, 103], [103, 104], [105, 110], [110, 111], [111, 115], [116, 118], [119, 129], [130, 132], [133, 140], [141, 144], [145, 150], [151, 155], [156, 165], [166, 167], [167, 168], [169, 176], [177, 182], [183, 184], [185, 189], [190, 196], [197, 199], [200, 203], [204, 212], [213, 218], [218, 219], [220, 225], [226, 227], [227, 228], [228, 232], [233, 235], [236, 237], [238, 250], [251, 255], [256, 262], [263, 267], [268, 270], [271, 277], [278, 282], [283, 289], [289, 290]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 9, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "breadth", "-", "first", "search", "and", "depth", "-", "first", "search", "techniques", ",", "but", "concludes", "in", "the", "end", "that", "the", "results", "represent", "expert", "systems", "that", "contain", "a", "lot", "of", "technical", "knowledge", "but", "do", "not", "illuminate", "the", "mental", "processes", "that", "humans", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses breadth-first search and depth-first search techniques, but concludes in the end that the results represent expert systems that contain a lot of technical knowledge but do not illuminate the mental processes that humans use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 67], [67, 68], [69, 72], [73, 82], [83, 85], [86, 89], [90, 93], [94, 98], [99, 102], [103, 110], [111, 120], [121, 127], [128, 135], [136, 140], [141, 148], [149, 150], [151, 154], [155, 157], [158, 167], [168, 177], [178, 181], [182, 184], [185, 188], [189, 199], [200, 203], [204, 210], [211, 220], [221, 225], [226, 232], [233, 236], [237, 239], [240, 245], [246, 250], [251, 258], [258, 259]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 3, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "synthesis", "is", "about", "how", "spoken", "language", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and synthesis is about how spoken language can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 32], [33, 35], [36, 41], [42, 45], [46, 52], [53, 61], [62, 65], [66, 68], [69, 79], [80, 82], [83, 90], [91, 96], [97, 106], [106, 107]]}
{"doc_key": "ai-train-42", "ner": [[9, 10, "algorithm"], [24, 26, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math\\", "theta", "^{*}/math", "is", "usually", "estimated", "using", "a", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^{", "*}", "=\\", "theta", "^{", "ML}", "/math", ")", "or", "Maximum", "A", "Posteriori", "(", "math\\", "theta", "^{*}", "=\\", "theta", "^{", "MAP}/math", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta^{*}/math is usually estimated using a Maximum Likelihood (math\\ theta^{*} =\\ theta^{ML}/math) or Maximum A Posteriori (math\\ theta^{*} =\\ theta^{MAP}/math) procedure.", "token2charspan": [[0, 4], [5, 10], [11, 16], [16, 25], [26, 28], [29, 36], [37, 46], [47, 52], [53, 54], [55, 62], [63, 73], [74, 75], [75, 79], [79, 80], [81, 86], [86, 88], [88, 90], [91, 93], [94, 99], [99, 101], [101, 104], [104, 109], [109, 110], [111, 113], [114, 121], [122, 123], [124, 134], [135, 136], [136, 141], [142, 147], [147, 151], [152, 154], [155, 160], [160, 162], [162, 171], [171, 172], [173, 182], [182, 183]]}
{"doc_key": "ai-train-43", "ner": [[6, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "lesser", "-", "used", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesizer", "for", "their", "speech", ",", "which", "produces", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some lesser-used languages use the open-source eSpeak synthesizer for their speech, which produces a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 11], [11, 12], [12, 16], [17, 26], [27, 30], [31, 34], [35, 39], [39, 40], [40, 46], [47, 53], [54, 65], [66, 69], [70, 75], [76, 82], [82, 83], [84, 89], [90, 98], [99, 100], [101, 108], [108, 109], [110, 117], [118, 123], [124, 128], [129, 132], [133, 135], [136, 145], [146, 148], [149, 159], [159, 160]]}
{"doc_key": "ai-train-44", "ner": [[1, 1, "programlang"], [39, 40, "programlang"], [42, 42, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 39, 40, "compare", "", false, false], [1, 1, 42, 42, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "mainly", "used", "by", "statisticians", "and", "other", "practitioners", "who", "need", "an", "environment", "for", "statistical", "computations", "and", "software", "development", ",", "it", "can", "also", "serve", "as", "a", "general", "-", "purpose", "toolkit", "for", "matrix", "computation", "-", "with", "benchmarks", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is mainly used by statisticians and other practitioners who need an environment for statistical computations and software development, it can also serve as a general-purpose toolkit for matrix computation - with benchmarks comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 20], [21, 25], [26, 28], [29, 42], [43, 46], [47, 52], [53, 66], [67, 70], [71, 75], [76, 78], [79, 90], [91, 94], [95, 106], [107, 119], [120, 123], [124, 132], [133, 144], [144, 145], [146, 148], [149, 152], [153, 157], [158, 163], [164, 166], [167, 168], [169, 176], [176, 177], [177, 184], [185, 192], [193, 196], [197, 203], [204, 215], [216, 217], [218, 222], [223, 233], [234, 244], [245, 247], [248, 251], [252, 258], [259, 261], [262, 268], [268, 269]]}
{"doc_key": "ai-train-45", "ner": [[0, 1, "algorithm"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 1, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "combining", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden that creates new frequencies by combining two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [76, 79], [80, 88], [89, 97], [98, 107], [108, 112], [113, 120], [121, 124], [125, 136], [137, 139], [140, 149], [150, 153], [154, 165], [165, 166]]}
{"doc_key": "ai-train-46", "ner": [[14, 14, "person"], [17, 17, "misc"], [21, 23, "organisation"], [26, 26, "organisation"], [28, 30, "misc"], [32, 33, "person"], [35, 35, "organisation"], [37, 39, "misc"], [41, 42, "person"], [44, 45, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[14, 14, 17, 17, "role", "actor_in", false, false], [17, 17, 21, 23, "artifact", "", false, false], [28, 30, 26, 26, "artifact", "", false, false], [32, 33, 28, 30, "role", "actor_in", false, false], [37, 39, 35, 35, "artifact", "", false, false], [41, 42, 37, 39, "role", "actor_in", false, false], [44, 45, 37, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Other", "films", "that", "helped", "put", "3D", "back", "on", "the", "map", "that", "month", "were", "the", "John", "Wayne", "film", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "starring", "Rita", "Hayworth", "and", "Paramount", "'s", "Money", "From", "Home", "starring", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Other films that helped put 3D back on the map that month were the John Wayne film Hondo (distributed by Warner Bros. ), Columbia's Miss Sadie Thompson starring Rita Hayworth and Paramount's Money From Home starring Dean Martin and Jerry Lewis.", "token2charspan": [[0, 5], [6, 11], [12, 16], [17, 23], [24, 27], [28, 30], [31, 35], [36, 38], [39, 42], [43, 46], [47, 51], [52, 57], [58, 62], [63, 66], [67, 71], [72, 77], [78, 82], [83, 88], [89, 90], [90, 101], [102, 104], [105, 111], [112, 116], [116, 117], [118, 119], [119, 120], [121, 129], [129, 131], [132, 136], [137, 142], [143, 151], [152, 160], [161, 165], [166, 174], [175, 178], [179, 188], [188, 190], [191, 196], [197, 201], [202, 206], [207, 215], [216, 220], [221, 227], [228, 231], [232, 237], [238, 243], [243, 244]]}
{"doc_key": "ai-train-47", "ner": [[0, 1, "product"], [3, 3, "field"], [7, 8, "task"], [15, 15, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 7, 8, "general-affiliation", "", false, false], [0, 1, 15, 15, "artifact", "", false, false], [7, 8, 3, 3, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "system", "for", "face", "recognition", "developed", "by", "a", "research", "group", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep learning system for face recognition developed by a research group at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 38], [39, 43], [44, 55], [56, 65], [66, 68], [69, 70], [71, 79], [80, 85], [86, 88], [89, 97], [97, 98]]}
{"doc_key": "ai-train-48", "ner": [[0, 2, "field"], [8, 8, "conference"], [15, 16, "field"], [25, 28, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 15, 16, "part-of", "subfield", false, false], [8, 8, 0, 2, "topic", "", false, false], [25, 28, 0, 2, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "regular", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "topic", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometry processing is a regular research topic at SIGGRAPH, the leading academic conference on computer graphics, and the main topic of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 32], [33, 41], [42, 47], [48, 50], [51, 59], [59, 60], [61, 64], [65, 72], [73, 81], [82, 92], [93, 95], [96, 104], [105, 113], [113, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 136], [137, 140], [141, 147], [148, 157], [158, 160], [161, 169], [170, 180], [180, 181]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [14, 16, "algorithm"], [12, 12, "algorithm"], [21, 23, "algorithm"], [19, 19, "algorithm"], [28, 30, "algorithm"], [26, 26, "algorithm"], [39, 42, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 9], "relations": [[12, 12, 14, 16, "named", "", false, false], [19, 19, 21, 23, "named", "", false, false], [26, 26, 28, 30, "named", "", false, false]], "relations_mapping_to_source": [1, 3, 5], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "one", "step", "using", "PCA", "(", "Principal", "Component", "Analysis", ")", ",", "LDA", "(", "linear", "discriminant", "analysis", ")", "or", "CCA", "(", "canonical", "correlation", "analysis", ")", "as", "a", "preprocessing", "step", ",", "followed", "by", "k", "-", "NN", "clustering", "on", "feature", "vectors", "in", "a", "reduced", "dimension", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in one step using PCA (Principal Component Analysis), LDA (linear discriminant analysis) or CCA (canonical correlation analysis) as a preprocessing step, followed by k -NN clustering on feature vectors in a reduced dimension space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 65], [66, 70], [71, 76], [77, 80], [81, 82], [82, 91], [92, 101], [102, 110], [110, 111], [111, 112], [113, 116], [117, 118], [118, 124], [125, 137], [138, 146], [146, 147], [148, 150], [151, 154], [155, 156], [156, 165], [166, 177], [178, 186], [186, 187], [188, 190], [191, 192], [193, 206], [207, 211], [211, 212], [213, 221], [222, 224], [225, 226], [227, 228], [228, 230], [231, 241], [242, 244], [245, 252], [253, 260], [261, 263], [264, 265], [266, 273], [274, 283], [284, 289], [289, 290]]}
{"doc_key": "ai-train-50", "ner": [[0, 3, "algorithm"], [10, 11, "field"], [13, 14, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 3, 10, 11, "related-to", "good_at", true, false], [0, 3, 13, 14, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "are", "excellent", "for", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that are excellent for machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 60], [61, 70], [71, 74], [75, 82], [83, 91], [92, 95], [96, 103], [104, 115], [115, 116]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [35, 38, "algorithm"], [39, 40, "researcher"], [42, 44, "researcher"], [46, 52, "misc"], [54, 63, "conference"], [65, 65, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [46, 52, 35, 38, "topic", "", false, false], [46, 52, 39, 40, "artifact", "", false, false], [46, 52, 42, 44, "artifact", "", false, false], [46, 52, 54, 63, "temporal", "", false, false], [65, 65, 54, 63, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "page", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "as", "histograms", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "page", "1", ":", "886-", "893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), page 1: 15-33, 2000 others use local features as histograms of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), page 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 127], [128, 129], [129, 130], [131, 133], [133, 134], [134, 136], [136, 137], [138, 142], [143, 149], [150, 153], [154, 159], [160, 168], [169, 171], [172, 182], [183, 185], [186, 194], [195, 204], [205, 207], [208, 213], [213, 214], [215, 216], [216, 217], [218, 224], [224, 225], [226, 236], [237, 239], [240, 248], [249, 258], [259, 262], [263, 268], [269, 278], [278, 279], [280, 284], [285, 293], [294, 301], [302, 312], [313, 315], [316, 324], [325, 331], [332, 335], [336, 343], [344, 355], [356, 357], [357, 361], [361, 362], [362, 363], [364, 368], [369, 370], [370, 371], [372, 376], [376, 379], [379, 380], [381, 385], [386, 397], [397, 398]]}
{"doc_key": "ai-train-52", "ner": [[1, 2, "algorithm"], [6, 8, "algorithm"], [12, 14, "task"], [16, 16, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 2, 6, 8, "type-of", "", false, false], [12, 14, 1, 2, "usage", "", true, false], [12, 14, 16, 16, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "functional", "learning", "in", "an", "unsupervised", "learning", "manner", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network used to learn functional learning in an unsupervised learning manner.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 61], [62, 67], [68, 78], [79, 87], [88, 90], [91, 93], [94, 106], [107, 115], [116, 122], [122, 123]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [5, 5, "organisation"], [10, 11, "field"], [13, 16, "field"], [20, 24, "organisation"], [26, 26, "organisation"], [32, 33, "field"], [35, 36, "field"], [42, 42, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 2, 5, 5, "role", "fellow_of", false, false], [0, 2, 10, 11, "related-to", "contributes_to", false, false], [0, 2, 13, 16, "related-to", "contributes_to", false, false], [0, 2, 20, 24, "role", "fellow_of", false, false], [0, 2, 32, 33, "related-to", "contributes_to", false, false], [0, 2, 35, 36, "related-to", "contributes_to", false, false], [26, 26, 20, 24, "named", "", false, false], [42, 42, 20, 24, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "IEEE", "for", "his", "contributions", "in", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "in", "pattern", "recognition", ",", "image", "processing", "and", "for", "his", "work", "with", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition, image processing and for his work with IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 28], [29, 32], [33, 36], [37, 50], [51, 53], [54, 62], [63, 69], [70, 73], [74, 79], [80, 90], [91, 94], [95, 96], [97, 103], [104, 106], [107, 110], [111, 124], [125, 136], [137, 140], [141, 148], [149, 160], [161, 162], [162, 166], [166, 167], [168, 171], [172, 175], [176, 189], [190, 192], [193, 200], [201, 212], [212, 213], [214, 219], [220, 230], [231, 234], [235, 238], [239, 242], [243, 247], [248, 252], [253, 257], [257, 258]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [24, 25, "researcher"], [27, 28, "organisation"], [30, 31, "researcher"], [34, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 9, 13, 15, "usage", "", false, false], [13, 15, 24, 25, "origin", "", true, false], [13, 15, 30, 31, "origin", "", true, false], [17, 17, 13, 15, "named", "", false, false], [24, 25, 27, 28, "physical", "", false, false], [24, 25, 27, 28, "role", "", false, false], [30, 31, 34, 36, "physical", "", false, false], [30, 31, 34, 36, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "made", "with", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "-", "based", "systems", "introduced", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was made with Connectionist Temporal Classification (CTC)-based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 44], [45, 49], [50, 63], [64, 72], [73, 87], [88, 89], [89, 92], [92, 93], [93, 94], [94, 99], [100, 107], [108, 118], [119, 121], [122, 126], [127, 133], [134, 136], [137, 143], [144, 152], [153, 156], [157, 164], [165, 171], [172, 174], [175, 178], [179, 189], [190, 192], [193, 200], [201, 203], [204, 208], [208, 209]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 7, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 7, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear-", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalization", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-fractional programming (LFP) is a generalization of linear programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 1, "researcher"], [8, 13, "misc"], [16, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 8, 13, "win-defeat", "", false, false], [8, 13, 16, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the International Conference on Machine Learning 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 104], [105, 107], [108, 115], [116, 124], [125, 129], [130, 133], [134, 138], [138, 139]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [25, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "implement", "the", "evolved", "neural", "network", "in", "these", "frameworks", "as", "inherited", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to implement the evolved neural network in these frameworks as inherited components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 131], [132, 135], [136, 143], [144, 150], [151, 158], [159, 161], [162, 167], [168, 178], [179, 181], [182, 191], [192, 202], [202, 203]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "and", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "illustrations", ")", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, and the algorithm first creates an alignment (see illustrations) between two sentences, the candidate translation string and the reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 67], [68, 77], [78, 83], [84, 91], [92, 94], [95, 104], [105, 106], [106, 109], [110, 123], [123, 124], [125, 132], [133, 136], [137, 146], [146, 147], [148, 151], [152, 161], [162, 173], [174, 180], [181, 184], [185, 188], [189, 198], [199, 210], [211, 217], [217, 218]]}
{"doc_key": "ai-train-59", "ner": [[6, 12, "conference"], [22, 22, "task"], [24, 25, "task"], [29, 30, "metrics"], [32, 38, "metrics"], [43, 46, "conference"], [48, 48, "conference"], [51, 51, "location"], [53, 53, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 12, 22, 22, "related-to", "subject_at", false, false], [6, 12, 24, 25, "related-to", "subject_at", false, false], [29, 30, 6, 12, "temporal", "", false, false], [32, 38, 29, 30, "named", "", true, false], [48, 48, 43, 46, "named", "", false, false], [51, 51, 53, 53, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "NIST", "'s", "annual", "conferences", "on", "document", "comprehension", ",", "where", "research", "groups", "submit", "their", "systems", "for", "both", "summarization", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "2014", ")", "."], "sentence-detokenized": "One of the metrics used at NIST's annual conferences on document comprehension, where research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December 2014).", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 31], [31, 33], [34, 40], [41, 52], [53, 55], [56, 64], [65, 78], [78, 79], [80, 85], [86, 94], [95, 101], [102, 108], [109, 114], [115, 122], [123, 126], [127, 131], [132, 145], [146, 149], [150, 161], [162, 167], [167, 168], [169, 171], [172, 175], [176, 181], [182, 188], [189, 190], [190, 196], [196, 197], [197, 205], [206, 216], [217, 220], [221, 228], [229, 239], [239, 240], [241, 243], [244, 252], [253, 255], [256, 262], [263, 274], [275, 285], [286, 293], [294, 295], [295, 299], [299, 300], [300, 301], [302, 310], [310, 311], [312, 318], [318, 319], [320, 328], [329, 333], [333, 334], [334, 335]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [10, 11, "programlang"], [15, 15, "product"], [21, 21, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 10, 11, "type-of", "", false, false], [6, 6, 21, 21, "named", "", false, false], [8, 8, 10, 11, "part-of", "", false, false], [8, 8, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "to", "run", "in", "Java", "with", "JShell", "(", "Java", "9", "minimum", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, to run in Java with JShell (Java 9 minimum): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 23], [24, 27], [28, 30], [31, 35], [36, 40], [41, 47], [48, 49], [49, 53], [54, 55], [56, 63], [63, 64], [64, 65], [66, 76], [77, 87], [88, 89], [90, 109], [110, 114], [115, 116], [117, 121]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLEU", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLEU metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "initiated", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "added", "restriction", "that", "the", "edges", "may", "only", "be", "from", "a", "limited", "set", "of", "possible", "relations", ",", "to", "make", "it", "easier", "to", "make", "algebras", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly initiated a project called Knowledge Graphs, which are semantic networks, but with the added restriction that the edges may only be from a limited set of possible relations, to make it easier to make algebras on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 118], [119, 120], [121, 128], [129, 135], [136, 145], [146, 152], [152, 153], [154, 159], [160, 163], [164, 172], [173, 181], [181, 182], [183, 186], [187, 191], [192, 195], [196, 201], [202, 213], [214, 218], [219, 222], [223, 228], [229, 232], [233, 237], [238, 240], [241, 245], [246, 247], [248, 255], [256, 259], [260, 262], [263, 271], [272, 281], [281, 282], [283, 285], [286, 290], [291, 293], [294, 300], [301, 303], [304, 308], [309, 317], [318, 320], [321, 324], [325, 330], [330, 331]]}
{"doc_key": "ai-train-63", "ner": [[0, 4, "product"], [17, 18, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 4, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "word", "processor", ",", "but", "also", "exist", "as", "a", "standalone", "program", "that", "can", "be", "activated", "from", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but also exist as a standalone program that can be activated from programs that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [76, 77], [78, 82], [83, 85], [86, 87], [88, 92], [93, 102], [102, 103], [104, 107], [108, 112], [113, 118], [119, 121], [122, 123], [124, 134], [135, 142], [143, 147], [148, 151], [152, 154], [155, 164], [165, 169], [170, 178], [179, 183], [184, 188], [189, 193], [194, 202], [203, 207], [207, 208]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 20, "conference"], [24, 31, "organisation"], [34, 36, "conference"], [38, 40, "conference"], [43, 45, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "he", "is", "the", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement Artificial Intelligence, and the Cognitive Science Society, and he is the editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 121], [122, 134], [134, 135], [136, 139], [140, 143], [144, 153], [154, 161], [162, 169], [169, 170], [171, 174], [175, 177], [178, 180], [181, 184], [185, 191], [192, 194], [195, 197], [198, 207], [208, 217], [217, 218], [219, 221], [222, 230], [231, 239], [239, 240], [241, 244], [245, 247], [248, 255], [256, 264], [264, 265]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "task"], [18, 19, "researcher"], [21, 22, "university"], [24, 25, "researcher"], [27, 30, "organisation"], [33, 33, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 18, 19, "origin", "", false, false], [0, 2, 24, 25, "origin", "", false, false], [4, 4, 0, 2, "named", "", false, false], [18, 19, 21, 22, "physical", "", false, false], [18, 19, 21, 22, "role", "", false, false], [24, 25, 27, 30, "role", "", false, false], [33, 33, 27, 30, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "be", "developed", "by", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear predictive coding (LPC), a form of speech coding, began to be developed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 68], [69, 78], [79, 81], [82, 90], [91, 98], [99, 101], [102, 108], [109, 119], [120, 123], [124, 129], [130, 135], [136, 138], [139, 145], [146, 155], [156, 159], [160, 169], [170, 171], [171, 174], [174, 175], [176, 178], [179, 183], [183, 184]]}
{"doc_key": "ai-train-66", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["Furthermore", ",", "if", "the", "signal", "is", "ergodic", ",", "all", "sample", "paths", "have", "the", "same", "time", "average", ",", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "mean", "square", "error", "context", "."], "sentence-detokenized": "Furthermore, if the signal is ergodic, all sample paths have the same time average, and thus mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the mean square error context.", "token2charspan": [[0, 11], [11, 12], [13, 15], [16, 19], [20, 26], [27, 29], [30, 37], [37, 38], [39, 42], [43, 49], [50, 55], [56, 60], [61, 64], [65, 69], [70, 74], [75, 82], [82, 83], [84, 87], [88, 92], [93, 98], [99, 100], [101, 102], [103, 104], [105, 106], [106, 107], [108, 109], [110, 111], [112, 113], [114, 115], [115, 116], [117, 118], [118, 119], [120, 123], [123, 124], [125, 127], [128, 135], [136, 137], [137, 138], [138, 139], [140, 141], [142, 143], [144, 145], [146, 147], [147, 148], [149, 150], [151, 152], [153, 154], [155, 156], [156, 157], [158, 159], [159, 160], [161, 164], [164, 165], [166, 167], [168, 172], [173, 175], [176, 179], [180, 184], [185, 191], [192, 197], [198, 205], [205, 206]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [14, 16, "algorithm"], [12, 12, "algorithm"], [21, 23, "algorithm"], [19, 19, "algorithm"], [28, 30, "algorithm"], [26, 26, "algorithm"], [35, 38, "algorithm"], [43, 44, "misc"], [47, 50, "algorithm"], [52, 53, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12], "relations": [[14, 16, 43, 44, "related-to", "", false, false], [12, 12, 14, 16, "named", "", false, false], [21, 23, 43, 44, "related-to", "", false, false], [19, 19, 21, 23, "named", "", false, false], [28, 30, 43, 44, "related-to", "", false, false], [26, 26, 28, 30, "named", "", false, false], [35, 38, 43, 44, "related-to", "", false, false], [47, 50, 52, 53, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 8], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "one", "step", "using", "PCA", "(", "principal", "component", "analysis", ")", ",", "LDA", "(", "linear", "discriminant", "analysis", ")", ",", "CCA", "(", "canonical", "correlation", "analysis", ")", "or", "NMF", "(", "non", "-negative", "matrix", "factorization", ")", "techniques", "as", "a", "preprocessing", "step", "followed", "by", "K", "-", "NN", "clustering", "on", "feature", "vectors", "in", "a", "reduced", "dimension", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in one step using PCA (principal component analysis), LDA (linear discriminant analysis), CCA (canonical correlation analysis) or NMF (non-negative matrix factorization) techniques as a preprocessing step followed by K-NN clustering on feature vectors in a reduced dimension space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 65], [66, 70], [71, 76], [77, 80], [81, 82], [82, 91], [92, 101], [102, 110], [110, 111], [111, 112], [113, 116], [117, 118], [118, 124], [125, 137], [138, 146], [146, 147], [147, 148], [149, 152], [153, 154], [154, 163], [164, 175], [176, 184], [184, 185], [186, 188], [189, 192], [193, 194], [194, 197], [197, 206], [207, 213], [214, 227], [227, 228], [229, 239], [240, 242], [243, 244], [245, 258], [259, 263], [264, 272], [273, 275], [276, 277], [277, 278], [278, 280], [281, 291], [292, 294], [295, 302], [303, 310], [311, 313], [314, 315], [316, 323], [324, 333], [334, 339], [339, 340]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 7, "task"], [9, 11, "task"], [29, 30, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 7, 9, 11, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognizing", "named", "entities", "in", "text", "is", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "the", "named", "entities", "mentioned", "in", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognizing named entities in text is Named Entity Recognition, while the task of determining the identity of the named entities mentioned in text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 46], [47, 49], [50, 55], [56, 62], [63, 74], [74, 75], [76, 81], [82, 85], [86, 90], [91, 93], [94, 105], [106, 109], [110, 118], [119, 121], [122, 125], [126, 131], [132, 140], [141, 150], [151, 153], [154, 158], [159, 161], [162, 168], [169, 175], [176, 183], [183, 184]]}
{"doc_key": "ai-train-70", "ner": [[29, 30, "algorithm"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "but", "from", "version", "0.8.0", "onwards", "they", "were", "released", "in", "a", "separate", "R", "package", "sigmoid", "in", "order", "to", "allow", "a", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, but from version 0.8.0 onwards they were released in a separate R package sigmoid in order to allow a more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 102], [103, 107], [108, 115], [116, 121], [122, 129], [130, 134], [135, 139], [140, 148], [149, 151], [152, 153], [154, 162], [163, 164], [165, 172], [173, 180], [181, 183], [184, 189], [190, 192], [193, 198], [199, 200], [201, 205], [206, 213], [214, 217], [217, 218]]}
{"doc_key": "ai-train-71", "ner": [[0, 2, "programlang"], [16, 20, "organisation"], [22, 22, "organisation"], [29, 29, "location"], [31, 31, "location"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 14, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 7, 8, "artifact", "", true, false], [0, 2, 10, 11, "artifact", "", true, false], [0, 2, 13, 14, "artifact", "", true, false], [22, 22, 16, 20, "named", "", false, false], [22, 22, 29, 29, "physical", "", false, false], [29, 29, 31, 31, "physical", "", false, false], [7, 8, 16, 20, "role", "", false, false], [10, 11, 16, 20, "role", "", false, false], [13, 14, 16, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "research", "firm", "in", "Cambridge", ",", "Massachusetts", "."], "sentence-detokenized": "The logo was created in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert at Bolt, Beranek and Newman (BBN), a research firm in Cambridge, Massachusetts.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 37], [38, 46], [46, 47], [48, 55], [56, 63], [64, 67], [68, 75], [76, 82], [83, 85], [86, 90], [90, 91], [92, 99], [100, 103], [104, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 119], [120, 128], [129, 133], [134, 136], [137, 146], [146, 147], [148, 161], [161, 162]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [16, 16, "field"], [21, 22, "algorithm"], [25, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 16, 16, "compare", "", false, false], [21, 22, 16, 16, "part-of", "", false, false], [25, 26, 16, 16, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "is", "comparable", "to", "conventional", "deep", "learning", "techniques", "that", "apply", "gradient", "descent", "to", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and is comparable to conventional deep learning techniques that apply gradient descent to a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 85], [86, 96], [97, 99], [100, 112], [113, 117], [118, 126], [127, 137], [138, 142], [143, 148], [149, 157], [158, 165], [166, 168], [169, 170], [171, 177], [178, 185], [186, 190], [191, 192], [193, 198], [199, 207], [207, 208]]}
{"doc_key": "ai-train-73", "ner": [[4, 6, "algorithm"], [56, 58, "metrics"], [60, 60, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[60, 60, 56, 58, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "the", "least", "squares", "method", "to", "fit", "a", "function", "in", "terms", "of", "a", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n/", "sub", ",", "we", "can", "then", "evaluate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use the least squares method to fit a function in terms of a hyperplane \u0177 = a + \u03b2 supT/sup x to the data (x sub i/sub, y sub i/sub) sub 1 \u2264 i \u2264n/sub, we can then evaluate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 13], [14, 19], [20, 27], [28, 34], [35, 37], [38, 41], [42, 43], [44, 52], [53, 55], [56, 61], [62, 64], [65, 66], [67, 77], [78, 79], [80, 81], [82, 83], [84, 85], [86, 87], [88, 92], [92, 93], [93, 96], [97, 98], [99, 101], [102, 105], [106, 110], [111, 112], [112, 113], [114, 117], [118, 119], [119, 120], [120, 123], [123, 124], [125, 126], [127, 130], [131, 132], [132, 133], [133, 136], [136, 137], [138, 141], [142, 143], [144, 145], [146, 147], [148, 151], [151, 154], [154, 155], [156, 158], [159, 162], [163, 167], [168, 176], [177, 180], [181, 184], [185, 190], [191, 194], [195, 199], [200, 207], [208, 213], [214, 215], [215, 218], [218, 219], [219, 220]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [31, 31, "country"], [33, 33, "country"], [35, 35, "country"], [37, 38, "country"], [40, 40, "country"], [42, 42, "country"], [44, 44, "country"], [46, 47, "country"], [49, 50, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "the", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, the Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 144], [145, 156], [156, 157], [158, 164], [164, 165], [166, 175], [175, 176], [177, 182], [183, 189], [189, 190], [191, 196], [196, 197], [198, 204], [204, 205], [206, 214], [214, 215], [216, 222], [223, 226], [227, 230], [231, 237], [238, 245], [245, 246]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 8, "field"], [13, 13, "organisation"], [16, 21, "university"], [27, 29, "organisation"], [31, 34, "university"], [39, 40, "university"], [42, 43, "university"], [46, 48, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 8, "topic", "", false, false], [3, 3, 13, 13, "origin", "", false, false], [3, 3, 16, 21, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "MSc", "in", "Electrical", "and", "Computer", "Engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "Ponts", "ParisTech", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a MSc in Electrical and Computer Engineering (2000) from Inria and the University of Nice Sophia Antipolis and has held permanent positions at Siemens Corporate Technology, \u00c9cole des Ponts ParisTech and visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 90], [91, 93], [94, 98], [99, 105], [106, 115], [116, 119], [120, 123], [124, 128], [129, 138], [139, 148], [149, 151], [152, 159], [160, 169], [170, 180], [180, 181], [182, 187], [188, 191], [192, 197], [198, 207], [208, 211], [212, 220], [221, 230], [231, 233], [234, 241], [242, 252], [252, 253], [254, 258], [259, 269], [270, 273], [274, 277], [278, 288], [289, 291], [292, 299], [299, 300]]}
{"doc_key": "ai-train-76", "ner": [[8, 9, "researcher"], [0, 0, "researcher"], [14, 15, "product"], [18, 18, "country"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 8, 9, "role", "licensing_patent_to", false, false], [0, 0, 18, 18, "physical", "", false, false], [20, 20, 0, 0, "artifact", "", false, false], [20, 20, 14, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberger", "licensed", "the", "original", "patent", "granted", "to", "inventor", "George", "Devol", "and", "developed", "the", "first", "industrial", "robot", "in", "the", "US", ",", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Engelberger licensed the original patent granted to inventor George Devol and developed the first industrial robot in the US, Unimate, in the 1950s.", "token2charspan": [[0, 11], [12, 20], [21, 24], [25, 33], [34, 40], [41, 48], [49, 51], [52, 60], [61, 67], [68, 73], [74, 77], [78, 87], [88, 91], [92, 97], [98, 108], [109, 114], [115, 117], [118, 121], [122, 124], [124, 125], [126, 133], [133, 134], [135, 137], [138, 141], [142, 147], [147, 148]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [12, 13, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", ",", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition, and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [38, 39], [40, 43], [44, 47], [48, 54], [55, 57], [58, 64], [65, 71], [72, 81], [81, 82]]}
{"doc_key": "ai-train-78", "ner": [[3, 3, "programlang"], [6, 8, "programlang"], [14, 14, "programlang"], [17, 17, "programlang"], [29, 29, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 3, 14, 14, "named", "", false, false], [6, 8, 3, 3, "origin", "descendant_of", false, false], [6, 8, 17, 17, "general-affiliation", "", false, false], [6, 8, 29, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Descendants", "of", "the", "CLIPS", "language", "include", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "rewritten", "in", "Java", ",", "it", "later", "grew", "up", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired"], "sentence-detokenized": "Descendants of the CLIPS language include Jess (the rule-based part of CLIPS rewritten in Java, it later grew up in a different direction), JESS was originally inspired", "token2charspan": [[0, 11], [12, 14], [15, 18], [19, 24], [25, 33], [34, 41], [42, 46], [47, 48], [48, 51], [52, 56], [56, 57], [57, 62], [63, 67], [68, 70], [71, 76], [77, 86], [87, 89], [90, 94], [94, 95], [96, 98], [99, 104], [105, 109], [110, 112], [113, 115], [116, 117], [118, 127], [128, 137], [137, 138], [138, 139], [140, 144], [145, 148], [149, 159], [160, 168]]}
{"doc_key": "ai-train-79", "ner": [[12, 14, "product"], [17, 18, "organisation"], [22, 23, "product"], [38, 39, "product"], [41, 43, "product"], [57, 58, "misc"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6], "relations": [[17, 18, 12, 14, "usage", "", false, false], [22, 23, 17, 18, "artifact", "", false, false], [38, 39, 17, 18, "origin", "", true, false], [38, 39, 57, 58, "related-to", "", true, false], [41, 43, 17, 18, "origin", "", true, false], [41, 43, 57, 58, "related-to", "", true, false]], "relations_mapping_to_source": [1, 2, 3, 4, 5, 6], "sentence": ["The", "company", "has", "also", "created", "flexible", "intelligent", "AGV", "applications", "and", "designed", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", "used", "for", "complex", "pick", "and", "place", "operations", "in", "gantry", "systems", "and", "industrial", "robotic", "arms", "used", "in", "leading", "automotive", "plants", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "The company has also created flexible intelligent AGV applications and designed the Motivity control system used by RMT Robotics to develop the ADAM iAGV (Self-Guided Vehicle) used for complex pick and place operations in gantry systems and industrial robotic arms used in leading automotive plants to move products from process to process in non-linear layouts.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 20], [21, 28], [29, 37], [38, 49], [50, 53], [54, 66], [67, 70], [71, 79], [80, 83], [84, 92], [93, 100], [101, 107], [108, 112], [113, 115], [116, 119], [120, 128], [129, 131], [132, 139], [140, 143], [144, 148], [149, 153], [154, 155], [155, 159], [159, 160], [160, 166], [167, 174], [174, 175], [176, 180], [181, 184], [185, 192], [193, 197], [198, 201], [202, 207], [208, 218], [219, 221], [222, 228], [229, 236], [237, 240], [241, 251], [252, 259], [260, 264], [265, 269], [270, 272], [273, 280], [281, 291], [292, 298], [299, 301], [302, 306], [307, 315], [316, 320], [321, 328], [329, 331], [332, 339], [340, 342], [343, 353], [354, 361], [361, 362]]}
{"doc_key": "ai-train-80", "ner": [[7, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameters", "\u03b2", "are", "typically", "estimated", "using", "maximum", "likelihood", "."], "sentence-detokenized": "The parameters \u03b2 are typically estimated using maximum likelihood.", "token2charspan": [[0, 3], [4, 14], [15, 16], [17, 20], [21, 30], [31, 40], [41, 46], [47, 54], [55, 65], [65, 66]]}
{"doc_key": "ai-train-81", "ner": [[5, 5, "metrics"], [7, 7, "metrics"], [9, 10, "metrics"]], "ner_mapping_to_source": [1, 2, 3], "relations": [], "relations_mapping_to_source": [], "sentence": ["Information", "retrieval", "metrics", "such", "as", "precision", "and", "recall", "or", "DCG", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics such as precision and recall or DCG are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [30, 34], [35, 37], [38, 47], [48, 51], [52, 58], [59, 61], [62, 65], [66, 69], [70, 76], [77, 80], [81, 90], [91, 94], [95, 102], [103, 105], [106, 107], [108, 122], [123, 129], [129, 130]]}
{"doc_key": "ai-train-82", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "contains", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 26], [27, 35], [36, 38], [39, 49], [50, 56], [57, 64], [65, 67], [68, 73], [74, 83], [84, 94], [95, 100], [100, 101], [102, 106], [107, 110], [111, 116], [117, 120], [121, 126], [127, 130], [131, 136], [137, 144], [144, 145]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [28, 29, "task"], [31, 32, "task"], [34, 35, "task"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7], "relations": [], "relations_mapping_to_source": [], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", ",", "including", ":", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "area", "growth", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, area growth and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [88, 89], [90, 99], [99, 100], [101, 106], [107, 119], [119, 120], [121, 128], [129, 139], [139, 140], [141, 145], [146, 156], [156, 157], [158, 164], [165, 174], [174, 175], [176, 180], [181, 187], [188, 191], [192, 197], [198, 207], [207, 208]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [12, 13, "field"], [19, 21, "misc"], [25, 31, "conference"], [33, 33, "conference"], [37, 39, "misc"], [43, 47, "conference"], [42, 49, "conference"], [53, 57, "conference"], [59, 59, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 12, 13, "related-to", "contributes_to", false, false], [0, 0, 19, 21, "win-defeat", "", false, false], [0, 0, 37, 39, "win-defeat", "", false, false], [19, 21, 25, 31, "temporal", "", false, false], [33, 33, 25, 31, "named", "", false, false], [37, 39, 43, 47, "temporal", "", false, false], [37, 39, 53, 57, "temporal", "", false, false], [42, 49, 43, 47, "named", "", false, false], [59, 59, 53, 57, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "at", "international", "conferences", "and", "in", "computer", "vision", "journals", "and", "has", "won", "the", "best", "paper", "award", "at", "the", "2012", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "and", "the", "best", "reviewer", "award", "at", "the", "2012", "Asian", "Conference", "on", "Computer", "Vision", "(", "ACCV", ")", "and", "2015", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "."], "sentence-detokenized": "Xu has published more than 50 papers at international conferences and in computer vision journals and has won the best paper award at the 2012 International Conference on Non-Photorealistic Rendering and Animation (NPAR) and the best reviewer award at the 2012 Asian Conference on Computer Vision (ACCV) and 2015 International Conference on Computer Vision (ICCV).", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 72], [73, 81], [82, 88], [89, 97], [98, 101], [102, 105], [106, 109], [110, 113], [114, 118], [119, 124], [125, 130], [131, 133], [134, 137], [138, 142], [143, 156], [157, 167], [168, 170], [171, 189], [190, 199], [200, 203], [204, 213], [214, 215], [215, 219], [219, 220], [221, 224], [225, 228], [229, 233], [234, 242], [243, 248], [249, 251], [252, 255], [256, 260], [261, 266], [267, 277], [278, 280], [281, 289], [290, 296], [297, 298], [298, 302], [302, 303], [304, 307], [308, 312], [313, 326], [327, 337], [338, 340], [341, 349], [350, 356], [357, 358], [358, 362], [362, 363], [363, 364]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 13, "researcher"], [16, 20, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 2, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [16, 20, 0, 0, "usage", "", false, false], [16, 20, 13, 13, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "an", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "project", "on", "artificial", "intelligence", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is an ontology language used by Doug Lenat's Cyc project on artificial intelligence.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 67], [68, 76], [77, 81], [82, 84], [85, 89], [90, 95], [95, 97], [98, 101], [102, 109], [110, 112], [113, 123], [124, 136], [136, 137]]}
{"doc_key": "ai-train-86", "ner": [[2, 3, "task"], [5, 7, "metrics"], [13, 16, "metrics"], [18, 25, "metrics"], [34, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "part-of", "", false, false], [13, 16, 5, 7, "named", "", false, false], [18, 25, 5, 7, "named", "", false, false], [34, 37, 5, 7, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "mean", "square", "error", ",", "often", "referred", "to", "as", "mean", "square", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "square", "error", ",", "can", "refer", "to", "the", "mean", "of", "the", "squared", "deviations", "of", "the", "predictions", "from", "the", "true", "values", "over", "an", "out", "-", "of", "-", "sample", "test", "area", "generated", "by", "a", "model", "estimated", "over", "a", "specific", "sample", "area", "."], "sentence-detokenized": "Also in regression analysis, mean square error, often referred to as mean square prediction error or out-of-sample mean square error, can refer to the mean of the squared deviations of the predictions from the true values over an out-of-sample test area generated by a model estimated over a specific sample area.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 33], [34, 40], [41, 46], [46, 47], [48, 53], [54, 62], [63, 65], [66, 68], [69, 73], [74, 80], [81, 91], [92, 97], [98, 100], [101, 104], [104, 105], [105, 107], [107, 108], [108, 114], [115, 119], [120, 126], [127, 132], [132, 133], [134, 137], [138, 143], [144, 146], [147, 150], [151, 155], [156, 158], [159, 162], [163, 170], [171, 181], [182, 184], [185, 188], [189, 200], [201, 205], [206, 209], [210, 214], [215, 221], [222, 226], [227, 229], [230, 233], [233, 234], [234, 236], [236, 237], [237, 243], [244, 248], [249, 253], [254, 263], [264, 266], [267, 268], [269, 274], [275, 284], [285, 289], [290, 291], [292, 300], [301, 307], [308, 312], [312, 313]]}
{"doc_key": "ai-train-87", "ner": [[5, 7, "algorithm"], [18, 23, "algorithm"]], "ner_mapping_to_source": [0, 2], "relations": [[5, 7, 18, 23, "named", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["The", "results", "show", "that", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptors", "having", "a", "slight", "advantage", "in", "terms", "of", "error", "detection", "rate", "at", "fixed", "FALSK", "positive", "rates", "across", "both", "datasets", "."], "sentence-detokenized": "The results show that the C-HOG and R-HOG block descriptors perform comparably, with the C-HOG descriptors having a slight advantage in terms of error detection rate at fixed FALSK positive rates across both datasets.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 21], [22, 25], [26, 27], [27, 28], [28, 31], [32, 35], [36, 38], [38, 41], [42, 47], [48, 59], [60, 67], [68, 78], [78, 79], [80, 84], [85, 88], [89, 90], [90, 91], [91, 94], [95, 106], [107, 113], [114, 115], [116, 122], [123, 132], [133, 135], [136, 141], [142, 144], [145, 150], [151, 160], [161, 165], [166, 168], [169, 174], [175, 180], [181, 189], [190, 195], [196, 202], [203, 207], [208, 216], [216, 217]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [17, 19, "algorithm"], [22, 24, "algorithm"], [26, 28, "algorithm"], [30, 31, "misc"], [36, 38, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 30, 31, "usage", "", false, false], [14, 15, 30, 31, "usage", "", false, false], [17, 19, 30, 31, "usage", "", false, false], [22, 24, 30, 31, "usage", "", false, false], [26, 28, 30, 31, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "the", "Fisherface", "algorithm", ",", "the", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neuronally", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using the Fisherface algorithm, the hidden Markov model, multilinear subspace learning using tensor representation, and neuronally motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 142], [143, 153], [154, 163], [163, 164], [165, 168], [169, 175], [176, 182], [183, 188], [188, 189], [190, 201], [202, 210], [211, 219], [220, 225], [226, 232], [233, 247], [247, 248], [249, 252], [253, 263], [264, 273], [274, 281], [282, 286], [287, 295], [295, 296]]}
{"doc_key": "ai-train-89", "ner": [[3, 7, "misc"], [18, 20, "location"], [38, 40, "location"], [54, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[18, 20, 3, 7, "temporal", "", false, false], [38, 40, 3, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "may", "now", "be", "banned", "from", "showing", "at", "the", "Scotiabank", "Theatre", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "from", "showing", "elsewhere", "(", "such", "as", "at", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "by", "a", "service", "like", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films may now be banned from showing at the Scotiabank Theatre Toronto - one of the festival's main venues - and from showing elsewhere (such as at TIFF Bell Lightbox and other local cinemas) if they are distributed by a service like Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 69], [70, 73], [74, 76], [77, 83], [84, 88], [89, 96], [97, 99], [100, 103], [104, 114], [115, 122], [123, 130], [131, 132], [133, 136], [137, 139], [140, 143], [144, 152], [152, 154], [155, 159], [160, 166], [167, 168], [169, 172], [173, 177], [178, 185], [186, 195], [196, 197], [197, 201], [202, 204], [205, 207], [208, 212], [213, 217], [218, 226], [227, 230], [231, 236], [237, 242], [243, 250], [250, 251], [252, 254], [255, 259], [260, 263], [264, 275], [276, 278], [279, 280], [281, 288], [289, 293], [294, 301], [301, 302]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 4, "researcher"], [5, 6, "organisation"], [12, 13, "researcher"], [23, 28, "product"], [36, 37, "researcher"], [39, 41, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 0, 5, 6, "related-to", "purchases", false, false], [2, 4, 12, 13, "named", "same", false, false], [2, 4, 36, 37, "named", "same", false, false], [5, 6, 2, 4, "origin", "founded_by", false, false], [23, 28, 0, 0, "artifact", "", false, false], [39, 41, 36, 37, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Unimation", "acquired", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "producing", "the", "Programmable", "Universal", "Assembly", "Machine", ",", "a", "new", "model", "of", "robotic", "arm", ",", "using", "Scheinman", "'s", "advanced", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation acquired Victor Scheinman's Vicarm Inc. in 1977, and with Scheinman's help, the company created and began producing the Programmable Universal Assembly Machine, a new model of robotic arm, using Scheinman's advanced VAL programming language.", "token2charspan": [[0, 9], [10, 18], [19, 25], [26, 35], [35, 37], [38, 44], [45, 49], [50, 52], [53, 57], [57, 58], [59, 62], [63, 67], [68, 77], [77, 79], [80, 84], [84, 85], [86, 89], [90, 97], [98, 105], [106, 109], [110, 115], [116, 125], [126, 129], [130, 142], [143, 152], [153, 161], [162, 169], [169, 170], [171, 172], [173, 176], [177, 182], [183, 185], [186, 193], [194, 197], [197, 198], [199, 204], [205, 214], [214, 216], [217, 225], [226, 229], [230, 241], [242, 250], [250, 251]]}
{"doc_key": "ai-train-91", "ner": [[0, 2, "product"], [10, 11, "algorithm"], [13, 17, "product"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 2, 10, 11, "origin", "implementation_of", false, false], [0, 2, 13, 17, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[12, 15, "product"], [20, 26, "misc"]], "ner_mapping_to_source": [1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "over", "20,000", "times", "according", "to", "Google", "Scholar", ",", "and", "it", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", "for", "2016", ",", "which", "is", "a", "sign", "that", "a", "paper", "has", "had", "exceptional", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited over 20,000 times according to Google Scholar, and it also received the IEEE Signal Processing Society Sustained Impact Award for 2016, which is a sign that a paper has had exceptional impact for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 46], [47, 52], [53, 62], [63, 65], [66, 72], [73, 80], [80, 81], [82, 85], [86, 88], [89, 93], [94, 102], [103, 106], [107, 111], [112, 118], [119, 129], [130, 137], [138, 147], [148, 154], [155, 160], [161, 164], [165, 169], [169, 170], [171, 176], [177, 179], [180, 181], [182, 186], [187, 191], [192, 193], [194, 199], [200, 203], [204, 207], [208, 219], [220, 226], [227, 230], [231, 233], [234, 239], [240, 242], [243, 248], [249, 254], [255, 266], [266, 267]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [25, 26, "product"], [36, 38, "product"], [41, 41, "organisation"], [42, 42, "product"], [47, 47, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 41, 41, "artifact", "", false, false], [25, 26, 0, 1, "related-to", "performs", false, false], [25, 26, 36, 38, "part-of", "", false, false], [41, 41, 47, 47, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "close", "to", "being", "completely", "indistinguishable", "from", "a", "real", "human", "voice", "with", "the", "introduction", "in", "2016", "of", "the", "voice", "processing", "and", "generation", "software", "Adobe", "Voco", ",", "a", "prototype", "planned", "to", "become", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", "."], "sentence-detokenized": "Speech synthesis is close to being completely indistinguishable from a real human voice with the introduction in 2016 of the voice processing and generation software Adobe Voco, a prototype planned to become part of the Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 25], [26, 28], [29, 34], [35, 45], [46, 63], [64, 68], [69, 70], [71, 75], [76, 81], [82, 87], [88, 92], [93, 96], [97, 109], [110, 112], [113, 117], [118, 120], [121, 124], [125, 130], [131, 141], [142, 145], [146, 156], [157, 165], [166, 171], [172, 176], [176, 177], [178, 179], [180, 189], [190, 197], [198, 200], [201, 207], [208, 212], [213, 215], [216, 219], [220, 225], [226, 234], [235, 240], [240, 241], [242, 245], [246, 254], [255, 262], [262, 263], [264, 265], [266, 275], [276, 280], [281, 287], [287, 288]]}
{"doc_key": "ai-train-94", "ner": [[0, 1, "researcher"], [7, 9, "organisation"], [15, 20, "organisation"], [27, 27, "conference"], [34, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 7, 9, "role", "", false, false], [0, 1, 15, 20, "role", "", false, false], [0, 1, 27, 27, "role", "", false, false], [0, 1, 34, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "member", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "and", "a", "founding", "member", "of", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary member of the Neuroscience Research Program, a fellow of the American Academy of Arts and Sciences, and a founding member of AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 125], [126, 127], [128, 136], [137, 143], [144, 146], [147, 151], [152, 155], [156, 157], [158, 166], [167, 173], [174, 176], [177, 180], [181, 189], [190, 199], [200, 203], [204, 209], [210, 218], [218, 219]]}
{"doc_key": "ai-train-95", "ner": [[8, 9, "task"], [11, 12, "task"], [16, 17, "task"], [24, 24, "misc"], [25, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 9, 16, 17, "cause-effect", "", false, false], [11, 12, 16, 17, "cause-effect", "", false, false], [25, 26, 16, 17, "topic", "", false, false], [25, 26, 24, 24, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["During", "the", "1990s", ",", "encouraged", "by", "successes", "in", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "into", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "During the 1990s, encouraged by successes in speech recognition and speech synthesis, research into speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 6], [7, 10], [11, 16], [16, 17], [18, 28], [29, 31], [32, 41], [42, 44], [45, 51], [52, 63], [64, 67], [68, 74], [75, 84], [84, 85], [86, 94], [95, 99], [100, 106], [107, 118], [119, 124], [125, 129], [130, 133], [134, 145], [146, 148], [149, 152], [153, 159], [160, 169], [170, 177], [177, 178]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [14, 15, "algorithm"], [19, 20, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 4, 8, 9, "role", "", false, false], [14, 15, 3, 4, "origin", "", false, false], [14, 15, 8, 9, "origin", "", false, false], [14, 15, 11, 12, "origin", "", false, false], [19, 20, 14, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisor", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "forget", "gate", "(", "also", "called", "keep", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisor J\u00fcrgen Schmidhuber and Fred Cummins introduced forget gate (also called keep gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 35], [36, 42], [43, 54], [55, 58], [59, 63], [64, 71], [72, 82], [83, 89], [90, 94], [95, 96], [96, 100], [101, 107], [108, 112], [113, 117], [117, 118], [119, 123], [124, 127], [128, 132], [133, 145], [145, 146]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 1, 3, "part-of", "", false, false], [9, 11, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sinc", "function", "is", "commonly", "defined", "by"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sinc function is commonly defined by", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 93], [94, 101], [102, 104]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [18, 21, "conference"], [24, 28, "organisation"], [30, 30, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 18, 21, "role", "", false, false], [9, 10, 24, 28, "role", "", false, false], [30, 30, 24, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 72], [72, 73], [74, 75], [76, 84], [85, 91], [92, 94], [95, 99], [100, 103], [104, 115], [116, 119], [120, 133], [134, 145], [146, 149], [150, 153], [154, 167], [168, 177], [178, 180], [181, 194], [195, 206], [207, 208], [208, 212], [212, 213], [213, 214]]}
{"doc_key": "ai-train-99", "ner": [[8, 13, "misc"], [18, 18, "misc"], [47, 49, "metrics"], [51, 56, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[51, 56, 47, 49, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "a", "dimensional", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPD", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "allows", "capturing", "the", "nonlinear", "products", "of", "the", "digital", "pre-distorter", "coefficients", "and", "minimizing", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "in", "order", "to", "solve", "for", "the", "digital", "pre-distorter", "coefficients", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In a dimensional polynomial-based memory (or memoryless) DPD, the distorted output of the nonlinear system must be oversampled at a rate that allows capturing the nonlinear products of the digital pre-distorter coefficients and minimizing the mean squared error (MSE), in order to solve for the digital pre-distorter coefficients.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 33], [34, 45], [46, 56], [56, 57], [57, 62], [63, 69], [70, 71], [71, 73], [74, 84], [84, 85], [86, 89], [89, 90], [91, 94], [95, 104], [105, 111], [112, 114], [115, 118], [119, 128], [129, 135], [136, 140], [141, 143], [144, 155], [156, 158], [159, 160], [161, 165], [166, 170], [171, 177], [178, 187], [188, 191], [192, 201], [202, 210], [211, 213], [214, 217], [218, 225], [226, 239], [240, 252], [253, 256], [257, 267], [268, 271], [272, 276], [277, 284], [285, 290], [291, 292], [292, 295], [295, 296], [296, 297], [298, 300], [301, 306], [307, 309], [310, 315], [316, 319], [320, 323], [324, 331], [332, 345], [346, 358], [358, 359]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [10, 10, "location"], [12, 13, "location"], [15, 16, "country"], [19, 19, "location"], [21, 21, "country"], [35, 41, "organisation"], [44, 47, "organisation"], [49, 49, "location"], [56, 57, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 10, 10, "physical", "", false, false], [0, 1, 44, 47, "physical", "", false, false], [0, 1, 56, 57, "role", "", false, false], [10, 10, 12, 13, "physical", "", false, false], [12, 13, 15, 16, "physical", "", false, false], [35, 41, 44, 47, "part-of", "", false, false], [44, 47, 49, 49, "physical", "", false, false], [56, 57, 35, 41, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "October", "5", ",", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldavian", "SSR", ",", "Soviet", "Union", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "an", "American", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "and", "leader", "of", "the", "laboratory", "'s", "InfoLab", "Group", "."], "sentence-detokenized": "Boris Katz, (born October 5, 1947 in Chi\u0219in\u0103u, Moldavian SSR, Soviet Union (now Chi\u0219in\u0103u, Moldova)) is an American research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and leader of the laboratory's InfoLab Group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 25], [26, 27], [27, 28], [29, 33], [34, 36], [37, 45], [45, 46], [47, 56], [57, 60], [60, 61], [62, 68], [69, 74], [75, 76], [76, 79], [80, 88], [88, 89], [90, 97], [97, 98], [98, 99], [100, 102], [103, 105], [106, 114], [115, 123], [124, 133], [134, 135], [135, 143], [144, 153], [153, 154], [155, 157], [158, 161], [162, 165], [166, 174], [175, 182], [183, 186], [187, 197], [198, 210], [211, 221], [222, 224], [225, 228], [229, 242], [243, 252], [253, 255], [256, 266], [267, 269], [270, 279], [280, 283], [284, 290], [291, 293], [294, 297], [298, 308], [308, 310], [311, 318], [319, 324], [324, 325]]}
