{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [29, 30, "field"], [31, 33, "researcher"], [35, 37, "researcher"], [39, 40, "researcher"], [42, 43, "researcher"], [45, 47, "researcher"], [49, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 20, "part-of", "", false, false], [3, 7, 19, 20, "usage", "", false, false], [3, 7, 29, 30, "part-of", "", false, false], [3, 7, 29, 30, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "opinion", "-", "based", "recommender", "system", "use", "various", "techniques", ",", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to opinion-based recommender system use various techniques, including text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 54], [55, 58], [59, 66], [67, 77], [77, 78], [79, 88], [89, 93], [94, 100], [100, 101], [102, 113], [114, 123], [123, 124], [125, 134], [135, 143], [144, 145], [145, 148], [149, 153], [154, 164], [165, 174], [175, 183], [183, 184], [185, 188], [189, 193], [194, 202], [203, 206], [206, 207], [208, 212], [212, 213], [214, 215], [215, 216], [217, 222], [222, 223], [224, 228], [229, 232], [232, 233], [234, 238], [239, 244], [244, 245], [246, 247], [247, 248], [249, 252], [252, 253], [254, 258], [259, 264], [264, 265], [266, 270], [271, 275], [275, 276], [277, 279], [280, 282], [282, 283], [284, 285], [285, 289], [289, 290], [290, 291], [291, 292], [293, 295], [296, 297], [297, 298], [298, 299], [299, 300], [301, 307], [307, 308]]}
{"doc_key": "ai-train-2", "ner": [[8, 8, "university"], [14, 15, "researcher"], [17, 18, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[14, 15, 8, 8, "physical", "", false, false], [14, 15, 8, 8, "role", "", false, false], [17, 18, 8, 8, "physical", "", false, false], [17, 18, 8, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Advocates", "of", "procedural", "representations", "were", "mainly", "centred", "at", "MIT", ",", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Advocates of procedural representations were mainly centred at MIT, under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 9], [10, 12], [13, 23], [24, 39], [40, 44], [45, 51], [52, 59], [60, 62], [63, 66], [66, 67], [68, 73], [74, 77], [78, 88], [89, 91], [92, 98], [99, 105], [106, 109], [110, 117], [118, 124], [124, 125]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [26, 26, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 26, 26, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "allows", "linear", "and", "non-linear", "problems", "to", "be", "solved", "numerically", ",", "and", "other", "numerical", "experiments", "to", "be", "performed", ",", "using", "software", "that", "is", "largely", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave allows linear and non-linear problems to be solved numerically, and other numerical experiments to be performed, using software that is largely compatible with MATLAB.", "token2charspan": [[0, 6], [7, 13], [14, 20], [21, 24], [25, 35], [36, 44], [45, 47], [48, 50], [51, 57], [58, 69], [69, 70], [71, 74], [75, 80], [81, 90], [91, 102], [103, 105], [106, 108], [109, 118], [118, 119], [120, 125], [126, 134], [135, 139], [140, 142], [143, 150], [151, 161], [162, 166], [167, 173], [173, 174]]}
{"doc_key": "ai-train-5", "ner": [[3, 4, "algorithm"], [6, 7, "misc"], [9, 10, "researcher"], [15, 17, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 4, 9, 10, "origin", "", false, false], [6, 7, 9, 10, "origin", "", false, false], [9, 10, 15, 17, "physical", "", false, false], [9, 10, 15, 17, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variations", "of", "the", "backpropagation", "algorithm", "and", "unsupervised", "methods", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variations of the backpropagation algorithm and unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 10], [11, 13], [14, 17], [18, 33], [34, 43], [44, 47], [48, 60], [61, 68], [69, 71], [72, 77], [78, 84], [85, 88], [89, 99], [100, 102], [103, 106], [107, 117], [118, 120], [121, 128], [129, 132], [133, 135], [136, 140], [141, 143], [144, 149], [150, 154], [154, 155], [156, 162], [163, 172], [173, 179], [180, 193], [193, 194], [195, 196], [196, 197], [197, 201], [202, 209]]}
{"doc_key": "ai-train-6", "ner": [[3, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalently", "using", "DCG", "notation", ":"], "sentence-detokenized": "or equivalently using DCG notation:", "token2charspan": [[0, 2], [3, 15], [16, 21], [22, 25], [26, 34], [34, 35]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 9, "algorithm"], [14, 17, "algorithm"], [20, 22, "algorithm"], [28, 30, "algorithm"], [26, 27, "algorithm"], [44, 45, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 9, "type-of", "", false, false], [0, 3, 14, 17, "usage", "part-of?", true, false], [14, 17, 20, 22, "compare", "", false, false], [28, 30, 20, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "apply", "competitive", "learning", ",", "as", "opposed", "to", "error", "correction", "learning", "(", "such", "as", "gradient", "descent", "back", "-", "propagation", ")", ",", "and", "in", "that", "they", "use", "a", "neighbourhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks in that they apply competitive learning, as opposed to error correction learning (such as gradient descent back-propagation), and in that they use a neighbourhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 84], [85, 96], [97, 105], [105, 106], [107, 109], [110, 117], [118, 120], [121, 126], [127, 137], [138, 146], [147, 148], [148, 152], [153, 155], [156, 164], [165, 172], [173, 177], [177, 178], [178, 189], [189, 190], [190, 191], [192, 195], [196, 198], [199, 203], [204, 208], [209, 212], [213, 214], [215, 228], [229, 237], [238, 240], [241, 249], [250, 253], [254, 265], [266, 276], [277, 279], [280, 283], [284, 289], [290, 295], [295, 296]]}
{"doc_key": "ai-train-8", "ner": [[10, 14, "organisation"], [27, 28, "misc"], [36, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "several", "organisations", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", ",", "which", "is", "then", "filtered", "in", "the", "noise", "floor", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "dubious", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "inhibiting", "circuits", "."], "sentence-detokenized": "Since the early 1990s, several organisations, including the Audio Engineering Society, have recommended that dynamic range measurements be made in the presence of an audio signal, which is then filtered in the noise floor measurement used to determine dynamic range. This avoids dubious measurements based on the use of blank media or inhibiting circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 30], [31, 44], [44, 45], [46, 55], [56, 59], [60, 65], [66, 77], [78, 85], [85, 86], [87, 91], [92, 103], [104, 108], [109, 116], [117, 122], [123, 135], [136, 138], [139, 143], [144, 146], [147, 150], [151, 159], [160, 162], [163, 165], [166, 171], [172, 178], [178, 179], [180, 185], [186, 188], [189, 193], [194, 202], [203, 205], [206, 209], [210, 215], [216, 221], [222, 233], [234, 238], [239, 241], [242, 251], [252, 259], [260, 265], [265, 266], [267, 271], [272, 278], [279, 286], [287, 299], [300, 305], [306, 308], [309, 312], [313, 316], [317, 319], [320, 325], [326, 331], [332, 334], [335, 345], [346, 354], [354, 355]]}
{"doc_key": "ai-train-9", "ner": [[5, 6, "misc"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [30, 34, "task"], [36, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[5, 6, 17, 18, "part-of", "concept_used_in", true, false], [5, 6, 20, 21, "part-of", "concept_used_in", false, false], [5, 6, 23, 24, "part-of", "concept_used_in", false, false], [5, 6, 26, 27, "part-of", "concept_used_in", false, false], [5, 6, 30, 34, "part-of", "concept_used_in", false, false], [5, 6, 36, 38, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["The", "technique", "used", "to", "create", "clean", "faces", "and", "use", "them", "for", "recognition", "is", "also", "used", "outside", "of", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "speech", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "The technique used to create clean faces and use them for recognition is also used outside of face recognition: handwriting recognition, lip reading, speech recognition, sign language/hand gesture interpretation and medical image analysis.", "token2charspan": [[0, 3], [4, 13], [14, 18], [19, 21], [22, 28], [29, 34], [35, 40], [41, 44], [45, 48], [49, 53], [54, 57], [58, 69], [70, 72], [73, 77], [78, 82], [83, 90], [91, 93], [94, 98], [99, 110], [110, 111], [112, 123], [124, 135], [135, 136], [137, 140], [141, 148], [148, 149], [150, 156], [157, 168], [168, 169], [170, 174], [175, 183], [183, 184], [184, 188], [189, 196], [197, 211], [212, 215], [216, 223], [224, 229], [230, 238], [238, 239]]}
{"doc_key": "ai-train-10", "ner": [[1, 3, "organisation"], [10, 14, "organisation"], [16, 16, "organisation"], [20, 23, "organisation"], [26, 30, "organisation"], [33, 36, "organisation"], [39, 43, "organisation"], [45, 45, "organisation"], [49, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 14, 1, 3, "part-of", "", false, false], [16, 16, 10, 14, "named", "", false, false], [20, 23, 1, 3, "part-of", "", false, false], [26, 30, 1, 3, "part-of", "", false, false], [33, 36, 1, 3, "part-of", "", false, false], [39, 43, 1, 3, "part-of", "", false, false], [45, 45, 39, 43, "named", "", false, false], [49, 54, 1, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "the", "umbrella", "organisation", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defence", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", ",", "which", "coordinated", "studies", "to", "inform", "strategic", "planners", "'", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was the umbrella organisation for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defence, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research, which coordinated studies to inform strategic planners' deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 39], [40, 48], [49, 61], [62, 65], [66, 69], [70, 78], [79, 90], [91, 94], [95, 100], [101, 115], [116, 117], [117, 121], [121, 122], [122, 123], [124, 127], [128, 130], [131, 141], [142, 144], [145, 151], [151, 152], [153, 156], [157, 159], [160, 170], [171, 173], [174, 182], [183, 187], [187, 188], [189, 192], [193, 195], [196, 206], [207, 209], [210, 217], [217, 218], [219, 222], [223, 230], [231, 239], [240, 248], [249, 257], [258, 264], [265, 266], [266, 271], [271, 272], [273, 276], [277, 280], [281, 287], [288, 290], [291, 296], [297, 305], [305, 306], [307, 312], [313, 324], [325, 332], [333, 335], [336, 342], [343, 352], [353, 361], [361, 362], [363, 376], [376, 377]]}
{"doc_key": "ai-train-11", "ner": [[5, 9, "metrics"], [10, 11, "algorithm"], [15, 17, "researcher"], [21, 21, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 9, 10, 11, "part-of", "", false, false], [15, 17, 21, 21, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "quick", "method", "of", "calculating", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "as", "an", "appendix", "to", "Bliss", "'", "work", "in", "1935", "."], "sentence-detokenized": "A quick method of calculating maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an appendix to Bliss' work in 1935.", "token2charspan": [[0, 1], [2, 7], [8, 14], [15, 17], [18, 29], [30, 37], [38, 48], [49, 58], [59, 62], [63, 66], [67, 73], [74, 79], [80, 83], [84, 92], [93, 95], [96, 102], [103, 109], [110, 112], [113, 115], [116, 124], [125, 127], [128, 133], [133, 134], [135, 139], [140, 142], [143, 147], [147, 148]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 15, "product"], [18, 18, "organisation"], [19, 20, "product"], [24, 24, "organisation"], [26, 26, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[19, 20, 14, 15, "usage", "uses_software", false, false], [19, 20, 18, 18, "artifact", "", false, false], [19, 20, 26, 26, "named", "", false, false], [26, 26, 24, 24, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "powers", "AltaVista", "'s", "BabelFish", "(", "which", "became", "Yahoo", "'s", "Babelfish", "on", "9", "May", "2008", ")", "."], "sentence-detokenized": "Several of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (which became Yahoo's Babelfish on 9 May 2008).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 39], [40, 46], [46, 47], [48, 52], [53, 55], [56, 62], [63, 72], [73, 76], [77, 80], [81, 88], [89, 95], [96, 100], [101, 107], [108, 117], [117, 119], [120, 129], [130, 131], [131, 136], [137, 143], [144, 149], [149, 151], [152, 161], [162, 164], [165, 166], [167, 170], [171, 175], [175, 176], [176, 177]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 22, "field"], [26, 27, "misc"], [31, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 31, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 31, 32, "related-to", "", true, false], [10, 11, 20, 22, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 31, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-driven", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-driven reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 202], [203, 216], [217, 225], [225, 226]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [14, 20, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 14, 20, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "method", "is", "to", "use", "the", "so", "-", "called", "RED", "measure", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "."], "sentence-detokenized": "The most common method is to use the so-called RED measure (Recall-Oriented Understudy for Gisting Evaluation).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 22], [23, 25], [26, 28], [29, 32], [33, 36], [37, 39], [39, 40], [40, 46], [47, 50], [51, 58], [59, 60], [60, 66], [66, 67], [67, 75], [76, 86], [87, 90], [91, 98], [99, 109], [109, 110], [110, 111]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [15, 15, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [0, 0, 15, 15, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "learning", "patterns", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides learning patterns, models and algorithms and can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 37], [37, 38], [39, 45], [46, 49], [50, 60], [61, 64], [65, 68], [69, 71], [72, 80], [81, 86], [87, 88], [89, 92], [93, 99], [100, 107], [107, 108], [109, 114], [115, 121], [121, 122], [123, 128], [129, 137], [137, 138], [139, 141], [142, 150], [151, 155], [155, 156]]}
{"doc_key": "ai-train-16", "ner": [[1, 3, "product"], [8, 8, "programlang"], [10, 11, "product"]], "ner_mapping_to_source": [0, 4, 5], "relations": [[1, 3, 10, 11, "related-to", "", true, false], [10, 11, 8, 8, "general-affiliation", "", true, false]], "relations_mapping_to_source": [2, 4], "sentence": ["The", "most", "recent", "version", ",", "entirely", "based", "on", "Java", "(", "Weka", "3", ")", ",", "whose", "development", "started", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "especially", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "The most recent version, entirely based on Java (Weka 3), whose development started in 1997, is now used in many different application areas, especially for educational and research purposes.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [23, 24], [25, 33], [34, 39], [40, 42], [43, 47], [48, 49], [49, 53], [54, 55], [55, 56], [56, 57], [58, 63], [64, 75], [76, 83], [84, 86], [87, 91], [91, 92], [93, 95], [96, 99], [100, 104], [105, 107], [108, 112], [113, 122], [123, 134], [135, 140], [140, 141], [142, 152], [153, 156], [157, 168], [169, 172], [173, 181], [182, 190], [190, 191]]}
{"doc_key": "ai-train-17", "ner": [[0, 1, "product"], [14, 21, "misc"], [24, 26, "misc"], [29, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 21, 0, 1, "topic", "", false, false], [14, 21, 24, 26, "win-defeat", "", false, false], [24, 26, 29, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "has", "made", "many", "interesting", "discoveries", "and", "has", "received", "much", "recognition", ",", "his", "paper", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "won", "the", "best", "paper", "award", "at", "the", "1982", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "."], "sentence-detokenized": "Eurisko has made many interesting discoveries and has received much recognition, his paper Heuretics: Theoretical and Study of Heuristic Rules won the best paper award at the 1982 Association for the Advancement of Artificial Intelligence.", "token2charspan": [[0, 7], [8, 11], [12, 16], [17, 21], [22, 33], [34, 45], [46, 49], [50, 53], [54, 62], [63, 67], [68, 79], [79, 80], [81, 84], [85, 90], [91, 100], [100, 101], [102, 113], [114, 117], [118, 123], [124, 126], [127, 136], [137, 142], [143, 146], [147, 150], [151, 155], [156, 161], [162, 167], [168, 170], [171, 174], [175, 179], [180, 191], [192, 195], [196, 199], [200, 211], [212, 214], [215, 225], [226, 238], [238, 239]]}
{"doc_key": "ai-train-18", "ner": [[8, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "account", "for", "multiple", "entities", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To account for multiple entities, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 10], [11, 14], [15, 23], [24, 32], [32, 33], [34, 35], [36, 44], [45, 50], [51, 55], [56, 58], [59, 69], [70, 73], [74, 78], [79, 86], [86, 87]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 13, "product"], [15, 16, "product"], [18, 19, "product"], [21, 23, "product"], [31, 32, "product"], [37, 40, "product"], [43, 44, "product"], [46, 47, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 31, 32, "type-of", "", false, false], [12, 13, 31, 32, "type-of", "", false, false], [15, 16, 31, 32, "type-of", "", false, false], [18, 19, 31, 32, "type-of", "", false, false], [21, 23, 31, 32, "type-of", "", false, false], [43, 44, 37, 40, "type-of", "", false, false], [46, 47, 37, 40, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "emergence", "of", "conversational", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "it", "is", "now", "possible", "to", "access", "voice", "portals", "via", "mobile", "devices", "and", "Far", "Field", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the emergence of conversational assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, it is now possible to access voice portals via mobile devices and Far Field smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 36], [37, 47], [48, 52], [53, 55], [56, 61], [61, 63], [64, 68], [68, 69], [70, 76], [77, 82], [82, 83], [84, 90], [91, 100], [100, 101], [102, 111], [112, 119], [120, 123], [124, 131], [131, 133], [134, 139], [139, 140], [141, 143], [144, 146], [147, 150], [151, 159], [160, 162], [163, 169], [170, 175], [176, 183], [184, 187], [188, 194], [195, 202], [203, 206], [207, 210], [211, 216], [217, 222], [223, 231], [232, 236], [237, 239], [240, 246], [247, 251], [252, 255], [256, 262], [263, 267], [267, 268]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [6, 8, "algorithm"], [11, 13, "algorithm"], [15, 16, "algorithm"], [19, 19, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 3, "type-of", "", false, false], [11, 13, 2, 3, "type-of", "", false, false], [15, 16, 2, 3, "type-of", "", false, false], [19, 19, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "the", "Naive", "Bayes", "classifier", ",", "the", "support", "vector", "machine", ",", "Gaussian", "mixtures", "and", "the", "network", "."], "sentence-detokenized": "Examples of supervised learning are the Naive Bayes classifier, the support vector machine, Gaussian mixtures and the network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 39], [40, 45], [46, 51], [52, 62], [62, 63], [64, 67], [68, 75], [76, 82], [83, 90], [90, 91], [92, 100], [101, 109], [110, 113], [114, 117], [118, 125], [125, 126]]}
{"doc_key": "ai-train-21", "ner": [[4, 5, "algorithm"], [27, 29, "algorithm"], [31, 31, "task"], [36, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 5, 27, 29, "part-of", "", true, false], [36, 37, 31, 31, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "derive", "mathematical", "regret", "bounds", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "math", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "for", "classification", ",", "which", "uses", "the", "hinge", "loss", "math", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to derive mathematical regret bounds O (\\ sqrt {T}) / math for the online version of the support vector machine for classification, which uses the hinge loss math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 39], [40, 52], [53, 59], [60, 66], [67, 68], [69, 70], [70, 71], [72, 76], [77, 78], [78, 79], [79, 80], [80, 81], [82, 83], [84, 88], [89, 92], [93, 96], [97, 103], [104, 111], [112, 114], [115, 118], [119, 126], [127, 133], [134, 141], [142, 145], [146, 160], [160, 161], [162, 167], [168, 172], [173, 176], [177, 182], [183, 187], [188, 192], [193, 194], [195, 197], [198, 199], [199, 200], [200, 201], [202, 204], [205, 208], [208, 209], [210, 211], [211, 212], [212, 213], [214, 215], [216, 217], [218, 219], [220, 222], [223, 224], [224, 225], [225, 226], [227, 231], [232, 233], [234, 236], [236, 237], [237, 239], [240, 241], [242, 246]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"], [26, 27, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", "and", "match", "moving", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, individual wildlife identification and match moving.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 102], [102, 103], [104, 111], [112, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 151], [152, 160], [161, 175], [176, 179], [180, 185], [186, 192], [192, 193]]}
{"doc_key": "ai-train-23", "ner": [[10, 11, "task"], [16, 17, "university"], [19, 21, "university"], [23, 24, "university"], [26, 27, "university"], [29, 34, "university"], [36, 38, "university"], [40, 42, "university"], [44, 45, "university"], [47, 52, "university"], [54, 54, "university"], [57, 61, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[10, 11, 16, 17, "related-to", "", true, false], [10, 11, 19, 21, "related-to", "", true, false], [10, 11, 23, 24, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 29, 34, "related-to", "", true, false], [10, 11, 36, 38, "related-to", "", true, false], [10, 11, 40, 42, "related-to", "", true, false], [10, 11, 44, 45, "related-to", "", true, false], [10, 11, 47, 52, "related-to", "", true, false], [10, 11, 54, 54, "related-to", "", true, false], [10, 11, 57, 61, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "conducting", "research", "on", "pose", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are conducting research on pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Science and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 47], [48, 56], [57, 59], [60, 64], [65, 75], [75, 76], [77, 86], [87, 93], [94, 96], [97, 102], [103, 113], [113, 114], [115, 123], [124, 130], [131, 141], [141, 142], [143, 146], [147, 159], [159, 160], [161, 169], [170, 180], [180, 181], [182, 192], [193, 195], [196, 206], [206, 207], [208, 211], [212, 217], [217, 218], [219, 229], [230, 232], [233, 240], [240, 241], [242, 247], [248, 256], [257, 262], [262, 263], [264, 267], [268, 274], [274, 275], [276, 284], [285, 295], [296, 298], [299, 306], [307, 310], [311, 321], [322, 323], [323, 327], [327, 328], [329, 332], [333, 343], [344, 346], [347, 357], [357, 358], [359, 365], [365, 366]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "Sigmoid", "Cross", "Entropy", "Loss", "function", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "Math", "0.1", "/", "Math", "."], "sentence-detokenized": "The Sigmoid Cross Entropy Loss function is used to predict K independent probability values in Math 0.1 / Math.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 25], [26, 30], [31, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 99], [100, 103], [104, 105], [106, 110], [110, 111]]}
{"doc_key": "ai-train-25", "ner": [[3, 5, "misc"], [7, 7, "field"], [9, 11, "field"], [13, 15, "university"], [18, 18, "country"], [22, 25, "misc"], [27, 30, "university"], [32, 32, "country"], [39, 39, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 5, 7, 7, "topic", "", false, false], [3, 5, 9, 11, "topic", "", false, false], [3, 5, 13, 15, "physical", "", true, false], [13, 15, 18, 18, "physical", "", false, false], [22, 25, 27, 30, "physical", "", true, false], [27, 30, 32, 32, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", ",", "the", "Netherlands", ",", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", ",", "Japan", ",", "before", "becoming", "a", "professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Chair in Mathematics and Computer Science at the University of Groningen, the Netherlands, and the Toshiba Endowed Chair at the Tokyo Institute of Technology, Japan, before becoming a professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 34], [35, 37], [38, 49], [50, 53], [54, 62], [63, 70], [71, 73], [74, 77], [78, 88], [89, 91], [92, 101], [101, 102], [103, 106], [107, 118], [118, 119], [120, 123], [124, 127], [128, 135], [136, 143], [144, 149], [150, 152], [153, 156], [157, 162], [163, 172], [173, 175], [176, 186], [186, 187], [188, 193], [193, 194], [195, 201], [202, 210], [211, 212], [213, 222], [223, 225], [226, 235], [235, 236]]}
{"doc_key": "ai-train-26", "ner": [[5, 6, "algorithm"], [11, 14, "algorithm"], [16, 16, "algorithm"], [20, 21, "researcher"], [23, 24, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 11, 14, "usage", "", true, false], [11, 14, 20, 21, "origin", "", false, false], [11, 14, 23, 24, "origin", "", false, false], [16, 16, 11, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "particularly", "used", "for", "recurrent", "neural", "networks", "is", "the", "1997", "Long", "Short", "Term", "Memory", "(", "LSTM", ")", "network", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "."], "sentence-detokenized": "Another technique particularly used for recurrent neural networks is the 1997 Long Short Term Memory (LSTM) network by Sepp Hochreiter & J\u00fcrgen Schmidhuber.", "token2charspan": [[0, 7], [8, 17], [18, 30], [31, 35], [36, 39], [40, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 77], [78, 82], [83, 88], [89, 93], [94, 100], [101, 102], [102, 106], [106, 107], [108, 115], [116, 118], [119, 123], [124, 134], [135, 136], [137, 143], [144, 155], [155, 156]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 9, "product"], [15, 15, "product"], [45, 45, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 4, 5, "general-affiliation", "", false, false], [8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", "onwards", ")", "makes", "this", "package", "very", "versatile", "as", "it", "can", "be", "used", "interactively", ",", "scripted", "and", "compiled", "in", "a", "similar", "way", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT up to version 5.34, Cling from version 6 onwards) makes this package very versatile as it can be used interactively, scripted and compiled in a similar way to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 43], [44, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [82, 89], [89, 90], [91, 96], [97, 101], [102, 109], [110, 114], [115, 124], [125, 127], [128, 130], [131, 134], [135, 137], [138, 142], [143, 156], [156, 157], [158, 166], [167, 170], [171, 179], [180, 182], [183, 184], [185, 192], [193, 196], [197, 199], [200, 210], [211, 219], [220, 224], [225, 227], [228, 234], [234, 235]]}
{"doc_key": "ai-train-28", "ner": [[0, 1, "product"], [20, 22, "field"], [26, 27, "task"], [29, 31, "task"], [33, 34, "task"], [36, 37, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 20, 22, "related-to", "", false, false], [26, 27, 20, 22, "part-of", "", false, false], [29, 31, 20, 22, "part-of", "", false, false], [33, 34, 20, 22, "part-of", "", false, false], [36, 37, 20, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Speech", "interfaces", "that", "interpret", "and", "manage", "conversational", "state", "are", "difficult", "to", "design", "because", "of", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Speech interfaces that interpret and manage conversational state are difficult to design because of the inherent difficulty of integrating complex natural language processing tasks such as coreference resolution, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 6], [7, 17], [18, 22], [23, 32], [33, 36], [37, 43], [44, 58], [59, 64], [65, 68], [69, 78], [79, 81], [82, 88], [89, 96], [97, 99], [100, 103], [104, 112], [113, 123], [124, 126], [127, 138], [139, 146], [147, 154], [155, 163], [164, 174], [175, 180], [181, 185], [186, 188], [189, 200], [201, 211], [211, 212], [213, 218], [219, 225], [226, 237], [237, 238], [239, 250], [251, 260], [261, 264], [265, 273], [274, 284], [284, 285]]}
{"doc_key": "ai-train-29", "ner": [[6, 7, "algorithm"], [10, 12, "algorithm"], [16, 17, "researcher"], [23, 26, "organisation"], [32, 33, "field"], [35, 36, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[6, 7, 16, 17, "origin", "", false, false], [6, 7, 32, 33, "part-of", "", false, false], [6, 7, 35, 36, "part-of", "", false, false], [10, 12, 16, 17, "origin", "", false, false], [10, 12, 32, 33, "part-of", "", false, false], [10, 12, 35, 36, "part-of", "", false, false], [16, 17, 23, 26, "physical", "", false, false], [16, 17, 23, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "the", "recurrent", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "by", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "AI", "Lab", "IDSIA", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed by J\u00fcrgen Schmidhuber's research group at the Swiss AI Lab IDSIA won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 26], [27, 36], [37, 43], [44, 52], [53, 56], [57, 61], [62, 73], [74, 80], [81, 89], [90, 99], [100, 102], [103, 109], [110, 121], [121, 123], [124, 132], [133, 138], [139, 141], [142, 145], [146, 151], [152, 154], [155, 158], [159, 164], [165, 168], [169, 174], [175, 188], [189, 201], [202, 204], [205, 212], [213, 224], [225, 228], [229, 236], [237, 245], [245, 246]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [7, 8, "product"], [10, 11, "product"], [15, 19, "task"], [21, 21, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 7, 8, "usage", "", false, false], [1, 3, 10, 11, "usage", "", false, false], [1, 3, 15, 19, "usage", "", true, false], [1, 3, 21, 21, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "the", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "text", "-", "to", "-", "speech", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use the SAPI 4 and SAPI 5 components to support text-to-speech and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 42], [43, 47], [48, 49], [50, 53], [54, 58], [59, 60], [61, 71], [72, 74], [75, 82], [83, 87], [87, 88], [88, 90], [90, 91], [91, 97], [98, 101], [102, 108], [108, 109]]}
{"doc_key": "ai-train-31", "ner": [[7, 12, "misc"], [14, 14, "field"], [17, 19, "university"], [26, 29, "field"], [32, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 12, 14, 14, "topic", "topic_of_award", false, false], [7, 12, 17, 19, "origin", "", true, false], [26, 29, 32, 35, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "a", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "the", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, a S. V. della laurea ad honorem in psychology from the University of Padua in 1995 and a doctorate in industrial design and engineering from the Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 35], [36, 38], [39, 41], [42, 47], [48, 54], [55, 57], [58, 65], [66, 68], [69, 79], [80, 84], [85, 88], [89, 99], [100, 102], [103, 108], [109, 111], [112, 116], [117, 120], [121, 122], [123, 132], [133, 135], [136, 146], [147, 153], [154, 157], [158, 169], [170, 174], [175, 178], [179, 184], [185, 195], [196, 198], [199, 209], [209, 210]]}
{"doc_key": "ai-train-32", "ner": [[8, 9, "researcher"], [15, 18, "organisation"], [20, 20, "location"], [22, 22, "researcher"], [33, 34, "misc"], [47, 49, "misc"], [65, 66, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[8, 9, 15, 18, "physical", "", false, false], [8, 9, 15, 18, "role", "", false, false], [15, 18, 20, 20, "physical", "", false, false], [22, 22, 33, 34, "related-to", "works_with", true, false], [22, 22, 47, 49, "related-to", "works_with", true, false], [22, 22, 65, 66, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", ",", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "with", "impaired", "multiplication", "but", "preserved", "subtraction", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobule", ")", "and", "others", "with", "impaired", "subtraction", "but", "preserved", "multiplication", "(", "associated", "with", "lesions", "of", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Together with his long-time collaborator, Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication but preserved subtraction (associated with lesions of the inferior parietal lobule) and others with impaired subtraction but preserved multiplication (associated with lesions of the intraparietal sulcus).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [40, 41], [42, 49], [50, 55], [55, 56], [57, 58], [59, 70], [71, 73], [74, 77], [78, 83], [83, 84], [84, 95], [96, 104], [105, 107], [108, 113], [113, 114], [115, 122], [123, 127], [128, 138], [139, 147], [148, 152], [153, 160], [161, 163], [164, 173], [174, 181], [182, 184], [185, 188], [189, 197], [198, 202], [203, 207], [208, 216], [217, 231], [232, 235], [236, 245], [246, 257], [258, 259], [259, 269], [270, 274], [275, 282], [283, 285], [286, 289], [290, 298], [299, 307], [308, 314], [314, 315], [316, 319], [320, 326], [327, 331], [332, 340], [341, 352], [353, 356], [357, 366], [367, 381], [382, 383], [383, 393], [394, 398], [399, 406], [407, 409], [410, 413], [414, 427], [428, 434], [434, 435], [435, 436]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [13, 16, "misc"], [18, 19, "misc"], [27, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 8, "topic", "", false, false], [18, 19, 6, 8, "topic", "", false, false], [27, 27, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "depictions", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "and", "the", "2016", "television", "adaptation", "of", "Westworld", ",", "have", "generated", "public", "sympathy", "for", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional depictions of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina, and the 2016 television adaptation of Westworld, have generated public sympathy for the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 35], [36, 38], [39, 51], [52, 63], [64, 70], [71, 73], [74, 79], [80, 84], [85, 87], [88, 91], [91, 92], [93, 103], [104, 116], [117, 120], [121, 123], [124, 131], [131, 132], [133, 136], [137, 140], [141, 145], [146, 156], [157, 167], [168, 170], [171, 180], [180, 181], [182, 186], [187, 196], [197, 203], [204, 212], [213, 216], [217, 220], [221, 227], [228, 238], [238, 239]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [10, 12, "algorithm"], [14, 15, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 12, 7, 8, "part-of", "", false, false], [14, 15, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [23, 24, "misc"], [29, 30, "misc"], [32, 34, "person"], [39, 40, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[23, 24, 0, 3, "artifact", "", false, false], [29, 30, 0, 3, "artifact", "", false, false], [29, 30, 32, 34, "role", "director_of", false, false], [29, 30, 39, 40, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "make", "greater", "use", "of", "3D", "films", "in", "special", "locations", "to", "impress", "audiences", ".", "Notable", "examples", "include", "Magic", "Trips", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "."], "sentence-detokenized": "The Walt Disney Company also began to make greater use of 3D films in special locations to impress audiences. Notable examples include Magic Trips (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 42], [43, 50], [51, 54], [55, 57], [58, 60], [61, 66], [67, 69], [70, 77], [78, 87], [88, 90], [91, 98], [99, 108], [108, 109], [110, 117], [118, 126], [127, 134], [135, 140], [141, 146], [147, 148], [148, 152], [152, 153], [154, 157], [158, 165], [166, 168], [169, 170], [170, 177], [178, 182], [183, 190], [190, 191], [192, 196], [196, 197], [198, 206], [207, 214], [215, 222], [222, 223], [223, 224]]}
{"doc_key": "ai-train-36", "ner": [[12, 14, "field"], [19, 24, "task"], [26, 26, "task"], [28, 28, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 14, "part-of", "", false, false], [26, 26, 12, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "labelling", "and", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing for tasks such as part-of-speech labelling and parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [95, 98], [99, 104], [105, 109], [110, 112], [113, 117], [117, 118], [118, 120], [120, 121], [121, 127], [128, 137], [138, 141], [142, 149], [150, 151], [151, 158], [158, 159], [160, 164], [164, 165], [165, 166]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [9, 12, "organisation"], [14, 15, "organisation"], [17, 17, "country"], [21, 24, "product"], [28, 29, "researcher"], [39, 39, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 12, 2, 3, "role", "introduces_to_market", true, false], [14, 15, 2, 3, "role", "introduces_to_market", true, false], [14, 15, 17, 17, "physical", "", false, false], [21, 24, 39, 39, "related-to", "sold_to", true, false], [28, 29, 21, 24, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", ".", "by", "KUKA", "robotics", "in", "Germany", ",", "and", "the", "universal", "programmable", "assembly", "machine", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", ",", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced in 1963 by Fuji Yusoki Kogyo. by KUKA robotics in Germany, and the universal programmable assembly machine was invented by Victor Scheinman in 1976, and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [71, 72], [73, 75], [76, 80], [81, 89], [90, 92], [93, 100], [100, 101], [102, 105], [106, 109], [110, 119], [120, 132], [133, 141], [142, 149], [150, 153], [154, 162], [163, 165], [166, 172], [173, 182], [183, 185], [186, 190], [190, 191], [192, 195], [196, 199], [200, 206], [207, 210], [211, 215], [216, 218], [219, 228], [228, 229]]}
{"doc_key": "ai-train-38", "ner": [[8, 8, "conference"], [10, 10, "researcher"], [19, 19, "field"], [32, 33, "researcher"], [39, 40, "researcher"], [50, 50, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[10, 10, 8, 8, "role", "president_of", false, false], [10, 10, 32, 33, "role", "colleagues", false, false], [19, 19, 50, 50, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "while", "president", "of", "the", "AAAI", ",", "Hayes", "launched", "a", "series", "of", "attacks", "on", "critics", "of", "AI", ",", "mostly", "couched", "in", "irony", ",", "and", "invented", "(", "with", "his", "colleague", "Kenneth", "Ford", ")", "an", "award", "named", "after", "Simon", "Newcomb", "for", "the", "most", "ridiculous", "argument", "refuting", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, while president of the AAAI, Hayes launched a series of attacks on critics of AI, mostly couched in irony, and invented (with his colleague Kenneth Ford) an award named after Simon Newcomb for the most ridiculous argument refuting the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 33], [34, 36], [37, 40], [41, 45], [45, 46], [47, 52], [53, 61], [62, 63], [64, 70], [71, 73], [74, 81], [82, 84], [85, 92], [93, 95], [96, 98], [98, 99], [100, 106], [107, 114], [115, 117], [118, 123], [123, 124], [125, 128], [129, 137], [138, 139], [139, 143], [144, 147], [148, 157], [158, 165], [166, 170], [170, 171], [172, 174], [175, 180], [181, 186], [187, 192], [193, 198], [199, 206], [207, 210], [211, 214], [215, 219], [220, 230], [231, 239], [240, 248], [249, 252], [253, 264], [265, 267], [268, 270], [270, 271]]}
{"doc_key": "ai-train-39", "ner": [[13, 15, "algorithm"], [38, 39, "algorithm"], [51, 53, "algorithm"], [57, 59, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 15, 38, 39, "named", "same", false, false], [51, 53, 13, 15, "type-of", "", false, false], [57, 59, 13, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "optimal", "value", "of", "mathematics", "alpha", "/", "math", "can", "be", "found", "using", "a", "linear", "search", "algorithm", ",", "i.e.", "the", "value", "of", "mathematics", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimises", "S", ",", "usually", "using", "a", "linear", "search", "in", "the", "interval", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "linear", "backtracking", "search", "such", "as", "the", "Armijo", "linear", "search", "."], "sentence-detokenized": "An optimal value of mathematics alpha/math can be found using a linear search algorithm, i.e. the value of mathematics alpha/math is determined by finding the value that minimises S, usually using a linear search in the interval math0\\ alpha 1/math or a linear backtracking search such as the Armijo linear search.", "token2charspan": [[0, 2], [3, 10], [11, 16], [17, 19], [20, 31], [32, 37], [37, 38], [38, 42], [43, 46], [47, 49], [50, 55], [56, 61], [62, 63], [64, 70], [71, 77], [78, 87], [87, 88], [89, 93], [94, 97], [98, 103], [104, 106], [107, 118], [119, 124], [124, 125], [125, 129], [130, 132], [133, 143], [144, 146], [147, 154], [155, 158], [159, 164], [165, 169], [170, 179], [180, 181], [181, 182], [183, 190], [191, 196], [197, 198], [199, 205], [206, 212], [213, 215], [216, 219], [220, 228], [229, 234], [234, 235], [236, 241], [242, 243], [243, 244], [244, 248], [249, 251], [252, 253], [254, 260], [261, 273], [274, 280], [281, 285], [286, 288], [289, 292], [293, 299], [300, 306], [307, 313], [313, 314]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [4, 4, "algorithm"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "breadth", "and", "depth", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "embody", "a", "great", "deal", "of", "technical", "knowledge", ",", "but", "do", "not", "shed", "much", "light", "on", "the", "mental", "processes", "that", "humans", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses breadth and depth search techniques, but ultimately concludes that the results represent expert systems that embody a great deal of technical knowledge, but do not shed much light on the mental processes that humans use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [21, 24], [25, 30], [31, 37], [38, 48], [48, 49], [50, 53], [54, 64], [65, 74], [75, 79], [80, 83], [84, 91], [92, 101], [102, 108], [109, 116], [117, 121], [122, 128], [129, 130], [131, 136], [137, 141], [142, 144], [145, 154], [155, 164], [164, 165], [166, 169], [170, 172], [173, 176], [177, 181], [182, 186], [187, 192], [193, 195], [196, 199], [200, 206], [207, 216], [217, 221], [222, 228], [229, 232], [233, 235], [236, 241], [242, 246], [247, 254], [254, 255]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 3, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "synthesis", "deals", "with", "the", "way", "in", "which", "spoken", "language", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and synthesis deals with the way in which spoken language can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 32], [33, 38], [39, 43], [44, 47], [48, 51], [52, 54], [55, 60], [61, 67], [68, 76], [77, 80], [81, 83], [84, 94], [95, 97], [98, 105], [106, 111], [112, 121], [121, 122]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"], [31, 33, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "mathematics", "of", "theta", "^", "{", "*}", "/", "math", "is", "normally", "estimated", "using", "a", "Maximum", "Likelihood", "procedure", "(", "math\\_theta", "^", "{", "*}", "=\\theta", "^", "{", "ML", "}", "/", "math", ")", "or", "Maximum", "A", "Posteriori", "procedure", "(", "math\\_theta", "^", "{", "*}", "=\\theta", "^", "{", "MAP", "}", "/", "math", ")"], "sentence-detokenized": "This mathematics of theta ^ {*} / math is normally estimated using a Maximum Likelihood procedure (math\\_theta ^ {*} =\\theta ^ {ML} / math) or Maximum A Posteriori procedure (math\\_theta ^ {*} =\\theta ^ {MAP} / math)", "token2charspan": [[0, 4], [5, 16], [17, 19], [20, 25], [26, 27], [28, 29], [29, 31], [32, 33], [34, 38], [39, 41], [42, 50], [51, 60], [61, 66], [67, 68], [69, 76], [77, 87], [88, 97], [98, 99], [99, 110], [111, 112], [113, 114], [114, 116], [117, 124], [125, 126], [127, 128], [128, 130], [130, 131], [132, 133], [134, 138], [138, 139], [140, 142], [143, 150], [151, 152], [153, 163], [164, 173], [174, 175], [175, 186], [187, 188], [189, 190], [190, 192], [193, 200], [201, 202], [203, 204], [204, 207], [207, 208], [209, 210], [211, 215], [215, 216]]}
{"doc_key": "ai-train-43", "ner": [[10, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "lesser", "-", "used", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesizer", "for", "their", "speech", ",", "producing", "an", "awkward", ",", "robotic", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some lesser-used languages use the open-source eSpeak synthesizer for their speech, producing an awkward, robotic voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 11], [11, 12], [12, 16], [17, 26], [27, 30], [31, 34], [35, 39], [39, 40], [40, 46], [47, 53], [54, 65], [66, 69], [70, 75], [76, 82], [82, 83], [84, 93], [94, 96], [97, 104], [104, 105], [106, 113], [114, 119], [120, 124], [125, 128], [129, 131], [132, 141], [142, 144], [145, 155], [155, 156]]}
{"doc_key": "ai-train-44", "ner": [[18, 18, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[18, 18, 35, 36, "compare", "", false, false], [18, 18, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "primarily", "used", "by", "statisticians", "and", "other", "practitioners", "requiring", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "R", "can", "also", "function", "as", "a", "general", "purpose", "toolkit", "for", "matrix", "computing", "-", "with", "performance", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although primarily used by statisticians and other practitioners requiring an environment for statistical computing and software development, R can also function as a general purpose toolkit for matrix computing - with performance comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 18], [19, 23], [24, 26], [27, 40], [41, 44], [45, 50], [51, 64], [65, 74], [75, 77], [78, 89], [90, 93], [94, 105], [106, 115], [116, 119], [120, 128], [129, 140], [140, 141], [142, 143], [144, 147], [148, 152], [153, 161], [162, 164], [165, 166], [167, 174], [175, 182], [183, 190], [191, 194], [195, 201], [202, 211], [212, 213], [214, 218], [219, 230], [231, 241], [242, 244], [245, 248], [249, 255], [256, 258], [259, 265], [265, 266]]}
{"doc_key": "ai-train-45", "ner": [[0, 1, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 3, 4, "part-of", "", false, false], [0, 1, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "engineer", "-", "inventor", "Reginald", "Fessenden", "that", "allows", "the", "creation", "of", "new", "frequencies", "by", "mixing", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian engineer-inventor Reginald Fessenden that allows the creation of new frequencies by mixing two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [75, 76], [76, 84], [85, 93], [94, 103], [104, 108], [109, 115], [116, 119], [120, 128], [129, 131], [132, 135], [136, 147], [148, 150], [151, 157], [158, 161], [162, 173], [173, 174]]}
{"doc_key": "ai-train-46", "ner": [[13, 14, "person"], [16, 16, "misc"], [20, 22, "organisation"], [25, 25, "organisation"], [27, 29, "misc"], [31, 32, "person"], [35, 35, "organisation"], [37, 39, "misc"], [41, 42, "person"], [44, 45, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 14, 16, 16, "role", "actor_in", false, false], [16, 16, 20, 22, "artifact", "", false, false], [27, 29, 25, 25, "artifact", "", false, false], [31, 32, 27, 29, "role", "actor_in", false, false], [37, 39, 35, 35, "artifact", "", false, false], [41, 42, 37, 39, "role", "actor_in", false, false], [44, 45, 37, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Several", "other", "films", "helped", "put", "3D", "back", "on", "the", "map", "that", "month", ":", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", ",", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Several other films helped put 3D back on the map that month: John Wayne's Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth, and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 7], [8, 13], [14, 19], [20, 26], [27, 30], [31, 33], [34, 38], [39, 41], [42, 45], [46, 49], [50, 54], [55, 60], [60, 61], [62, 66], [67, 72], [72, 74], [75, 80], [81, 82], [82, 93], [94, 96], [97, 103], [104, 108], [108, 109], [109, 110], [110, 111], [112, 120], [120, 122], [123, 127], [128, 133], [134, 142], [143, 147], [148, 152], [153, 161], [161, 162], [163, 166], [167, 176], [176, 178], [179, 184], [185, 189], [190, 194], [195, 199], [200, 204], [205, 211], [212, 215], [216, 221], [222, 227], [227, 228]]}
{"doc_key": "ai-train-47", "ner": [[0, 1, "product"], [3, 4, "field"], [5, 6, "task"], [14, 14, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 5, 6, "general-affiliation", "", false, false], [0, 1, 14, 14, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "research", "group", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a research group at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 81], [82, 84], [85, 93], [93, 94]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [15, 16, "field"], [25, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [25, 27, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "topic", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the leading academic conference on computer graphics, and the main topic of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 91], [92, 94], [95, 103], [104, 112], [112, 113], [114, 117], [118, 121], [122, 126], [127, 132], [133, 135], [136, 139], [140, 146], [147, 155], [156, 166], [167, 176], [176, 177]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [36, 36, "misc"], [41, 43, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 15, 36, 36, "general-affiliation", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 36, 36, "general-affiliation", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 36, 36, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "k", "-", "NN", "clustering", "on", "the", "feature", "vectors", "in", "a", "reduced", "dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by k-NN clustering on the feature vectors in a reduced dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [153, 155], [156, 165], [166, 177], [178, 186], [187, 188], [188, 191], [191, 192], [193, 203], [204, 206], [207, 208], [209, 223], [224, 228], [228, 229], [230, 238], [239, 241], [242, 243], [243, 244], [244, 246], [247, 257], [258, 260], [261, 264], [265, 272], [273, 280], [281, 283], [284, 285], [286, 293], [294, 305], [306, 311], [311, 312]]}
{"doc_key": "ai-train-50", "ner": [[0, 3, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 3, 9, 10, "related-to", "good_at", true, false], [0, 3, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computer", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computer models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 39], [40, 46], [47, 51], [52, 57], [58, 60], [61, 68], [69, 77], [78, 81], [82, 89], [90, 101], [101, 102]]}
{"doc_key": "ai-train-51", "ner": [[0, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [37, 40, "algorithm"], [41, 42, "researcher"], [44, 46, "researcher"], [48, 54, "misc"], [56, 65, "conference"], [67, 67, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 0, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [48, 54, 37, 40, "topic", "", false, false], [48, 54, 41, 42, "artifact", "", false, false], [48, 54, 44, 46, "artifact", "", false, false], [48, 54, 56, 65, "temporal", "", false, false], [67, 67, 56, 65, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["C", ".", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "the", "histogram", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005"], "sentence-detokenized": "C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as the histogram of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005", "token2charspan": [[0, 1], [1, 2], [3, 15], [16, 19], [20, 22], [23, 29], [29, 30], [31, 32], [33, 42], [43, 53], [54, 63], [64, 70], [70, 71], [72, 85], [86, 93], [94, 96], [97, 105], [106, 112], [113, 114], [114, 118], [118, 119], [119, 120], [121, 126], [127, 128], [128, 129], [130, 132], [132, 133], [133, 135], [135, 136], [137, 141], [142, 148], [149, 152], [153, 158], [159, 167], [168, 172], [173, 175], [176, 179], [180, 189], [190, 192], [193, 201], [202, 211], [212, 214], [215, 220], [220, 221], [222, 223], [223, 224], [225, 231], [231, 232], [233, 243], [244, 246], [247, 255], [256, 265], [266, 269], [270, 275], [276, 285], [285, 286], [287, 291], [292, 300], [301, 308], [309, 319], [320, 322], [323, 331], [332, 338], [339, 342], [343, 350], [351, 362], [363, 364], [364, 368], [368, 369], [369, 370], [371, 376], [377, 378], [378, 379], [380, 387], [387, 388], [389, 393]]}
{"doc_key": "ai-train-52", "ner": [[1, 1, "algorithm"], [6, 8, "algorithm"], [12, 13, "task"], [11, 11, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 1, 6, 8, "type-of", "", false, false], [12, 13, 1, 1, "usage", "", true, false], [12, 13, 11, 11, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "for", "unsupervised", "feature", "learning", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network used for unsupervised feature learning.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 62], [63, 75], [76, 83], [84, 92], [92, 93]]}
{"doc_key": "ai-train-53", "ner": [[0, 3, "researcher"], [7, 7, "organisation"], [15, 16, "field"], [18, 19, "field"], [25, 29, "organisation"], [31, 31, "organisation"], [40, 41, "field"], [43, 44, "field"], [48, 48, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 3, 7, 7, "role", "fellow_of", false, false], [0, 3, 15, 16, "related-to", "contributes_to", false, false], [0, 3, 18, 19, "related-to", "contributes_to", false, false], [0, 3, 25, 29, "role", "fellow_of", false, false], [0, 3, 40, 41, "related-to", "contributes_to", false, false], [0, 3, 43, 44, "related-to", "contributes_to", false, false], [31, 31, 25, 29, "named", "", false, false], [48, 48, 25, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Dr.", "Haralick", "is", "a", "member", "of", "the", "IEEE", "for", "his", "contributions", "to", "the", "field", "of", "computer", "vision", "and", "image", "processing", "and", "a", "member", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "the", "field", "of", "pattern", "recognition", ",", "image", "processing", "and", "service", "to", "IAPR", "."], "sentence-detokenized": "Dr. Haralick is a member of the IEEE for his contributions to the field of computer vision and image processing and a member of the International Association for Pattern Recognition (IAPR) for his contributions to the field of pattern recognition, image processing and service to IAPR.", "token2charspan": [[0, 3], [4, 12], [13, 15], [16, 17], [18, 24], [25, 27], [28, 31], [32, 36], [37, 40], [41, 44], [45, 58], [59, 61], [62, 65], [66, 71], [72, 74], [75, 83], [84, 90], [91, 94], [95, 100], [101, 111], [112, 115], [116, 117], [118, 124], [125, 127], [128, 131], [132, 145], [146, 157], [158, 161], [162, 169], [170, 181], [182, 183], [183, 187], [187, 188], [189, 192], [193, 196], [197, 210], [211, 213], [214, 217], [218, 223], [224, 226], [227, 234], [235, 246], [246, 247], [248, 253], [254, 264], [265, 268], [269, 276], [277, 279], [280, 284], [284, 285]]}
{"doc_key": "ai-train-54", "ner": [[9, 10, "task"], [15, 17, "algorithm"], [19, 19, "algorithm"], [23, 24, "researcher"], [26, 27, "organisation"], [29, 30, "researcher"], [33, 35, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 10, 15, 17, "usage", "", false, false], [15, 17, 23, 24, "origin", "", true, false], [15, 17, 29, 30, "origin", "", true, false], [19, 19, 15, 17, "named", "", false, false], [23, 24, 26, 27, "physical", "", false, false], [23, 24, 26, 27, "role", "", false, false], [29, 30, 33, 35, "physical", "", false, false], [29, 30, 33, 35, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "with", "systems", "based", "on", "connectionist", "temporal", "classification", "(", "CTC", ")", "presented", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was with systems based on connectionist temporal classification (CTC) presented by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 44], [45, 52], [53, 58], [59, 61], [62, 75], [76, 84], [85, 99], [100, 101], [101, 104], [104, 105], [106, 115], [116, 118], [119, 123], [124, 130], [131, 133], [134, 140], [141, 149], [150, 153], [154, 161], [162, 168], [169, 171], [172, 175], [176, 186], [187, 189], [190, 197], [198, 200], [201, 205], [205, 206]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear-", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalization", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-fractional programming (LFP) is a generalization of linear programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 1, "researcher"], [9, 14, "misc"], [17, 24, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 9, 14, "win-defeat", "", false, false], [9, 14, 17, 24, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Dr.", "Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "2011", "&", "2012", ","], "sentence-detokenized": "Dr. Lafferty has received numerous awards, including two Test-of-Time awards at the International Conference on Machine Learning 2011 & 2012,", "token2charspan": [[0, 3], [4, 12], [13, 16], [17, 25], [26, 34], [35, 41], [41, 42], [43, 52], [53, 56], [57, 61], [61, 62], [62, 64], [64, 65], [65, 69], [70, 76], [77, 79], [80, 83], [84, 97], [98, 108], [109, 111], [112, 119], [120, 128], [129, 133], [134, 135], [136, 140], [140, 141]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [24, 25, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "able", "to", "deploy", "the", "neural", "network", "developed", "in", "these", "frameworks", "as", "inheritable", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are able to deploy the neural network developed in these frameworks as inheritable components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 118], [119, 121], [122, 128], [129, 132], [133, 139], [140, 147], [148, 157], [158, 160], [161, 166], [167, 177], [178, 180], [181, 192], [193, 203], [203, 204]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLUE", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ".", "The", "algorithm", "first", "creates", "an", "alignment", "(", "see", "illustrations", ")", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLUE, the basic unit of evaluation is the sentence. The algorithm first creates an alignment (see illustrations) between two sentences, the candidate translation string and the reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 73], [74, 79], [80, 87], [88, 90], [91, 100], [101, 102], [102, 105], [106, 119], [119, 120], [121, 128], [129, 132], [133, 142], [142, 143], [144, 147], [148, 157], [158, 169], [170, 176], [177, 180], [181, 184], [185, 194], [195, 206], [207, 213], [213, 214]]}
{"doc_key": "ai-train-59", "ner": [[6, 11, "conference"], [20, 20, "task"], [22, 23, "task"], [27, 28, "metrics"], [30, 36, "metrics"], [41, 44, "conference"], [46, 46, "conference"], [49, 49, "location"], [51, 51, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 11, 20, 20, "related-to", "subject_at", false, false], [6, 11, 22, 23, "related-to", "subject_at", false, false], [27, 28, 6, 11, "temporal", "", false, false], [30, 36, 27, 28, "named", "", true, false], [46, 46, 41, 44, "named", "", false, false], [49, 49, 51, 51, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "the", "annual", "NIST", "document", "comprehension", "conferences", ",", "where", "research", "groups", "submit", "their", "systems", "for", "summarisation", "and", "translation", "tasks", ",", "is", "the", "RED", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", ")", "."], "sentence-detokenized": "One of the metrics used at the annual NIST document comprehension conferences, where research groups submit their systems for summarisation and translation tasks, is the RED metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014).", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 30], [31, 37], [38, 42], [43, 51], [52, 65], [66, 77], [77, 78], [79, 84], [85, 93], [94, 100], [101, 107], [108, 113], [114, 121], [122, 125], [126, 139], [140, 143], [144, 155], [156, 161], [161, 162], [163, 165], [166, 169], [170, 173], [174, 180], [181, 182], [182, 188], [188, 189], [189, 197], [198, 208], [209, 212], [213, 220], [221, 231], [231, 232], [233, 235], [236, 244], [245, 247], [248, 254], [255, 266], [267, 277], [278, 285], [286, 287], [287, 291], [291, 292], [292, 293], [294, 302], [302, 303], [304, 310], [310, 311], [312, 320], [321, 322], [323, 327], [327, 328], [328, 329]]}
{"doc_key": "ai-train-60", "ner": [[7, 7, "programlang"], [9, 9, "product"], [11, 12, "programlang"], [16, 16, "product"], [22, 22, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 7, 11, 12, "type-of", "", false, false], [7, 7, 22, 22, "named", "", false, false], [9, 9, 11, 12, "part-of", "", false, false], [9, 9, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "to", "be", "executed", "in", "Java", "with", "JShell", "(", "Java", "9", "minimum", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, to be executed in Java with JShell (Java 9 minimum): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 23], [24, 26], [27, 35], [36, 38], [39, 43], [44, 48], [49, 55], [56, 57], [57, 61], [62, 63], [64, 71], [71, 72], [72, 73], [74, 84], [85, 95], [96, 97], [98, 117], [118, 122], [123, 124], [125, 129]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLUE", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLUE metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "launched", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "additional", "constraint", "that", "the", "edges", "are", "limited", "to", "a", "restricted", "set", "of", "possible", "relations", ",", "in", "order", "to", "facilitate", "algebra", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly launched a project called Knowledge Graphs, which are semantic networks, but with the additional constraint that the edges are limited to a restricted set of possible relations, in order to facilitate algebra on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 117], [118, 119], [120, 127], [128, 134], [135, 144], [145, 151], [151, 152], [153, 158], [159, 162], [163, 171], [172, 180], [180, 181], [182, 185], [186, 190], [191, 194], [195, 205], [206, 216], [217, 221], [222, 225], [226, 231], [232, 235], [236, 243], [244, 246], [247, 248], [249, 259], [260, 263], [264, 266], [267, 275], [276, 285], [285, 286], [287, 289], [290, 295], [296, 298], [299, 309], [310, 317], [318, 320], [321, 324], [325, 330], [330, 331]]}
{"doc_key": "ai-train-63", "ner": [[0, 1, "product"], [14, 15, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 14, 15, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "integrated", "into", "a", "larger", "program", ",", "such", "as", "a", "word", "processor", ",", "but", "they", "are", "also", "available", "as", "a", "stand", "-", "alone", "application", "that", "can", "be", "activated", "from", "programs", "that", "deal", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often integrated into a larger program, such as a word processor, but they are also available as a stand-alone application that can be activated from programs that deal with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 42], [43, 47], [48, 49], [50, 56], [57, 64], [64, 65], [66, 70], [71, 73], [74, 75], [76, 80], [81, 90], [90, 91], [92, 95], [96, 100], [101, 104], [105, 109], [110, 119], [120, 122], [123, 124], [125, 130], [130, 131], [131, 136], [137, 148], [149, 153], [154, 157], [158, 160], [161, 170], [171, 175], [176, 184], [185, 189], [190, 194], [195, 199], [200, 208], [209, 213], [213, 214]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 20, "conference"], [24, 26, "organisation"], [31, 33, "conference"], [35, 37, "conference"], [40, 42, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement Artificial Intelligence, and the Cognitive Science Society, and editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 121], [122, 134], [134, 135], [136, 139], [140, 143], [144, 153], [154, 161], [162, 169], [169, 170], [171, 174], [175, 181], [182, 184], [185, 187], [188, 197], [198, 207], [207, 208], [209, 211], [212, 220], [221, 229], [229, 230], [231, 234], [235, 237], [238, 245], [246, 254], [254, 255]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [5, 7, "algorithm"], [10, 11, "task"], [20, 21, "researcher"], [23, 24, "university"], [26, 27, "researcher"], [29, 32, "organisation"], [34, 34, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 20, 21, "origin", "", false, false], [0, 2, 26, 27, "origin", "", false, false], [5, 7, 0, 2, "named", "", false, false], [20, 21, 23, 24, "physical", "", false, false], [20, 21, 23, 24, "role", "", false, false], [26, 27, 29, 32, "role", "", false, false], [34, 34, 29, 32, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "develop", "with", "the", "work", "of", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear predictive coding (LPC), a form of speech coding, began to develop with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 73], [74, 78], [79, 82], [83, 87], [88, 90], [91, 99], [100, 107], [108, 110], [111, 117], [118, 128], [129, 132], [133, 138], [139, 144], [145, 147], [148, 154], [155, 164], [165, 168], [169, 178], [179, 180], [180, 183], [183, 184], [185, 187], [188, 192], [192, 193]]}
{"doc_key": "ai-train-66", "ner": [[59, 61, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "also", "ergodic", ",", "all", "the", "sampling", "paths", "have", "the", "same", "time", "average", "and", "therefore", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "the", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is also ergodic, all the sampling paths have the same time average and therefore mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the sense of the mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 21], [22, 29], [29, 30], [31, 34], [35, 38], [39, 47], [48, 53], [54, 58], [59, 62], [63, 67], [68, 72], [73, 80], [81, 84], [85, 94], [95, 100], [101, 102], [103, 104], [105, 106], [107, 108], [108, 109], [110, 111], [112, 113], [114, 115], [116, 117], [117, 118], [119, 120], [120, 121], [122, 125], [125, 126], [127, 129], [130, 137], [138, 139], [139, 140], [140, 141], [142, 143], [144, 145], [146, 147], [148, 149], [149, 150], [151, 152], [153, 154], [155, 156], [157, 158], [158, 159], [160, 161], [161, 162], [163, 166], [166, 167], [168, 169], [170, 174], [175, 177], [178, 181], [182, 187], [188, 190], [191, 194], [195, 199], [200, 206], [207, 212], [212, 213]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [34, 36, "algorithm"], [38, 38, "algorithm"], [43, 44, "misc"], [48, 50, "algorithm"], [54, 55, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[13, 15, 43, 44, "related-to", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 43, 44, "related-to", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 43, 44, "related-to", "", false, false], [31, 31, 27, 29, "named", "", false, false], [34, 36, 43, 44, "related-to", "", false, false], [38, 38, 34, 36, "named", "", false, false], [48, 50, 54, 55, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorisation", "(", "NMF", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "K", "-", "NN", "clustering", "on", "the", "feature", "vectors", "in", "the", "reduced", "dimension", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorisation (NMF) techniques as a pre-processing step, followed by K-NN clustering on the feature vectors in the reduced dimension space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [152, 153], [154, 163], [164, 175], [176, 184], [185, 186], [186, 189], [189, 190], [191, 193], [194, 206], [207, 213], [214, 227], [228, 229], [229, 232], [232, 233], [234, 244], [245, 247], [248, 249], [250, 264], [265, 269], [269, 270], [271, 279], [280, 282], [283, 284], [284, 285], [285, 287], [288, 298], [299, 301], [302, 305], [306, 313], [314, 321], [322, 324], [325, 328], [329, 336], [337, 346], [347, 352], [352, 353]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [11, 13, "task"], [31, 32, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 11, 13, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognising", "named", "entities", "in", "a", "text", "is", "called", "named", "entity", "recognition", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "a", "text", "is", "called", "entity", "linking", "."], "sentence-detokenized": "The task of recognising named entities in a text is called named entity recognition, while the task of determining the identity of named entities mentioned in a text is called entity linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 43], [44, 48], [49, 51], [52, 58], [59, 64], [65, 71], [72, 83], [83, 84], [85, 90], [91, 94], [95, 99], [100, 102], [103, 114], [115, 118], [119, 127], [128, 130], [131, 136], [137, 145], [146, 155], [156, 158], [159, 160], [161, 165], [166, 168], [169, 175], [176, 182], [183, 190], [190, 191]]}
{"doc_key": "ai-train-70", "ner": [[1, 1, "algorithm"], [28, 28, "programlang"], [27, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 27, 27, "part-of", "", true, false], [27, 27, 28, 28, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "from", "version", "0.8.0", "onwards", "they", "have", "been", "released", "in", "a", "separate", "sigmoid", "R", "package", ",", "in", "order", "to", "allow", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, from version 0.8.0 onwards they have been released in a separate sigmoid R package, in order to allow more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 103], [104, 111], [112, 117], [118, 125], [126, 130], [131, 135], [136, 140], [141, 149], [150, 152], [153, 154], [155, 163], [164, 171], [172, 173], [174, 181], [181, 182], [183, 185], [186, 191], [192, 194], [195, 200], [201, 205], [206, 213], [214, 217], [217, 218]]}
{"doc_key": "ai-train-71", "ner": [[1, 2, "programlang"], [7, 11, "organisation"], [13, 13, "organisation"], [20, 20, "location"], [22, 22, "location"], [25, 26, "researcher"], [28, 29, "researcher"], [31, 32, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 2, 25, 26, "artifact", "", true, false], [1, 2, 28, 29, "artifact", "", true, false], [1, 2, 31, 32, "artifact", "", true, false], [13, 13, 7, 11, "named", "", false, false], [13, 13, 20, 20, "physical", "", false, false], [20, 20, 22, 22, "physical", "", false, false], [25, 26, 7, 11, "role", "", false, false], [28, 29, 7, 11, "role", "", false, false], [31, 32, 7, 11, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "research", "firm", "in", "Cambridge", ",", "Massachusetts", ",", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The logo was created in 1967 at Bolt, Beranek and Newman (BBN), a research firm in Cambridge, Massachusetts, by Wally Feurzeig, Cynthia Solomon and Seymour Papert.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 36], [36, 37], [38, 45], [46, 49], [50, 56], [57, 58], [58, 61], [61, 62], [62, 63], [64, 65], [66, 74], [75, 79], [80, 82], [83, 92], [92, 93], [94, 107], [107, 108], [109, 111], [112, 117], [118, 126], [126, 127], [128, 135], [136, 143], [144, 147], [148, 155], [156, 162], [162, 163]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [7, 8, "field"], [17, 18, "field"], [22, 23, "algorithm"], [28, 29, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 7, 8, "part-of", "", false, false], [0, 1, 17, 18, "compare", "", false, false], [22, 23, 17, 18, "part-of", "", false, false], [28, 29, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuro", "-evolution", "is", "commonly", "used", "within", "the", "reinforcement", "learning", "paradigm", ",", "and", "can", "be", "compared", "to", "conventional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "on", "a", "fixed", "topology", "neural", "network", "."], "sentence-detokenized": "Neuro-evolution is commonly used within the reinforcement learning paradigm, and can be compared to conventional deep learning techniques that use gradient descent on a fixed topology neural network.", "token2charspan": [[0, 5], [5, 15], [16, 18], [19, 27], [28, 32], [33, 39], [40, 43], [44, 57], [58, 66], [67, 75], [75, 76], [77, 80], [81, 84], [85, 87], [88, 96], [97, 99], [100, 112], [113, 117], [118, 126], [127, 137], [138, 142], [143, 146], [147, 155], [156, 163], [164, 166], [167, 168], [169, 174], [175, 183], [184, 190], [191, 198], [198, 199]]}
{"doc_key": "ai-train-73", "ner": [[3, 4, "algorithm"], [56, 58, "metrics"], [60, 60, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[60, 60, 56, 58, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "least", "squares", "to", "fit", "a", "function", "in", "the", "form", "of", "a", "hyperplane", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "then", "we", "could", "evaluate", "the", "fit", "using", "the", "mean", "square", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use least squares to fit a function in the form of a hyperplane \u0177 = a + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264n / sub, then we could evaluate the fit using the mean square error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 15], [16, 23], [24, 26], [27, 30], [31, 32], [33, 41], [42, 44], [45, 48], [49, 53], [54, 56], [57, 58], [59, 69], [70, 71], [72, 73], [74, 75], [76, 77], [78, 79], [80, 84], [85, 86], [87, 90], [91, 92], [93, 95], [96, 99], [100, 104], [105, 106], [106, 107], [108, 111], [112, 113], [114, 115], [116, 119], [119, 120], [121, 122], [123, 126], [127, 128], [129, 130], [131, 134], [134, 135], [136, 139], [140, 141], [142, 143], [144, 145], [146, 148], [149, 150], [151, 154], [154, 155], [156, 160], [161, 163], [164, 169], [170, 178], [179, 182], [183, 186], [187, 192], [193, 196], [197, 201], [202, 208], [209, 214], [215, 216], [216, 219], [219, 220], [220, 221]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [48, 48, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "locations", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "UK", "."], "sentence-detokenized": "The company has international locations in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the UK.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 39], [40, 42], [43, 52], [52, 53], [54, 60], [60, 61], [62, 68], [68, 69], [70, 75], [75, 76], [77, 84], [84, 85], [86, 91], [91, 92], [93, 98], [98, 99], [100, 105], [105, 106], [107, 112], [112, 113], [114, 123], [123, 124], [125, 131], [131, 132], [133, 141], [141, 142], [143, 154], [154, 155], [156, 162], [162, 163], [164, 173], [173, 174], [175, 180], [181, 187], [187, 188], [189, 194], [194, 195], [196, 202], [202, 203], [204, 212], [212, 213], [214, 220], [221, 224], [225, 228], [229, 231], [231, 232]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [6, 9, "field"], [14, 14, "organisation"], [17, 24, "university"], [29, 31, "organisation"], [33, 37, "university"], [41, 42, "university"], [44, 45, "university"], [48, 50, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 6, 9, "topic", "", false, false], [3, 3, 14, 14, "origin", "", false, false], [3, 3, 17, 24, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "D.Sc", ".", "in", "Electrical", "and", "Computer", "Engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", ".", "He", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a D.Sc. in Electrical and Computer Engineering (2000) from Inria and the University of Nice Sophia Antipolis. He has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech and visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 15], [15, 16], [17, 19], [20, 30], [31, 34], [35, 43], [44, 55], [56, 57], [57, 61], [61, 62], [63, 67], [68, 73], [74, 77], [78, 81], [82, 92], [93, 95], [96, 100], [101, 107], [108, 117], [117, 118], [119, 121], [122, 125], [126, 130], [131, 140], [141, 150], [151, 153], [154, 161], [162, 171], [172, 182], [182, 183], [184, 189], [190, 193], [194, 199], [200, 209], [210, 213], [214, 222], [223, 232], [233, 235], [236, 243], [244, 254], [254, 255], [256, 260], [261, 271], [272, 275], [276, 279], [280, 290], [291, 293], [294, 301], [301, 302]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [0, 0, "researcher"], [13, 14, "product"], [17, 17, "country"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 8, "role", "licensing_patent_to", false, false], [0, 0, 17, 17, "physical", "", false, false], [20, 20, 0, 0, "artifact", "", false, false], [20, 20, 13, 14, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberger", "licensed", "the", "original", "patent", "from", "inventor", "George", "Devol", "and", "developed", "the", "first", "industrial", "robot", "in", "the", "US", ",", "the", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Engelberger licensed the original patent from inventor George Devol and developed the first industrial robot in the US, the Unimate, in the 1950s.", "token2charspan": [[0, 11], [12, 20], [21, 24], [25, 33], [34, 40], [41, 45], [46, 54], [55, 61], [62, 67], [68, 71], [72, 81], [82, 85], [86, 91], [92, 102], [103, 108], [109, 111], [112, 115], [116, 118], [118, 119], [120, 123], [124, 131], [131, 132], [133, 135], [136, 139], [140, 145], [145, 146]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[5, 5, "programlang"], [8, 8, "programlang"], [12, 12, "programlang"], [18, 18, "programlang"], [28, 28, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 5, 12, 12, "named", "", false, false], [8, 8, 5, 5, "origin", "descendant_of", false, false], [8, 8, 18, 18, "general-affiliation", "", false, false], [8, 8, 28, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Among", "the", "descendants", "of", "the", "CLIPS", "language", "is", "Jess", "(", "part", "of", "CLIPS", "based", "on", "rules", "rewritten", "in", "Java", ",", "it", "then", "evolved", "in", "another", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by"], "sentence-detokenized": "Among the descendants of the CLIPS language is Jess (part of CLIPS based on rules rewritten in Java, it then evolved in another direction), JESS was originally inspired by", "token2charspan": [[0, 5], [6, 9], [10, 21], [22, 24], [25, 28], [29, 34], [35, 43], [44, 46], [47, 51], [52, 53], [53, 57], [58, 60], [61, 66], [67, 72], [73, 75], [76, 81], [82, 91], [92, 94], [95, 99], [99, 100], [101, 103], [104, 108], [109, 116], [117, 119], [120, 127], [128, 137], [137, 138], [138, 139], [140, 144], [145, 148], [149, 159], [160, 168], [169, 171]]}
{"doc_key": "ai-train-79", "ner": [[6, 6, "product"], [11, 13, "product"], [16, 17, "organisation"], [21, 22, "product"], [42, 43, "product"], [45, 47, "product"], [64, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[11, 13, 6, 6, "type-of", "", false, false], [16, 17, 11, 13, "usage", "", false, false], [21, 22, 16, 17, "artifact", "", false, false], [42, 43, 16, 17, "origin", "", true, false], [42, 43, 64, 65, "related-to", "", true, false], [45, 47, 16, 17, "origin", "", true, false], [45, 47, 64, 65, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["It", "has", "also", "created", "flexible", "intelligent", "AGV", "applications", ",", "designing", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "its", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", ",", "which", "is", "used", "for", "complex", "pick", "and", "place", "operations", "in", "conjunction", "with", "gantry", "systems", "and", "industrial", "robot", "arms", ",", "used", "in", "leading", "automotive", "supply", "plants", "to", "move", "products", "from", "one", "process", "to", "another", "in", "non-linear", "layouts", "."], "sentence-detokenized": "It has also created flexible intelligent AGV applications, designing the Motivity control system used by RMT Robotics to develop its ADAM iAGV (Self-Guided Vehicle), which is used for complex pick and place operations in conjunction with gantry systems and industrial robot arms, used in leading automotive supply plants to move products from one process to another in non-linear layouts.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 19], [20, 28], [29, 40], [41, 44], [45, 57], [57, 58], [59, 68], [69, 72], [73, 81], [82, 89], [90, 96], [97, 101], [102, 104], [105, 108], [109, 117], [118, 120], [121, 128], [129, 132], [133, 137], [138, 142], [143, 144], [144, 148], [148, 149], [149, 155], [156, 163], [163, 164], [164, 165], [166, 171], [172, 174], [175, 179], [180, 183], [184, 191], [192, 196], [197, 200], [201, 206], [207, 217], [218, 220], [221, 232], [233, 237], [238, 244], [245, 252], [253, 256], [257, 267], [268, 273], [274, 278], [278, 279], [280, 284], [285, 287], [288, 295], [296, 306], [307, 313], [314, 320], [321, 323], [324, 328], [329, 337], [338, 342], [343, 346], [347, 354], [355, 357], [358, 365], [366, 368], [369, 379], [380, 387], [387, 388]]}
{"doc_key": "ai-train-80", "ner": [[7, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "\u03b2", "parameters", "are", "usually", "estimated", "by", "maximum", "likelihood", "."], "sentence-detokenized": "The \u03b2 parameters are usually estimated by maximum likelihood.", "token2charspan": [[0, 3], [4, 5], [6, 16], [17, 20], [21, 28], [29, 38], [39, 41], [42, 49], [50, 60], [60, 61]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [5, 5, "metrics"], [7, 7, "metrics"], [9, 9, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 0, 1, "part-of", "", false, false], [7, 7, 0, 1, "part-of", "", false, false], [9, 9, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", "such", "as", "precision", "and", "recall", "or", "GCI", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics such as precision and recall or GCI are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [30, 34], [35, 37], [38, 47], [48, 51], [52, 58], [59, 61], [62, 65], [66, 69], [70, 76], [77, 80], [81, 90], [91, 94], [95, 102], [103, 105], [106, 107], [108, 122], [123, 129], [129, 130]]}
{"doc_key": "ai-train-82", "ner": [[6, 6, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "contains", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 26], [27, 35], [36, 38], [39, 49], [50, 56], [57, 64], [65, 67], [68, 73], [74, 83], [84, 94], [95, 100], [100, 101], [102, 106], [107, 110], [111, 116], [117, 120], [121, 126], [127, 130], [131, 136], [137, 144], [144, 145]]}
{"doc_key": "ai-train-83", "ner": [[5, 7, "product"], [13, 14, "field"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [30, 31, "task"], [33, 34, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[13, 14, 5, 7, "usage", "", false, true], [18, 19, 13, 14, "part-of", "", false, false], [21, 22, 13, 14, "part-of", "", false, false], [24, 25, 13, 14, "part-of", "", false, false], [27, 28, 13, 14, "part-of", "", false, false], [30, 31, 13, 14, "part-of", "", false, false], [33, 34, 13, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "CNPs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", ",", "including", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growing", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, CNPs have been used in a variety of image processing applications, including image segmentation, feature generation, face extraction, motion detection, region growing and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 26], [27, 31], [32, 36], [37, 41], [42, 44], [45, 46], [47, 54], [55, 57], [58, 63], [64, 74], [75, 87], [87, 88], [89, 98], [99, 104], [105, 117], [117, 118], [119, 126], [127, 137], [137, 138], [139, 143], [144, 154], [154, 155], [156, 162], [163, 172], [172, 173], [174, 180], [181, 188], [189, 192], [193, 198], [199, 208], [208, 209]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 17, "field"], [21, 24, "misc"], [26, 32, "conference"], [34, 34, "conference"], [39, 41, "misc"], [44, 48, "conference"], [49, 50, "conference"], [52, 56, "conference"], [58, 58, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 16, 17, "related-to", "contributes_to", false, false], [0, 0, 21, 24, "win-defeat", "", false, false], [0, 0, 39, 41, "win-defeat", "", false, false], [21, 24, 26, 32, "temporal", "", false, false], [34, 34, 26, 32, "named", "", false, false], [39, 41, 44, 48, "temporal", "", false, false], [39, 41, 52, 56, "temporal", "", false, false], [49, 50, 44, 48, "named", "", false, false], [58, 58, 52, 56, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "and", "the", "Best", "Reviewer", "Award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision and won the Best Paper Award at the International Conference on Non-Photorealistic Rendering and Animation (NPAR) 2012 and the Best Reviewer Award at the Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 160], [161, 171], [172, 174], [175, 193], [194, 203], [204, 207], [208, 217], [218, 219], [219, 223], [223, 224], [225, 229], [230, 233], [234, 237], [238, 242], [243, 251], [252, 257], [258, 260], [261, 264], [265, 270], [271, 281], [282, 284], [285, 293], [294, 300], [301, 305], [306, 310], [311, 314], [315, 328], [329, 339], [340, 342], [343, 351], [352, 358], [359, 360], [360, 364], [364, 365], [366, 370], [370, 371]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 14, "researcher"], [16, 18, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 2, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [16, 18, 0, 0, "usage", "", false, false], [16, 18, 13, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "an", "ontological", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is an ontological language used by Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 70], [71, 79], [80, 84], [85, 87], [88, 92], [93, 98], [98, 100], [101, 104], [105, 115], [116, 123], [123, 124]]}
{"doc_key": "ai-train-86", "ner": [[2, 3, "task"], [6, 9, "metrics"], [16, 21, "metrics"], [23, 29, "metrics"], [39, 40, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 9, 2, 3, "part-of", "", false, false], [16, 21, 6, 9, "named", "", false, false], [23, 29, 6, 9, "named", "", false, false], [39, 40, 6, 9, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "the", "root", "mean", "square", "error", ",", "often", "referred", "to", "as", "the", "root", "mean", "square", "error", "of", "prediction", "or", "root", "mean", "square", "error", "out", "of", "sample", ",", "can", "refer", "to", "the", "mean", "value", "of", "the", "squared", "deviations", "of", "predictions", "from", "TRUE", "values", ",", "over", "an", "out", "-", "of", "-", "sample", "test", "space", ",", "generated", "by", "a", "model", "estimated", "over", "a", "particular", "sample", "space", "."], "sentence-detokenized": "Also in regression analysis, the root mean square error, often referred to as the root mean square error of prediction or root mean square error out of sample, can refer to the mean value of the squared deviations of predictions from TRUE values, over an out-of-sample test space, generated by a model estimated over a particular sample space.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 32], [33, 37], [38, 42], [43, 49], [50, 55], [55, 56], [57, 62], [63, 71], [72, 74], [75, 77], [78, 81], [82, 86], [87, 91], [92, 98], [99, 104], [105, 107], [108, 118], [119, 121], [122, 126], [127, 131], [132, 138], [139, 144], [145, 148], [149, 151], [152, 158], [158, 159], [160, 163], [164, 169], [170, 172], [173, 176], [177, 181], [182, 187], [188, 190], [191, 194], [195, 202], [203, 213], [214, 216], [217, 228], [229, 233], [234, 238], [239, 245], [245, 246], [247, 251], [252, 254], [255, 258], [258, 259], [259, 261], [261, 262], [262, 268], [269, 273], [274, 279], [279, 280], [281, 290], [291, 293], [294, 295], [296, 301], [302, 311], [312, 316], [317, 318], [319, 329], [330, 336], [337, 342], [342, 343]]}
{"doc_key": "ai-train-87", "ner": [[6, 8, "algorithm"], [10, 11, "algorithm"], [19, 22, "algorithm"], [33, 34, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 8, 10, 11, "compare", "", false, false], [6, 8, 19, 22, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "terms", "of", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptors", "maintaining", "a", "slight", "advantage", "in", "detection", "failure", "rate", "for", "fixed", "false", "positive", "rates", "on", "both", "datasets", "."], "sentence-detokenized": "In terms of results, the C-HOG and R-HOG block descriptors perform comparably, with the C-HOG descriptors maintaining a slight advantage in detection failure rate for fixed false positive rates on both datasets.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 19], [19, 20], [21, 24], [25, 26], [26, 27], [27, 30], [31, 34], [35, 37], [37, 40], [41, 46], [47, 58], [59, 66], [67, 77], [77, 78], [79, 83], [84, 87], [88, 89], [89, 90], [90, 93], [94, 105], [106, 117], [118, 119], [120, 126], [127, 136], [137, 139], [140, 149], [150, 157], [158, 162], [163, 166], [167, 172], [173, 178], [179, 187], [188, 193], [194, 196], [197, 201], [202, 210], [210, 211]]}
{"doc_key": "ai-train-88", "ner": [[6, 8, "algorithm"], [10, 10, "misc"], [12, 14, "algorithm"], [16, 17, "algorithm"], [20, 21, "algorithm"], [24, 26, "algorithm"], [28, 30, "algorithm"], [32, 33, "misc"], [38, 40, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 8, 10, 10, "usage", "", false, false], [12, 14, 32, 33, "usage", "", false, false], [16, 17, 32, 33, "usage", "", false, false], [20, 21, 32, 33, "usage", "", false, false], [24, 26, 32, 33, "usage", "", false, false], [28, 30, 32, 33, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["The", "most", "common", "recognition", "algorithms", "are", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "the", "Fisherface", "algorithm", ",", "the", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "neural", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "The most common recognition algorithms are principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using the Fisherface algorithm, the hidden Markov model, multilinear subspace learning using tensor representation, and neural motivated dynamic link matching.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 27], [28, 38], [39, 42], [43, 52], [53, 62], [63, 71], [72, 77], [78, 88], [88, 89], [90, 96], [97, 109], [110, 118], [118, 119], [120, 127], [128, 136], [137, 142], [143, 146], [147, 157], [158, 167], [167, 168], [169, 172], [173, 179], [180, 186], [187, 192], [192, 193], [194, 205], [206, 214], [215, 223], [224, 229], [230, 236], [237, 251], [251, 252], [253, 256], [257, 263], [264, 273], [274, 281], [282, 286], [287, 295], [295, 296]]}
{"doc_key": "ai-train-89", "ner": [[3, 7, "misc"], [17, 19, "location"], [37, 39, "location"], [52, 52, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 19, 3, 7, "temporal", "", false, false], [37, 39, 3, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "of", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "may", "no", "longer", "be", "screened", "at", "the", "Scotiabank", "Theatre", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "may", "be", "screened", "elsewhere", "(", "including", "at", "TIFF", "Bell", "Lightbox", "and", "other", "local", "theatres", ")", "if", "distributed", "by", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "As of the 2019 Toronto International Film Festival, films may no longer be screened at the Scotiabank Theatre Toronto - one of the festival's main venues - and may be screened elsewhere (including at TIFF Bell Lightbox and other local theatres) if distributed by a service such as Netflix.", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 14], [15, 22], [23, 36], [37, 41], [42, 50], [50, 51], [52, 57], [58, 61], [62, 64], [65, 71], [72, 74], [75, 83], [84, 86], [87, 90], [91, 101], [102, 109], [110, 117], [118, 119], [120, 123], [124, 126], [127, 130], [131, 139], [139, 141], [142, 146], [147, 153], [154, 155], [156, 159], [160, 163], [164, 166], [167, 175], [176, 185], [186, 187], [187, 196], [197, 199], [200, 204], [205, 209], [210, 218], [219, 222], [223, 228], [229, 234], [235, 243], [243, 244], [245, 247], [248, 259], [260, 262], [263, 264], [265, 272], [273, 277], [278, 280], [281, 288], [288, 289]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [5, 6, "researcher"], [2, 3, "organisation"], [12, 12, "researcher"], [23, 26, "product"], [36, 36, "researcher"], [38, 40, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 0, 2, 3, "related-to", "purchases", false, false], [5, 6, 12, 12, "named", "same", false, false], [5, 6, 36, 36, "named", "same", false, false], [2, 3, 5, 6, "origin", "founded_by", false, false], [23, 26, 0, 0, "artifact", "", false, false], [38, 40, 36, 36, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Unimation", "purchased", "Vicarm", "Inc.", "from", "Victor", "Scheinman", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "producing", "the", "Universal", "Programmable", "Assembly", "Machine", ",", "a", "new", "model", "of", "robotic", "arm", ",", "using", "Scheinman", "'s", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation purchased Vicarm Inc. from Victor Scheinman in 1977, and with Scheinman's help, the company created and began producing the Universal Programmable Assembly Machine, a new model of robotic arm, using Scheinman's VAL programming language.", "token2charspan": [[0, 9], [10, 19], [20, 26], [27, 31], [32, 36], [37, 43], [44, 53], [54, 56], [57, 61], [61, 62], [63, 66], [67, 71], [72, 81], [81, 83], [84, 88], [88, 89], [90, 93], [94, 101], [102, 109], [110, 113], [114, 119], [120, 129], [130, 133], [134, 143], [144, 156], [157, 165], [166, 173], [173, 174], [175, 176], [177, 180], [181, 186], [187, 189], [190, 197], [198, 201], [201, 202], [203, 208], [209, 218], [218, 220], [221, 224], [225, 236], [237, 245], [245, 246]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [10, 11, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 10, 11, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[2, 2, "metrics"], [12, 13, "product"], [19, 25, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[2, 2, 12, 13, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "over", "20,000", "times", "according", "to", "Google", "Scholar", ".", "It", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", "for", "2016", ",", "which", "indicates", "that", "a", "paper", "has", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited over 20,000 times according to Google Scholar. It also received the IEEE Signal Processing Society Sustained Impact Award for 2016, which indicates that a paper has an unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 46], [47, 52], [53, 62], [63, 65], [66, 72], [73, 80], [80, 81], [82, 84], [85, 89], [90, 98], [99, 102], [103, 107], [108, 114], [115, 125], [126, 133], [134, 143], [144, 150], [151, 156], [157, 160], [161, 165], [165, 166], [167, 172], [173, 182], [183, 187], [188, 189], [190, 195], [196, 199], [200, 202], [203, 212], [213, 217], [218, 224], [225, 228], [229, 231], [232, 237], [238, 240], [241, 246], [247, 252], [253, 264], [264, 265]]}
{"doc_key": "ai-train-93", "ner": [[0, 3, "task"], [26, 27, "product"], [42, 44, "product"], [47, 47, "organisation"], [48, 48, "product"], [53, 53, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 3, 47, 47, "artifact", "", false, false], [26, 27, 0, 3, "related-to", "performs", false, false], [26, 27, 42, 44, "part-of", "", false, false], [47, 47, 53, 53, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Text", "-", "to", "-speech", "is", "on", "the", "verge", "of", "being", "completely", "indistinguishable", "from", "the", "voice", "of", "a", "real", "human", "being", "with", "the", "introduction", "in", "2016", "of", "Adobe", "Voco", "voice", "editing", "and", "generation", "software", ",", "a", "prototype", "intended", "to", "be", "part", "of", "the", "Adobe", "Creative", "suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", "."], "sentence-detokenized": "Text-to-speech is on the verge of being completely indistinguishable from the voice of a real human being with the introduction in 2016 of Adobe Voco voice editing and generation software, a prototype intended to be part of the Adobe Creative suite, and DeepMind WaveNet, a prototype from Google.", "token2charspan": [[0, 4], [4, 5], [5, 7], [7, 14], [15, 17], [18, 20], [21, 24], [25, 30], [31, 33], [34, 39], [40, 50], [51, 68], [69, 73], [74, 77], [78, 83], [84, 86], [87, 88], [89, 93], [94, 99], [100, 105], [106, 110], [111, 114], [115, 127], [128, 130], [131, 135], [136, 138], [139, 144], [145, 149], [150, 155], [156, 163], [164, 167], [168, 178], [179, 187], [187, 188], [189, 190], [191, 200], [201, 209], [210, 212], [213, 215], [216, 220], [221, 223], [224, 227], [228, 233], [234, 242], [243, 248], [248, 249], [250, 253], [254, 262], [263, 270], [270, 271], [272, 273], [274, 283], [284, 288], [289, 295], [295, 296]]}
{"doc_key": "ai-train-94", "ner": [[0, 3, "researcher"], [8, 10, "organisation"], [16, 21, "organisation"], [28, 28, "conference"], [36, 40, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 3, 8, 10, "role", "", false, false], [0, 3, 16, 21, "role", "", false, false], [0, 3, 28, 28, "role", "", false, false], [0, 3, 36, 40, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Dr.", "Poggio", "is", "an", "Honorary", "Fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "Fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "Founding", "Fellow", "of", "the", "AAAI", ",", "and", "a", "Founding", "Fellow", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Dr. Poggio is an Honorary Fellow of the Neuroscience Research Program, a Fellow of the American Academy of Arts and Sciences, a Founding Fellow of the AAAI, and a Founding Fellow of the McGovern Institute for Brain Research.", "token2charspan": [[0, 3], [4, 10], [11, 13], [14, 16], [17, 25], [26, 32], [33, 35], [36, 39], [40, 52], [53, 61], [62, 69], [69, 70], [71, 72], [73, 79], [80, 82], [83, 86], [87, 95], [96, 103], [104, 106], [107, 111], [112, 115], [116, 124], [124, 125], [126, 127], [128, 136], [137, 143], [144, 146], [147, 150], [151, 155], [155, 156], [157, 160], [161, 162], [163, 171], [172, 178], [179, 181], [182, 185], [186, 194], [195, 204], [205, 208], [209, 214], [215, 223], [223, 224]]}
{"doc_key": "ai-train-95", "ner": [[9, 10, "task"], [12, 12, "task"], [16, 17, "task"], [24, 24, "misc"], [25, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 10, 16, 17, "cause-effect", "", false, false], [12, 12, 16, 17, "cause-effect", "", false, false], [25, 26, 16, 17, "topic", "", false, false], [25, 26, 24, 24, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "encouraged", "by", "the", "success", "of", "speech", "recognition", "and", "synthesis", ",", "research", "into", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, encouraged by the success of speech recognition and synthesis, research into speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 31], [32, 39], [40, 42], [43, 49], [50, 61], [62, 65], [66, 75], [75, 76], [77, 85], [86, 90], [91, 97], [98, 109], [110, 115], [116, 120], [121, 124], [125, 136], [137, 139], [140, 143], [144, 150], [151, 160], [161, 168], [168, 169]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 16, "algorithm"], [22, 23, "algorithm"], [27, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 16, 3, 4, "origin", "", false, false], [15, 16, 8, 9, "origin", "", false, false], [15, 16, 11, 12, "origin", "", false, false], [15, 16, 27, 27, "part-of", "", false, false], [22, 23, 15, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisor", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "Oblivion", "Door", "(", "also", "known", "as", "the", "Holding", "Door", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisor J\u00fcrgen Schmidhuber and Fred Cummins introduced the Oblivion Door (also known as the Holding Door) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 35], [36, 42], [43, 54], [55, 58], [59, 63], [64, 71], [72, 82], [83, 86], [87, 95], [96, 100], [101, 102], [102, 106], [107, 112], [113, 115], [116, 119], [120, 127], [128, 132], [132, 133], [134, 138], [139, 142], [143, 147], [148, 160], [160, 161]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 1, 3, "part-of", "", false, false], [9, 11, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sinc", "function", "is", "usually", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sinc function is usually defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103], [104, 111]]}
{"doc_key": "ai-train-98", "ner": [[3, 7, "field"], [9, 10, "researcher"], [17, 20, "conference"], [23, 27, "organisation"], [29, 29, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[3, 7, 9, 10, "origin", "coined_term", false, false], [9, 10, 17, 20, "role", "", false, false], [9, 10, 23, 27, "role", "", false, false], [29, 29, 23, 27, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "\"", "computational", "linguistics", "\"", "was", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term \"computational linguistics\" was coined by David Hays, a founding member of the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 10], [10, 23], [24, 35], [35, 36], [37, 40], [41, 47], [48, 50], [51, 56], [57, 61], [61, 62], [63, 64], [65, 73], [74, 80], [81, 83], [84, 87], [88, 99], [100, 103], [104, 117], [118, 129], [130, 133], [134, 137], [138, 151], [152, 161], [162, 164], [165, 178], [179, 190], [191, 192], [192, 196], [196, 197], [197, 198]]}
{"doc_key": "ai-train-99", "ner": [[9, 14, "misc"], [19, 19, "misc"], [37, 39, "metrics"], [41, 41, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[41, 41, 37, 39, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "the", "one", "-dimensional", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPD", ",", "in", "order", "to", "solve", "the", "coefficients", "of", "the", "polynomials", "of", "the", "numerical", "predistorter", "and", "minimise", "the", "mean", "square", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "non-linear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "allows", "the", "capture", "of", "the", "non-linear", "products", "of", "the", "order", "of", "the", "numerical", "predistorter", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In the one-dimensional polynomial-based memory (or memoryless) DPD, in order to solve the coefficients of the polynomials of the numerical predistorter and minimise the mean square error (MSE), the distorted output of the non-linear system must be oversampled at a rate that allows the capture of the non-linear products of the order of the numerical predistorter.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 35], [36, 39], [39, 51], [52, 62], [62, 63], [63, 68], [69, 75], [76, 77], [77, 79], [80, 90], [90, 91], [92, 95], [95, 96], [97, 99], [100, 105], [106, 108], [109, 114], [115, 118], [119, 131], [132, 134], [135, 138], [139, 150], [151, 153], [154, 157], [158, 167], [168, 180], [181, 184], [185, 193], [194, 197], [198, 202], [203, 209], [210, 215], [216, 217], [217, 220], [220, 221], [221, 222], [223, 226], [227, 236], [237, 243], [244, 246], [247, 250], [251, 261], [262, 268], [269, 273], [274, 276], [277, 288], [289, 291], [292, 293], [294, 298], [299, 303], [304, 310], [311, 314], [315, 322], [323, 325], [326, 329], [330, 340], [341, 349], [350, 352], [353, 356], [357, 362], [363, 365], [366, 369], [370, 379], [380, 392], [392, 393]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [10, 10, "location"], [12, 13, "location"], [15, 16, "country"], [20, 20, "location"], [22, 22, "country"], [37, 43, "organisation"], [46, 49, "organisation"], [51, 51, "location"], [58, 59, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 10, 10, "physical", "", false, false], [0, 1, 46, 49, "physical", "", false, false], [0, 1, 58, 59, "role", "", false, false], [10, 10, 12, 13, "physical", "", false, false], [12, 13, 15, 16, "physical", "", false, false], [37, 43, 46, 49, "part-of", "", false, false], [46, 49, 51, 51, "physical", "", false, false], [58, 59, 37, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "October", "5", ",", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldavian", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "an", "American", "senior", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "and", "head", "of", "the", "laboratory", "'s", "InfoLab", "group", "."], "sentence-detokenized": "Boris Katz, (born October 5, 1947 in Chi\u0219in\u0103u, Moldavian SSR, Soviet Union, (now Chi\u0219in\u0103u, Moldova)) is an American senior research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and head of the laboratory's InfoLab group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 25], [26, 27], [27, 28], [29, 33], [34, 36], [37, 45], [45, 46], [47, 56], [57, 60], [60, 61], [62, 68], [69, 74], [74, 75], [76, 77], [77, 80], [81, 89], [89, 90], [91, 98], [98, 99], [99, 100], [101, 103], [104, 106], [107, 115], [116, 122], [123, 131], [132, 141], [142, 143], [143, 151], [152, 161], [161, 162], [163, 165], [166, 169], [170, 173], [174, 182], [183, 190], [191, 194], [195, 205], [206, 218], [219, 229], [230, 232], [233, 236], [237, 250], [251, 260], [261, 263], [264, 274], [275, 277], [278, 287], [288, 291], [292, 296], [297, 299], [300, 303], [304, 314], [314, 316], [317, 324], [325, 330], [330, 331]]}
