{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [30, 31, "field"], [32, 34, "researcher"], [36, 38, "researcher"], [40, 41, "researcher"], [43, 44, "researcher"], [46, 48, "researcher"], [50, 51, "researcher"], [53, 54, "researcher"], [56, 57, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 20, "part-of", "", false, false], [3, 7, 19, 20, "usage", "", false, false], [3, 7, 30, 31, "part-of", "", false, false], [3, 7, 30, 31, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "to", "opinion", "-", "based", "recommendation", "system", "use", "various", "techniques", ",", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "Multimodal", "sentiment", "analysis", ")", ",", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches to opinion-based recommendation system use various techniques, including text mining, information retrieval, sentiment analysis (see also Multimodal sentiment analysis), and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019), 21 (5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 50], [51, 57], [58, 61], [62, 69], [70, 80], [80, 81], [82, 91], [92, 96], [97, 103], [103, 104], [105, 116], [117, 126], [126, 127], [128, 137], [138, 146], [147, 148], [148, 151], [152, 156], [157, 167], [168, 177], [178, 186], [186, 187], [187, 188], [189, 192], [193, 197], [198, 206], [207, 210], [210, 211], [212, 216], [216, 217], [218, 219], [219, 220], [221, 226], [226, 227], [228, 232], [233, 236], [236, 237], [238, 242], [243, 248], [248, 249], [250, 251], [251, 252], [253, 256], [256, 257], [258, 262], [263, 268], [268, 269], [270, 274], [275, 279], [279, 280], [281, 283], [284, 286], [286, 287], [288, 289], [289, 293], [293, 294], [294, 295], [296, 298], [299, 300], [300, 301], [301, 302], [302, 303], [304, 310], [310, 311]]}
{"doc_key": "ai-train-2", "ner": [[9, 9, "university"], [13, 14, "researcher"], [16, 17, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[13, 14, 9, 9, "physical", "", false, false], [13, 14, 9, 9, "role", "", false, false], [16, 17, 9, 9, "physical", "", false, false], [16, 17, 9, 9, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "proponents", "of", "procedural", "representations", "were", "mainly", "concentrated", "at", "MIT", ",", "led", "by", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The proponents of procedural representations were mainly concentrated at MIT, led by Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 3], [4, 14], [15, 17], [18, 28], [29, 44], [45, 49], [50, 56], [57, 69], [70, 72], [73, 76], [76, 77], [78, 81], [82, 84], [85, 91], [92, 98], [99, 102], [103, 110], [111, 117], [117, 118]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [22, 22, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 22, 22, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "to", "numerically", "solve", "linear", "and", "nonlinear", "problems", "and", "perform", "other", "numerical", "experiments", "using", "a", "program", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps to numerically solve linear and nonlinear problems and perform other numerical experiments using a program that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 27], [28, 33], [34, 40], [41, 44], [45, 54], [55, 63], [64, 67], [68, 75], [76, 81], [82, 91], [92, 103], [104, 109], [110, 111], [112, 119], [120, 124], [125, 127], [128, 134], [135, 145], [146, 150], [151, 157], [157, 158]]}
{"doc_key": "ai-train-5", "ner": [[3, 6, "algorithm"], [10, 11, "misc"], [13, 14, "researcher"], [19, 21, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 6, 13, 14, "origin", "", false, false], [10, 11, 13, 14, "origin", "", false, false], [13, 14, 19, 21, "physical", "", false, false], [13, 14, 19, 21, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", "as", "well", "as", "unsupervised", "methods", "from", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm as well as unsupervised methods from Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [43, 45], [46, 50], [51, 53], [54, 66], [67, 74], [75, 79], [80, 85], [86, 92], [93, 96], [97, 107], [108, 110], [111, 114], [115, 125], [126, 128], [129, 136], [137, 140], [141, 143], [144, 148], [149, 151], [152, 157], [158, 162], [162, 163], [164, 170], [171, 180], [181, 187], [188, 201], [201, 202], [203, 204], [204, 205], [205, 209], [210, 217]]}
{"doc_key": "ai-train-6", "ner": [[3, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalently", "using", "DCG", "notation", ":"], "sentence-detokenized": "or equivalently using DCG notation:", "token2charspan": [[0, 2], [3, 15], [16, 21], [22, 25], [26, 34], [34, 35]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 9, "algorithm"], [14, 16, "algorithm"], [19, 21, "algorithm"], [25, 25, "algorithm"], [27, 28, "algorithm"], [44, 46, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 9, "type-of", "", false, false], [0, 3, 14, 16, "usage", "part-of?", true, false], [14, 16, 19, 21, "compare", "", false, false], [25, 25, 19, 21, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organizing", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "implement", "competitive", "learning", "as", "opposed", "to", "error", "-correction", "learning", ",", "such", "as", "backpropagation", "with", "gradient", "descent", ")", ",", "and", "in", "the", "sense", "that", "they", "use", "a", "neighborhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organizing maps differ from other artificial neural networks in that they implement competitive learning as opposed to error-correction learning, such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 88], [89, 100], [101, 109], [110, 112], [113, 120], [121, 123], [124, 129], [129, 140], [141, 149], [149, 150], [151, 155], [156, 158], [159, 174], [175, 179], [180, 188], [189, 196], [196, 197], [197, 198], [199, 202], [203, 205], [206, 209], [210, 215], [216, 220], [221, 225], [226, 229], [230, 231], [232, 244], [245, 253], [254, 256], [257, 265], [266, 269], [270, 281], [282, 292], [293, 295], [296, 299], [300, 305], [306, 311], [311, 312]]}
{"doc_key": "ai-train-8", "ner": [[15, 17, "organisation"], [30, 34, "misc"], [39, 41, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "it", "has", "been", "recommended", "by", "several", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "that", "dynamic", "range", "measurements", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", ",", "which", "is", "then", "filtered", "into", "the", "noise", "floor", "measurement", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "muting", "circuits", "."], "sentence-detokenized": "Since the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that dynamic range measurements be made in the presence of an audio signal, which is then filtered into the noise floor measurement used to determine dynamic range. This avoids questionable measurements based on the use of blank media or muting circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 25], [26, 29], [30, 34], [35, 46], [47, 49], [50, 57], [58, 69], [69, 70], [71, 80], [81, 84], [85, 90], [91, 102], [103, 110], [110, 111], [112, 116], [117, 124], [125, 130], [131, 143], [144, 146], [147, 151], [152, 154], [155, 158], [159, 167], [168, 170], [171, 173], [174, 179], [180, 186], [186, 187], [188, 193], [194, 196], [197, 201], [202, 210], [211, 215], [216, 219], [220, 225], [226, 231], [232, 243], [244, 248], [249, 251], [252, 261], [262, 269], [270, 275], [275, 276], [277, 281], [282, 288], [289, 301], [302, 314], [315, 320], [321, 323], [324, 327], [328, 331], [332, 334], [335, 340], [341, 346], [347, 349], [350, 356], [357, 365], [365, 366]]}
{"doc_key": "ai-train-9", "ner": [[5, 6, "misc"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [29, 29, "task"], [30, 33, "task"], [35, 37, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[5, 6, 17, 18, "part-of", "concept_used_in", true, false], [5, 6, 20, 21, "part-of", "concept_used_in", false, false], [5, 6, 23, 24, "part-of", "concept_used_in", false, false], [5, 6, 26, 27, "part-of", "concept_used_in", false, false], [5, 6, 29, 29, "part-of", "concept_used_in", false, false], [5, 6, 30, 33, "part-of", "concept_used_in", false, false], [5, 6, 35, 37, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "technique", "used", "to", "create", "idiosyncratic", "faces", "and", "use", "them", "for", "recognition", "is", "also", "used", "outside", "of", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "gesture", "interpretation", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "The technique used to create idiosyncratic faces and use them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language/gesture interpretation and medical image analysis.", "token2charspan": [[0, 3], [4, 13], [14, 18], [19, 21], [22, 28], [29, 42], [43, 48], [49, 52], [53, 56], [57, 61], [62, 65], [66, 77], [78, 80], [81, 85], [86, 90], [91, 98], [99, 101], [102, 106], [107, 118], [118, 119], [120, 131], [132, 143], [143, 144], [145, 148], [149, 156], [156, 157], [158, 163], [164, 175], [175, 176], [177, 181], [182, 190], [190, 191], [191, 198], [199, 213], [214, 217], [218, 225], [226, 231], [232, 240], [240, 241]]}
{"doc_key": "ai-train-10", "ner": [[1, 3, "organisation"], [10, 13, "organisation"], [15, 15, "organisation"], [19, 22, "organisation"], [25, 29, "organisation"], [32, 35, "organisation"], [42, 42, "organisation"], [38, 44, "organisation"], [48, 51, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 13, 1, 3, "part-of", "", false, false], [15, 15, 10, 13, "named", "", false, false], [19, 22, 1, 3, "part-of", "", false, false], [25, 29, 1, 3, "part-of", "", false, false], [32, 35, 1, 3, "part-of", "", false, false], [42, 42, 1, 3, "part-of", "", false, false], [38, 44, 42, 42, "named", "", false, false], [48, 51, 1, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "an", "umbrella", "for", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "coordinated", "studies", "to", "inform", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 38], [39, 47], [48, 51], [52, 55], [56, 64], [65, 76], [77, 80], [81, 86], [87, 101], [102, 103], [103, 107], [107, 108], [108, 109], [110, 113], [114, 116], [117, 127], [128, 130], [131, 137], [137, 138], [139, 142], [143, 145], [146, 156], [157, 159], [160, 168], [169, 173], [173, 174], [175, 178], [179, 181], [182, 192], [193, 195], [196, 203], [203, 204], [205, 208], [209, 216], [217, 225], [226, 234], [235, 243], [244, 250], [251, 252], [252, 257], [257, 258], [259, 262], [263, 266], [267, 273], [274, 276], [277, 282], [283, 291], [292, 303], [304, 311], [312, 314], [315, 321], [322, 331], [332, 340], [341, 343], [344, 349], [350, 363], [363, 364]]}
{"doc_key": "ai-train-11", "ner": [[5, 6, "metrics"], [10, 11, "algorithm"], [15, 16, "researcher"], [21, 21, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 10, 11, "part-of", "", false, false], [15, 16, 21, 21, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "rapid", "method", "for", "computing", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "as", "an", "appendix", "to", "Bliss", "'", "work", "in", "1935", "."], "sentence-detokenized": "A rapid method for computing maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an appendix to Bliss' work in 1935.", "token2charspan": [[0, 1], [2, 7], [8, 14], [15, 18], [19, 28], [29, 36], [37, 47], [48, 57], [58, 61], [62, 65], [66, 72], [73, 78], [79, 82], [83, 91], [92, 94], [95, 101], [102, 108], [109, 111], [112, 114], [115, 123], [124, 126], [127, 132], [132, 133], [134, 138], [139, 141], [142, 146], [146, 147]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 15, "product"], [20, 20, "product"], [23, 23, "organisation"], [25, 25, "product"]], "ner_mapping_to_source": [0, 1, 3, 4, 5], "relations": [[20, 20, 14, 15, "usage", "uses_software", false, false], [20, 20, 25, 25, "named", "", false, false], [25, 25, 23, 23, "artifact", "", true, false]], "relations_mapping_to_source": [0, 2, 3], "sentence": ["Several", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "powers", "AltaVista", "'s", "BabelFish", "(", "now", "Yahoo", "'s", "Babelfish", "as", "of", "9", "May", "2008", ")", "."], "sentence-detokenized": "Several of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (now Yahoo's Babelfish as of 9 May 2008).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 39], [40, 46], [46, 47], [48, 52], [53, 55], [56, 62], [63, 72], [73, 76], [77, 80], [81, 88], [89, 95], [96, 100], [101, 107], [108, 117], [117, 119], [120, 129], [130, 131], [131, 134], [135, 140], [140, 142], [143, 152], [153, 155], [156, 158], [159, 160], [161, 164], [165, 169], [169, 170], [170, 171]]}
{"doc_key": "ai-train-13", "ner": [[2, 2, "researcher"], [6, 7, "researcher"], [9, 10, "researcher"], [19, 21, "field"], [25, 26, "misc"], [30, 31, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[2, 2, 19, 21, "related-to", "", true, false], [2, 2, 25, 26, "related-to", "", true, false], [2, 2, 30, 31, "related-to", "", true, false], [6, 7, 19, 21, "related-to", "", true, false], [6, 7, 25, 26, "related-to", "", true, false], [6, 7, 30, 31, "related-to", "", true, false], [9, 10, 19, 21, "related-to", "", true, false], [9, 10, 25, 26, "related-to", "", true, false], [9, 10, 30, 31, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealized", "intelligent", "agents", "and", "reward", "motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002 Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealized intelligent agents and reward motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [8, 14], [14, 15], [16, 24], [25, 29], [30, 36], [37, 48], [49, 52], [53, 58], [59, 63], [63, 64], [65, 74], [75, 78], [79, 88], [89, 90], [91, 103], [104, 110], [111, 113], [114, 124], [125, 132], [133, 145], [146, 151], [152, 154], [155, 164], [165, 176], [177, 183], [184, 187], [188, 194], [195, 204], [205, 218], [219, 227], [227, 228]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 19, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 19, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measure", "."], "sentence-detokenized": "The most common way is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 25], [26, 29], [30, 33], [34, 36], [36, 37], [37, 43], [44, 49], [50, 51], [51, 57], [57, 58], [58, 66], [67, 77], [78, 81], [82, 89], [90, 100], [100, 101], [102, 109], [109, 110]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [15, 15, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [0, 0, 15, 15, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "learning", "schemas", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides learning schemas, models and algorithms and can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 85], [86, 87], [88, 91], [92, 98], [99, 106], [106, 107], [108, 113], [114, 120], [120, 121], [122, 127], [128, 136], [136, 137], [138, 140], [141, 149], [150, 154], [154, 155]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [13, 14, "task"], [18, 20, "misc"], [33, 35, "programlang"], [38, 39, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 13, 14, "related-to", "", false, false], [0, 0, 38, 39, "related-to", "", true, false], [18, 20, 0, 0, "part-of", "", false, false], [38, 39, 33, 35, "general-affiliation", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["tity", "contains", "a", "collection", "of", "visualization", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modeling", ",", "along", "with", "graphical", "user", "interfaces", "for", "easy", "access", "to", "these", "functions", ".", "but", "the", "most", "recent", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "for", "which", "development", "began", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "particularly", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "tity contains a collection of visualization tools and algorithms for data analysis and predictive modeling, along with graphical user interfaces for easy access to these functions. but the most recent fully Java-based version (Weka 3), for which development began in 1997, is now used in many different application areas, particularly for educational and research purposes.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 106], [106, 107], [108, 113], [114, 118], [119, 128], [129, 133], [134, 144], [145, 148], [149, 153], [154, 160], [161, 163], [164, 169], [170, 179], [179, 180], [181, 184], [185, 188], [189, 193], [194, 200], [201, 206], [207, 211], [211, 212], [212, 217], [218, 225], [226, 227], [227, 231], [232, 233], [233, 234], [234, 235], [236, 239], [240, 245], [246, 257], [258, 263], [264, 266], [267, 271], [271, 272], [273, 275], [276, 279], [280, 284], [285, 287], [288, 292], [293, 302], [303, 314], [315, 320], [320, 321], [322, 334], [335, 338], [339, 350], [351, 354], [355, 363], [364, 372], [372, 373]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [13, 20, "misc"], [23, 25, "misc"], [28, 36, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 20, 0, 0, "topic", "", false, false], [13, 20, 23, 25, "win-defeat", "", false, false], [23, 25, 28, 36, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "received", "considerable", "recognition", ",", "with", "his", "paper", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "winning", "the", "best", "paper", "award", "at", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "in", "1982", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and received considerable recognition, with his paper Heuretics: Theoretical and Study of Heuristic Rules winning the best paper award at the Association for the Advancement of Artificial Intelligence in 1982.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 54], [55, 67], [68, 79], [79, 80], [81, 85], [86, 89], [90, 95], [96, 105], [105, 106], [107, 118], [119, 122], [123, 128], [129, 131], [132, 141], [142, 147], [148, 155], [156, 159], [160, 164], [165, 170], [171, 176], [177, 179], [180, 183], [184, 195], [196, 199], [200, 203], [204, 215], [216, 218], [219, 229], [230, 242], [243, 245], [246, 250], [250, 251]]}
{"doc_key": "ai-train-18", "ner": [[8, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "account", "for", "multiple", "entities", ",", "a", "separate", "joi", "nt", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To account for multiple entities, a separate joint loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 10], [11, 14], [15, 23], [24, 32], [32, 33], [34, 35], [36, 44], [45, 48], [48, 50], [51, 55], [56, 58], [59, 69], [70, 73], [74, 78], [79, 86], [86, 87]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 13, "product"], [15, 16, "product"], [18, 19, "product"], [21, 23, "product"], [25, 26, "product"], [35, 39, "product"], [42, 43, "product"], [45, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 25, 26, "type-of", "", false, false], [12, 13, 25, 26, "type-of", "", false, false], [15, 16, 25, 26, "type-of", "", false, false], [18, 19, 25, 26, "type-of", "", false, false], [21, 23, 25, 26, "type-of", "", false, false], [42, 43, 35, 39, "type-of", "", false, false], [45, 46, 35, 39, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "emergence", "of", "chat", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "Far", "Field", "voice", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the emergence of chat assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, voice portals can now be accessed via mobile devices and Far Field voice smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 26], [27, 37], [38, 42], [43, 45], [46, 51], [51, 53], [54, 58], [58, 59], [60, 66], [67, 72], [72, 73], [74, 80], [81, 90], [90, 91], [92, 101], [102, 109], [110, 113], [114, 121], [121, 123], [124, 129], [129, 130], [131, 136], [137, 144], [145, 148], [149, 152], [153, 155], [156, 164], [165, 168], [169, 175], [176, 183], [184, 187], [188, 191], [192, 197], [198, 203], [204, 209], [210, 218], [219, 223], [224, 226], [227, 233], [234, 238], [239, 242], [243, 249], [250, 254], [254, 255]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [6, 8, "algorithm"], [11, 13, "algorithm"], [15, 16, "algorithm"], [19, 19, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 3, "type-of", "", false, false], [11, 13, 2, 3, "type-of", "", false, false], [15, 16, 2, 3, "type-of", "", false, false], [19, 19, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "the", "Naive", "Bayes", "classifier", ",", "the", "support", "vector", "machine", ",", "Gaussians", "mixtures", "and", "the", "network", "."], "sentence-detokenized": "Examples of supervised learning are the Naive Bayes classifier, the support vector machine, Gaussians mixtures and the network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 39], [40, 45], [46, 51], [52, 62], [62, 63], [64, 67], [68, 75], [76, 82], [83, 90], [90, 91], [92, 101], [102, 110], [111, 114], [115, 118], [119, 126], [126, 127]]}
{"doc_key": "ai-train-21", "ner": [[4, 5, "algorithm"], [25, 27, "algorithm"], [29, 29, "task"], [34, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 5, 25, 27, "part-of", "", true, false], [34, 36, 29, 29, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "mathematically", "extract", "O", "(", "\\", "sqrt", "{", "T", "})", "/mathematical", "regret", "bounds", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "for", "classification", ",", "which", "uses", "the", "loss", "of", "articulation", "math", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to mathematically extract O(\\ sqrt {T})/mathematical regret bounds for the online version of the support vector machine for classification, which uses the loss of articulation math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 47], [48, 55], [56, 57], [57, 58], [58, 59], [60, 64], [65, 66], [66, 67], [67, 69], [69, 82], [83, 89], [90, 96], [97, 100], [101, 104], [105, 111], [112, 119], [120, 122], [123, 126], [127, 134], [135, 141], [142, 149], [150, 153], [154, 168], [168, 169], [170, 175], [176, 180], [181, 184], [185, 189], [190, 192], [193, 205], [206, 210], [211, 212], [213, 215], [216, 217], [217, 218], [218, 219], [220, 222], [223, 226], [226, 227], [228, 229], [229, 230], [230, 231], [232, 233], [234, 235], [236, 237], [238, 240], [241, 242], [242, 243], [243, 244], [245, 249], [250, 251], [252, 254], [254, 255], [255, 257], [258, 259], [260, 264]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 24, "task"], [26, 27, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", "and", "movie", "animation", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, individual wildlife identification and movie animation.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 88], [88, 89], [90, 92], [93, 102], [102, 103], [104, 111], [112, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 151], [152, 160], [161, 175], [176, 179], [180, 185], [186, 195], [195, 196]]}
{"doc_key": "ai-train-23", "ner": [[6, 7, "task"], [12, 13, "university"], [15, 17, "university"], [19, 20, "university"], [22, 23, "university"], [25, 30, "university"], [32, 34, "university"], [36, 38, "university"], [40, 41, "university"], [46, 48, "university"], [43, 50, "university"], [53, 57, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[6, 7, 12, 13, "related-to", "", true, false], [6, 7, 15, 17, "related-to", "", true, false], [6, 7, 19, 20, "related-to", "", true, false], [6, 7, 22, 23, "related-to", "", true, false], [6, 7, 25, 30, "related-to", "", true, false], [6, 7, 32, 34, "related-to", "", true, false], [6, 7, 36, 38, "related-to", "", true, false], [6, 7, 40, 41, "related-to", "", true, false], [6, 7, 46, 48, "related-to", "", true, false], [6, 7, 43, 50, "related-to", "", true, false], [6, 7, 53, 57, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Several", "groups", "and", "companies", "are", "researching", "pose", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", "at", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", "at", "Irvine", "."], "sentence-detokenized": "Several groups and companies are researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California at San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Science and Technology (NUST) and University of California at Irvine.", "token2charspan": [[0, 7], [8, 14], [15, 18], [19, 28], [29, 32], [33, 44], [45, 49], [50, 60], [60, 61], [62, 71], [72, 78], [79, 81], [82, 87], [88, 98], [98, 99], [100, 108], [109, 115], [116, 126], [126, 127], [128, 131], [132, 144], [144, 145], [146, 154], [155, 165], [165, 166], [167, 177], [178, 180], [181, 191], [192, 194], [195, 198], [199, 204], [204, 205], [206, 216], [217, 219], [220, 227], [227, 228], [229, 234], [235, 243], [244, 249], [249, 250], [251, 254], [255, 261], [261, 262], [263, 271], [272, 282], [283, 285], [286, 293], [294, 297], [298, 308], [309, 310], [310, 314], [314, 315], [316, 319], [320, 330], [331, 333], [334, 344], [345, 347], [348, 354], [354, 355]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoidal", "Cross", "entropy", "loss", "function", "is", "used", "to", "predict", "K", "independent", "probability", "values", "at", "math", "0.1", "/", "math", "."], "sentence-detokenized": "The sigmoidal Cross entropy loss function is used to predict K independent probability values at math 0.1/math.", "token2charspan": [[0, 3], [4, 13], [14, 19], [20, 27], [28, 32], [33, 41], [42, 44], [45, 49], [50, 52], [53, 60], [61, 62], [63, 74], [75, 86], [87, 93], [94, 96], [97, 101], [102, 105], [105, 106], [106, 110], [110, 111]]}
{"doc_key": "ai-train-25", "ner": [[3, 6, "misc"], [7, 7, "field"], [9, 10, "field"], [13, 15, "university"], [18, 18, "country"], [21, 23, "misc"], [26, 29, "university"], [31, 31, "country"], [37, 37, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 6, 7, 7, "topic", "", false, false], [3, 6, 9, 10, "topic", "", false, false], [3, 6, 13, 15, "physical", "", true, false], [13, 15, 18, 18, "physical", "", false, false], [21, 23, 26, 29, "physical", "", true, false], [26, 29, 31, 31, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["He", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "before", "becoming", "a", "professor", "at", "Cambridge", "."], "sentence-detokenized": "He held the Johann Bernoulli Chair in Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan before becoming a professor at Cambridge.", "token2charspan": [[0, 2], [3, 7], [8, 11], [12, 18], [19, 28], [29, 34], [35, 37], [38, 49], [50, 53], [54, 62], [63, 70], [71, 73], [74, 77], [78, 88], [89, 91], [92, 101], [102, 104], [105, 108], [109, 120], [121, 124], [125, 128], [129, 136], [137, 144], [145, 150], [151, 153], [154, 157], [158, 163], [164, 173], [174, 176], [177, 187], [188, 190], [191, 196], [197, 203], [204, 212], [213, 214], [215, 224], [225, 227], [228, 237], [237, 238]]}
{"doc_key": "ai-train-26", "ner": [[7, 8, "algorithm"], [14, 17, "algorithm"], [13, 19, "algorithm"], [23, 24, "researcher"], [26, 27, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 8, 14, 17, "usage", "", true, false], [14, 17, 23, 24, "origin", "", false, false], [14, 17, 26, 27, "origin", "", false, false], [13, 19, 14, 17, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "that", "is", "particularly", "used", "for", "recurrent", "neural", "networks", "is", "the", "1997", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "network", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "."], "sentence-detokenized": "Another technique that is particularly used for recurrent neural networks is the 1997 long short-term memory (LSTM) network by Sepp Hochreiter & J\u00fcrgen Schmidhuber.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 25], [26, 38], [39, 43], [44, 47], [48, 57], [58, 64], [65, 73], [74, 76], [77, 80], [81, 85], [86, 90], [91, 96], [96, 97], [97, 101], [102, 108], [109, 110], [110, 114], [114, 115], [116, 123], [124, 126], [127, 131], [132, 142], [143, 144], [145, 151], [152, 163], [163, 164]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 9, "product"], [15, 15, "product"], [47, 47, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 4, 5, "general-affiliation", "", false, false], [8, 9, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "flexible", ",", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "operations", "in", "a", "manner", "similar", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT up to version 5.34, Cling from version 6) makes this package very flexible, as it can be used in interactive, scripted and compiled operations in a manner similar to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 43], [44, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [81, 82], [83, 88], [89, 93], [94, 101], [102, 106], [107, 115], [115, 116], [117, 119], [120, 122], [123, 126], [127, 129], [130, 134], [135, 137], [138, 149], [149, 150], [151, 159], [160, 163], [164, 172], [173, 183], [184, 186], [187, 188], [189, 195], [196, 203], [204, 206], [207, 217], [218, 226], [227, 231], [232, 234], [235, 241], [241, 242]]}
{"doc_key": "ai-train-28", "ner": [[0, 2, "product"], [21, 23, "field"], [27, 28, "task"], [30, 32, "task"], [34, 35, "task"], [37, 38, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 2, 21, 23, "related-to", "", false, false], [27, 28, 21, 23, "part-of", "", false, false], [30, 32, 21, 23, "part-of", "", false, false], [34, 35, 21, 23, "part-of", "", false, false], [37, 38, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Voice", "user", "interfaces", "that", "interpret", "and", "manage", "conversational", "state", "are", "challenging", "to", "design", "because", "of", "the", "inherent", "difficulty", "of", "incorporating", "complex", "natural", "language", "processing", "tasks", "such", "as", "reference", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Voice user interfaces that interpret and manage conversational state are challenging to design because of the inherent difficulty of incorporating complex natural language processing tasks such as reference resolution, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 5], [6, 10], [11, 21], [22, 26], [27, 36], [37, 40], [41, 47], [48, 62], [63, 68], [69, 72], [73, 84], [85, 87], [88, 94], [95, 102], [103, 105], [106, 109], [110, 118], [119, 129], [130, 132], [133, 146], [147, 154], [155, 162], [163, 171], [172, 182], [183, 188], [189, 193], [194, 196], [197, 206], [207, 217], [217, 218], [219, 224], [225, 231], [232, 243], [243, 244], [245, 256], [257, 266], [267, 270], [271, 279], [280, 290], [290, 291]]}
{"doc_key": "ai-train-29", "ner": [[5, 6, "algorithm"], [9, 11, "algorithm"], [15, 17, "researcher"], [22, 26, "organisation"], [32, 33, "field"], [35, 36, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 15, 17, "origin", "", false, false], [5, 6, 32, 33, "part-of", "", false, false], [5, 6, 35, 36, "part-of", "", false, false], [9, 11, 15, 17, "origin", "", false, false], [9, 11, 32, 33, "part-of", "", false, false], [9, 11, 35, 36, "part-of", "", false, false], [15, 17, 22, 26, "physical", "", false, false], [15, 17, 22, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "feedforward", "neural", "networks", "developed", "in", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "artificial", "intelligence", "laboratory", "IDSIA", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in J\u00fcrgen Schmidhuber's research group at the Swiss artificial intelligence laboratory IDSIA won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 69], [70, 76], [77, 85], [86, 95], [96, 98], [99, 105], [106, 117], [117, 119], [120, 128], [129, 134], [135, 137], [138, 141], [142, 147], [148, 158], [159, 171], [172, 182], [183, 188], [189, 192], [193, 198], [199, 212], [213, 225], [226, 228], [229, 236], [237, 248], [249, 252], [253, 260], [261, 269], [269, 270]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 11, "misc"], [13, 13, "field"], [16, 18, "university"], [25, 28, "field"], [31, 34, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 11, 13, 13, "topic", "topic_of_award", false, false], [7, 11, 16, 18, "origin", "", true, false], [25, 28, 31, 34, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "a", "S.V.", "della", "laurea", "ad", "honorem", "in", "Psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "PhD", "in", "Industrial", "Design", "and", "Engineering", "from", "the", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, a S.V. della laurea ad honorem in Psychology from the University of Padua in 1995 and a PhD in Industrial Design and Engineering from the Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 35], [36, 40], [41, 46], [47, 53], [54, 56], [57, 64], [65, 67], [68, 78], [79, 83], [84, 87], [88, 98], [99, 101], [102, 107], [108, 110], [111, 115], [116, 119], [120, 121], [122, 125], [126, 128], [129, 139], [140, 146], [147, 150], [151, 162], [163, 167], [168, 171], [172, 177], [178, 188], [189, 191], [192, 202], [202, 203]]}
{"doc_key": "ai-train-32", "ner": [[7, 8, "researcher"], [14, 17, "organisation"], [19, 19, "location"], [21, 21, "researcher"], [32, 33, "misc"], [46, 48, "misc"], [64, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[7, 8, 14, 17, "physical", "", false, false], [7, 8, 14, 17, "role", "", false, false], [14, 17, 19, 19, "physical", "", false, false], [21, 21, 32, 33, "related-to", "works_with", true, false], [21, 21, 46, 48, "related-to", "works_with", true, false], [21, 21, 64, 65, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "with", "reduced", "proliferation", "but", "preserved", "ablation", "(", "associated", "with", "lesions", "in", "the", "inferior", "parietal", "lobe", ")", "and", "others", "with", "reduced", "ablation", "but", "preserved", "proliferation", "(", "associated", "with", "lesions", "in", "the", "intracerebellar", "groove", ")", "."], "sentence-detokenized": "Together with his long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with reduced proliferation but preserved ablation (associated with lesions in the inferior parietal lobe) and others with reduced ablation but preserved proliferation (associated with lesions in the intracerebellar groove).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [41, 48], [49, 54], [54, 55], [56, 57], [58, 69], [70, 72], [73, 76], [77, 82], [82, 83], [83, 94], [95, 103], [104, 106], [107, 112], [112, 113], [114, 121], [122, 126], [127, 137], [138, 146], [147, 151], [152, 159], [160, 162], [163, 172], [173, 180], [181, 183], [184, 187], [188, 196], [197, 201], [202, 206], [207, 214], [215, 228], [229, 232], [233, 242], [243, 251], [252, 253], [253, 263], [264, 268], [269, 276], [277, 279], [280, 283], [284, 292], [293, 301], [302, 306], [306, 307], [308, 311], [312, 318], [319, 323], [324, 331], [332, 340], [341, 344], [345, 354], [355, 368], [369, 370], [370, 380], [381, 385], [386, 393], [394, 396], [397, 400], [401, 416], [417, 423], [423, 424], [424, 425]]}
{"doc_key": "ai-train-33", "ner": [[6, 8, "product"], [13, 16, "misc"], [18, 19, "misc"], [26, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 16, 6, 8, "topic", "", false, false], [18, 19, 6, 8, "topic", "", false, false], [26, 26, 6, 8, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", "and", "the", "2016", "TV", "adaptation", "of", "Westworld", "have", "made", "audiences", "sympathetic", "to", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have made audiences sympathetic to the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 56], [57, 68], [69, 75], [76, 78], [79, 84], [85, 89], [90, 92], [93, 96], [96, 97], [98, 108], [109, 121], [122, 125], [126, 128], [129, 136], [137, 140], [141, 144], [145, 149], [150, 152], [153, 163], [164, 166], [167, 176], [177, 181], [182, 186], [187, 196], [197, 208], [209, 211], [212, 215], [216, 222], [223, 233], [233, 234]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [10, 12, "algorithm"], [14, 15, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 12, 7, 8, "part-of", "", false, false], [14, 15, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [24, 25, "misc"], [30, 31, "misc"], [33, 35, "person"], [40, 41, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[24, 25, 0, 3, "artifact", "", false, false], [30, 31, 0, 3, "artifact", "", false, false], [30, 31, 33, 35, "role", "director_of", false, false], [30, 31, 40, 41, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "make", "more", "use", "of", "3D", "films", "in", "special", "venues", "to", "impress", "audiences", ",", "with", "notable", "examples", "being", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "."], "sentence-detokenized": "The Walt Disney Company also began to make more use of 3D films in special venues to impress audiences, with notable examples being Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 42], [43, 47], [48, 51], [52, 54], [55, 57], [58, 63], [64, 66], [67, 74], [75, 81], [82, 84], [85, 92], [93, 102], [102, 103], [104, 108], [109, 116], [117, 125], [126, 131], [132, 137], [138, 146], [147, 148], [148, 152], [152, 153], [154, 157], [158, 165], [166, 168], [169, 170], [170, 177], [178, 182], [183, 190], [190, 191], [192, 196], [196, 197], [198, 206], [207, 214], [215, 222], [222, 223], [223, 224]]}
{"doc_key": "ai-train-36", "ner": [[12, 14, "field"], [19, 24, "task"], [26, 27, "task"], [29, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 14, "part-of", "", false, false], [26, 27, 12, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "analysis", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing for tasks such as part-of-speech tagging and syntactic analysis (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [95, 98], [99, 104], [105, 109], [110, 112], [113, 117], [117, 118], [118, 120], [120, 121], [121, 127], [128, 135], [136, 139], [140, 149], [150, 158], [159, 160], [160, 167], [167, 168], [169, 173], [173, 174], [174, 175]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [9, 12, "organisation"], [15, 16, "organisation"], [18, 18, "country"], [22, 25, "product"], [29, 30, "researcher"], [40, 40, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 12, 2, 3, "role", "introduces_to_market", true, false], [15, 16, 2, 3, "role", "introduces_to_market", true, false], [15, 16, 18, 18, "physical", "", false, false], [22, 25, 40, 40, "related-to", "sold_to", true, false], [29, 30, 22, 25, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletizing", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", "Company", ";", "by", "KUKA", "robotics", "in", "Germany", ",", "and", "the", "programmable", "universal", "assembly", "machine", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", ",", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletizing robot was introduced in 1963 by Fuji Yusoki Kogyo Company; by KUKA robotics in Germany, and the programmable universal assembly machine was invented by Victor Scheinman in 1976, and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [72, 79], [79, 80], [81, 83], [84, 88], [89, 97], [98, 100], [101, 108], [108, 109], [110, 113], [114, 117], [118, 130], [131, 140], [141, 149], [150, 157], [158, 161], [162, 170], [171, 173], [174, 180], [181, 190], [191, 193], [194, 198], [198, 199], [200, 203], [204, 207], [208, 214], [215, 218], [219, 223], [224, 226], [227, 236], [236, 237]]}
{"doc_key": "ai-train-38", "ner": [[10, 10, "conference"], [12, 12, "researcher"], [19, 19, "field"], [34, 35, "researcher"], [39, 40, "researcher"], [55, 55, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[12, 12, 10, 10, "role", "president_of", false, false], [12, 12, 34, 35, "role", "colleagues", false, false], [19, 19, 55, 55, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "while", "serving", "as", "president", "of", "the", "AAAI", ",", "Hayes", "launched", "a", "series", "of", "attacks", "on", "AI", "critics", ",", "mostly", "couched", "in", "ironic", "terms", ",", "and", "(", "along", "with", "his", "colleague", "Kenneth", "Ford", ")", "devised", "a", "Simon", "Newcomb", "Prize", ",", "to", "be", "awarded", "for", "the", "most", "ridiculous", "argument", "disproving", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, while serving as president of the AAAI, Hayes launched a series of attacks on AI critics, mostly couched in ironic terms, and (along with his colleague Kenneth Ford) devised a Simon Newcomb Prize, to be awarded for the most ridiculous argument disproving the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 23], [24, 31], [32, 34], [35, 44], [45, 47], [48, 51], [52, 56], [56, 57], [58, 63], [64, 72], [73, 74], [75, 81], [82, 84], [85, 92], [93, 95], [96, 98], [99, 106], [106, 107], [108, 114], [115, 122], [123, 125], [126, 132], [133, 138], [138, 139], [140, 143], [144, 145], [145, 150], [151, 155], [156, 159], [160, 169], [170, 177], [178, 182], [182, 183], [184, 191], [192, 193], [194, 199], [200, 207], [208, 213], [213, 214], [215, 217], [218, 220], [221, 228], [229, 232], [233, 236], [237, 241], [242, 252], [253, 261], [262, 272], [273, 276], [277, 288], [289, 291], [292, 294], [294, 295]]}
{"doc_key": "ai-train-39", "ner": [[11, 13, "algorithm"], [34, 35, "algorithm"], [47, 49, "algorithm"], [53, 55, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[11, 13, 34, 35, "named", "same", false, false], [47, 49, 11, 13, "type-of", "", false, false], [53, 55, 11, 13, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "optimal", "value", "for", "math\\alpha/", "math", "can", "be", "found", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math\\alpha/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimizes", "S", ",", "usually", "using", "a", "line", "search", "in", "the", "interval", "math", "0\\", "alpha", "1", "/", "math", "or", "a", "backtracking", "line", "search", "such", "as", "the", "Armijo", "line", "search", "."], "sentence-detokenized": "An optimal value for math\\alpha/math can be found using a line search algorithm, i.e. the size of math\\alpha/math is determined by finding the value that minimizes S, usually using a line search in the interval math0\\alpha 1/math or a backtracking line search such as the Armijo line search.", "token2charspan": [[0, 2], [3, 10], [11, 16], [17, 20], [21, 32], [32, 36], [37, 40], [41, 43], [44, 49], [50, 55], [56, 57], [58, 62], [63, 69], [70, 79], [79, 80], [81, 85], [86, 89], [90, 94], [95, 97], [98, 109], [109, 113], [114, 116], [117, 127], [128, 130], [131, 138], [139, 142], [143, 148], [149, 153], [154, 163], [164, 165], [165, 166], [167, 174], [175, 180], [181, 182], [183, 187], [188, 194], [195, 197], [198, 201], [202, 210], [211, 215], [215, 217], [217, 222], [223, 224], [224, 225], [225, 229], [230, 232], [233, 234], [235, 247], [248, 252], [253, 259], [260, 264], [265, 267], [268, 271], [272, 278], [279, 283], [284, 290], [290, 291]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 11, "algorithm"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "Breadth", "-", "first", "search", "and", "Depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "that", "embody", "a", "lot", "of", "technical", "knowledge", ",", "but", "does", "not", "shed", "much", "light", "on", "the", "mental", "processes", "that", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses Breadth-first search and Depth-first search techniques, but ultimately concludes that the results represent expert systems that embody a lot of technical knowledge, but does not shed much light on the mental processes that people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 67], [67, 68], [69, 72], [73, 83], [84, 93], [94, 98], [99, 102], [103, 110], [111, 120], [121, 127], [128, 135], [136, 140], [141, 147], [148, 149], [150, 153], [154, 156], [157, 166], [167, 176], [176, 177], [178, 181], [182, 186], [187, 190], [191, 195], [196, 200], [201, 206], [207, 209], [210, 213], [214, 220], [221, 230], [231, 235], [236, 242], [243, 246], [247, 249], [250, 255], [256, 260], [261, 268], [268, 269]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "are", "concerned", "with", "how", "speech", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis are concerned with how speech can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 43], [44, 53], [54, 58], [59, 62], [63, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 98], [99, 104], [105, 114], [114, 115]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"], [33, 35, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "mathematical", "\\", "theta", "^", "{", "*}", "/", "math", "is", "normally", "estimated", "using", "a", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "Maximum", "A", "Posteriori", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "MAP", "}", "/", "math", ")", "procedure", "."], "sentence-detokenized": "This mathematical \\ theta ^ {*} / math is normally estimated using a Maximum Likelihood (math\\ theta ^ {*} =\\ theta ^ {ML} / math) or Maximum A Posteriori (math\\ theta ^ {*} =\\ theta ^ {MAP} / math) procedure.", "token2charspan": [[0, 4], [5, 17], [18, 19], [20, 25], [26, 27], [28, 29], [29, 31], [32, 33], [34, 38], [39, 41], [42, 50], [51, 60], [61, 66], [67, 68], [69, 76], [77, 87], [88, 89], [89, 93], [93, 94], [95, 100], [101, 102], [103, 104], [104, 106], [107, 109], [110, 115], [116, 117], [118, 119], [119, 121], [121, 122], [123, 124], [125, 129], [129, 130], [131, 133], [134, 141], [142, 143], [144, 154], [155, 156], [156, 160], [160, 161], [162, 167], [168, 169], [170, 171], [171, 173], [174, 176], [177, 182], [183, 184], [185, 186], [186, 189], [189, 190], [191, 192], [193, 197], [197, 198], [199, 208], [208, 209]]}
{"doc_key": "ai-train-43", "ner": [[10, 11, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "lesser", "-", "used", "languages", "use", "the", "open", "-", "source", "eSpeak", "synthesizer", "for", "their", "speech", ",", "producing", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some lesser-used languages use the open-source eSpeak synthesizer for their speech, producing a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 11], [11, 12], [12, 16], [17, 26], [27, 30], [31, 34], [35, 39], [39, 40], [40, 46], [47, 53], [54, 65], [66, 69], [70, 75], [76, 82], [82, 83], [84, 93], [94, 95], [96, 103], [103, 104], [105, 112], [113, 118], [119, 123], [124, 127], [128, 130], [131, 140], [141, 143], [144, 154], [154, 155]]}
{"doc_key": "ai-train-44", "ner": [[19, 19, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[19, 19, 35, 36, "compare", "", false, false], [19, 19, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "used", "primarily", "by", "statisticians", "and", "other", "professionals", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "R", "can", "also", "function", "as", "a", "general", "matrix", "computing", "toolkit", "-", "with", "performance", "benchmarks", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although used primarily by statisticians and other professionals who need an environment for statistical computing and software development, R can also function as a general matrix computing toolkit - with performance benchmarks comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 13], [14, 23], [24, 26], [27, 40], [41, 44], [45, 50], [51, 64], [65, 68], [69, 73], [74, 76], [77, 88], [89, 92], [93, 104], [105, 114], [115, 118], [119, 127], [128, 139], [139, 140], [141, 142], [143, 146], [147, 151], [152, 160], [161, 163], [164, 165], [166, 173], [174, 180], [181, 190], [191, 198], [199, 200], [201, 205], [206, 217], [218, 228], [229, 239], [240, 242], [243, 246], [247, 253], [254, 256], [257, 263], [263, 264]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 4, "part-of", "", false, false], [0, 0, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "-", "engineer", "Reginald", "Fessenden", ",", "which", "creates", "new", "frequencies", "by", "combining", "the", "mixing", "of", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden, which creates new frequencies by combining the mixing of two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [75, 76], [76, 84], [85, 93], [94, 103], [103, 104], [105, 110], [111, 118], [119, 122], [123, 134], [135, 137], [138, 147], [148, 151], [152, 158], [159, 161], [162, 165], [166, 177], [177, 178]]}
{"doc_key": "ai-train-46", "ner": [[15, 16, "person"], [17, 18, "misc"], [22, 24, "organisation"], [27, 27, "organisation"], [29, 31, "misc"], [33, 34, "person"], [38, 40, "misc"], [42, 43, "person"], [45, 46, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 7, 8, 9], "relations": [[15, 16, 17, 18, "role", "actor_in", false, false], [17, 18, 22, 24, "artifact", "", false, false], [29, 31, 27, 27, "artifact", "", false, false], [33, 34, 29, 31, "role", "actor_in", false, false], [42, 43, 38, 40, "role", "actor_in", false, false], [45, 46, 38, 40, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["Several", "other", "films", "that", "helped", "bring", "3D", "back", "to", "the", "forefro", "nt", "that", "month", "were", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Several other films that helped bring 3D back to the forefront that month were John Wayne's Hondo (distributed by Warner Bros. ), Columbia's Miss Sadie Thompson with Rita Hayworth and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 7], [8, 13], [14, 19], [20, 24], [25, 31], [32, 37], [38, 40], [41, 45], [46, 48], [49, 52], [53, 60], [60, 62], [63, 67], [68, 73], [74, 78], [79, 83], [84, 89], [89, 91], [92, 97], [98, 99], [99, 110], [111, 113], [114, 120], [121, 125], [125, 126], [127, 128], [128, 129], [130, 138], [138, 140], [141, 145], [146, 151], [152, 160], [161, 165], [166, 170], [171, 179], [180, 183], [184, 193], [193, 195], [196, 201], [202, 206], [207, 211], [212, 216], [217, 221], [222, 228], [229, 232], [233, 238], [239, 244], [244, 245]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 4, "field"], [5, 6, "task"], [11, 11, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 5, 6, "general-affiliation", "", false, false], [0, 0, 11, 11, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "Facebook", "research", "team", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a Facebook research team.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 84], [85, 89], [89, 90]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [15, 16, "field"], [25, 28, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [25, 28, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "the", "main", "theme", "of", "the", "annual", "Symposium", "on", "Geometry", "Processing", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the leading academic conference on computer graphics, and the main theme of the annual Symposium on Geometry Processing.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 91], [92, 94], [95, 103], [104, 112], [112, 113], [114, 117], [118, 121], [122, 126], [127, 132], [133, 135], [136, 139], [140, 146], [147, 156], [157, 159], [160, 168], [169, 179], [179, 180]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [12, 16, "algorithm"], [20, 21, "algorithm"], [19, 23, "algorithm"], [26, 26, "algorithm"], [27, 30, "algorithm"], [35, 35, "misc"], [40, 42, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 8, 9], "relations": [[20, 21, 35, 35, "general-affiliation", "", false, false], [19, 23, 20, 21, "named", "", false, false], [26, 26, 35, 35, "general-affiliation", "", false, false], [27, 30, 26, 26, "named", "", false, false]], "relations_mapping_to_source": [2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "one", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "preprocessing", "step", ",", "followed", "by", "k", "-", "NN", "clustering", "into", "feature", "vectors", "in", "reduced-dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) techniques as a preprocessing step, followed by k -NN clustering into feature vectors in reduced-dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [153, 155], [156, 165], [166, 177], [178, 186], [187, 188], [188, 191], [191, 192], [193, 203], [204, 206], [207, 208], [209, 222], [223, 227], [227, 228], [229, 237], [238, 240], [241, 242], [243, 244], [244, 246], [247, 257], [258, 262], [263, 270], [271, 278], [279, 281], [282, 301], [302, 307], [307, 308]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 10, "misc"], [13, 17, "conference"], [19, 19, "conference"], [36, 39, "algorithm"], [40, 41, "researcher"], [43, 45, "researcher"], [47, 53, "misc"], [55, 64, "conference"], [66, 66, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 10, 1, 2, "artifact", "", false, false], [7, 10, 4, 5, "artifact", "", false, false], [7, 10, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [47, 53, 36, 39, "topic", "", false, false], [47, 53, 40, 41, "artifact", "", false, false], [47, 53, 43, 45, "artifact", "", false, false], [47, 53, 55, 64, "temporal", "", false, false], [66, 66, 55, 64, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histograms", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others use local features such as histograms of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 143], [144, 150], [151, 154], [155, 160], [161, 169], [170, 174], [175, 177], [178, 188], [189, 191], [192, 200], [201, 210], [211, 213], [214, 219], [219, 220], [221, 222], [222, 223], [224, 230], [230, 231], [232, 242], [243, 245], [246, 254], [255, 264], [265, 268], [269, 274], [275, 284], [284, 285], [286, 290], [291, 299], [300, 307], [308, 318], [319, 321], [322, 330], [331, 337], [338, 341], [342, 349], [350, 361], [362, 363], [363, 367], [367, 368], [368, 369], [370, 375], [376, 377], [377, 378], [379, 386], [386, 387], [388, 392], [393, 404], [404, 405]]}
{"doc_key": "ai-train-52", "ner": [[0, 2, "algorithm"], [7, 9, "algorithm"], [13, 13, "task"], [16, 17, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 7, 9, "type-of", "", false, false], [13, 13, 0, 2, "usage", "", true, false], [13, 13, 16, 17, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "auto", "-encoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "for", "learning", "features", "in", "an", "unsupervised", "learning", "manner", "."], "sentence-detokenized": "An auto-encoder is a type of artificial neural network used for learning features in an unsupervised learning manner.", "token2charspan": [[0, 2], [3, 7], [7, 15], [16, 18], [19, 20], [21, 25], [26, 28], [29, 39], [40, 46], [47, 54], [55, 59], [60, 63], [64, 72], [73, 81], [82, 84], [85, 87], [88, 100], [101, 109], [110, 116], [116, 117]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [6, 6, "organisation"], [11, 12, "field"], [14, 15, "field"], [24, 25, "organisation"], [21, 27, "organisation"], [33, 34, "field"], [36, 37, "field"], [44, 44, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 2, 6, 6, "role", "fellow_of", false, false], [0, 2, 11, 12, "related-to", "contributes_to", false, false], [0, 2, 14, 15, "related-to", "contributes_to", false, false], [0, 2, 24, 25, "role", "fellow_of", false, false], [0, 2, 33, 34, "related-to", "contributes_to", false, false], [0, 2, 36, 37, "related-to", "contributes_to", false, false], [21, 27, 24, 25, "named", "", false, false], [44, 44, 24, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "member", "of", "the", "IEEE", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", "and", "a", "member", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "pattern", "recognition", ",", "image", "processing", ",", "and", "for", "his", "service", "to", "IAPR", "."], "sentence-detokenized": "Haralick is a member of the IEEE for his contributions to computer vision and image processing and a member of the International Association for Pattern Recognition (IAPR) for his contributions to pattern recognition, image processing, and for his service to IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 40], [41, 54], [55, 57], [58, 66], [67, 73], [74, 77], [78, 83], [84, 94], [95, 98], [99, 100], [101, 107], [108, 110], [111, 114], [115, 128], [129, 140], [141, 144], [145, 152], [153, 164], [165, 166], [166, 170], [170, 171], [172, 175], [176, 179], [180, 193], [194, 196], [197, 204], [205, 216], [216, 217], [218, 223], [224, 234], [234, 235], [236, 239], [240, 243], [244, 247], [248, 255], [256, 258], [259, 263], [263, 264]]}
{"doc_key": "ai-train-54", "ner": [[4, 9, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [24, 25, "researcher"], [27, 28, "organisation"], [30, 31, "researcher"], [34, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 9, 13, 15, "usage", "", false, false], [13, 15, 24, 25, "origin", "", true, false], [13, 15, 30, 31, "origin", "", true, false], [17, 17, 13, 15, "named", "", false, false], [24, 25, 27, 28, "physical", "", false, false], [24, 25, 27, 28, "role", "", false, false], [30, 31, 34, 36, "physical", "", false, false], [30, 31, 34, 36, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "made", "with", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "-", "based", "systems", "presented", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "in", "2014", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was made with Connectionist Temporal Classification (CTC)-based systems presented by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 44], [45, 49], [50, 63], [64, 72], [73, 87], [88, 89], [89, 92], [92, 93], [93, 94], [94, 99], [100, 107], [108, 117], [118, 120], [121, 125], [126, 132], [133, 135], [136, 142], [143, 151], [152, 155], [156, 163], [164, 170], [171, 173], [174, 177], [178, 188], [189, 191], [192, 199], [200, 202], [203, 207], [207, 208]]}
{"doc_key": "ai-train-55", "ner": [[0, 4, "algorithm"], [11, 11, "algorithm"], [10, 13, "algorithm"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[10, 13, 11, 11, "named", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["Linear-", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalization", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-fractional programming (LFP) is a generalization of linear programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 13, "misc"], [16, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 13, "win-defeat", "", false, false], [8, 13, 16, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "2011", "&", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the International Conference on Machine Learning 2011 & 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 104], [105, 107], [108, 115], [116, 124], [125, 129], [130, 131], [132, 136], [136, 137]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [24, 25, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "are", "capable", "of", "deploying", "the", "neural", "network", "developed", "in", "these", "frameworks", "as", "inheritable", "components", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments are capable of deploying the neural network developed in these frameworks as inheritable components.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 121], [122, 124], [125, 134], [135, 138], [139, 145], [146, 153], [154, 163], [164, 166], [167, 172], [173, 183], [184, 186], [187, 198], [199, 209], [209, 210]]}
{"doc_key": "ai-train-58", "ner": [[2, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "images", ")", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see images) between two sentences, the candidate translation string and the reference translation string.", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 73], [74, 79], [80, 87], [88, 90], [91, 100], [101, 102], [102, 105], [106, 112], [112, 113], [114, 121], [122, 125], [126, 135], [135, 136], [137, 140], [141, 150], [151, 162], [163, 169], [170, 173], [174, 177], [178, 187], [188, 199], [200, 206], [206, 207]]}
{"doc_key": "ai-train-59", "ner": [[7, 12, "conference"], [21, 21, "task"], [23, 24, "task"], [28, 38, "metrics"], [30, 36, "metrics"], [45, 46, "conference"], [43, 48, "conference"], [51, 51, "location"], [53, 53, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[7, 12, 21, 21, "related-to", "subject_at", false, false], [7, 12, 23, 24, "related-to", "subject_at", false, false], [28, 38, 7, 12, "temporal", "", false, false], [30, 36, 28, 38, "named", "", true, false], [43, 48, 45, 46, "named", "", false, false], [51, 51, 53, 53, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "the", "annual", "NIST", "conferences", "on", "document", "comprehension", ",", "where", "research", "teams", "submit", "their", "systems", "for", "summarization", "and", "translation", "work", ",", "is", "the", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "metric", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used at the annual NIST conferences on document comprehension, where research teams submit their systems for summarization and translation work, is the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 30], [31, 37], [38, 42], [43, 54], [55, 57], [58, 66], [67, 80], [80, 81], [82, 87], [88, 96], [97, 102], [103, 109], [110, 115], [116, 123], [124, 127], [128, 141], [142, 145], [146, 157], [158, 162], [162, 163], [164, 166], [167, 170], [171, 176], [177, 178], [178, 184], [184, 185], [185, 193], [194, 204], [205, 208], [209, 216], [217, 227], [227, 228], [229, 235], [235, 236], [237, 239], [240, 248], [249, 251], [252, 258], [259, 270], [271, 281], [282, 289], [290, 291], [291, 295], [295, 296], [296, 297], [298, 306], [306, 307], [308, 314], [314, 315], [316, 324], [325, 326], [327, 331], [331, 332]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [10, 11, "programlang"], [16, 16, "product"], [22, 22, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 10, 11, "type-of", "", false, false], [6, 6, 22, 22, "named", "", false, false], [8, 8, 10, 11, "part-of", "", false, false], [8, 8, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "to", "run", "in", "Java", "with", "JShell", "(", "Java", "9", "at", "least", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, to run in Java with JShell (Java 9 at least): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 23], [24, 27], [28, 30], [31, 35], [36, 40], [41, 47], [48, 49], [49, 53], [54, 55], [56, 58], [59, 64], [64, 65], [65, 66], [67, 77], [78, 88], [89, 90], [91, 110], [111, 115], [116, 117], [118, 122]]}
{"doc_key": "ai-train-61", "ner": [[1, 6, "metrics"], [7, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[1, 6, 7, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLEU", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLEU metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [10, 12, "university"], [15, 17, "university"], [24, 25, "product"], [29, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 12, 6, 6, "physical", "", false, false], [15, 17, 6, 6, "physical", "", false, false], [24, 25, 10, 12, "origin", "", false, false], [24, 25, 15, 17, "origin", "", false, false], [24, 25, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "additional", "constraint", "that", "edges", "are", "restricted", "to", "come", "from", "a", "limited", "set", "of", "possible", "relations", ",", "in", "order", "to", "facilitate", "algebras", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called Knowledge Graphs, which are semantic networks, but with the additional constraint that edges are restricted to come from a limited set of possible relations, in order to facilitate algebras on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 116], [117, 118], [119, 126], [127, 133], [134, 143], [144, 150], [150, 151], [152, 157], [158, 161], [162, 170], [171, 179], [179, 180], [181, 184], [185, 189], [190, 193], [194, 204], [205, 215], [216, 220], [221, 226], [227, 230], [231, 241], [242, 244], [245, 249], [250, 254], [255, 256], [257, 264], [265, 268], [269, 271], [272, 280], [281, 290], [290, 291], [292, 294], [295, 300], [301, 303], [304, 314], [315, 323], [324, 326], [327, 330], [331, 336], [336, 337]]}
{"doc_key": "ai-train-63", "ner": [[0, 2, "product"], [17, 18, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checks", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", ",", "such", "as", "a", "text", "editor", ",", "but", "they", "are", "also", "available", "as", "a", "stand", "-", "alone", "application", "that", "can", "be", "activated", "from", "within", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checks are most often implemented as a feature of a larger program, such as a text editor, but they are also available as a stand-alone application that can be activated from within programs that work with editable text.", "token2charspan": [[0, 7], [8, 14], [15, 18], [19, 23], [24, 29], [30, 41], [42, 44], [45, 46], [47, 54], [55, 57], [58, 59], [60, 66], [67, 74], [74, 75], [76, 80], [81, 83], [84, 85], [86, 90], [91, 97], [97, 98], [99, 102], [103, 107], [108, 111], [112, 116], [117, 126], [127, 129], [130, 131], [132, 137], [137, 138], [138, 143], [144, 155], [156, 160], [161, 164], [165, 167], [168, 177], [178, 182], [183, 189], [190, 198], [199, 203], [204, 208], [209, 213], [214, 222], [223, 227], [227, 228]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 21, "conference"], [25, 28, "organisation"], [36, 38, "conference"], [40, 42, "conference"], [45, 47, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "the", "Society", "for", "Cognitive", "Science", ",", "and", "an", "editor", "of", "the", "journals", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence, and the Society for Cognitive Science, and an editor of the journals J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [137, 138], [139, 142], [143, 146], [147, 154], [155, 158], [159, 168], [169, 176], [176, 177], [178, 181], [182, 184], [185, 191], [192, 194], [195, 198], [199, 207], [208, 210], [211, 220], [221, 230], [230, 231], [232, 234], [235, 243], [244, 252], [252, 253], [254, 257], [258, 260], [261, 268], [269, 277], [277, 278]]}
{"doc_key": "ai-train-65", "ner": [[1, 2, "algorithm"], [0, 4, "algorithm"], [10, 11, "task"], [21, 22, "researcher"], [24, 25, "university"], [27, 28, "researcher"], [30, 33, "organisation"], [35, 37, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[1, 2, 10, 11, "type-of", "", false, false], [1, 2, 21, 22, "origin", "", false, false], [1, 2, 27, 28, "origin", "", false, false], [0, 4, 1, 2, "named", "", false, false], [21, 22, 24, 25, "physical", "", false, false], [21, 22, 24, 25, "role", "", false, false], [27, 28, 30, 33, "role", "", false, false], [35, 37, 30, 33, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "began", "to", "be", "developed", "with", "the", "work", "of", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "in", "1966", "."], "sentence-detokenized": "Linear predictive coding (LPC), a form of speech coding, began to be developed with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 62], [63, 65], [66, 68], [69, 78], [79, 83], [84, 87], [88, 92], [93, 95], [96, 104], [105, 112], [113, 115], [116, 122], [123, 133], [134, 137], [138, 143], [144, 149], [150, 152], [153, 159], [160, 169], [170, 173], [174, 183], [184, 185], [185, 188], [188, 189], [190, 192], [193, 197], [197, 198]]}
{"doc_key": "ai-train-66", "ner": [[58, 60, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "further", "ergodic", ",", "all", "sample", "paths", "exhibit", "the", "same", "mean", "time", "average", "and", "therefore", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is further ergodic, all sample paths exhibit the same mean time average and therefore mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the sense of mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 24], [25, 32], [32, 33], [34, 37], [38, 44], [45, 50], [51, 58], [59, 62], [63, 67], [68, 72], [73, 77], [78, 85], [86, 89], [90, 99], [100, 105], [106, 107], [108, 109], [110, 111], [112, 113], [113, 114], [115, 116], [117, 118], [119, 120], [121, 122], [122, 123], [124, 125], [125, 126], [127, 130], [130, 131], [132, 134], [135, 142], [143, 144], [144, 145], [145, 146], [147, 148], [149, 150], [151, 152], [153, 154], [154, 155], [156, 157], [158, 159], [160, 161], [162, 163], [163, 164], [165, 166], [166, 167], [168, 171], [171, 172], [173, 174], [175, 179], [180, 182], [183, 186], [187, 192], [193, 195], [196, 200], [201, 207], [208, 213], [213, 214]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [12, 16, "algorithm"], [20, 20, "algorithm"], [19, 23, "algorithm"], [26, 26, "algorithm"], [27, 30, "algorithm"], [33, 35, "algorithm"], [33, 37, "algorithm"], [42, 43, "misc"], [47, 49, "algorithm"], [52, 53, "misc"]], "ner_mapping_to_source": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[20, 20, 42, 43, "related-to", "", false, false], [19, 23, 20, 20, "named", "", false, false], [26, 26, 42, 43, "related-to", "", false, false], [27, 30, 26, 26, "named", "", false, false], [33, 35, 42, 43, "related-to", "", false, false], [33, 37, 33, 35, "named", "", false, false], [47, 49, 52, 53, "related-to", "", true, false]], "relations_mapping_to_source": [2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "one", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorization", "(", "NMF", ")", "techniques", "as", "a", "preprocessing", "step", ",", "followed", "by", "K", "-", "NN", "clustering", "into", "feature", "vectors", "in", "reduced-dimensional", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorization (NMF) techniques as a preprocessing step, followed by K-NN clustering into feature vectors in reduced-dimensional space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [152, 153], [154, 163], [164, 175], [176, 184], [185, 186], [186, 189], [189, 190], [191, 193], [194, 206], [207, 213], [214, 227], [228, 229], [229, 232], [232, 233], [234, 244], [245, 247], [248, 249], [250, 263], [264, 268], [268, 269], [270, 278], [279, 281], [282, 283], [283, 284], [284, 286], [287, 297], [298, 302], [303, 310], [311, 318], [319, 321], [322, 341], [342, 347], [347, 348]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [10, 10, "programlang"], [16, 16, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[16, 16, 3, 3, "related-to", "program_type_compatible_with", false, false], [16, 16, 5, 5, "related-to", "program_type_compatible_with", false, false], [16, 16, 7, 7, "related-to", "program_type_compatible_with", false, false], [16, 16, 10, 10, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", ",", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX, or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [40, 41], [42, 44], [45, 49], [50, 53], [54, 56], [57, 63], [64, 72], [73, 77], [78, 84], [84, 85]]}
{"doc_key": "ai-train-69", "ner": [[3, 7, "task"], [11, 12, "task"], [30, 31, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 7, 11, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "identifying", "named", "entities", "in", "text", "is", "called", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "identifying", "the", "identity", "of", "named", "entities", "referred", "to", "in", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of identifying named entities in text is called Named Entity Recognition, while the task of identifying the identity of named entities referred to in text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 46], [47, 49], [50, 56], [57, 62], [63, 69], [70, 81], [81, 82], [83, 88], [89, 92], [93, 97], [98, 100], [101, 112], [113, 116], [117, 125], [126, 128], [129, 134], [135, 143], [144, 152], [153, 155], [156, 158], [159, 163], [164, 166], [167, 173], [174, 180], [181, 188], [188, 189]]}
{"doc_key": "ai-train-70", "ner": [[1, 1, "algorithm"], [28, 28, "programlang"], [29, 29, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 29, 29, "part-of", "", true, false], [29, 29, 28, 28, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "but", "from", "version", "0.8.0", "onwards", ",", "these", "were", "released", "in", "a", "separate", "R", "sigmoid", "package", "to", "allow", "for", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, but from version 0.8.0 onwards, these were released in a separate R sigmoid package to allow for more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 102], [103, 107], [108, 115], [116, 121], [122, 129], [129, 130], [131, 136], [137, 141], [142, 150], [151, 153], [154, 155], [156, 164], [165, 166], [167, 174], [175, 182], [183, 185], [186, 191], [192, 195], [196, 200], [201, 208], [209, 212], [212, 213]]}
{"doc_key": "ai-train-71", "ner": [[0, 0, "programlang"], [6, 10, "organisation"], [12, 12, "organisation"], [19, 19, "location"], [21, 21, "location"], [24, 25, "researcher"], [27, 28, "researcher"], [30, 31, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 0, 24, 25, "artifact", "", true, false], [0, 0, 27, 28, "artifact", "", true, false], [0, 0, 30, 31, "artifact", "", true, false], [12, 12, 6, 10, "named", "", false, false], [12, 12, 19, 19, "physical", "", false, false], [19, 19, 21, 21, "physical", "", false, false], [24, 25, 6, 10, "role", "", false, false], [27, 28, 6, 10, "role", "", false, false], [30, 31, 6, 10, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Logo", "was", "created", "in", "1967", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "research", "firm", "in", "Cambridge", ",", "Massachusetts", ",", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Logo was created in 1967 at Bolt, Beranek and Newman (BBN), a research firm in Cambridge, Massachusetts, by Wally Feurzeig, Cynthia Solomon and Seymour Papert.", "token2charspan": [[0, 4], [5, 8], [9, 16], [17, 19], [20, 24], [25, 27], [28, 32], [32, 33], [34, 41], [42, 45], [46, 52], [53, 54], [54, 57], [57, 58], [58, 59], [60, 61], [62, 70], [71, 75], [76, 78], [79, 88], [88, 89], [90, 103], [103, 104], [105, 107], [108, 113], [114, 122], [122, 123], [124, 131], [132, 139], [140, 143], [144, 151], [152, 158], [158, 159]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [17, 18, "field"], [22, 23, "algorithm"], [26, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 17, 18, "compare", "", false, false], [22, 23, 17, 18, "part-of", "", false, false], [26, 27, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "contrasted", "with", "conventional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "in", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and can be contrasted with conventional deep learning techniques that use gradient descent in a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 86], [87, 89], [90, 100], [101, 105], [106, 118], [119, 123], [124, 132], [133, 143], [144, 148], [149, 152], [153, 161], [162, 169], [170, 172], [173, 174], [175, 181], [182, 189], [190, 194], [195, 196], [197, 202], [203, 211], [211, 212]]}
{"doc_key": "ai-train-73", "ner": [[3, 4, "algorithm"], [57, 58, "metrics"], [56, 60, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[56, 60, 57, 58, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "use", "least", "squares", "to", "fit", "a", "function", "in", "the", "form", "of", "a", "hyperplane", "\u0177", "=", "\u03b1", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", ",", "we", "could", "then", "evaluate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we use least squares to fit a function in the form of a hyperplane \u0177 = \u03b1 + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264n / sub, we could then evaluate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 15], [16, 23], [24, 26], [27, 30], [31, 32], [33, 41], [42, 44], [45, 48], [49, 53], [54, 56], [57, 58], [59, 69], [70, 71], [72, 73], [74, 75], [76, 77], [78, 79], [80, 84], [85, 86], [87, 90], [91, 92], [93, 95], [96, 99], [100, 104], [105, 106], [106, 107], [108, 111], [112, 113], [114, 115], [116, 119], [119, 120], [121, 122], [123, 126], [127, 128], [129, 130], [131, 134], [134, 135], [136, 139], [140, 141], [142, 143], [144, 145], [146, 148], [149, 150], [151, 154], [154, 155], [156, 158], [159, 164], [165, 169], [170, 178], [179, 182], [183, 186], [187, 192], [193, 196], [197, 201], [202, 209], [210, 215], [216, 217], [217, 220], [220, 221], [221, 222]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 32, "country"], [34, 34, "country"], [36, 36, "country"], [38, 38, "country"], [40, 52, "country"], [43, 54, "country"], [45, 56, "country"], [47, 58, "country"], [49, 60, "country"], [63, 64, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "facilities", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international facilities in Australia, Brazil, Canada, China, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 40], [41, 43], [44, 53], [53, 54], [55, 61], [61, 62], [63, 69], [69, 70], [71, 76], [76, 77], [78, 83], [83, 84], [85, 92], [92, 93], [94, 99], [99, 100], [101, 106], [106, 107], [108, 113], [113, 114], [115, 120], [120, 121], [122, 131], [131, 132], [133, 139], [139, 140], [141, 149], [149, 150], [151, 159], [159, 160], [161, 172], [172, 173], [174, 180], [180, 181], [182, 191], [191, 192], [193, 198], [199, 205], [205, 206], [207, 212], [212, 213], [214, 220], [220, 221], [222, 230], [230, 231], [232, 238], [238, 239], [240, 245], [246, 252], [252, 253], [254, 259], [259, 260], [261, 267], [267, 268], [269, 277], [277, 278], [279, 285], [286, 289], [290, 293], [294, 300], [301, 308], [308, 309]]}
{"doc_key": "ai-train-75", "ner": [[3, 4, "misc"], [5, 8, "field"], [13, 13, "organisation"], [16, 20, "university"], [26, 28, "organisation"], [30, 37, "university"], [41, 42, "university"], [44, 45, "university"], [48, 50, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 4, 5, 8, "topic", "", false, false], [3, 4, 13, 13, "origin", "", false, false], [3, 4, 16, 20, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "(", "2000", ")", "from", "Inria", "and", "the", "Sophia", "Antipolis", "University", "of", "Nice", "and", "has", "held", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "as", "well", "as", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering (2000) from Inria and the Sophia Antipolis University of Nice and has held positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, as well as visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 86], [87, 96], [97, 107], [108, 110], [111, 115], [116, 119], [120, 123], [124, 128], [129, 138], [139, 141], [142, 149], [150, 159], [160, 170], [170, 171], [172, 177], [178, 181], [182, 187], [188, 197], [197, 198], [199, 201], [202, 206], [207, 209], [210, 218], [219, 228], [229, 231], [232, 239], [240, 250], [250, 251], [252, 256], [257, 267], [268, 271], [272, 275], [276, 286], [287, 289], [290, 297], [297, 298]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [10, 10, "researcher"], [14, 15, "product"], [18, 19, "country"], [21, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 7, 8, "role", "licensing_patent_to", false, false], [10, 10, 18, 19, "physical", "", false, false], [21, 24, 10, 10, "artifact", "", false, false], [21, 24, 14, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Licensing", "the", "original", "patent", "granted", "to", "inventor", "George", "Devol", ",", "Engelberger", "developed", "the", "first", "industrial", "robot", "in", "the", "United", "States", ",", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Licensing the original patent granted to inventor George Devol, Engelberger developed the first industrial robot in the United States, Unimate, in the 1950s.", "token2charspan": [[0, 9], [10, 13], [14, 22], [23, 29], [30, 37], [38, 40], [41, 49], [50, 56], [57, 62], [62, 63], [64, 75], [76, 85], [86, 89], [90, 95], [96, 106], [107, 112], [113, 115], [116, 119], [120, 126], [127, 133], [133, 134], [135, 142], [142, 143], [144, 146], [147, 150], [151, 156], [156, 157]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[4, 4, "programlang"], [7, 7, "programlang"], [15, 15, "programlang"], [18, 18, "programlang"], [28, 28, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[4, 4, 15, 15, "named", "", false, false], [7, 7, 4, 4, "origin", "descendant_of", false, false], [7, 7, 18, 18, "general-affiliation", "", false, false], [7, 7, 28, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "descendants", "of", "the", "CLIPS", "language", "include", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "rewritten", "in", "Java", ",", "later", "developed", "in", "a", "different", "direction", ")", ",", "JESS", "was", "originally", "inspired"], "sentence-detokenized": "The descendants of the CLIPS language include Jess (the rule-based part of CLIPS rewritten in Java, later developed in a different direction), JESS was originally inspired", "token2charspan": [[0, 3], [4, 15], [16, 18], [19, 22], [23, 28], [29, 37], [38, 45], [46, 50], [51, 52], [52, 55], [56, 60], [60, 61], [61, 66], [67, 71], [72, 74], [75, 80], [81, 90], [91, 93], [94, 98], [98, 99], [100, 105], [106, 115], [116, 118], [119, 120], [121, 130], [131, 140], [140, 141], [141, 142], [143, 147], [148, 151], [152, 162], [163, 171]]}
{"doc_key": "ai-train-79", "ner": [[5, 5, "product"], [10, 12, "product"], [15, 16, "organisation"], [20, 21, "product"], [42, 44, "product"], [46, 48, "product"], [66, 67, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[10, 12, 5, 5, "type-of", "", false, false], [15, 16, 10, 12, "usage", "", false, false], [20, 21, 15, 16, "artifact", "", false, false], [42, 44, 15, 16, "origin", "", true, false], [42, 44, 66, 67, "related-to", "", true, false], [46, 48, 15, 16, "origin", "", true, false], [46, 48, 66, 67, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["He", "also", "created", "flexible", "intelligent", "AGV", "applications", ",", "designing", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "self", "-", "driving", "vehicle", ")", ",", "which", "is", "used", "for", "complex", "pick", "and", "place", "operations", ",", "in", "combination", "with", "gantry", "crane", "systems", "and", "industrial", "robotic", "arms", ",", "used", "in", "first", "-", "class", "automotive", "supply", "factories", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "He also created flexible intelligent AGV applications, designing the Motivity control system used by RMT Robotics to develop the ADAM iAGV (self-driving vehicle), which is used for complex pick and place operations, in combination with gantry crane systems and industrial robotic arms, used in first-class automotive supply factories to move products from process to process in non-linear layouts.", "token2charspan": [[0, 2], [3, 7], [8, 15], [16, 24], [25, 36], [37, 40], [41, 53], [53, 54], [55, 64], [65, 68], [69, 77], [78, 85], [86, 92], [93, 97], [98, 100], [101, 104], [105, 113], [114, 116], [117, 124], [125, 128], [129, 133], [134, 138], [139, 140], [140, 144], [144, 145], [145, 152], [153, 160], [160, 161], [161, 162], [163, 168], [169, 171], [172, 176], [177, 180], [181, 188], [189, 193], [194, 197], [198, 203], [204, 214], [214, 215], [216, 218], [219, 230], [231, 235], [236, 242], [243, 248], [249, 256], [257, 260], [261, 271], [272, 279], [280, 284], [284, 285], [286, 290], [291, 293], [294, 299], [299, 300], [300, 305], [306, 316], [317, 323], [324, 333], [334, 336], [337, 341], [342, 350], [351, 355], [356, 363], [364, 366], [367, 374], [375, 377], [378, 388], [389, 396], [396, 397]]}
{"doc_key": "ai-train-80", "ner": [[7, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "b", "parameters", "are", "usually", "estimated", "with", "maximum", "likelihood", "."], "sentence-detokenized": "The b parameters are usually estimated with maximum likelihood.", "token2charspan": [[0, 3], [4, 5], [6, 16], [17, 20], [21, 28], [29, 38], [39, 43], [44, 51], [52, 62], [62, 63]]}
{"doc_key": "ai-train-81", "ner": [[0, 5, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 0, 5, "part-of", "", false, false], [8, 8, 0, 5, "part-of", "", false, false], [10, 10, 0, 5, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", ",", "such", "as", "accuracy", "and", "recall", "or", "DCG", ",", "are", "useful", "for", "evaluating", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics, such as accuracy and recall or DCG, are useful for evaluating the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [29, 30], [31, 35], [36, 38], [39, 47], [48, 51], [52, 58], [59, 61], [62, 65], [65, 66], [67, 70], [71, 77], [78, 81], [82, 92], [93, 96], [97, 104], [105, 107], [108, 109], [110, 124], [125, 131], [131, 132]]}
{"doc_key": "ai-train-82", "ner": [[6, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "includes", "hundreds", "of", "industrial", "robots", "working", "in", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "workers", "."], "sentence-detokenized": "A typical factory includes hundreds of industrial robots working in fully automated production lines, with one robot for every ten workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 26], [27, 35], [36, 38], [39, 49], [50, 56], [57, 64], [65, 67], [68, 73], [74, 83], [84, 94], [95, 100], [100, 101], [102, 106], [107, 110], [111, 116], [117, 120], [121, 126], [127, 130], [131, 138], [138, 139]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [13, 14, "field"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [29, 30, "task"], [32, 33, "task"], [36, 37, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[13, 14, 5, 5, "usage", "", false, true], [20, 21, 13, 14, "part-of", "", false, false], [23, 24, 13, 14, "part-of", "", false, false], [26, 27, 13, 14, "part-of", "", false, false], [29, 30, 13, 14, "part-of", "", false, false], [32, 33, 13, 14, "part-of", "", false, false], [36, 37, 13, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", ",", "such", "as", ":", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "development", ",", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a variety of image processing applications, such as: image segmentation, feature generation, face extraction, motion detection, region development, and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 55], [56, 58], [59, 64], [65, 75], [76, 88], [88, 89], [90, 94], [95, 97], [97, 98], [99, 104], [105, 117], [117, 118], [119, 126], [127, 137], [137, 138], [139, 143], [144, 154], [154, 155], [156, 162], [163, 172], [172, 173], [174, 180], [181, 192], [192, 193], [194, 197], [198, 203], [204, 213], [213, 214]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 17, "field"], [21, 23, "misc"], [27, 33, "conference"], [30, 35, "conference"], [39, 41, "misc"], [44, 48, "conference"], [49, 50, "conference"], [53, 56, "conference"], [58, 58, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 16, 17, "related-to", "contributes_to", false, false], [0, 0, 21, 23, "win-defeat", "", false, false], [0, 0, 39, 41, "win-defeat", "", false, false], [21, 23, 27, 33, "temporal", "", false, false], [30, 35, 27, 33, "named", "", false, false], [39, 41, 44, 48, "temporal", "", false, false], [39, 41, 53, 56, "temporal", "", false, false], [49, 50, 44, 48, "named", "", false, false], [58, 58, 53, 56, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "best", "paper", "award", "at", "the", "2012", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "and", "the", "best", "referee", "award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision and won the best paper award at the 2012 International Conference on Non-Photorealistic Rendering and Animation (NPAR) and the best referee award at the Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 151], [152, 165], [166, 176], [177, 179], [180, 198], [199, 208], [209, 212], [213, 222], [223, 224], [224, 228], [228, 229], [230, 233], [234, 237], [238, 242], [243, 250], [251, 256], [257, 259], [260, 263], [264, 269], [270, 280], [281, 283], [284, 292], [293, 299], [300, 304], [305, 309], [310, 313], [314, 327], [328, 338], [339, 341], [342, 350], [351, 357], [358, 359], [359, 363], [363, 364], [365, 369], [369, 370]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 13, "researcher"], [15, 18, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 2, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [15, 18, 0, 0, "usage", "", false, false], [15, 18, 13, 13, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "an", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is an ontology language used by Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 67], [68, 76], [77, 81], [82, 84], [85, 89], [90, 95], [95, 97], [98, 101], [102, 112], [113, 120], [120, 121]]}
{"doc_key": "ai-train-86", "ner": [[3, 4, "task"], [7, 9, "metrics"], [16, 19, "metrics"], [21, 28, "metrics"], [37, 39, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 9, 3, 4, "part-of", "", false, false], [16, 19, 7, 9, "named", "", false, false], [21, 28, 7, 9, "named", "", false, false], [37, 39, 7, 9, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", ",", "in", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "of", "the", "squared", "deviations", "of", "predictions", "from", "the", "TRUE", "values", ",", "in", "an", "out", "-", "of", "-", "sample", "test", "space", ",", "produced", "by", "a", "model", "estimated", "in", "a", "particular", "sample", "space", "."], "sentence-detokenized": "Also, in regression analysis, the mean squared error, often referred to as the mean squared prediction error or out-of-sample mean squared error, can refer to the average of the squared deviations of predictions from the TRUE values, in an out-of-sample test space, produced by a model estimated in a particular sample space.", "token2charspan": [[0, 4], [4, 5], [6, 8], [9, 19], [20, 28], [28, 29], [30, 33], [34, 38], [39, 46], [47, 52], [52, 53], [54, 59], [60, 68], [69, 71], [72, 74], [75, 78], [79, 83], [84, 91], [92, 102], [103, 108], [109, 111], [112, 115], [115, 116], [116, 118], [118, 119], [119, 125], [126, 130], [131, 138], [139, 144], [144, 145], [146, 149], [150, 155], [156, 158], [159, 162], [163, 170], [171, 173], [174, 177], [178, 185], [186, 196], [197, 199], [200, 211], [212, 216], [217, 220], [221, 225], [226, 232], [232, 233], [234, 236], [237, 239], [240, 243], [243, 244], [244, 246], [246, 247], [247, 253], [254, 258], [259, 264], [264, 265], [266, 274], [275, 277], [278, 279], [280, 285], [286, 295], [296, 298], [299, 300], [301, 311], [312, 318], [319, 324], [324, 325]]}
{"doc_key": "ai-train-87", "ner": [[6, 8, "algorithm"], [10, 11, "algorithm"], [20, 23, "algorithm"], [34, 35, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 8, 10, 11, "compare", "", false, false], [6, 8, 20, 23, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "terms", "of", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "have", "comparable", "performance", ",", "with", "the", "C", "-", "HOG", "descriptors", "maintaining", "a", "slight", "advantage", "in", "detection", "failure", "rate", "at", "constant", "FALSE", "positive", "rates", "on", "both", "datasets", "."], "sentence-detokenized": "In terms of results, the C-HOG and R-HOG block descriptors have comparable performance, with the C-HOG descriptors maintaining a slight advantage in detection failure rate at constant FALSE positive rates on both datasets.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 19], [19, 20], [21, 24], [25, 26], [26, 27], [27, 30], [31, 34], [35, 37], [37, 40], [41, 46], [47, 58], [59, 63], [64, 74], [75, 86], [86, 87], [88, 92], [93, 96], [97, 98], [98, 99], [99, 102], [103, 114], [115, 126], [127, 128], [129, 135], [136, 145], [146, 148], [149, 158], [159, 166], [167, 171], [172, 174], [175, 183], [184, 189], [190, 198], [199, 204], [205, 207], [208, 212], [213, 221], [221, 222]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [18, 19, "algorithm"], [21, 23, "algorithm"], [25, 27, "algorithm"], [29, 30, "misc"], [33, 35, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 29, 30, "usage", "", false, false], [14, 15, 29, 30, "usage", "", false, false], [18, 19, 29, 30, "usage", "", false, false], [21, 23, 29, 30, "usage", "", false, false], [25, 27, 29, 30, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenmodes", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "with", "the", "Fisherface", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", ",", "and", "dynamic", "link", "matching", "with", "neural", "stimuli", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenmodes, linear discriminant analysis, elastic matching with the Fisherface algorithm, hidden Markov model, multilinear subspace learning using tensor representation, and dynamic link matching with neural stimuli.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 137], [138, 141], [142, 152], [153, 162], [162, 163], [164, 170], [171, 177], [178, 183], [183, 184], [185, 196], [197, 205], [206, 214], [215, 220], [221, 227], [228, 242], [242, 243], [244, 247], [248, 255], [256, 260], [261, 269], [270, 274], [275, 281], [282, 289], [289, 290]]}
{"doc_key": "ai-train-89", "ner": [[3, 7, "misc"], [18, 20, "location"], [37, 39, "location"], [54, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[18, 20, 3, 7, "temporal", "", false, false], [37, 39, 3, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "of", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "can", "now", "be", "restricted", "from", "screening", "at", "the", "Scotiabank", "Theatre", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "shown", "elsewhere", "(", "such", "as", "at", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "they", "are", "distributed", "by", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "As of the 2019 Toronto International Film Festival, films can now be restricted from screening at the Scotiabank Theatre Toronto - one of the festival's main venues - and shown elsewhere (such as at TIFF Bell Lightbox and other local cinemas) if they are distributed by a service such as Netflix.", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 14], [15, 22], [23, 36], [37, 41], [42, 50], [50, 51], [52, 57], [58, 61], [62, 65], [66, 68], [69, 79], [80, 84], [85, 94], [95, 97], [98, 101], [102, 112], [113, 120], [121, 128], [129, 130], [131, 134], [135, 137], [138, 141], [142, 150], [150, 152], [153, 157], [158, 164], [165, 166], [167, 170], [171, 176], [177, 186], [187, 188], [188, 192], [193, 195], [196, 198], [199, 203], [204, 208], [209, 217], [218, 221], [222, 227], [228, 233], [234, 241], [241, 242], [243, 245], [246, 250], [251, 254], [255, 266], [267, 269], [270, 271], [272, 279], [280, 284], [285, 287], [288, 295], [295, 296]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [2, 2, "researcher"], [5, 6, "organisation"], [12, 13, "researcher"], [23, 27, "product"], [41, 43, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6], "relations": [[0, 0, 5, 6, "related-to", "purchases", false, false], [2, 2, 12, 13, "named", "same", false, false], [5, 6, 2, 2, "origin", "founded_by", false, false], [23, 27, 0, 0, "artifact", "", false, false]], "relations_mapping_to_source": [0, 1, 3, 4], "sentence": ["Unimation", "bought", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "producing", "the", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "a", "new", "model", "of", "robotic", "arm", ",", "and", "using", "Scheinman", "'s", "pioneering", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation bought Victor Scheinman's Vicarm Inc. in 1977, and with Scheinman's help, the company created and began producing the Programmable Universal Machine for Assembly, a new model of robotic arm, and using Scheinman's pioneering VAL programming language.", "token2charspan": [[0, 9], [10, 16], [17, 23], [24, 33], [33, 35], [36, 42], [43, 47], [48, 50], [51, 55], [55, 56], [57, 60], [61, 65], [66, 75], [75, 77], [78, 82], [82, 83], [84, 87], [88, 95], [96, 103], [104, 107], [108, 113], [114, 123], [124, 127], [128, 140], [141, 150], [151, 158], [159, 162], [163, 171], [171, 172], [173, 174], [175, 178], [179, 184], [185, 187], [188, 195], [196, 199], [199, 200], [201, 204], [205, 210], [211, 220], [220, 222], [223, 233], [234, 237], [238, 249], [250, 258], [258, 259]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [10, 13, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 10, 13, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[2, 2, "metrics"], [12, 13, "product"], [19, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[2, 2, 12, 13, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "2004", "SSIM", "paper", "has", "been", "cited", "over", "20,000", "times", "according", "to", "Google", "Scholar", ",", "and", "also", "received", "the", "IEEE", "Signal", "Processing", "Society", "'s", "Sustained", "Impact", "Award", "for", "2016", ",", "indicative", "of", "a", "paper", "with", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "The 2004 SSIM paper has been cited over 20,000 times according to Google Scholar, and also received the IEEE Signal Processing Society's Sustained Impact Award for 2016, indicative of a paper with unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 3], [4, 8], [9, 13], [14, 19], [20, 23], [24, 28], [29, 34], [35, 39], [40, 46], [47, 52], [53, 62], [63, 65], [66, 72], [73, 80], [80, 81], [82, 85], [86, 90], [91, 99], [100, 103], [104, 108], [109, 115], [116, 126], [127, 134], [134, 136], [137, 146], [147, 153], [154, 159], [160, 163], [164, 168], [168, 169], [170, 180], [181, 183], [184, 185], [186, 191], [192, 196], [197, 206], [207, 211], [212, 218], [219, 222], [223, 225], [226, 231], [232, 234], [235, 240], [241, 246], [247, 258], [258, 259]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [23, 24, "product"], [39, 41, "product"], [44, 44, "organisation"], [45, 45, "product"], [50, 50, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 44, 44, "artifact", "", false, false], [23, 24, 0, 1, "related-to", "performs", false, false], [23, 24, 39, 41, "part-of", "", false, false], [44, 44, 50, 50, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "has", "reached", "the", "point", "of", "being", "completely", "indistinguishable", "from", "the", "voice", "of", "a", "real", "person", "with", "the", "introduction", "in", "2016", "of", "Adobe", "Voco", "voice", "editing", "and", "creation", "software", ",", "a", "prototype", "planned", "to", "be", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", "."], "sentence-detokenized": "Speech synthesis has reached the point of being completely indistinguishable from the voice of a real person with the introduction in 2016 of Adobe Voco voice editing and creation software, a prototype planned to be part of the Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google.", "token2charspan": [[0, 6], [7, 16], [17, 20], [21, 28], [29, 32], [33, 38], [39, 41], [42, 47], [48, 58], [59, 76], [77, 81], [82, 85], [86, 91], [92, 94], [95, 96], [97, 101], [102, 108], [109, 113], [114, 117], [118, 130], [131, 133], [134, 138], [139, 141], [142, 147], [148, 152], [153, 158], [159, 166], [167, 170], [171, 179], [180, 188], [188, 189], [190, 191], [192, 201], [202, 209], [210, 212], [213, 215], [216, 220], [221, 223], [224, 227], [228, 233], [234, 242], [243, 248], [248, 249], [250, 253], [254, 262], [263, 270], [270, 271], [272, 273], [274, 283], [284, 288], [289, 295], [295, 296]]}
{"doc_key": "ai-train-94", "ner": [[0, 2, "researcher"], [7, 9, "organisation"], [15, 20, "organisation"], [27, 27, "conference"], [34, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 2, 7, 9, "role", "", false, false], [0, 2, 15, 20, "role", "", false, false], [0, 2, 27, 27, "role", "", false, false], [0, 2, 34, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "member", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "and", "a", "founding", "member", "of", "the", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary member of the Neuroscience Research Program, a member of the American Academy of Arts and Sciences and a founding member of the AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [121, 124], [125, 126], [127, 135], [136, 142], [143, 145], [146, 149], [150, 154], [155, 158], [159, 160], [161, 169], [170, 176], [177, 179], [180, 183], [184, 192], [193, 202], [203, 206], [207, 212], [213, 221], [221, 222]]}
{"doc_key": "ai-train-95", "ner": [[9, 9, "task"], [8, 11, "task"], [15, 16, "task"], [23, 23, "misc"], [24, 25, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 9, 15, 16, "cause-effect", "", false, false], [8, 11, 15, 16, "cause-effect", "", false, false], [24, 25, 15, 16, "topic", "", false, false], [24, 25, 23, 23, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["During", "the", "1990s", ",", "encouraged", "by", "successes", "in", "speech", "recognition", "and", "synthesis", ",", "research", "into", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "During the 1990s, encouraged by successes in speech recognition and synthesis, research into speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 6], [7, 10], [11, 16], [16, 17], [18, 28], [29, 31], [32, 41], [42, 44], [45, 51], [52, 63], [64, 67], [68, 77], [77, 78], [79, 87], [88, 92], [93, 99], [100, 111], [112, 117], [118, 122], [123, 126], [127, 138], [139, 141], [142, 145], [146, 152], [153, 162], [163, 170], [170, 171]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [15, 16, "algorithm"], [21, 22, "algorithm"], [26, 26, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [15, 16, 3, 4, "origin", "", false, false], [15, 16, 8, 9, "origin", "", false, false], [15, 16, 11, 12, "origin", "", false, false], [15, 16, 26, 26, "part-of", "", false, false], [21, 22, 15, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisor", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "oblivion", "gate", "(", "also", "called", "the", "preservation", "gate", ")", "to", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisor J\u00fcrgen Schmidhuber and Fred Cummins introduced the oblivion gate (also called the preservation gate) to the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 35], [36, 42], [43, 54], [55, 58], [59, 63], [64, 71], [72, 82], [83, 86], [87, 95], [96, 100], [101, 102], [102, 106], [107, 113], [114, 117], [118, 130], [131, 135], [135, 136], [137, 139], [140, 143], [144, 148], [149, 161], [161, 162]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 12, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 12, 1, 3, "part-of", "", false, false], [9, 12, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "function", "sinc", "is", "usually", "defined", "as", "follows"], "sentence-detokenized": "In digital signal processing and information theory, the normalized function sinc is usually defined as follows", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 76], [77, 81], [82, 84], [85, 92], [93, 100], [101, 103], [104, 111]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [18, 21, "conference"], [24, 28, "organisation"], [30, 30, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 18, 21, "role", "", false, false], [9, 10, 24, 28, "role", "", false, false], [30, 30, 24, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 72], [72, 73], [74, 75], [76, 84], [85, 91], [92, 94], [95, 99], [100, 103], [104, 115], [116, 119], [120, 133], [134, 145], [146, 149], [150, 153], [154, 167], [168, 177], [178, 180], [181, 194], [195, 206], [207, 208], [208, 212], [212, 213], [213, 214]]}
{"doc_key": "ai-train-99", "ner": [[8, 12, "misc"], [9, 9, "misc"], [34, 35, "metrics"], [33, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[33, 37, 34, 35, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one-dimensional", "DPD", "with", "polynomial", "memory", "(", "or", "without", "memory", ")", ",", "in", "order", "to", "solve", "the", "polynomial", "coefficients", "of", "the", "digital", "pre-modulator", "and", "minimize", "the", "mean", "square", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "allows", "the", "capture", "of", "the", "nonlinear", "products", "of", "the", "order", "of", "the", "digital", "pre-modulator", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional DPD with polynomial memory (or without memory), in order to solve the polynomial coefficients of the digital pre-modulator and minimize the mean square error (MSE), the distorted output of the nonlinear system must be oversampled at a rate that allows the capture of the nonlinear products of the order of the digital pre-modulator.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 47], [48, 51], [52, 56], [57, 67], [68, 74], [75, 76], [76, 78], [79, 86], [87, 93], [93, 94], [94, 95], [96, 98], [99, 104], [105, 107], [108, 113], [114, 117], [118, 128], [129, 141], [142, 144], [145, 148], [149, 156], [157, 170], [171, 174], [175, 183], [184, 187], [188, 192], [193, 199], [200, 205], [206, 207], [207, 210], [210, 211], [211, 212], [213, 216], [217, 226], [227, 233], [234, 236], [237, 240], [241, 250], [251, 257], [258, 262], [263, 265], [266, 277], [278, 280], [281, 282], [283, 287], [288, 292], [293, 299], [300, 303], [304, 311], [312, 314], [315, 318], [319, 328], [329, 337], [338, 340], [341, 344], [345, 350], [351, 353], [354, 357], [358, 365], [366, 379], [379, 380]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [10, 10, "location"], [12, 13, "location"], [15, 16, "country"], [20, 20, "location"], [22, 22, "country"], [36, 42, "organisation"], [45, 48, "organisation"], [50, 50, "location"], [57, 59, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 10, 10, "physical", "", false, false], [0, 1, 45, 48, "physical", "", false, false], [0, 1, 57, 59, "role", "", false, false], [10, 10, 12, 13, "physical", "", false, false], [12, 13, 15, 16, "physical", "", false, false], [36, 42, 45, 48, "part-of", "", false, false], [45, 48, 50, 50, "physical", "", false, false], [57, 59, 36, 42, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "October", "5", ",", "1947", ",", "Chi\u0219in\u0103u", ",", "Moldovan", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chi\u0219in\u0103u", ",", "Moldova", ")", ")", "is", "a", "senior", "American", "researcher", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", ",", "Cambridge", ",", "and", "head", "of", "the", "Laboratory", "'s", "InfoLab", "group", "."], "sentence-detokenized": "Boris Katz, (born October 5, 1947, Chi\u0219in\u0103u, Moldovan SSR, Soviet Union, (now Chi\u0219in\u0103u, Moldova)) is a senior American researcher (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, Cambridge, and head of the Laboratory's InfoLab group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 25], [26, 27], [27, 28], [29, 33], [33, 34], [35, 43], [43, 44], [45, 53], [54, 57], [57, 58], [59, 65], [66, 71], [71, 72], [73, 74], [74, 77], [78, 86], [86, 87], [88, 95], [95, 96], [96, 97], [98, 100], [101, 102], [103, 109], [110, 118], [119, 129], [130, 131], [131, 139], [140, 149], [149, 150], [151, 153], [154, 157], [158, 161], [162, 170], [171, 178], [179, 182], [183, 193], [194, 206], [207, 217], [218, 220], [221, 224], [225, 238], [239, 248], [249, 251], [252, 262], [262, 263], [264, 273], [273, 274], [275, 278], [279, 283], [284, 286], [287, 290], [291, 301], [301, 303], [304, 311], [312, 317], [317, 318]]}
