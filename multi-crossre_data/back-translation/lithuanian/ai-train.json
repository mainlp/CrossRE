{"doc_key": "ai-train-1", "ner": [[1, 5, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [29, 30, "field"], [31, 33, "researcher"], [35, 37, "researcher"], [39, 40, "researcher"], [42, 43, "researcher"], [45, 47, "researcher"], [49, 50, "researcher"], [52, 53, "researcher"], [55, 56, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[1, 5, 13, 14, "part-of", "", false, false], [1, 5, 13, 14, "usage", "", false, false], [1, 5, 16, 17, "part-of", "", false, false], [1, 5, 16, 17, "usage", "", false, false], [1, 5, 19, 20, "part-of", "", false, false], [1, 5, 19, 20, "usage", "", false, false], [1, 5, 29, 30, "part-of", "", false, false], [1, 5, 29, 30, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "opinion", "-", "based", "recommender", "system", "approaches", "use", "a", "variety", "of", "methods", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "Multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular opinion-based recommender system approaches use a variety of methods including text mining, information retrieval, sentiment analysis (see also Multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5): e12957.", "token2charspan": [[0, 7], [8, 15], [15, 16], [16, 21], [22, 33], [34, 40], [41, 51], [52, 55], [56, 57], [58, 65], [66, 68], [69, 76], [77, 86], [87, 91], [92, 98], [98, 99], [100, 111], [112, 121], [121, 122], [123, 132], [133, 141], [142, 143], [143, 146], [147, 151], [152, 162], [163, 172], [173, 181], [181, 182], [183, 186], [187, 191], [192, 200], [201, 204], [204, 205], [206, 210], [210, 211], [212, 213], [213, 214], [215, 220], [220, 221], [222, 226], [227, 230], [230, 231], [232, 236], [237, 242], [242, 243], [244, 245], [245, 246], [247, 250], [250, 251], [252, 256], [257, 262], [262, 263], [264, 268], [269, 273], [273, 274], [275, 277], [278, 280], [280, 281], [282, 283], [283, 287], [287, 288], [288, 289], [289, 290], [291, 293], [294, 295], [295, 296], [296, 297], [297, 298], [299, 305], [305, 306]]}
{"doc_key": "ai-train-2", "ner": [[8, 8, "university"], [10, 11, "researcher"], [13, 14, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 11, 8, 8, "physical", "", false, false], [10, 11, 8, 8, "role", "", false, false], [13, 14, 8, 8, "physical", "", false, false], [13, 14, 8, 8, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "proponents", "of", "procedural", "representations", "were", "mainly", "at", "MIT", "under", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The proponents of procedural representations were mainly at MIT under Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 3], [4, 14], [15, 17], [18, 28], [29, 44], [45, 49], [50, 56], [57, 59], [60, 63], [64, 69], [70, 76], [77, 83], [84, 87], [88, 95], [96, 102], [102, 103]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 1, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["\"", "Octave", "helps", "you", "to", "numerically", "solve", "linear", "and", "non-linear", "problems", "and", "carry", "out", "other", "numerical", "experiments", "using", "a", "commonly", "used", "MATLAB", "-", "compatible", "program", "."], "sentence-detokenized": "\"Octave helps you to numerically solve linear and non-linear problems and carry out other numerical experiments using a commonly used MATLAB-compatible program.", "token2charspan": [[0, 1], [1, 7], [8, 13], [14, 17], [18, 20], [21, 32], [33, 38], [39, 45], [46, 49], [50, 60], [61, 69], [70, 73], [74, 79], [80, 83], [84, 89], [90, 99], [100, 111], [112, 117], [118, 119], [120, 128], [129, 133], [134, 140], [140, 141], [141, 151], [152, 159], [159, 160]]}
{"doc_key": "ai-train-5", "ner": [[2, 6, "algorithm"], [11, 13, "misc"], [14, 16, "researcher"], [22, 24, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[2, 6, 14, 16, "origin", "", false, false], [11, 13, 14, 16, "origin", "", false, false], [14, 16, 22, 24, "physical", "", false, false], [14, 16, 22, 24, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", ",", "as", "well", "as", "unsupervised", "methods", "developed", "by", "Geoff", "Hinton", "and", "his", "colleagues", "at", "the", "University", "of", "Toronto", ",", "can", "be", "used", "to", "train", "deep", ",", "highly", "non-linear", "neural", "architectures", ",", "{{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm, as well as unsupervised methods developed by Geoff Hinton and his colleagues at the University of Toronto, can be used to train deep, highly non-linear neural architectures, {{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [42, 43], [44, 46], [47, 51], [52, 54], [55, 67], [68, 75], [76, 85], [86, 88], [89, 94], [95, 101], [102, 105], [106, 109], [110, 120], [121, 123], [124, 127], [128, 138], [139, 141], [142, 149], [149, 150], [151, 154], [155, 157], [158, 162], [163, 165], [166, 171], [172, 176], [176, 177], [178, 184], [185, 195], [196, 202], [203, 216], [216, 217], [218, 220], [220, 224], [225, 232]]}
{"doc_key": "ai-train-6", "ner": [[3, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalent", "using", "DCG", "notation", ":"], "sentence-detokenized": "or equivalent using DCG notation:", "token2charspan": [[0, 2], [3, 13], [14, 19], [20, 23], [24, 32], [32, 33]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 10, "algorithm"], [14, 17, "algorithm"], [18, 22, "algorithm"], [23, 25, "algorithm"], [27, 31, "algorithm"], [42, 47, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 10, "type-of", "", false, false], [0, 3, 14, 17, "usage", "part-of?", true, false], [14, 17, 18, 22, "compare", "", false, false], [23, 25, 18, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "use", "competitive", "learning", "rather", "than", "error", "-correction", "learning", "(", "e.g.", "back", "-", "propagation", "with", "gradient", "descent", ")", ",", "and", "in", "that", "they", "use", "a", "neighbourhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks in that they use competitive learning rather than error-correction learning (e.g. back-propagation with gradient descent), and in that they use a neighbourhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 82], [83, 94], [95, 103], [104, 110], [111, 115], [116, 121], [121, 132], [133, 141], [142, 143], [143, 147], [148, 152], [152, 153], [153, 164], [165, 169], [170, 178], [179, 186], [186, 187], [187, 188], [189, 192], [193, 195], [196, 200], [201, 205], [206, 209], [210, 211], [212, 225], [226, 234], [235, 237], [238, 246], [247, 250], [251, 262], [263, 273], [274, 276], [277, 280], [281, 286], [287, 292], [292, 293]]}
{"doc_key": "ai-train-8", "ner": [[10, 12, "organisation"], [26, 32, "misc"], [35, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "several", "bodies", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "should", "be", "measured", "in", "the", "presence", "of", "an", "audio", "signal", ",", "which", "is", "then", "filtered", "by", "measuring", "the", "noise", "level", "used", "to", "determine", "the", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "made", "using", "blank", "media", "or", "trip", "circuits", "."], "sentence-detokenized": "Since the early 1990s, several bodies, including the Audio Engineering Society, have recommended that dynamic range should be measured in the presence of an audio signal, which is then filtered by measuring the noise level used to determine the dynamic range. This avoids questionable measurements made using blank media or trip circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 58], [59, 70], [71, 78], [78, 79], [80, 84], [85, 96], [97, 101], [102, 109], [110, 115], [116, 122], [123, 125], [126, 134], [135, 137], [138, 141], [142, 150], [151, 153], [154, 156], [157, 162], [163, 169], [169, 170], [171, 176], [177, 179], [180, 184], [185, 193], [194, 196], [197, 206], [207, 210], [211, 216], [217, 222], [223, 227], [228, 230], [231, 240], [241, 244], [245, 252], [253, 258], [258, 259], [260, 264], [265, 271], [272, 284], [285, 297], [298, 302], [303, 308], [309, 314], [315, 320], [321, 323], [324, 328], [329, 337], [337, 338]]}
{"doc_key": "ai-train-9", "ner": [[7, 9, "misc"], [14, 15, "task"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [28, 28, "task"], [26, 32, "task"], [34, 36, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[7, 9, 14, 15, "part-of", "concept_used_in", true, false], [7, 9, 17, 18, "part-of", "concept_used_in", false, false], [7, 9, 20, 21, "part-of", "concept_used_in", false, false], [7, 9, 23, 24, "part-of", "concept_used_in", false, false], [7, 9, 28, 28, "part-of", "concept_used_in", false, false], [7, 9, 26, 32, "part-of", "concept_used_in", false, false], [7, 9, 34, 36, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "methodology", "used", "to", "create", "and", "use", "own", "faces", "for", "recognition", "also", "extends", "beyond", "face", "recognition", "to", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "interpretation", "of", "sign", "language", "/", "hand", "gestures", "and", "medical", "image", "analysis", "."], "sentence-detokenized": "The methodology used to create and use own faces for recognition also extends beyond face recognition to handwriting recognition, lip reading, voice recognition, interpretation of sign language/hand gestures and medical image analysis.", "token2charspan": [[0, 3], [4, 15], [16, 20], [21, 23], [24, 30], [31, 34], [35, 38], [39, 42], [43, 48], [49, 52], [53, 64], [65, 69], [70, 77], [78, 84], [85, 89], [90, 101], [102, 104], [105, 116], [117, 128], [128, 129], [130, 133], [134, 141], [141, 142], [143, 148], [149, 160], [160, 161], [162, 176], [177, 179], [180, 184], [185, 193], [193, 194], [194, 198], [199, 207], [208, 211], [212, 219], [220, 225], [226, 234], [234, 235]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [11, 16, "organisation"], [18, 18, "organisation"], [22, 27, "organisation"], [31, 36, "organisation"], [40, 45, "organisation"], [49, 53, "organisation"], [55, 55, "organisation"], [59, 64, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[11, 16, 0, 3, "part-of", "", false, false], [18, 18, 11, 16, "named", "", false, false], [22, 27, 0, 3, "part-of", "", false, false], [31, 36, 0, 3, "part-of", "", false, false], [40, 45, 0, 3, "part-of", "", false, false], [49, 53, 0, 3, "part-of", "", false, false], [55, 55, 49, 53, "named", "", false, false], [59, 64, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "the", "umbrella", "for", "research", "coordinated", "by", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", "(", "DOE", ")", ",", "the", "US", "Department", "of", "Commerce", "(", "NIST", ")", ",", "the", "US", "Department", "of", "Defense", "(", "DoD", ")", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "(", "ONR", ")", ",", "and", "used", "by", "the", "drafters", "of", "the", "strategic", "plans", "for", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was the umbrella for research coordinated by the National Aeronautics and Space Administration (NASA), the US Department of Energy (DOE), the US Department of Commerce (NIST), the US Department of Defense (DoD), the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research (ONR), and used by the drafters of the strategic plans for deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 39], [40, 48], [49, 52], [53, 61], [62, 73], [74, 76], [77, 80], [81, 89], [90, 101], [102, 105], [106, 111], [112, 126], [127, 128], [128, 132], [132, 133], [133, 134], [135, 138], [139, 141], [142, 152], [153, 155], [156, 162], [163, 164], [164, 167], [167, 168], [168, 169], [170, 173], [174, 176], [177, 187], [188, 190], [191, 199], [200, 201], [201, 205], [205, 206], [206, 207], [208, 211], [212, 214], [215, 225], [226, 228], [229, 236], [237, 238], [238, 241], [241, 242], [242, 243], [244, 247], [248, 255], [256, 264], [265, 273], [274, 282], [283, 289], [290, 291], [291, 296], [296, 297], [298, 301], [302, 305], [306, 312], [313, 315], [316, 321], [322, 330], [331, 332], [332, 335], [335, 336], [336, 337], [338, 341], [342, 346], [347, 349], [350, 353], [354, 362], [363, 365], [366, 369], [370, 379], [380, 385], [386, 389], [390, 403], [403, 404]]}
{"doc_key": "ai-train-11", "ner": [[8, 9, "metrics"], [13, 18, "algorithm"], [0, 1, "researcher"], [21, 21, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 9, 13, 18, "part-of", "", false, false], [0, 1, 21, 21, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Ronald", "Fisher", "proposed", "a", "fast", "method", "for", "calculating", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "in", "1935", "as", "a", "complement", "to", "Bliss", "'", "work", "."], "sentence-detokenized": "Ronald Fisher proposed a fast method for calculating maximum likelihood estimates for the probit model in 1935 as a complement to Bliss' work.", "token2charspan": [[0, 6], [7, 13], [14, 22], [23, 24], [25, 29], [30, 36], [37, 40], [41, 52], [53, 60], [61, 71], [72, 81], [82, 85], [86, 89], [90, 96], [97, 102], [103, 105], [106, 110], [111, 113], [114, 115], [116, 126], [127, 129], [130, 135], [135, 136], [137, 141], [141, 142]]}
{"doc_key": "ai-train-12", "ner": [[12, 13, "product"], [16, 18, "product"], [20, 21, "organisation"], [22, 22, "product"], [30, 31, "organisation"], [32, 33, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[22, 22, 16, 18, "usage", "uses_software", false, false], [22, 22, 20, 21, "artifact", "", false, false], [22, 22, 32, 33, "named", "", false, false], [32, 33, 30, 31, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "of", "these", "applications", "are", "available", "on", "the", "Internet", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "used", "in", "AltaVista", "'s", "BabelFish", "application", "(", "since", "9", "May", "2008", ",", "Yahoo", "'s", "Babelfish", "application", ")", "."], "sentence-detokenized": "Several of these applications are available on the Internet, such as Google Translate and the SYSTRAN system used in AltaVista's BabelFish application (since 9 May 2008, Yahoo's Babelfish application).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 29], [30, 33], [34, 43], [44, 46], [47, 50], [51, 59], [59, 60], [61, 65], [66, 68], [69, 75], [76, 85], [86, 89], [90, 93], [94, 101], [102, 108], [109, 113], [114, 116], [117, 126], [126, 128], [129, 138], [139, 150], [151, 152], [152, 157], [158, 159], [160, 163], [164, 168], [168, 169], [170, 175], [175, 177], [178, 187], [188, 199], [199, 200], [200, 201]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [20, 22, "field"], [26, 27, "misc"], [31, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 20, 22, "related-to", "", true, false], [3, 3, 26, 27, "related-to", "", true, false], [3, 3, 31, 32, "related-to", "", true, false], [7, 8, 20, 22, "related-to", "", true, false], [7, 8, 26, 27, "related-to", "", true, false], [7, 8, 31, 32, "related-to", "", true, false], [10, 11, 20, 22, "related-to", "", true, false], [10, 11, 26, 27, "related-to", "", true, false], [10, 11, 31, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hutter", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hutter, together with J\u00fcrgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 15], [15, 16], [17, 25], [26, 30], [31, 37], [38, 49], [50, 53], [54, 59], [60, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[7, 7, "metrics"], [9, 15, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[7, 7, 9, 15, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "commonly", "used", "method", "is", "the", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "method", "."], "sentence-detokenized": "The most commonly used method is the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) method.", "token2charspan": [[0, 3], [4, 8], [9, 17], [18, 22], [23, 29], [30, 32], [33, 36], [37, 42], [43, 44], [44, 50], [50, 51], [51, 59], [60, 70], [71, 74], [75, 82], [83, 93], [93, 94], [95, 101], [101, 102]]}
{"doc_key": "ai-train-15", "ner": [[0, 1, "product"], [14, 14, "programlang"], [16, 16, "programlang"], [19, 20, "researcher"], [22, 25, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 14, 14, "related-to", "", false, false], [0, 1, 16, 16, "related-to", "", false, false], [19, 20, 22, 25, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["\"", "RapidMiner", "provides", "learning", "schemes", ",", "models", "and", "algorithms", "that", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013"], "sentence-detokenized": "\"RapidMiner provides learning schemes, models and algorithms that can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013", "token2charspan": [[0, 1], [1, 11], [12, 20], [21, 29], [30, 37], [37, 38], [39, 45], [46, 49], [50, 60], [61, 65], [66, 69], [70, 72], [73, 81], [82, 87], [88, 89], [90, 93], [94, 100], [101, 108], [108, 109], [110, 115], [116, 122], [122, 123], [124, 129], [130, 138], [138, 139], [140, 142], [143, 151], [152, 156]]}
{"doc_key": "ai-train-16", "ner": [[11, 15, "product"]], "ner_mapping_to_source": [5], "relations": [], "relations_mapping_to_source": [], "sentence": ["However", ",", "the", "most", "recent", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "whose", "development", "started", "in", "1997", ",", "is", "now", "used", "in", "a", "wide", "range", "of", "applications", ",", "especially", "for", "educational", "and", "research", "purposes", "."], "sentence-detokenized": "However, the most recent fully Java-based version (Weka 3), whose development started in 1997, is now used in a wide range of applications, especially for educational and research purposes.", "token2charspan": [[0, 7], [7, 8], [9, 12], [13, 17], [18, 24], [25, 30], [31, 35], [35, 36], [36, 41], [42, 49], [50, 51], [51, 55], [56, 57], [57, 58], [58, 59], [60, 65], [66, 77], [78, 85], [86, 88], [89, 93], [93, 94], [95, 97], [98, 101], [102, 106], [107, 109], [110, 111], [112, 116], [117, 122], [123, 125], [126, 138], [138, 139], [140, 150], [151, 154], [155, 166], [167, 170], [171, 179], [180, 188], [188, 189]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [14, 19, "misc"], [20, 23, "misc"], [25, 32, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 19, 0, 0, "topic", "", false, false], [14, 19, 20, 23, "win-defeat", "", false, false], [20, 23, 25, 32, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "received", "much", "recognition", ",", "and", "his", "paper", "\"", "Heuristics", "\"", ":", "was", "awarded", "the", "best", "paper", "award", "at", "the", "1982", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "conference", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and received much recognition, and his paper \"Heuristics\": was awarded the best paper award at the 1982 Association for the Advancement of Artificial Intelligence conference.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 54], [55, 59], [60, 71], [71, 72], [73, 76], [77, 80], [81, 86], [87, 88], [88, 98], [98, 99], [99, 100], [101, 104], [105, 112], [113, 116], [117, 121], [122, 127], [128, 133], [134, 136], [137, 140], [141, 145], [146, 157], [158, 161], [162, 165], [166, 177], [178, 180], [181, 191], [192, 204], [205, 215], [215, 216]]}
{"doc_key": "ai-train-18", "ner": [[8, 14, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["To", "account", "for", "multiple", "entities", ",", "a", "separate", "hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "To account for multiple entities, a separate hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 10], [11, 14], [15, 23], [24, 32], [32, 33], [34, 35], [36, 44], [45, 50], [51, 55], [56, 58], [59, 69], [70, 73], [74, 78], [79, 86], [86, 87]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 14, "product"], [16, 17, "product"], [19, 21, "product"], [23, 25, "product"], [27, 28, "product"], [37, 43, "product"], [44, 45, "product"], [47, 48, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 27, 28, "type-of", "", false, false], [12, 14, 27, 28, "type-of", "", false, false], [16, 17, 27, 28, "type-of", "", false, false], [19, 21, 27, 28, "type-of", "", false, false], [23, 25, 27, 28, "type-of", "", false, false], [44, 45, 37, 43, "type-of", "", false, false], [47, 48, 37, 43, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "chat", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "'s", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "'s", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "Far", "Field", "voice", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of chat assistants such as Apple's Siri, Amazon's Alexa, Google Assistant, Microsoft's Cortana and Samsung's Bixby, voice portals can now be accessed via mobile devices and Far Field voice smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 23], [24, 34], [35, 39], [40, 42], [43, 48], [48, 50], [51, 55], [55, 56], [57, 63], [63, 65], [66, 71], [71, 72], [73, 79], [80, 89], [89, 90], [91, 100], [100, 102], [103, 110], [111, 114], [115, 122], [122, 124], [125, 130], [130, 131], [132, 137], [138, 145], [146, 149], [150, 153], [154, 156], [157, 165], [166, 169], [170, 176], [177, 184], [185, 188], [189, 192], [193, 198], [199, 204], [205, 210], [211, 219], [220, 224], [225, 227], [228, 234], [235, 239], [240, 243], [244, 250], [251, 255], [255, 256]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 14, "algorithm"], [16, 16, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "type-of", "", false, false], [9, 11, 2, 3, "type-of", "", false, false], [13, 14, 2, 3, "type-of", "", false, false], [16, 16, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", ":", "Naive", "Bayes", "classifier", ",", "support", "vector", "machine", ",", "Gaussian", "mixtures", "and", "network", "."], "sentence-detokenized": "Examples of supervised learning: Naive Bayes classifier, support vector machine, Gaussian mixtures and network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [31, 32], [33, 38], [39, 44], [45, 55], [55, 56], [57, 64], [65, 71], [72, 79], [79, 80], [81, 89], [90, 98], [99, 102], [103, 110], [110, 111]]}
{"doc_key": "ai-train-21", "ner": [[0, 2, "algorithm"], [24, 26, "algorithm"], [27, 27, "task"], [29, 30, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 24, 26, "part-of", "", true, false], [29, 30, 27, 27, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "OSD", "algorithm", "can", "be", "used", "to", "determine", "mathematical", "O", "(", "\\", "sqrt", "{", "T", "})", "/mathematical", "regret", "bounds", "for", "an", "online", "version", "of", "support", "vector", "machine", "classification", "using", "hinge", "loss", "mathematical", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "The OSD algorithm can be used to determine mathematical O(\\ sqrt {T})/mathematical regret bounds for an online version of support vector machine classification using hinge loss mathematical v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\}/math", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 24], [25, 29], [30, 32], [33, 42], [43, 55], [56, 57], [57, 58], [58, 59], [60, 64], [65, 66], [66, 67], [67, 69], [69, 82], [83, 89], [90, 96], [97, 100], [101, 103], [104, 110], [111, 118], [119, 121], [122, 129], [130, 136], [137, 144], [145, 159], [160, 165], [166, 171], [172, 176], [177, 189], [190, 191], [192, 194], [195, 196], [196, 197], [197, 198], [199, 201], [202, 205], [205, 206], [207, 208], [208, 209], [209, 210], [211, 212], [213, 214], [215, 216], [217, 219], [220, 221], [221, 222], [222, 223], [224, 228], [229, 230], [231, 233], [233, 234], [234, 236], [236, 237], [237, 241]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 27, "task"], [26, 26, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "fusion", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "wildlife", "identification", "and", "moving", "matching", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image fusion, 3D modelling, gesture recognition, video tracking, individual wildlife identification and moving matching.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 85], [85, 86], [87, 89], [90, 99], [99, 100], [101, 108], [109, 120], [120, 121], [122, 127], [128, 136], [136, 137], [138, 148], [149, 157], [158, 172], [173, 176], [177, 183], [184, 192], [192, 193]]}
{"doc_key": "ai-train-23", "ner": [[0, 1, "task"], [17, 18, "university"], [20, 22, "university"], [24, 25, "university"], [27, 28, "university"], [31, 38, "university"], [39, 41, "university"], [44, 46, "university"], [48, 49, "university"], [52, 57, "university"], [59, 59, "university"], [64, 68, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[0, 1, 17, 18, "related-to", "", true, false], [0, 1, 20, 22, "related-to", "", true, false], [0, 1, 24, 25, "related-to", "", true, false], [0, 1, 27, 28, "related-to", "", true, false], [0, 1, 31, 38, "related-to", "", true, false], [0, 1, 39, 41, "related-to", "", true, false], [0, 1, 44, 46, "related-to", "", true, false], [0, 1, 48, 49, "related-to", "", true, false], [0, 1, 52, 57, "related-to", "", true, false], [0, 1, 59, 59, "related-to", "", true, false], [0, 1, 64, 68, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Posture", "assessment", "is", "being", "researched", "by", "a", "wide", "range", "of", "groups", "and", "companies", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "the", "University", "of", "California", "at", "San", "Diego", ",", "the", "University", "of", "Toronto", ",", "the", "Paris", "Central", "School", ",", "ETH", "Zurich", ",", "the", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", ",", "and", "the", "University", "of", "California", "at", "Irvine", "."], "sentence-detokenized": "Posture assessment is being researched by a wide range of groups and companies, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, the University of California at San Diego, the University of Toronto, the Paris Central School, ETH Zurich, the National University of Science and Technology (NUST), and the University of California at Irvine.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 27], [28, 38], [39, 41], [42, 43], [44, 48], [49, 54], [55, 57], [58, 64], [65, 68], [69, 78], [78, 79], [80, 89], [90, 96], [97, 99], [100, 105], [106, 116], [116, 117], [118, 126], [127, 133], [134, 144], [144, 145], [146, 149], [150, 162], [162, 163], [164, 172], [173, 183], [183, 184], [185, 188], [189, 199], [200, 202], [203, 213], [214, 216], [217, 220], [221, 226], [226, 227], [228, 231], [232, 242], [243, 245], [246, 253], [253, 254], [255, 258], [259, 264], [265, 272], [273, 279], [279, 280], [281, 284], [285, 291], [291, 292], [293, 296], [297, 305], [306, 316], [317, 319], [320, 327], [328, 331], [332, 342], [343, 344], [344, 348], [348, 349], [349, 350], [351, 354], [355, 358], [359, 369], [370, 372], [373, 383], [384, 386], [387, 393], [393, 394]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "predict", "K", "independent", "probability", "values", "in", "maths", "0,1", "/", "maths", "."], "sentence-detokenized": "The sigmoid function Cross entropy loss is used to predict K independent probability values in maths 0,1 / maths.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 26], [27, 34], [35, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 94], [95, 100], [101, 104], [105, 106], [107, 112], [112, 113]]}
{"doc_key": "ai-train-25", "ner": [[10, 12, "misc"], [13, 14, "field"], [16, 18, "field"], [19, 22, "university"], [23, 25, "country"], [28, 32, "misc"], [33, 37, "university"], [38, 38, "country"], [4, 4, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[10, 12, 13, 14, "topic", "", false, false], [10, 12, 16, 18, "topic", "", false, false], [10, 12, 19, 22, "physical", "", true, false], [19, 22, 23, 25, "physical", "", false, false], [28, 32, 33, 37, "physical", "", true, false], [33, 37, 38, 38, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Prior", "to", "becoming", "a", "Cambridge", "Professor", ",", "he", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Computer", "Science", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "."], "sentence-detokenized": "Prior to becoming a Cambridge Professor, he held the Johann Bernoulli Chair in Mathematics and Computer Science at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan.", "token2charspan": [[0, 5], [6, 8], [9, 17], [18, 19], [20, 29], [30, 39], [39, 40], [41, 43], [44, 48], [49, 52], [53, 59], [60, 69], [70, 75], [76, 78], [79, 90], [91, 94], [95, 103], [104, 111], [112, 114], [115, 118], [119, 129], [130, 132], [133, 142], [143, 145], [146, 149], [150, 161], [162, 165], [166, 169], [170, 177], [178, 185], [186, 191], [192, 194], [195, 198], [199, 204], [205, 214], [215, 217], [218, 228], [229, 231], [232, 237], [237, 238]]}
{"doc_key": "ai-train-26", "ner": [[6, 7, "algorithm"], [12, 16, "algorithm"], [18, 18, "algorithm"], [23, 24, "researcher"], [21, 27, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 7, 12, 16, "usage", "", true, false], [12, 16, 23, 24, "origin", "", false, false], [12, 16, 21, 27, "origin", "", false, false], [18, 18, 12, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "method", ",", "particularly", "used", "for", "recurrent", "neural", "networks", ",", "is", "the", "Long", "Short", "-", "Term", "Memory", "(", "LSTM", ")", "network", "developed", "by", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another method, particularly used for recurrent neural networks, is the Long Short-Term Memory (LSTM) network developed by Sepp Hochreiter and J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 14], [14, 15], [16, 28], [29, 33], [34, 37], [38, 47], [48, 54], [55, 63], [63, 64], [65, 67], [68, 71], [72, 76], [77, 82], [82, 83], [83, 87], [88, 94], [95, 96], [96, 100], [100, 101], [102, 109], [110, 119], [120, 122], [123, 127], [128, 138], [139, 142], [143, 149], [150, 161], [162, 164], [165, 169], [169, 170]]}
{"doc_key": "ai-train-27", "ner": [[0, 2, "programlang"], [5, 6, "product"], [12, 12, "product"], [41, 41, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 0, 2, "general-affiliation", "", false, false], [5, 6, 12, 12, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", "onwards", ")", "makes", "this", "package", "very", "versatile", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "modes", ",", "similar", "to", "commercial", "products", "like", "MATLAB", "."], "sentence-detokenized": "The C++ interpreter (CINT up to version 5.34, Cling from version 6 onwards) makes this package very versatile as it can be used in interactive, scripted and compiled modes, similar to commercial products like MATLAB.", "token2charspan": [[0, 3], [4, 5], [5, 7], [8, 19], [20, 21], [21, 23], [23, 25], [26, 28], [29, 31], [32, 39], [40, 44], [44, 45], [46, 51], [52, 56], [57, 64], [65, 66], [67, 74], [74, 75], [76, 81], [82, 86], [87, 94], [95, 99], [100, 109], [110, 112], [113, 115], [116, 119], [120, 122], [123, 127], [128, 130], [131, 142], [142, 143], [144, 152], [153, 156], [157, 165], [166, 171], [171, 172], [173, 180], [181, 183], [184, 194], [195, 203], [204, 208], [209, 215], [215, 216]]}
{"doc_key": "ai-train-28", "ner": [[0, 3, "product"], [21, 23, "field"], [27, 28, "task"], [30, 32, "task"], [34, 35, "task"], [37, 38, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 3, 21, 23, "related-to", "", false, false], [27, 28, 21, 23, "part-of", "", false, false], [30, 32, 21, 23, "part-of", "", false, false], [34, 35, 21, 23, "part-of", "", false, false], [37, 38, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Voice", "user", "interfaces", "that", "interpret", "and", "control", "the", "state", "of", "a", "conversation", "are", "difficult", "to", "design", "because", "of", "the", "complexity", "of", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "named", "object", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Voice user interfaces that interpret and control the state of a conversation are difficult to design because of the complexity of natural language processing tasks such as coreference resolution, named object recognition, information retrieval and dialogue management.", "token2charspan": [[0, 5], [6, 10], [11, 21], [22, 26], [27, 36], [37, 40], [41, 48], [49, 52], [53, 58], [59, 61], [62, 63], [64, 76], [77, 80], [81, 90], [91, 93], [94, 100], [101, 108], [109, 111], [112, 115], [116, 126], [127, 129], [130, 137], [138, 146], [147, 157], [158, 163], [164, 168], [169, 171], [172, 183], [184, 194], [194, 195], [196, 201], [202, 208], [209, 220], [220, 221], [222, 233], [234, 243], [244, 247], [248, 256], [257, 267], [267, 268]]}
{"doc_key": "ai-train-29", "ner": [[5, 6, "algorithm"], [9, 10, "algorithm"], [14, 16, "researcher"], [19, 25, "organisation"], [31, 32, "field"], [34, 35, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 14, 16, "origin", "", false, false], [5, 6, 31, 32, "part-of", "", false, false], [5, 6, 34, 35, "part-of", "", false, false], [9, 10, 14, 16, "origin", "", false, false], [9, 10, 31, 32, "part-of", "", false, false], [9, 10, 34, 35, "part-of", "", false, false], [14, 16, 19, 25, "physical", "", false, false], [14, 16, 19, 25, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "neural", "networks", "developed", "by", "J\u00fcrgen", "Schmidhuber", "'s", "research", "group", "at", "the", "Swiss", "Artificial", "Intelligence", "Lab", "IDSIA", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep neural networks developed by J\u00fcrgen Schmidhuber's research group at the Swiss Artificial Intelligence Lab IDSIA won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 64], [65, 73], [74, 83], [84, 86], [87, 93], [94, 105], [105, 107], [108, 116], [117, 122], [123, 125], [126, 129], [130, 135], [136, 146], [147, 159], [160, 163], [164, 169], [170, 173], [174, 179], [180, 193], [194, 206], [207, 209], [210, 217], [218, 229], [230, 233], [234, 241], [242, 250], [250, 251]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [13, 14, "task"], [16, 16, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 13, 14, "usage", "", true, false], [1, 3, 16, 16, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "for", "speech", "synthesis", "and", "language", "support", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components for speech synthesis and language support.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 71], [72, 78], [79, 88], [89, 92], [93, 101], [102, 109], [109, 110]]}
{"doc_key": "ai-train-31", "ner": [[9, 25, "misc"], [14, 15, "field"], [16, 21, "university"], [27, 32, "field"], [33, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 25, 14, 15, "topic", "topic_of_award", false, false], [9, 25, 16, 21, "origin", "", true, false], [27, 32, 33, 36, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "has", "been", "awarded", "two", "honorary", "degrees", ":", "the", "S.V.", "della", "Laurea", "ad", "Honorem", "in", "Psychology", "from", "the", "University", "of", "Padua", "in", "1995", ",", "and", "a", "PhD", "in", "Industrial", "Design", "and", "Engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He has been awarded two honorary degrees: the S.V. della Laurea ad Honorem in Psychology from the University of Padua in 1995, and a PhD in Industrial Design and Engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 19], [20, 23], [24, 32], [33, 40], [40, 41], [42, 45], [46, 50], [51, 56], [57, 63], [64, 66], [67, 74], [75, 77], [78, 88], [89, 93], [94, 97], [98, 108], [109, 111], [112, 117], [118, 120], [121, 125], [125, 126], [127, 130], [131, 132], [133, 136], [137, 139], [140, 150], [151, 157], [158, 161], [162, 173], [174, 178], [179, 184], [185, 195], [196, 198], [199, 209], [209, 210]]}
{"doc_key": "ai-train-32", "ner": [[7, 10, "researcher"], [13, 17, "organisation"], [18, 19, "location"], [21, 21, "researcher"], [34, 36, "misc"], [44, 48, "misc"], [65, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[7, 10, 13, 17, "physical", "", false, false], [7, 10, 13, 17, "role", "", false, false], [13, 17, 18, 19, "physical", "", false, false], [21, 21, 34, 36, "related-to", "works_with", true, false], [21, 21, 44, 48, "related-to", "works_with", true, false], [21, 21, 65, 65, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "who", "had", "damage", "to", "multiplication", "in", "different", "regions", "of", "the", "parietal", "lobe", "but", "preserved", "subtraction", "(", "related", "to", "lesions", "in", "the", "inferior", "parietal", "lobe", ")", ",", "and", "other", "patients", "who", "had", "damage", "to", "subtraction", "but", "preserved", "multiplication", "(", "related", "to", "intra-parietal", "lesions", ")", "."], "sentence-detokenized": "Together with his long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients who had damage to multiplication in different regions of the parietal lobe but preserved subtraction (related to lesions in the inferior parietal lobe), and other patients who had damage to subtraction but preserved multiplication (related to intra-parietal lesions).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [41, 48], [49, 54], [54, 55], [56, 57], [58, 69], [70, 72], [73, 76], [77, 82], [82, 83], [83, 94], [95, 103], [104, 106], [107, 112], [112, 113], [114, 121], [122, 126], [127, 137], [138, 146], [147, 150], [151, 154], [155, 161], [162, 164], [165, 179], [180, 182], [183, 192], [193, 200], [201, 203], [204, 207], [208, 216], [217, 221], [222, 225], [226, 235], [236, 247], [248, 249], [249, 256], [257, 259], [260, 267], [268, 270], [271, 274], [275, 283], [284, 292], [293, 297], [297, 298], [298, 299], [300, 303], [304, 309], [310, 318], [319, 322], [323, 326], [327, 333], [334, 336], [337, 348], [349, 352], [353, 362], [363, 377], [378, 379], [379, 386], [387, 389], [390, 404], [405, 412], [412, 413], [413, 414]]}
{"doc_key": "ai-train-33", "ner": [[5, 7, "product"], [12, 15, "misc"], [17, 18, "misc"], [25, 25, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 15, 5, 7, "topic", "", false, false], [17, 18, 5, 7, "topic", "", false, false], [25, 25, 5, 7, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Recently", ",", "fictional", "depictions", "of", "artificially", "intelligent", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "and", "the", "2016", "TV", "adaptation", "Westworld", ",", "have", "led", "to", "audience", "sympathy", "for", "robots", "themselves", "."], "sentence-detokenized": "Recently, fictional depictions of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina, and the 2016 TV adaptation Westworld, have led to audience sympathy for robots themselves.", "token2charspan": [[0, 8], [8, 9], [10, 19], [20, 30], [31, 33], [34, 46], [47, 58], [59, 65], [66, 68], [69, 74], [75, 79], [80, 82], [83, 86], [86, 87], [88, 98], [99, 111], [112, 115], [116, 118], [119, 126], [126, 127], [128, 131], [132, 135], [136, 140], [141, 143], [144, 154], [155, 164], [164, 165], [166, 170], [171, 174], [175, 177], [178, 186], [187, 195], [196, 199], [200, 206], [207, 217], [217, 218]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [10, 12, "algorithm"], [14, 15, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 12, 7, 8, "part-of", "", false, false], [14, 15, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 6, "organisation"], [23, 24, "misc"], [31, 32, "misc"], [35, 37, "person"], [42, 43, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[23, 24, 0, 6, "artifact", "", false, false], [31, 32, 0, 6, "artifact", "", false, false], [31, 32, 35, 37, "role", "director_of", false, false], [31, 32, 42, 43, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["\"", "The", "Walt", "Disney", "Company", "has", "also", "started", "to", "increase", "the", "use", "of", "3D", "films", "in", "special", "locations", "to", "impress", "audiences", ":", "\"", "Magical", "Journeys", "\"", "(", "1982", ")", "and", "\"", "Captain", "EO", "\"", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "are", "notable", "examples", "."], "sentence-detokenized": "\"The Walt Disney Company has also started to increase the use of 3D films in special locations to impress audiences: \"Magical Journeys\" (1982) and \"Captain EO\" (Francis Ford Coppola, 1986, starring Michael Jackson) are notable examples.", "token2charspan": [[0, 1], [1, 4], [5, 9], [10, 16], [17, 24], [25, 28], [29, 33], [34, 41], [42, 44], [45, 53], [54, 57], [58, 61], [62, 64], [65, 67], [68, 73], [74, 76], [77, 84], [85, 94], [95, 97], [98, 105], [106, 115], [115, 116], [117, 118], [118, 125], [126, 134], [134, 135], [136, 137], [137, 141], [141, 142], [143, 146], [147, 148], [148, 155], [156, 158], [158, 159], [160, 161], [161, 168], [169, 173], [174, 181], [181, 182], [183, 187], [187, 188], [189, 197], [198, 205], [206, 213], [213, 214], [215, 218], [219, 226], [227, 235], [235, 236]]}
{"doc_key": "ai-train-36", "ner": [[12, 14, "field"], [19, 24, "task"], [26, 27, "task"], [29, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 14, "part-of", "", false, false], [26, 27, 12, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "gained", "popularity", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "analysis", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has gained popularity in the field of natural language processing for tasks such as part-of-speech tagging and syntactic analysis (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 53], [54, 56], [57, 60], [61, 66], [67, 69], [70, 77], [78, 86], [87, 97], [98, 101], [102, 107], [108, 112], [113, 115], [116, 120], [120, 121], [121, 123], [123, 124], [124, 130], [131, 138], [139, 142], [143, 152], [153, 161], [162, 163], [163, 170], [170, 171], [172, 176], [176, 177], [177, 178]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [7, 14, "organisation"], [16, 17, "organisation"], [25, 27, "country"], [20, 24, "product"], [34, 34, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6], "relations": [[7, 14, 2, 3, "role", "introduces_to_market", true, false], [16, 17, 2, 3, "role", "introduces_to_market", true, false], [16, 17, 25, 27, "physical", "", false, false], [20, 24, 34, 34, "related-to", "sold_to", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "by", "Fuji", "Yusoki", "Kogyo", "Company", "in", "1963", "and", "in", "1976", "KUKA", "robotics", "invented", "a", "programmable", "universal", "assembly", "machine", "in", "Germany", ",", "the", "design", "of", "which", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced by Fuji Yusoki Kogyo Company in 1963 and in 1976 KUKA robotics invented a programmable universal assembly machine in Germany, the design of which was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 57], [58, 63], [64, 71], [72, 74], [75, 79], [80, 83], [84, 86], [87, 91], [92, 96], [97, 105], [106, 114], [115, 116], [117, 129], [130, 139], [140, 148], [149, 156], [157, 159], [160, 167], [167, 168], [169, 172], [173, 179], [180, 182], [183, 188], [189, 192], [193, 197], [198, 200], [201, 210], [210, 211]]}
{"doc_key": "ai-train-38", "ner": [[8, 8, "conference"], [10, 10, "researcher"], [30, 31, "researcher"], [35, 36, "researcher"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[10, 10, 8, 8, "role", "president_of", false, false], [10, 10, 30, 31, "role", "colleagues", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "the", "mid-1990s", ",", "as", "President", "of", "the", "AAAI", ",", "Hayes", "launched", "an", "attack", "on", "critics", "of", "AI", ",", "which", "was", "mostly", "ironic", ",", "and", "(", "together", "with", "his", "colleague", "Kenneth", "Ford", ")", "devised", "the", "Simon", "Newcomb", "Award", ",", "to", "be", "given", "for", "the", "most", "absurd", "argument", "against", "AI", "."], "sentence-detokenized": "In the mid-1990s, as President of the AAAI, Hayes launched an attack on critics of AI, which was mostly ironic, and (together with his colleague Kenneth Ford) devised the Simon Newcomb Award, to be given for the most absurd argument against AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 20], [21, 30], [31, 33], [34, 37], [38, 42], [42, 43], [44, 49], [50, 58], [59, 61], [62, 68], [69, 71], [72, 79], [80, 82], [83, 85], [85, 86], [87, 92], [93, 96], [97, 103], [104, 110], [110, 111], [112, 115], [116, 117], [117, 125], [126, 130], [131, 134], [135, 144], [145, 152], [153, 157], [157, 158], [159, 166], [167, 170], [171, 176], [177, 184], [185, 190], [190, 191], [192, 194], [195, 197], [198, 203], [204, 207], [208, 211], [212, 216], [217, 223], [224, 232], [233, 240], [241, 243], [243, 244]]}
{"doc_key": "ai-train-39", "ner": [[11, 15, "algorithm"], [35, 36, "algorithm"], [48, 52, "algorithm"], [54, 56, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[11, 15, 35, 36, "named", "same", false, false], [48, 52, 11, 15, "type-of", "", false, false], [54, 56, 11, 15, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "of", "math\\alpha/", "math", "can", "be", "found", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math\\alpha/", "math", "is", "determined", "by", "searching", "for", "a", "value", "that", "minimises", "S", ",", "usually", "using", "a", "line", "search", "in", "the", "range", "math", "0\\", "alpha", "1", "/", "math", "or", "a", "reverse", "line", "search", ",", "e.g.", "an", "Arrhenius", "line", "search", "."], "sentence-detokenized": "The optimal value of math\\alpha/math can be found using a line search algorithm, i.e. the size of math\\alpha/math is determined by searching for a value that minimises S, usually using a line search in the range math0\\alpha 1/math or a reverse line search, e.g. an Arrhenius line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 20], [21, 32], [32, 36], [37, 40], [41, 43], [44, 49], [50, 55], [56, 57], [58, 62], [63, 69], [70, 79], [79, 80], [81, 85], [86, 89], [90, 94], [95, 97], [98, 109], [109, 113], [114, 116], [117, 127], [128, 130], [131, 140], [141, 144], [145, 146], [147, 152], [153, 157], [158, 167], [168, 169], [169, 170], [171, 178], [179, 184], [185, 186], [187, 191], [192, 198], [199, 201], [202, 205], [206, 211], [212, 216], [216, 218], [218, 223], [224, 225], [225, 226], [226, 230], [231, 233], [234, 235], [236, 243], [244, 248], [249, 255], [255, 256], [257, 261], [262, 264], [265, 274], [275, 279], [280, 286], [286, 287]]}
{"doc_key": "ai-train-40", "ner": [[2, 3, "algorithm"], [5, 6, "algorithm"], [16, 21, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "broad", "search", "and", "deep", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "reflect", "expert", "systems", "that", "are", "rich", "in", "technical", "knowledge", ",", "but", "do", "not", "capture", "the", "mental", "processes", "that", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses broad search and deep search techniques, but ultimately concludes that the results reflect expert systems that are rich in technical knowledge, but do not capture the mental processes that people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 18], [19, 25], [26, 29], [30, 34], [35, 41], [42, 52], [52, 53], [54, 57], [58, 68], [69, 78], [79, 83], [84, 87], [88, 95], [96, 103], [104, 110], [111, 118], [119, 123], [124, 127], [128, 132], [133, 135], [136, 145], [146, 155], [155, 156], [157, 160], [161, 163], [164, 167], [168, 175], [176, 179], [180, 186], [187, 196], [197, 201], [202, 208], [209, 212], [213, 215], [216, 221], [222, 226], [227, 234], [234, 235]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "are", "concerned", "with", "how", "spoken", "language", "can", "be", "understood", "or", "created", "using", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis are concerned with how spoken language can be understood or created using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 43], [44, 53], [54, 58], [59, 62], [63, 69], [70, 78], [79, 82], [83, 85], [86, 96], [97, 99], [100, 107], [108, 113], [114, 123], [123, 124]]}
{"doc_key": "ai-train-42", "ner": [[9, 10, "algorithm"], [24, 26, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "theta", "^{*}/math", "is", "usually", "calculated", "using", "a", "maximum", "likelihood", "(", "math", "theta", "^{", "*}", "=", "theta", "theta", "^{", "ML}", "/math", ")", "or", "maximum", "a", "posteriori", "(", "math", "theta", "^{", "*}", "=", "theta", "theta", "^{", "MAP}", "/math", ")", "procedure", "."], "sentence-detokenized": "This math theta^{*}/math is usually calculated using a maximum likelihood (math theta^{*} = theta theta^{ML}/math) or maximum a posteriori (math theta^{*} = theta theta^{MAP}/math) procedure.", "token2charspan": [[0, 4], [5, 9], [10, 15], [15, 24], [25, 27], [28, 35], [36, 46], [47, 52], [53, 54], [55, 62], [63, 73], [74, 75], [75, 79], [80, 85], [85, 87], [87, 89], [90, 91], [92, 97], [98, 103], [103, 105], [105, 108], [108, 113], [113, 114], [115, 117], [118, 125], [126, 127], [128, 138], [139, 140], [140, 144], [145, 150], [150, 152], [152, 154], [155, 156], [157, 162], [163, 168], [168, 170], [170, 174], [174, 179], [179, 180], [181, 190], [190, 191]]}
{"doc_key": "ai-train-43", "ner": [[11, 17, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "common", "languages", "are", "spoken", "using", "the", "open", "-", "source", "eSpeak", "synthesiser", ",", "which", "results", "in", "a", "robotic", ",", "awkward", "voice", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less common languages are spoken using the open-source eSpeak synthesiser, which results in a robotic, awkward voice that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 26], [27, 30], [31, 37], [38, 43], [44, 47], [48, 52], [52, 53], [53, 59], [60, 66], [67, 78], [78, 79], [80, 85], [86, 93], [94, 96], [97, 98], [99, 106], [106, 107], [108, 115], [116, 121], [122, 126], [127, 130], [131, 133], [134, 143], [144, 146], [147, 157], [157, 158]]}
{"doc_key": "ai-train-44", "ner": [[1, 20, "programlang"], [34, 35, "programlang"], [37, 37, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 20, 34, 35, "compare", "", false, false], [1, 20, 37, 37, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "mainly", "used", "by", "statisticians", "and", "other", "professionals", "who", "need", "a", "statistical", "computing", "and", "software", "development", "environment", ",", "it", "can", "also", "act", "as", "a", "general", "matrix", "computing", "toolbox", "with", "performance", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is mainly used by statisticians and other professionals who need a statistical computing and software development environment, it can also act as a general matrix computing toolbox with performance comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 20], [21, 25], [26, 28], [29, 42], [43, 46], [47, 52], [53, 66], [67, 70], [71, 75], [76, 77], [78, 89], [90, 99], [100, 103], [104, 112], [113, 124], [125, 136], [136, 137], [138, 140], [141, 144], [145, 149], [150, 153], [154, 156], [157, 158], [159, 166], [167, 173], [174, 183], [184, 191], [192, 196], [197, 208], [209, 219], [220, 222], [223, 226], [227, 233], [234, 236], [237, 243], [243, 244]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 4, "part-of", "", false, false], [0, 0, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "and", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "mixing", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor and engineer Reginald Fessenden that creates new frequencies by mixing two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [76, 79], [80, 88], [89, 97], [98, 107], [108, 112], [113, 120], [121, 124], [125, 136], [137, 139], [140, 146], [147, 150], [151, 162], [162, 163]]}
{"doc_key": "ai-train-46", "ner": [[15, 17, "person"], [18, 18, "misc"], [22, 24, "organisation"], [27, 28, "organisation"], [29, 31, "misc"], [33, 34, "person"], [36, 36, "organisation"], [38, 40, "misc"], [42, 43, "person"], [45, 46, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[15, 17, 18, 18, "role", "actor_in", false, false], [18, 18, 22, 24, "artifact", "", false, false], [29, 31, 27, 28, "artifact", "", false, false], [33, 34, 29, 31, "role", "actor_in", false, false], [38, 40, 36, 36, "artifact", "", false, false], [42, 43, 38, 40, "role", "actor_in", false, false], [45, 46, 38, 40, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Some", "of", "the", "other", "films", "that", "put", "3D", "back", "on", "the", "map", "that", "month", "were", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", "and", "Paramount", "'s", "Cash", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Some of the other films that put 3D back on the map that month were John Wayne's Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth and Paramount's Cash From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 4], [5, 7], [8, 11], [12, 17], [18, 23], [24, 28], [29, 32], [33, 35], [36, 40], [41, 43], [44, 47], [48, 51], [52, 56], [57, 62], [63, 67], [68, 72], [73, 78], [78, 80], [81, 86], [87, 88], [88, 99], [100, 102], [103, 109], [110, 114], [114, 115], [115, 116], [116, 117], [118, 126], [126, 128], [129, 133], [134, 139], [140, 148], [149, 153], [154, 158], [159, 167], [168, 171], [172, 181], [181, 183], [184, 188], [189, 193], [194, 198], [199, 203], [204, 208], [209, 215], [216, 219], [220, 225], [226, 231], [231, 232]]}
{"doc_key": "ai-train-47", "ner": [[0, 1, "product"], [4, 5, "field"], [6, 7, "task"], [11, 12, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 7, "general-affiliation", "", false, false], [0, 1, 11, 12, "artifact", "", false, false], [6, 7, 4, 5, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["\"", "DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "developed", "by", "Facebook", "'s", "research", "team", "."], "sentence-detokenized": "\"DeepFace is a deep learning facial recognition system developed by Facebook's research team.", "token2charspan": [[0, 1], [1, 9], [10, 12], [13, 14], [15, 19], [20, 28], [29, 35], [36, 47], [48, 54], [55, 64], [65, 67], [68, 76], [76, 78], [79, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [14, 16, "field"], [25, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 14, 16, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [25, 27, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "frequent", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "academic", "conference", "on", "computer", "graphics", ",", "and", "a", "major", "theme", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a frequent research topic at SIGGRAPH, the leading academic conference on computer graphics, and a major theme of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 33], [34, 42], [43, 48], [49, 51], [52, 60], [60, 61], [62, 65], [66, 73], [74, 82], [83, 93], [94, 96], [97, 105], [106, 114], [114, 115], [116, 119], [120, 121], [122, 127], [128, 133], [134, 136], [137, 140], [141, 147], [148, 156], [157, 167], [168, 177], [177, 178]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [34, 34, "misc"], [43, 45, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 15, 34, 34, "general-affiliation", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 34, 34, "general-affiliation", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 34, 34, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "as", "preprocessing", "step", ",", "followed", "by", "clustering", "based", "on", "the", "k", "-", "NN", "feature", "vectors", "in", "the", "dimensionality", "-", "reduction", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) as preprocessing step, followed by clustering based on the k -NN feature vectors in the dimensionality-reduction space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [158, 160], [161, 170], [171, 182], [183, 191], [192, 193], [193, 196], [196, 197], [198, 200], [201, 214], [215, 219], [219, 220], [221, 229], [230, 232], [233, 243], [244, 249], [250, 252], [253, 256], [257, 258], [259, 260], [260, 262], [263, 270], [271, 278], [279, 281], [282, 285], [286, 300], [300, 301], [301, 310], [311, 316], [316, 317]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [11, 12, "field"], [14, 15, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 11, 12, "related-to", "good_at", true, false], [0, 2, 14, 15, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "are", "well", "suited", "for", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that are well suited for machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 60], [61, 65], [66, 72], [73, 76], [77, 84], [85, 93], [94, 97], [98, 105], [106, 117], [117, 118]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [35, 38, "algorithm"], [39, 40, "researcher"], [42, 44, "researcher"], [46, 52, "misc"], [54, 63, "conference"], [65, 65, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [46, 52, 35, 38, "topic", "", false, false], [46, 52, 39, 40, "artifact", "", false, false], [46, 52, 42, 44, "artifact", "", false, false], [46, 52, 54, 63, "temporal", "", false, false], [65, 65, 54, 63, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000.Others", "use", "local", "features", "such", "as", "histograms", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005.Descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000.Others use local features such as histograms of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005.Descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 150], [151, 154], [155, 160], [161, 169], [170, 174], [175, 177], [178, 188], [189, 191], [192, 200], [201, 210], [211, 213], [214, 219], [219, 220], [221, 222], [222, 223], [224, 230], [230, 231], [232, 242], [243, 245], [246, 254], [255, 264], [265, 268], [269, 274], [275, 284], [284, 285], [286, 290], [291, 299], [300, 307], [308, 318], [319, 321], [322, 330], [331, 337], [338, 341], [342, 349], [350, 361], [362, 363], [363, 367], [367, 368], [368, 369], [370, 375], [376, 377], [377, 378], [379, 386], [386, 387], [388, 404], [404, 405]]}
{"doc_key": "ai-train-52", "ner": [[0, 1, "algorithm"], [6, 8, "algorithm"], [13, 16, "task"], [15, 15, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 8, "type-of", "", false, false], [13, 16, 0, 1, "usage", "", true, false], [13, 16, 15, 15, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autocoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "a", "function", "by", "unsupervised", "learning", "."], "sentence-detokenized": "An autocoder is a type of artificial neural network used to learn a function by unsupervised learning.", "token2charspan": [[0, 2], [3, 12], [13, 15], [16, 17], [18, 22], [23, 25], [26, 36], [37, 43], [44, 51], [52, 56], [57, 59], [60, 65], [66, 67], [68, 76], [77, 79], [80, 92], [93, 101], [101, 102]]}
{"doc_key": "ai-train-53", "ner": [[0, 0, "researcher"], [4, 6, "organisation"], [11, 12, "field"], [14, 15, "field"], [20, 25, "organisation"], [27, 27, "organisation"], [33, 34, "field"], [36, 37, "field"], [42, 42, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 0, 4, 6, "role", "fellow_of", false, false], [0, 0, 11, 12, "related-to", "contributes_to", false, false], [0, 0, 14, 15, "related-to", "contributes_to", false, false], [0, 0, 20, 25, "role", "fellow_of", false, false], [0, 0, 33, 34, "related-to", "contributes_to", false, false], [0, 0, 36, 37, "related-to", "contributes_to", false, false], [27, 27, 20, 25, "named", "", false, false], [42, 42, 20, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "the", "IEEE", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "his", "contributions", "to", "pattern", "recognition", ",", "image", "processing", ",", "and", "service", "to", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of the IEEE for his contributions to computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions to pattern recognition, image processing, and service to IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 40], [41, 54], [55, 57], [58, 66], [67, 73], [74, 77], [78, 83], [84, 94], [95, 98], [99, 100], [101, 107], [108, 110], [111, 114], [115, 128], [129, 140], [141, 144], [145, 152], [153, 164], [165, 166], [166, 170], [170, 171], [172, 175], [176, 179], [180, 193], [194, 196], [197, 204], [205, 216], [216, 217], [218, 223], [224, 234], [234, 235], [236, 239], [240, 247], [248, 250], [251, 255], [255, 256]]}
{"doc_key": "ai-train-54", "ner": [[6, 8, "task"], [14, 16, "algorithm"], [18, 18, "algorithm"], [24, 26, "researcher"], [28, 29, "organisation"], [31, 32, "researcher"], [35, 37, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[6, 8, 14, 16, "usage", "", false, false], [14, 16, 24, 26, "origin", "", true, false], [14, 16, 31, 32, "origin", "", true, false], [18, 18, 14, 16, "named", "", false, false], [24, 26, 28, 29, "physical", "", false, false], [24, 26, 28, 29, "role", "", false, false], [31, 32, 35, 37, "physical", "", false, false], [31, 32, 35, 37, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "to", "develop", "a", "definitive", "ASR", "system", "was", "a", "system", "based", "on", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", ",", "introduced", "in", "2014", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "."], "sentence-detokenized": "The first attempt to develop a definitive ASR system was a system based on Connectionist Temporal Classification (CTC), introduced in 2014 by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 28], [29, 30], [31, 41], [42, 45], [46, 52], [53, 56], [57, 58], [59, 65], [66, 71], [72, 74], [75, 88], [89, 97], [98, 112], [113, 114], [114, 117], [117, 118], [118, 119], [120, 130], [131, 133], [134, 138], [139, 141], [142, 146], [147, 153], [154, 156], [157, 163], [164, 172], [173, 176], [177, 184], [185, 191], [192, 194], [195, 198], [199, 209], [210, 212], [213, 220], [220, 221]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear-", "Fractional", "Programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "Linear", "Programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-Fractional Programming (LFP) is a generalisation of Linear Programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 13, "misc"], [14, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 13, "win-defeat", "", false, false], [8, 13, 14, 23, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Machine", "Learning", "Conference", "in", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the International Machine Learning Conference in 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 101], [102, 110], [111, 121], [122, 124], [125, 129], [130, 133], [134, 138], [138, 139]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [22, 23, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "emergence", "of", "component", "-", "based", "systems", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "can", "implement", "the", "neural", "network", "developed", "in", "these", "systems", "as", "legacy", "components", "."], "sentence-detokenized": "With the emergence of component-based systems such as .NET and Java, component-based development environments can implement the neural network developed in these systems as legacy components.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 31], [31, 32], [32, 37], [38, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 113], [114, 123], [124, 127], [128, 134], [135, 142], [143, 152], [153, 155], [156, 161], [162, 169], [170, 172], [173, 179], [180, 190], [190, 191]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "so", "the", "algorithm", "first", "creates", "an", "alignment", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "(", "see", "illustrations", ")", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, so the algorithm first creates an alignment between two sentences, the candidate translation string and the reference translation string (see illustrations).", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 62], [63, 66], [67, 76], [77, 82], [83, 90], [91, 93], [94, 103], [104, 111], [112, 115], [116, 125], [125, 126], [127, 130], [131, 140], [141, 152], [153, 159], [160, 163], [164, 167], [168, 177], [178, 189], [190, 196], [197, 198], [198, 201], [202, 215], [215, 216], [216, 217]]}
{"doc_key": "ai-train-59", "ner": [[6, 13, "conference"], [21, 21, "task"], [23, 24, "task"], [28, 29, "metrics"], [31, 37, "metrics"], [43, 46, "conference"], [48, 48, "conference"], [51, 51, "location"], [53, 56, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 13, 21, 21, "related-to", "subject_at", false, false], [6, 13, 23, 24, "related-to", "subject_at", false, false], [28, 29, 6, 13, "temporal", "", false, false], [31, 37, 28, 29, "named", "", true, false], [48, 48, 43, 46, "named", "", false, false], [51, 51, 53, 56, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "NIST", "'s", "annual", "document", "understanding", "conferences", ",", "where", "groups", "of", "researchers", "submit", "their", "systems", "for", "summarisation", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "2014", "."], "sentence-detokenized": "One of the metrics used at NIST's annual document understanding conferences, where groups of researchers submit their systems for summarisation and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation), In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 31], [31, 33], [34, 40], [41, 49], [50, 63], [64, 75], [75, 76], [77, 82], [83, 89], [90, 92], [93, 104], [105, 111], [112, 117], [118, 125], [126, 129], [130, 143], [144, 147], [148, 159], [160, 165], [165, 166], [167, 169], [170, 173], [174, 179], [180, 186], [187, 188], [188, 194], [194, 195], [195, 203], [204, 214], [215, 218], [219, 226], [227, 237], [237, 238], [238, 239], [240, 242], [243, 251], [252, 254], [255, 261], [262, 273], [274, 284], [285, 292], [293, 294], [294, 298], [298, 299], [299, 300], [301, 309], [309, 310], [311, 317], [317, 318], [319, 327], [328, 332], [332, 333]]}
{"doc_key": "ai-train-60", "ner": [[5, 5, "programlang"], [7, 7, "product"], [11, 12, "programlang"], [15, 15, "product"], [21, 21, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 5, 11, 12, "type-of", "", false, false], [5, 5, 21, 21, "named", "", false, false], [7, 7, 11, 12, "part-of", "", false, false], [7, 7, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Same", "implementation", ",", "run", "in", "Java", "with", "JShell", "(", "at", "least", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Same implementation, run in Java with JShell (at least Java 9): codejshell scriptfile /codesyntaxhighlight lang = java", "token2charspan": [[0, 4], [5, 19], [19, 20], [21, 24], [25, 27], [28, 32], [33, 37], [38, 44], [45, 46], [46, 48], [49, 54], [55, 59], [60, 61], [61, 62], [62, 63], [64, 74], [75, 85], [86, 87], [87, 106], [107, 111], [112, 113], [114, 118]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [7, 10, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 7, 10, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metrics", "are", "based", "on", "the", "BLEU", "metrics", ",", "but", "with", "some", "changes", "."], "sentence-detokenized": "The NIST metrics are based on the BLEU metrics, but with some changes.", "token2charspan": [[0, 3], [4, 8], [9, 16], [17, 20], [21, 26], [27, 29], [30, 33], [34, 38], [39, 46], [46, 47], [48, 51], [52, 56], [57, 61], [62, 69], [69, 70]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [12, 12, "university"], [10, 14, "university"], [22, 26, "product"], [28, 31, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[12, 12, 6, 6, "physical", "", false, false], [10, 14, 6, 6, "physical", "", false, false], [22, 26, 12, 12, "origin", "", false, false], [22, 26, 10, 14, "origin", "", false, false], [22, 26, 28, 31, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "Universities", "of", "Groningen", "and", "Twente", ",", "jointly", "launched", "a", "project", "called", "\"", "Knowledge", "Graphs", "\"", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "additional", "constraint", "that", "edges", "can", "only", "be", "drawn", "from", "a", "limited", "set", "of", "possible", "links", ",", "to", "facilitate", "the", "construction", "of", "graph", "algebras", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the Universities of Groningen and Twente, jointly launched a project called \"Knowledge Graphs\", which are semantic networks, but with the additional constraint that edges can only be drawn from a limited set of possible links, to facilitate the construction of graph algebras.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 59], [60, 62], [63, 72], [73, 76], [77, 83], [83, 84], [85, 92], [93, 101], [102, 103], [104, 111], [112, 118], [119, 120], [120, 129], [130, 136], [136, 137], [137, 138], [139, 144], [145, 148], [149, 157], [158, 166], [166, 167], [168, 171], [172, 176], [177, 180], [181, 191], [192, 202], [203, 207], [208, 213], [214, 217], [218, 222], [223, 225], [226, 231], [232, 236], [237, 238], [239, 246], [247, 250], [251, 253], [254, 262], [263, 268], [268, 269], [270, 272], [273, 283], [284, 287], [288, 300], [301, 303], [304, 309], [310, 318], [318, 319]]}
{"doc_key": "ai-train-63", "ner": [[0, 1, "product"], [16, 17, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 16, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "used", "as", "a", "function", "of", "a", "larger", "application", "such", "as", "a", "word", "processor", ",", "but", "they", "can", "also", "be", "used", "as", "stand", "-", "alone", "applications", "that", "can", "be", "activated", "from", "within", "the", "applications", "that", "work", "with", "the", "text", "being", "edited", "."], "sentence-detokenized": "Grammar checkers are most often used as a function of a larger application such as a word processor, but they can also be used as stand-alone applications that can be activated from within the applications that work with the text being edited.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 36], [37, 39], [40, 41], [42, 50], [51, 53], [54, 55], [56, 62], [63, 74], [75, 79], [80, 82], [83, 84], [85, 89], [90, 99], [99, 100], [101, 104], [105, 109], [110, 113], [114, 118], [119, 121], [122, 126], [127, 129], [130, 135], [135, 136], [136, 141], [142, 154], [155, 159], [160, 163], [164, 166], [167, 176], [177, 181], [182, 188], [189, 192], [193, 205], [206, 210], [211, 215], [216, 220], [221, 224], [225, 229], [230, 235], [236, 242], [242, 243]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 21, "conference"], [25, 27, "organisation"], [33, 35, "conference"], [37, 39, "conference"], [41, 43, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "an", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence, and the Cognitive Science Society, and an editor of J. Automated Reasoning, J. Learning Sciences and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [137, 138], [139, 142], [143, 146], [147, 156], [157, 164], [165, 172], [172, 173], [174, 177], [178, 180], [181, 187], [188, 190], [191, 193], [194, 203], [204, 213], [213, 214], [215, 217], [218, 226], [227, 235], [236, 239], [240, 242], [243, 250], [251, 259], [259, 260]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 7, "algorithm"], [10, 11, "task"], [18, 19, "researcher"], [31, 33, "university"], [21, 22, "researcher"], [24, 27, "organisation"], [29, 33, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 18, 19, "origin", "", false, false], [0, 2, 21, 22, "origin", "", false, false], [4, 7, 0, 2, "named", "", false, false], [18, 19, 31, 33, "physical", "", false, false], [18, 19, 31, 33, "role", "", false, false], [21, 22, 24, 27, "role", "", false, false], [29, 33, 24, 27, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "was", "developed", "in", "1966", "by", "Fumitada", "Itakura", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "at", "Nagoya", "University", "."], "sentence-detokenized": "Linear Predictive Coding (LPC), a form of speech coding, was developed in 1966 by Fumitada Itakura and Shuzo Saito of Nippon Telegraph and Telephone (NTT) at Nagoya University.", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 60], [61, 70], [71, 73], [74, 78], [79, 81], [82, 90], [91, 98], [99, 102], [103, 108], [109, 114], [115, 117], [118, 124], [125, 134], [135, 138], [139, 148], [149, 150], [150, 153], [153, 154], [155, 157], [158, 164], [165, 175], [175, 176]]}
{"doc_key": "ai-train-66", "ner": [[55, 58, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "ergodic", ",", "all", "sample", "paths", "have", "the", "same", "time", "average", ",", "so", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "terms", "of", "root", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is ergodic, all sample paths have the same time average, so mathR _ x ^ {n / T _ 0} (\\ tau) = widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in terms of root mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 24], [24, 25], [26, 29], [30, 36], [37, 42], [43, 47], [48, 51], [52, 56], [57, 61], [62, 69], [69, 70], [71, 73], [74, 79], [80, 81], [82, 83], [84, 85], [86, 87], [87, 88], [89, 90], [91, 92], [93, 94], [95, 96], [96, 97], [98, 99], [99, 100], [101, 104], [104, 105], [106, 107], [108, 115], [116, 117], [117, 118], [118, 119], [120, 121], [122, 123], [124, 125], [126, 127], [127, 128], [129, 130], [131, 132], [133, 134], [135, 136], [136, 137], [138, 139], [139, 140], [141, 144], [144, 145], [146, 147], [148, 152], [153, 155], [156, 161], [162, 164], [165, 169], [170, 174], [175, 181], [182, 187], [187, 188]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [34, 36, "algorithm"], [38, 38, "algorithm"], [43, 47, "misc"], [54, 56, "algorithm"], [50, 53, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[13, 15, 43, 47, "related-to", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 43, 47, "related-to", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 43, 47, "related-to", "", false, false], [31, 31, 27, 29, "named", "", false, false], [34, 36, 43, 47, "related-to", "", false, false], [38, 38, 34, 36, "named", "", false, false], [54, 56, 50, 53, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorisation", "(", "NMF", ")", "techniques", "as", "a", "preprocessing", "step", ",", "followed", "by", "clustering", "of", "feature", "vectors", "in", "the", "K", "-", "NN", "method", "in", "the", "dimensionality", "-", "reduced", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorisation (NMF) techniques as a preprocessing step, followed by clustering of feature vectors in the K-NN method in the dimensionality-reduced space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 63], [64, 66], [67, 68], [69, 75], [76, 80], [81, 86], [87, 96], [97, 106], [107, 115], [116, 117], [117, 120], [120, 121], [121, 122], [123, 129], [130, 142], [143, 151], [152, 153], [153, 156], [156, 157], [157, 158], [159, 168], [169, 180], [181, 189], [190, 191], [191, 194], [194, 195], [196, 198], [199, 211], [212, 218], [219, 232], [233, 234], [234, 237], [237, 238], [239, 249], [250, 252], [253, 254], [255, 268], [269, 273], [273, 274], [275, 283], [284, 286], [287, 297], [298, 300], [301, 308], [309, 316], [317, 319], [320, 323], [324, 325], [325, 326], [326, 328], [329, 335], [336, 338], [339, 342], [343, 357], [357, 358], [358, 365], [366, 371], [371, 372]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 9, "task"], [12, 16, "task"], [30, 31, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 9, 12, 16, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "identifying", "the", "named", "entities", "in", "a", "text", "is", "called", "named", "entity", "identification", ",", "and", "the", "task", "of", "identifying", "the", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "entity", "mapping", "."], "sentence-detokenized": "The task of identifying the named entities in a text is called named entity identification, and the task of identifying the named entities mentioned in the text is called entity mapping.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 27], [28, 33], [34, 42], [43, 45], [46, 47], [48, 52], [53, 55], [56, 62], [63, 68], [69, 75], [76, 90], [90, 91], [92, 95], [96, 99], [100, 104], [105, 107], [108, 119], [120, 123], [124, 129], [130, 138], [139, 148], [149, 151], [152, 155], [156, 160], [161, 163], [164, 170], [171, 177], [178, 185], [185, 186]]}
{"doc_key": "ai-train-70", "ner": [[0, 1, "algorithm"], [27, 27, "programlang"], [29, 31, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 29, 31, "part-of", "", true, false], [29, 31, 27, 27, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "and", "since", "version", "0.8.0", "they", "have", "been", "released", "in", "a", "separate", "R", "package", "sigmoid", "to", "allow", "wider", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, and since version 0.8.0 they have been released in a separate R package sigmoid to allow wider use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 102], [103, 108], [109, 116], [117, 122], [123, 127], [128, 132], [133, 137], [138, 146], [147, 149], [150, 151], [152, 160], [161, 162], [163, 170], [171, 178], [179, 181], [182, 187], [188, 193], [194, 197], [197, 198]]}
{"doc_key": "ai-train-71", "ner": [[0, 1, "programlang"], [16, 20, "organisation"], [22, 25, "organisation"], [26, 26, "location"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 15, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 5, 6, 7], "relations": [[0, 1, 7, 8, "artifact", "", true, false], [0, 1, 10, 11, "artifact", "", true, false], [0, 1, 13, 15, "artifact", "", true, false], [22, 25, 16, 20, "named", "", false, false], [22, 25, 26, 26, "physical", "", false, false], [7, 8, 16, 20, "role", "", false, false], [10, 11, 16, 20, "role", "", false, false], [13, 15, 16, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 6, 7, 8], "sentence": ["The", "logo", "was", "designed", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "of", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "Cambridge", ",", "Massachusetts", "research", "firm", "."], "sentence-detokenized": "The logo was designed in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert of Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts research firm.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 21], [22, 24], [25, 29], [30, 32], [33, 38], [39, 47], [47, 48], [49, 56], [57, 64], [65, 68], [69, 76], [77, 83], [84, 86], [87, 91], [91, 92], [93, 100], [101, 104], [105, 111], [112, 113], [113, 116], [116, 117], [117, 118], [119, 120], [121, 130], [130, 131], [132, 145], [146, 154], [155, 159], [159, 160]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [17, 18, "field"], [23, 32, "algorithm"], [25, 28, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false], [0, 1, 17, 18, "compare", "", false, false], [23, 32, 17, 18, "part-of", "", false, false], [25, 28, 17, 18, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "commonly", "used", "as", "part", "of", "the", "reinforcement", "learning", "paradigm", "and", "can", "be", "contrasted", "with", "conventional", "deep", "learning", "approaches", ",", "which", "use", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is commonly used as part of the reinforcement learning paradigm and can be contrasted with conventional deep learning approaches, which use gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 26], [27, 31], [32, 34], [35, 39], [40, 42], [43, 46], [47, 60], [61, 69], [70, 78], [79, 82], [83, 86], [87, 89], [90, 100], [101, 105], [106, 118], [119, 123], [124, 132], [133, 143], [143, 144], [145, 150], [151, 154], [155, 163], [164, 171], [172, 174], [175, 176], [177, 183], [184, 191], [192, 196], [197, 198], [199, 204], [205, 213], [213, 214]]}
{"doc_key": "ai-train-73", "ner": [[40, 41, "algorithm"], [52, 54, "metrics"], [56, 56, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[56, 56, 52, 54, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "fit", "a", "hyperplane", "function", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264", "n/", "sub", "using", "the", "least", "squares", "method", ",", "then", "we", "can", "assess", "the", "fit", "using", "the", "mean", "square", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we fit a hyperplane function \u0177 = a + \u03b2 supT / sup x to the data (x sub i/sub, y sub i/sub) sub 1 \u2264 i \u2264 n/sub using the least squares method, then we can assess the fit using the mean square error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 11], [12, 22], [23, 31], [32, 33], [34, 35], [36, 37], [38, 39], [40, 41], [42, 46], [47, 48], [49, 52], [53, 54], [55, 57], [58, 61], [62, 66], [67, 68], [68, 69], [70, 73], [74, 75], [75, 76], [76, 79], [79, 80], [81, 82], [83, 86], [87, 88], [88, 89], [89, 92], [92, 93], [94, 97], [98, 99], [100, 101], [102, 103], [104, 105], [106, 108], [108, 111], [112, 117], [118, 121], [122, 127], [128, 135], [136, 142], [142, 143], [144, 148], [149, 151], [152, 155], [156, 162], [163, 166], [167, 170], [171, 176], [177, 180], [181, 185], [186, 192], [193, 198], [199, 200], [200, 203], [203, 204], [204, 205]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [47, 48, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "UK", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the UK.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 152], [152, 153], [154, 160], [160, 161], [162, 171], [171, 172], [173, 178], [179, 185], [185, 186], [187, 192], [192, 193], [194, 200], [200, 201], [202, 210], [210, 211], [212, 218], [219, 222], [223, 226], [227, 229], [229, 230]]}
{"doc_key": "ai-train-75", "ner": [[3, 8, "misc"], [3, 8, "field"], [16, 16, "organisation"], [13, 22, "university"], [30, 32, "organisation"], [34, 39, "university"], [43, 43, "university"], [45, 45, "university"], [48, 50, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 8, 3, 8, "topic", "", false, false], [3, 8, 16, 16, "origin", "", false, false], [3, 8, 13, 22, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "(", "2000", ")", "from", "the", "University", "of", "Inria", "and", "Sophia", "Antipolis", "University", "of", "Nice", ",", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "and", "visiting", "positions", "at", "Rutgers", ",", "Yale", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering (2000) from the University of Inria and Sophia Antipolis University of Nice, and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, and visiting positions at Rutgers, Yale and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 69], [70, 80], [81, 83], [84, 89], [90, 93], [94, 100], [101, 110], [111, 121], [122, 124], [125, 129], [129, 130], [131, 134], [135, 138], [139, 143], [144, 153], [154, 163], [164, 166], [167, 174], [175, 184], [185, 195], [195, 196], [197, 202], [203, 206], [207, 212], [213, 222], [222, 223], [224, 227], [228, 236], [237, 246], [247, 249], [250, 257], [257, 258], [259, 263], [264, 267], [268, 271], [272, 282], [283, 285], [286, 293], [293, 294]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [10, 10, "researcher"], [14, 14, "product"], [18, 19, "country"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 7, 8, "role", "licensing_patent_to", false, false], [10, 10, 18, 19, "physical", "", false, false], [24, 24, 10, 10, "artifact", "", false, false], [24, 24, 14, 14, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Using", "an", "original", "patent", "granted", "to", "inventor", "George", "Devol", ",", "Engelberger", "created", "the", "first", "industrial", "robot", "in", "the", "United", "States", "in", "the", "1960s", "-", "Unimate", "."], "sentence-detokenized": "Using an original patent granted to inventor George Devol, Engelberger created the first industrial robot in the United States in the 1960s - Unimate.", "token2charspan": [[0, 5], [6, 8], [9, 17], [18, 24], [25, 32], [33, 35], [36, 44], [45, 51], [52, 57], [57, 58], [59, 70], [71, 78], [79, 82], [83, 88], [89, 99], [100, 105], [106, 108], [109, 112], [113, 119], [120, 126], [127, 129], [130, 133], [134, 139], [140, 141], [142, 149], [149, 150]]}
{"doc_key": "ai-train-77", "ner": [[4, 6, "task"], [11, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[4, 4, "programlang"], [7, 7, "programlang"], [15, 15, "programlang"], [18, 18, "programlang"], [28, 28, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[4, 4, 15, 15, "named", "", false, false], [7, 7, 4, 4, "origin", "descendant_of", false, false], [7, 7, 18, 18, "general-affiliation", "", false, false], [7, 7, 28, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "descendants", "of", "the", "CLIPS", "language", "are", "Jess", "(", "the", "rules", "-", "based", "part", "of", "CLIPS", "rewritten", "in", "Java", ",", "later", "evolved", "in", "the", "other", "direction", ")", ",", "JESS", "was", "originally", "inspired", "by"], "sentence-detokenized": "The descendants of the CLIPS language are Jess (the rules-based part of CLIPS rewritten in Java, later evolved in the other direction), JESS was originally inspired by", "token2charspan": [[0, 3], [4, 15], [16, 18], [19, 22], [23, 28], [29, 37], [38, 41], [42, 46], [47, 48], [48, 51], [52, 57], [57, 58], [58, 63], [64, 68], [69, 71], [72, 77], [78, 87], [88, 90], [91, 95], [95, 96], [97, 102], [103, 110], [111, 113], [114, 117], [118, 123], [124, 133], [133, 134], [134, 135], [136, 140], [141, 144], [145, 155], [156, 164], [165, 167]]}
{"doc_key": "ai-train-79", "ner": [[6, 6, "product"], [11, 15, "product"], [16, 17, "organisation"], [21, 22, "product"], [40, 41, "product"], [43, 47, "product"], [62, 64, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[11, 15, 6, 6, "type-of", "", false, false], [16, 17, 11, 15, "usage", "", false, false], [21, 22, 16, 17, "artifact", "", false, false], [40, 41, 16, 17, "origin", "", true, false], [40, 41, 62, 64, "related-to", "", true, false], [43, 47, 16, 17, "origin", "", true, false], [43, 47, 62, 64, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["She", "has", "also", "developed", "flexible", "advanced", "AGV", "applications", ",", "designed", "the", "Motivity", "control", "system", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "a", "self", "-", "driving", "vehicle", ")", "used", "for", "complex", "assembly", "and", "layout", "operations", ",", "together", "with", "gantry", "systems", "and", "industrial", "robotic", "arms", "used", "in", "tier", "-", "one", "automotive", "supply", "factories", "to", "move", "products", "from", "one", "process", "to", "another", "in", "non-linear", "layouts", "."], "sentence-detokenized": "She has also developed flexible advanced AGV applications, designed the Motivity control system used by RMT Robotics to develop the ADAM iAGV (a self-driving vehicle) used for complex assembly and layout operations, together with gantry systems and industrial robotic arms used in tier-one automotive supply factories to move products from one process to another in non-linear layouts.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 22], [23, 31], [32, 40], [41, 44], [45, 57], [57, 58], [59, 67], [68, 71], [72, 80], [81, 88], [89, 95], [96, 100], [101, 103], [104, 107], [108, 116], [117, 119], [120, 127], [128, 131], [132, 136], [137, 141], [142, 143], [143, 144], [145, 149], [149, 150], [150, 157], [158, 165], [165, 166], [167, 171], [172, 175], [176, 183], [184, 192], [193, 196], [197, 203], [204, 214], [214, 215], [216, 224], [225, 229], [230, 236], [237, 244], [245, 248], [249, 259], [260, 267], [268, 272], [273, 277], [278, 280], [281, 285], [285, 286], [286, 289], [290, 300], [301, 307], [308, 317], [318, 320], [321, 325], [326, 334], [335, 339], [340, 343], [344, 351], [352, 354], [355, 362], [363, 365], [366, 376], [377, 384], [384, 385]]}
{"doc_key": "ai-train-80", "ner": [[8, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameters", "\u03b2", "are", "usually", "estimated", "using", "the", "maximum", "likelihood", "method", "."], "sentence-detokenized": "The parameters \u03b2 are usually estimated using the maximum likelihood method.", "token2charspan": [[0, 3], [4, 14], [15, 16], [17, 20], [21, 28], [29, 38], [39, 44], [45, 48], [49, 56], [57, 67], [68, 74], [74, 75]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [5, 5, "metrics"], [7, 7, "metrics"], [10, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 0, 1, "part-of", "", false, false], [7, 7, 0, 1, "part-of", "", false, false], [10, 10, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "indicators", "such", "as", "precision", "and", "recall", ",", "or", "DCG", ",", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval indicators such as precision and recall, or DCG, are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 32], [33, 37], [38, 40], [41, 50], [51, 54], [55, 61], [61, 62], [63, 65], [66, 69], [69, 70], [71, 74], [75, 81], [82, 85], [86, 95], [96, 99], [100, 107], [108, 110], [111, 112], [113, 127], [128, 134], [134, 135]]}
{"doc_key": "ai-train-82", "ner": [[9, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "there", "are", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "with", "ten", "people", "per", "robot", "."], "sentence-detokenized": "In a typical factory, there are hundreds of industrial robots working on fully automated production lines, with ten people per robot.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 27], [28, 31], [32, 40], [41, 43], [44, 54], [55, 61], [62, 69], [70, 72], [73, 78], [79, 88], [89, 99], [100, 105], [105, 106], [107, 111], [112, 115], [116, 122], [123, 126], [127, 132], [132, 133]]}
{"doc_key": "ai-train-83", "ner": [[5, 9, "product"], [14, 16, "field"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [28, 29, "task"], [31, 32, "task"], [34, 35, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[14, 16, 5, 9, "usage", "", false, true], [19, 20, 14, 16, "part-of", "", false, false], [22, 23, 14, 16, "part-of", "", false, false], [25, 26, 14, 16, "part-of", "", false, false], [28, 29, 14, 16, "part-of", "", false, false], [31, 32, 14, 16, "part-of", "", false, false], [34, 35, 14, 16, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "past", "decade", ",", "PCNNs", "have", "been", "used", "in", "a", "wide", "range", "of", "image", "processing", "applications", ",", "including", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "enhancement", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the past decade, PCNNs have been used in a wide range of image processing applications, including image segmentation, feature generation, face extraction, motion detection, region enhancement and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [20, 21], [22, 27], [28, 32], [33, 37], [38, 42], [43, 45], [46, 47], [48, 52], [53, 58], [59, 61], [62, 67], [68, 78], [79, 91], [91, 92], [93, 102], [103, 108], [109, 121], [121, 122], [123, 130], [131, 141], [141, 142], [143, 147], [148, 158], [158, 159], [160, 166], [167, 176], [176, 177], [178, 184], [185, 196], [197, 200], [201, 206], [207, 216], [216, 217]]}
{"doc_key": "ai-train-84", "ner": [[0, 1, "researcher"], [15, 17, "field"], [23, 27, "misc"], [29, 35, "conference"], [37, 37, "conference"], [41, 43, "misc"], [44, 50, "conference"], [51, 52, "conference"], [55, 59, "conference"], [61, 61, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 15, 17, "related-to", "contributes_to", false, false], [0, 1, 23, 27, "win-defeat", "", false, false], [0, 1, 41, 43, "win-defeat", "", false, false], [23, 27, 29, 35, "temporal", "", false, false], [37, 37, 29, 35, "named", "", false, false], [41, 43, 44, 50, "temporal", "", false, false], [41, 43, 55, 59, "temporal", "", false, false], [51, 52, 44, 50, "named", "", false, false], [61, 61, 55, 59, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", ",", "and", "has", "won", "the", "best", "paper", "award", "at", "the", "2012", "International", "Conference", "on", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "and", "the", "best", "reviewer", "award", "at", "the", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "the", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision, and has won the best paper award at the 2012 International Conference on Non-Photorealistic Rendering and Animation (NPAR) and the best reviewer award at the Asian Conference on Computer Vision ACCV 2012 and the International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [110, 111], [112, 115], [116, 119], [120, 123], [124, 127], [128, 132], [133, 138], [139, 144], [145, 147], [148, 151], [152, 156], [157, 170], [171, 181], [182, 184], [185, 203], [204, 213], [214, 217], [218, 227], [228, 229], [229, 233], [233, 234], [235, 238], [239, 242], [243, 247], [248, 256], [257, 262], [263, 265], [266, 269], [270, 275], [276, 286], [287, 289], [290, 298], [299, 305], [306, 310], [311, 315], [316, 319], [320, 323], [324, 337], [338, 348], [349, 351], [352, 360], [361, 367], [368, 369], [369, 373], [373, 374], [375, 379], [379, 380]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [1, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 15, "researcher"], [16, 18, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 1, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [16, 18, 0, 0, "usage", "", false, false], [16, 18, 13, 15, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "computer", "science", "and", "artificial", "intelligence", "is", "the", "ontology", "language", "used", "in", "Doug", "Lenato", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "CycL in computer science and artificial intelligence is the ontology language used in Doug Lenato's Cyc artificial project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 59], [60, 68], [69, 77], [78, 82], [83, 85], [86, 90], [91, 97], [97, 99], [100, 103], [104, 114], [115, 122], [122, 123]]}
{"doc_key": "ai-train-86", "ner": [[0, 4, "task"], [5, 7, "metrics"], [14, 17, "metrics"], [20, 22, "metrics"], [31, 33, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 0, 4, "part-of", "", false, false], [14, 17, 5, 7, "named", "", false, false], [20, 22, 5, 7, "named", "", false, false], [31, 33, 5, 7, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "squared", "prediction", "error", "or", "the", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "of", "the", "squared", "deviations", "of", "the", "predictions", "from", "the", "TRUE", "values", "obtained", "from", "a", "model", "estimated", "over", "a", "given", "sample", "space", "."], "sentence-detokenized": "In regression analysis, the mean squared error, often referred to as the mean squared prediction error or the mean squared error, can refer to the average of the squared deviations of the predictions from the TRUE values obtained from a model estimated over a given sample space.", "token2charspan": [[0, 2], [3, 13], [14, 22], [22, 23], [24, 27], [28, 32], [33, 40], [41, 46], [46, 47], [48, 53], [54, 62], [63, 65], [66, 68], [69, 72], [73, 77], [78, 85], [86, 96], [97, 102], [103, 105], [106, 109], [110, 114], [115, 122], [123, 128], [128, 129], [130, 133], [134, 139], [140, 142], [143, 146], [147, 154], [155, 157], [158, 161], [162, 169], [170, 180], [181, 183], [184, 187], [188, 199], [200, 204], [205, 208], [209, 213], [214, 220], [221, 229], [230, 234], [235, 236], [237, 242], [243, 252], [253, 257], [258, 259], [260, 265], [266, 272], [273, 278], [278, 279]]}
{"doc_key": "ai-train-87", "ner": [[5, 7, "algorithm"], [9, 10, "algorithm"], [18, 21, "algorithm"], [37, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 7, 9, 10, "compare", "", false, false], [5, 7, 18, 21, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "results", "show", "that", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "perform", "similarly", ",", "while", "the", "C", "-", "HOG", "descriptors", "maintain", "a", "slight", "advantage", "in", "terms", "of", "detection", "miss", "rate", "for", "a", "fixed", "number", "of", "false", "positives", "in", "both", "data", "sets", "."], "sentence-detokenized": "The results show that the C-HOG and R-HOG block descriptors perform similarly, while the C-HOG descriptors maintain a slight advantage in terms of detection miss rate for a fixed number of false positives in both data sets.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 21], [22, 25], [26, 27], [27, 28], [28, 31], [32, 35], [36, 38], [38, 41], [42, 47], [48, 59], [60, 67], [68, 77], [77, 78], [79, 84], [85, 88], [89, 90], [90, 91], [91, 94], [95, 106], [107, 115], [116, 117], [118, 124], [125, 134], [135, 137], [138, 143], [144, 146], [147, 156], [157, 161], [162, 166], [167, 170], [171, 172], [173, 178], [179, 185], [186, 188], [189, 194], [195, 204], [205, 207], [208, 212], [213, 217], [218, 222], [222, 223]]}
{"doc_key": "ai-train-88", "ner": [[6, 8, "algorithm"], [10, 10, "misc"], [12, 14, "algorithm"], [16, 17, "algorithm"], [20, 22, "algorithm"], [24, 26, "algorithm"], [28, 30, "algorithm"], [32, 33, "misc"], [38, 40, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 8, 10, 10, "usage", "", false, false], [12, 14, 32, 33, "usage", "", false, false], [16, 17, 32, 33, "usage", "", false, false], [20, 22, 32, 33, "usage", "", false, false], [24, 26, 32, 33, "usage", "", false, false], [28, 30, 32, 33, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["The", "most", "popular", "recognition", "algorithms", "are", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "the", "Fisher", "surface", "algorithm", ",", "hidden", "Markov", "model", ",", "multi-linear", "subspace", "learning", "using", "tensor", "mapping", "and", "neural", "-", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "The most popular recognition algorithms are principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using the Fisher surface algorithm, hidden Markov model, multi-linear subspace learning using tensor mapping and neural-motivated dynamic link matching.", "token2charspan": [[0, 3], [4, 8], [9, 16], [17, 28], [29, 39], [40, 43], [44, 53], [54, 63], [64, 72], [73, 78], [79, 89], [89, 90], [91, 97], [98, 110], [111, 119], [119, 120], [121, 128], [129, 137], [138, 143], [144, 147], [148, 154], [155, 162], [163, 172], [172, 173], [174, 180], [181, 187], [188, 193], [193, 194], [195, 207], [208, 216], [217, 225], [226, 231], [232, 238], [239, 246], [247, 250], [251, 257], [257, 258], [258, 267], [268, 275], [276, 280], [281, 289], [289, 290]]}
{"doc_key": "ai-train-89", "ner": [[5, 10, "misc"], [16, 19, "location"], [36, 38, "location"], [53, 53, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[16, 19, 5, 10, "temporal", "", false, false], [36, 38, 5, 10, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "in", "2019", ",", "films", "from", "the", "Toronto", "International", "Film", "Festival", "can", "be", "screened", "outside", "the", "Scotiabank", "Theatre", "in", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "and", "shown", "elsewhere", "(", "e.g.", "at", "the", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", ",", "if", "they", "are", "distributed", "by", "a", "service", "like", "Netflix", "."], "sentence-detokenized": "Starting in 2019, films from the Toronto International Film Festival can be screened outside the Scotiabank Theatre in Toronto - one of the festival's main venues - and shown elsewhere (e.g. at the TIFF Bell Lightbox and other local cinemas), if they are distributed by a service like Netflix.", "token2charspan": [[0, 8], [9, 11], [12, 16], [16, 17], [18, 23], [24, 28], [29, 32], [33, 40], [41, 54], [55, 59], [60, 68], [69, 72], [73, 75], [76, 84], [85, 92], [93, 96], [97, 107], [108, 115], [116, 118], [119, 126], [127, 128], [129, 132], [133, 135], [136, 139], [140, 148], [148, 150], [151, 155], [156, 162], [163, 164], [165, 168], [169, 174], [175, 184], [185, 186], [186, 190], [191, 193], [194, 197], [198, 202], [203, 207], [208, 216], [217, 220], [221, 226], [227, 232], [233, 240], [240, 241], [241, 242], [243, 245], [246, 250], [251, 254], [255, 266], [267, 269], [270, 271], [272, 279], [280, 284], [285, 292], [292, 293]]}
{"doc_key": "ai-train-90", "ner": [[1, 1, "organisation"], [3, 5, "researcher"], [7, 8, "organisation"], [12, 14, "researcher"], [20, 25, "product"], [33, 34, "researcher"], [38, 40, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[1, 1, 7, 8, "related-to", "purchases", false, false], [3, 5, 12, 14, "named", "same", false, false], [3, 5, 33, 34, "named", "same", false, false], [7, 8, 3, 5, "origin", "founded_by", false, false], [20, 25, 1, 1, "artifact", "", false, false], [38, 40, 33, 34, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["1977", "Unimation", "acquires", "Victor", "Scheinman", "'s", "company", "Vicarm", "Inc.", "and", ",", "with", "Scheinman", "'s", "help", ",", "developed", "and", "launched", "the", "Programmable", "Universal", "Assembly", "Machine", ",", "a", "new", "model", "of", "robotic", "arm", ",", "using", "Scheinman", "'s", "cutting", "-", "edge", "programming", "language", "VAL", "."], "sentence-detokenized": "1977 Unimation acquires Victor Scheinman's company Vicarm Inc. and, with Scheinman's help, developed and launched the Programmable Universal Assembly Machine, a new model of robotic arm, using Scheinman's cutting-edge programming language VAL.", "token2charspan": [[0, 4], [5, 14], [15, 23], [24, 30], [31, 40], [40, 42], [43, 50], [51, 57], [58, 62], [63, 66], [66, 67], [68, 72], [73, 82], [82, 84], [85, 89], [89, 90], [91, 100], [101, 104], [105, 113], [114, 117], [118, 130], [131, 140], [141, 149], [150, 157], [157, 158], [159, 160], [161, 164], [165, 170], [171, 173], [174, 181], [182, 185], [185, 186], [187, 192], [193, 202], [202, 204], [205, 212], [212, 213], [213, 217], [218, 229], [230, 238], [239, 242], [242, 243]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [8, 11, "algorithm"], [12, 17, "product"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 1, 8, 11, "origin", "implementation_of", false, false], [0, 1, 12, 17, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "implementation", "of", "the", "Java", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source implementation of the Java C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 36], [37, 39], [40, 43], [44, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[8, 8, "metrics"], [3, 6, "product"], [25, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[8, 8, 3, 6, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["\"", "According", "to", "Google", "Scholar", ",", "the", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", ",", "and", "in", "2016", "it", "was", "awarded", "the", "IEEE", "Signal", "Processing", "Society", "'s", "Continuous", "Impact", "Award", ",", "which", "means", "that", "a", "paper", "has", "had", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "\"According to Google Scholar, the 2004 SSIM paper has been cited more than 20,000 times, and in 2016 it was awarded the IEEE Signal Processing Society's Continuous Impact Award, which means that a paper has had an unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 1], [1, 10], [11, 13], [14, 20], [21, 28], [28, 29], [30, 33], [34, 38], [39, 43], [44, 49], [50, 53], [54, 58], [59, 64], [65, 69], [70, 74], [75, 81], [82, 87], [87, 88], [89, 92], [93, 95], [96, 100], [101, 103], [104, 107], [108, 115], [116, 119], [120, 124], [125, 131], [132, 142], [143, 150], [150, 152], [153, 163], [164, 170], [171, 176], [176, 177], [178, 183], [184, 189], [190, 194], [195, 196], [197, 202], [203, 206], [207, 210], [211, 213], [214, 223], [224, 228], [229, 235], [236, 239], [240, 242], [243, 248], [249, 251], [252, 257], [258, 263], [264, 275], [275, 276]]}
{"doc_key": "ai-train-93", "ner": [[0, 1, "task"], [23, 27, "product"], [37, 39, "product"], [45, 45, "organisation"], [46, 46, "product"], [42, 43, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 45, 45, "artifact", "", false, false], [23, 27, 0, 1, "related-to", "performs", false, false], [23, 27, 37, 39, "part-of", "", false, false], [45, 45, 42, 43, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "is", "almost", "indistinguishable", "from", "the", "real", "human", "voice", ",", "thanks", "to", "the", "launch", "in", "2016", "of", "voice", "editing", "and", "generation", "software", "Adobe", "Voco", ",", "which", "is", "expected", "to", "be", "a", "prototype", "as", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "Google", "'s", "prototype", "DeepMind", "WaveNet", "."], "sentence-detokenized": "Speech synthesis is almost indistinguishable from the real human voice, thanks to the launch in 2016 of voice editing and generation software Adobe Voco, which is expected to be a prototype as part of the Adobe Creative Suite, and Google's prototype DeepMind WaveNet.", "token2charspan": [[0, 6], [7, 16], [17, 19], [20, 26], [27, 44], [45, 49], [50, 53], [54, 58], [59, 64], [65, 70], [70, 71], [72, 78], [79, 81], [82, 85], [86, 92], [93, 95], [96, 100], [101, 103], [104, 109], [110, 117], [118, 121], [122, 132], [133, 141], [142, 147], [148, 152], [152, 153], [154, 159], [160, 162], [163, 171], [172, 174], [175, 177], [178, 179], [180, 189], [190, 192], [193, 197], [198, 200], [201, 204], [205, 210], [211, 219], [220, 225], [225, 226], [227, 230], [231, 237], [237, 239], [240, 249], [250, 258], [259, 266], [266, 267]]}
{"doc_key": "ai-train-94", "ner": [[0, 2, "researcher"], [7, 9, "organisation"], [15, 20, "organisation"], [26, 27, "conference"], [34, 39, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 2, 7, 9, "role", "", false, false], [0, 2, 15, 20, "role", "", false, false], [0, 2, 26, 27, "role", "", false, false], [0, 2, 34, 39, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "Honorary", "Fellow", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "Fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "Founding", "Fellow", "of", "the", "AAAI", ",", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an Honorary Fellow of the Neuroscience Research Program, a Fellow of the American Academy of Arts and Sciences, a Founding Fellow of the AAAI, and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 123], [124, 132], [133, 139], [140, 142], [143, 146], [147, 151], [151, 152], [153, 156], [157, 158], [159, 167], [168, 174], [175, 177], [178, 181], [182, 190], [191, 200], [201, 204], [205, 210], [211, 219], [219, 220]]}
{"doc_key": "ai-train-95", "ner": [[8, 9, "task"], [11, 11, "task"], [15, 17, "task"], [21, 21, "misc"], [22, 23, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[8, 9, 15, 17, "cause-effect", "", false, false], [11, 11, 15, 17, "cause-effect", "", false, false], [22, 23, 15, 17, "topic", "", false, false], [22, 23, 21, 21, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "stimulated", "by", "advances", "in", "speech", "recognition", "and", "synthesis", ",", "research", "into", "language", "translation", "was", "launched", "under", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, stimulated by advances in speech recognition and synthesis, research into language translation was launched under the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 36], [37, 39], [40, 46], [47, 58], [59, 62], [63, 72], [72, 73], [74, 82], [83, 87], [88, 96], [97, 108], [109, 112], [113, 121], [122, 127], [128, 131], [132, 138], [139, 148], [149, 156], [156, 157]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [14, 15, "algorithm"], [21, 28, "algorithm"], [27, 27, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [14, 15, 3, 4, "origin", "", false, false], [14, 15, 8, 9, "origin", "", false, false], [14, 15, 11, 12, "origin", "", false, false], [14, 15, 27, 27, "part-of", "", false, false], [21, 28, 14, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisors", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "forget", "gates", "(", "also", "known", "as", "\"", "keep", "gates", "\"", ")", "in", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999, Felix Gers and his advisors J\u00fcrgen Schmidhuber and Fred Cummins introduced forget gates (also known as \"keep gates\") in the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 90], [91, 96], [97, 98], [98, 102], [103, 108], [109, 111], [112, 113], [113, 117], [118, 123], [123, 124], [124, 125], [126, 128], [129, 132], [133, 137], [138, 150], [150, 151]]}
{"doc_key": "ai-train-97", "ner": [[2, 3, "field"], [0, 8, "field"], [9, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 2, 3, "part-of", "", false, false], [9, 11, 0, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalised", "sinc", "function", "is", "usually", "defined", "as"], "sentence-detokenized": "In digital signal processing and information theory, the normalised sinc function is usually defined as", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [8, 9, "researcher"], [14, 17, "conference"], [20, 24, "organisation"], [26, 26, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 8, 9, "origin", "coined_term", false, false], [8, 9, 14, 17, "role", "", false, false], [8, 9, 20, 24, "role", "", false, false], [26, 26, 20, 24, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "was", "first", "coined", "by", "David", "Hays", ",", "founder", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "for", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics was first coined by David Hays, founder of the Association for Computational Linguistics and the International Committee for Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 38], [39, 44], [45, 51], [52, 54], [55, 60], [61, 65], [65, 66], [67, 74], [75, 77], [78, 81], [82, 93], [94, 97], [98, 111], [112, 123], [124, 127], [128, 131], [132, 145], [146, 155], [156, 159], [160, 173], [174, 185], [186, 187], [187, 191], [191, 192], [192, 193]]}
{"doc_key": "ai-train-99", "ner": [[7, 18, "misc"], [19, 19, "misc"], [37, 39, "metrics"], [41, 44, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[41, 44, 37, 39, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "October", "2011", "In", "one", "-", "dimensional", "memory", "-", "based", "(", "or", "memoryless", ")", "polynomial", "DPD", ",", "to", "solve", "the", "problem", "of", "the", "polynomial", "coefficients", "of", "the", "digital", "predistortion", "and", "to", "minimise", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distorted", "nonlinear", "output", "of", "the", "system", "has", "to", "be", "intercepted", "at", "a", "rate", "sufficient", "to", "capture", "the", "nonlinear", "products", "of", "the", "digital", "predistortion", "order", "."], "sentence-detokenized": "59, pp. 2547-2553, October 2011 In one-dimensional memory-based (or memoryless) polynomial DPD, to solve the problem of the polynomial coefficients of the digital predistortion and to minimise the mean squared error (MSE), the distorted nonlinear output of the system has to be intercepted at a rate sufficient to capture the nonlinear products of the digital predistortion order.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 26], [27, 31], [32, 34], [35, 38], [38, 39], [39, 50], [51, 57], [57, 58], [58, 63], [64, 65], [65, 67], [68, 78], [78, 79], [80, 90], [91, 94], [94, 95], [96, 98], [99, 104], [105, 108], [109, 116], [117, 119], [120, 123], [124, 134], [135, 147], [148, 150], [151, 154], [155, 162], [163, 176], [177, 180], [181, 183], [184, 192], [193, 196], [197, 201], [202, 209], [210, 215], [216, 217], [217, 220], [220, 221], [221, 222], [223, 226], [227, 236], [237, 246], [247, 253], [254, 256], [257, 260], [261, 267], [268, 271], [272, 274], [275, 277], [278, 289], [290, 292], [293, 294], [295, 299], [300, 310], [311, 313], [314, 321], [322, 325], [326, 335], [336, 344], [345, 347], [348, 351], [352, 359], [360, 373], [374, 379], [379, 380]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [15, 15, "location"], [17, 18, "location"], [20, 21, "country"], [24, 24, "location"], [26, 30, "country"], [46, 53, "organisation"], [41, 44, "organisation"], [54, 55, "location"], [62, 62, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 15, 15, "physical", "", false, false], [0, 1, 41, 44, "physical", "", false, false], [0, 1, 62, 62, "role", "", false, false], [15, 15, 17, 18, "physical", "", false, false], [17, 18, 20, 21, "physical", "", false, false], [46, 53, 41, 44, "part-of", "", false, false], [41, 44, 54, 55, "physical", "", false, false], [62, 62, 46, 53, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", "(", "born", "5", "October", "1947", ")", "Katz", ",", "born", "1947", ",", "1947", ",", "Kishinev", ",", "Moldavian", "SSR", ",", "Soviet", "Union", "(", "now", "Kishinev", ",", "Moldova", ")", ")", "is", "an", "American", "senior", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "Massachusetts", "Institute", "of", "Technology", "(", "MIT", ")", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "in", "Cambridge", ",", "and", "group", "leader", "of", "the", "InfoLab", "."], "sentence-detokenized": "Boris Katz (born 5 October 1947) Katz, born 1947, 1947, Kishinev, Moldavian SSR, Soviet Union (now Kishinev, Moldova)) is an American senior research scientist (computer scientist) at the Massachusetts Institute of Technology (MIT) Computer Science and Artificial Intelligence Laboratory in Cambridge, and group leader of the InfoLab.", "token2charspan": [[0, 5], [6, 10], [11, 12], [12, 16], [17, 18], [19, 26], [27, 31], [31, 32], [33, 37], [37, 38], [39, 43], [44, 48], [48, 49], [50, 54], [54, 55], [56, 64], [64, 65], [66, 75], [76, 79], [79, 80], [81, 87], [88, 93], [94, 95], [95, 98], [99, 107], [107, 108], [109, 116], [116, 117], [117, 118], [119, 121], [122, 124], [125, 133], [134, 140], [141, 149], [150, 159], [160, 161], [161, 169], [170, 179], [179, 180], [181, 183], [184, 187], [188, 201], [202, 211], [212, 214], [215, 225], [226, 227], [227, 230], [230, 231], [232, 240], [241, 248], [249, 252], [253, 263], [264, 276], [277, 287], [288, 290], [291, 300], [300, 301], [302, 305], [306, 311], [312, 318], [319, 321], [322, 325], [326, 333], [333, 334]]}
