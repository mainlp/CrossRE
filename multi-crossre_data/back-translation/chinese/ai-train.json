{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [12, 13, "field"], [15, 16, "task"], [18, 19, "task"], [23, 25, "task"], [28, 29, "field"], [30, 32, "researcher"], [34, 36, "researcher"], [38, 39, "researcher"], [41, 42, "researcher"], [44, 46, "researcher"], [50, 51, "researcher"], [53, 54, "researcher"], [56, 57, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 12, 13, "part-of", "", false, false], [3, 7, 12, 13, "usage", "", false, false], [3, 7, 15, 16, "part-of", "", false, false], [3, 7, 15, 16, "usage", "", false, false], [3, 7, 18, 19, "part-of", "", false, false], [3, 7, 18, 19, "usage", "", false, false], [3, 7, 28, 29, "part-of", "", false, false], [3, 7, 28, 29, "usage", "", false, false], [23, 25, 18, 19, "part-of", "", false, false], [23, 25, 18, 19, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "for", "opinion", "-", "based", "recommender", "systems", "utilize", "various", "techniques", "including", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y", ".", "Zhu", ",", "Y", ".", "C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches for opinion-based recommender systems utilize various techniques including text mining, information retrieval, sentiment analysis (see also multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y. C. Liang, R.C. Guan, D. Xu, (2019),, 21 (5 ): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 30], [30, 31], [31, 36], [37, 48], [49, 56], [57, 64], [65, 72], [73, 83], [84, 93], [94, 98], [99, 105], [105, 106], [107, 118], [119, 128], [128, 129], [130, 139], [140, 148], [149, 150], [150, 153], [154, 158], [159, 169], [170, 179], [180, 188], [188, 189], [190, 193], [194, 198], [199, 207], [208, 211], [211, 212], [213, 217], [217, 218], [219, 220], [220, 221], [222, 227], [227, 228], [229, 233], [234, 237], [237, 238], [239, 243], [244, 249], [249, 250], [251, 252], [252, 253], [254, 257], [257, 258], [259, 260], [260, 261], [262, 264], [265, 270], [270, 271], [272, 276], [277, 281], [281, 282], [283, 285], [286, 288], [288, 289], [290, 291], [291, 295], [295, 296], [296, 297], [297, 298], [299, 301], [302, 303], [303, 304], [305, 306], [306, 307], [308, 314], [314, 315]]}
{"doc_key": "ai-train-2", "ner": [[7, 7, "university"], [13, 14, "researcher"], [16, 17, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[13, 14, 7, 7, "physical", "", false, false], [13, 14, 7, 7, "role", "", false, false], [16, 17, 7, 7, "physical", "", false, false], [16, 17, 7, 7, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Advocates", "of", "procedural", "representation", "are", "concentrated", "at", "MIT", ",", "under", "the", "leadership", "of", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Advocates of procedural representation are concentrated at MIT, under the leadership of Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 9], [10, 12], [13, 23], [24, 38], [39, 42], [43, 55], [56, 58], [59, 62], [62, 63], [64, 69], [70, 73], [74, 84], [85, 87], [88, 94], [95, 101], [102, 105], [106, 113], [114, 120], [120, 121]]}
{"doc_key": "ai-train-3", "ner": [[10, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [22, 22, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 22, 22, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "to", "solve", "linear", "and", "non-linear", "problems", "numerically", "and", "to", "perform", "other", "numerical", "experiments", "using", "software", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps to solve linear and non-linear problems numerically and to perform other numerical experiments using software that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 21], [22, 28], [29, 32], [33, 43], [44, 52], [53, 64], [65, 68], [69, 71], [72, 79], [80, 85], [86, 95], [96, 107], [108, 113], [114, 122], [123, 127], [128, 130], [131, 137], [138, 148], [149, 153], [154, 160], [160, 161]]}
{"doc_key": "ai-train-5", "ner": [[3, 5, "algorithm"], [8, 10, "misc"], [11, 12, "researcher"], [16, 19, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 5, 11, 12, "origin", "", false, false], [8, 10, 11, 12, "origin", "", false, false], [11, 12, 16, 19, "physical", "", false, false], [11, 12, 16, 19, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["A", "variant", "of", "the", "backpropagation", "algorithm", "and", "the", "unsupervised", "method", "of", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "structures", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "A variant of the backpropagation algorithm and the unsupervised method of Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural structures, {{cite journal", "token2charspan": [[0, 1], [2, 9], [10, 12], [13, 16], [17, 32], [33, 42], [43, 46], [47, 50], [51, 63], [64, 70], [71, 73], [74, 79], [80, 86], [87, 90], [91, 101], [102, 104], [105, 108], [109, 119], [120, 122], [123, 130], [131, 134], [135, 137], [138, 142], [143, 145], [146, 151], [152, 156], [156, 157], [158, 164], [165, 174], [175, 181], [182, 192], [192, 193], [194, 195], [195, 196], [196, 200], [201, 208]]}
{"doc_key": "ai-train-6", "ner": [[4, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalent", "using", "the", "DCG", "notation", "."], "sentence-detokenized": "or equivalent using the DCG notation.", "token2charspan": [[0, 2], [3, 13], [14, 19], [20, 23], [24, 27], [28, 36], [36, 37]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 9, "algorithm"], [13, 14, "algorithm"], [17, 19, "algorithm"], [23, 24, "algorithm"], [26, 27, "algorithm"], [37, 40, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 9, "type-of", "", false, false], [0, 3, 13, 14, "usage", "part-of?", true, false], [13, 14, 17, 19, "compare", "", false, false], [23, 24, 17, 19, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "because", "they", "use", "competitive", "learning", "rather", "than", "error", "-correcting", "learning", ",", "such", "as", "back", "propagation", "with", "gradient", "descent", ")", "and", "they", "use", "adjacency", "functions", "to", "maintain", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks because they use competitive learning rather than error-correcting learning, such as back propagation with gradient descent) and they use adjacency functions to maintain the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 73], [74, 78], [79, 82], [83, 94], [95, 103], [104, 110], [111, 115], [116, 121], [121, 132], [133, 141], [141, 142], [143, 147], [148, 150], [151, 155], [156, 167], [168, 172], [173, 181], [182, 189], [189, 190], [191, 194], [195, 199], [200, 203], [204, 213], [214, 223], [224, 226], [227, 235], [236, 239], [240, 251], [252, 262], [263, 265], [266, 269], [270, 275], [276, 281], [281, 282]]}
{"doc_key": "ai-train-8", "ner": [[11, 16, "organisation"], [29, 31, "misc"], [38, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "a", "number", "of", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "have", "recommended", "that", "dynamic", "range", "measurements", "should", "be", "made", "in", "the", "presence", "of", "an", "audio", "signal", "and", "then", "filtered", "out", "in", "the", "noise", "floor", "measurements", "used", "to", "determine", "dynamic", "range", ".", "This", "avoids", "making", "dubious", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "muted", "circuits", "."], "sentence-detokenized": "Since the early 1990s, a number of authorities, including the Audio Engineering Society, have recommended that dynamic range measurements should be made in the presence of an audio signal and then filtered out in the noise floor measurements used to determine dynamic range. This avoids making dubious measurements based on the use of blank media or muted circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 24], [25, 31], [32, 34], [35, 46], [46, 47], [48, 57], [58, 61], [62, 67], [68, 79], [80, 87], [87, 88], [89, 93], [94, 105], [106, 110], [111, 118], [119, 124], [125, 137], [138, 144], [145, 147], [148, 152], [153, 155], [156, 159], [160, 168], [169, 171], [172, 174], [175, 180], [181, 187], [188, 191], [192, 196], [197, 205], [206, 209], [210, 212], [213, 216], [217, 222], [223, 228], [229, 241], [242, 246], [247, 249], [250, 259], [260, 267], [268, 273], [273, 274], [275, 279], [280, 286], [287, 293], [294, 301], [302, 314], [315, 320], [321, 323], [324, 327], [328, 331], [332, 334], [335, 340], [341, 346], [347, 349], [350, 355], [356, 364], [364, 365]]}
{"doc_key": "ai-train-9", "ner": [[5, 5, "misc"], [18, 19, "task"], [21, 22, "task"], [24, 25, "task"], [27, 28, "task"], [30, 30, "task"], [31, 34, "task"], [36, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[5, 5, 18, 19, "part-of", "concept_used_in", true, false], [5, 5, 21, 22, "part-of", "concept_used_in", false, false], [5, 5, 24, 25, "part-of", "concept_used_in", false, false], [5, 5, 27, 28, "part-of", "concept_used_in", false, false], [5, 5, 30, 30, "part-of", "concept_used_in", false, false], [5, 5, 31, 34, "part-of", "concept_used_in", false, false], [5, 5, 36, 38, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "techniques", "used", "to", "create", "eigenfaces", "and", "use", "them", "for", "recognition", "are", "also", "used", "in", "areas", "other", "than", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "speech", "recognition", ",", "sign", "language", "/", "gesture", "interpretation", "and", "medical", "imaging", "analysis", "."], "sentence-detokenized": "The techniques used to create eigenfaces and use them for recognition are also used in areas other than face recognition: handwriting recognition, lip reading, speech recognition, sign language/gesture interpretation and medical imaging analysis.", "token2charspan": [[0, 3], [4, 14], [15, 19], [20, 22], [23, 29], [30, 40], [41, 44], [45, 48], [49, 53], [54, 57], [58, 69], [70, 73], [74, 78], [79, 83], [84, 86], [87, 92], [93, 98], [99, 103], [104, 108], [109, 120], [120, 121], [122, 133], [134, 145], [145, 146], [147, 150], [151, 158], [158, 159], [160, 166], [167, 178], [178, 179], [180, 184], [185, 193], [193, 194], [194, 201], [202, 216], [217, 220], [221, 228], [229, 236], [237, 245], [245, 246]]}
{"doc_key": "ai-train-10", "ner": [[0, 5, "organisation"], [12, 16, "organisation"], [18, 18, "organisation"], [21, 25, "organisation"], [28, 32, "organisation"], [35, 38, "organisation"], [40, 45, "organisation"], [47, 47, "organisation"], [51, 55, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[12, 16, 0, 5, "part-of", "", false, false], [18, 18, 12, 16, "named", "", false, false], [21, 25, 0, 5, "part-of", "", false, false], [28, 32, 0, 5, "part-of", "", false, false], [35, 38, 0, 5, "part-of", "", false, false], [40, 45, 0, 5, "part-of", "", false, false], [47, 47, 40, 45, "named", "", false, false], [51, 55, 0, 5, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "(", "NSF", ")", "is", "the", "agency", "of", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "U.S.", "Department", "of", "Energy", ",", "the", "U.S.", "Department", "of", "Commerce", "NIST", ",", "the", "U.S.", "Department", "of", "Defense", ",", "the", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "that", "coordinates", "research", "to", "inform", "the", "deliberations", "of", "strategic", "planners", "."], "sentence-detokenized": "The National Science Foundation (NSF) is the agency of the National Aeronautics and Space Administration (NASA), the U.S. Department of Energy, the U.S. Department of Commerce NIST, the U.S. Department of Defense, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research that coordinates research to inform the deliberations of strategic planners.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 33], [33, 36], [36, 37], [38, 40], [41, 44], [45, 51], [52, 54], [55, 58], [59, 67], [68, 79], [80, 83], [84, 89], [90, 104], [105, 106], [106, 110], [110, 111], [111, 112], [113, 116], [117, 121], [122, 132], [133, 135], [136, 142], [142, 143], [144, 147], [148, 152], [153, 163], [164, 166], [167, 175], [176, 180], [180, 181], [182, 185], [186, 190], [191, 201], [202, 204], [205, 212], [212, 213], [214, 217], [218, 225], [226, 234], [235, 243], [244, 252], [253, 259], [260, 261], [261, 266], [266, 267], [268, 271], [272, 275], [276, 282], [283, 285], [286, 291], [292, 300], [301, 305], [306, 317], [318, 326], [327, 329], [330, 336], [337, 340], [341, 354], [355, 357], [358, 367], [368, 376], [376, 377]]}
{"doc_key": "ai-train-11", "ner": [[5, 6, "metrics"], [9, 11, "algorithm"], [14, 15, "researcher"], [22, 22, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 9, 11, "part-of", "", false, false], [14, 15, 22, 22, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "fast", "method", "for", "calculating", "maximum", "likelihood", "estimates", "of", "probit", "models", "was", "proposed", "by", "Ronald", "Fisher", "in", "1935", "as", "an", "appendix", "to", "Bliss", "'s", "work", "."], "sentence-detokenized": "A fast method for calculating maximum likelihood estimates of probit models was proposed by Ronald Fisher in 1935 as an appendix to Bliss's work.", "token2charspan": [[0, 1], [2, 6], [7, 13], [14, 17], [18, 29], [30, 37], [38, 48], [49, 58], [59, 61], [62, 68], [69, 75], [76, 79], [80, 88], [89, 91], [92, 98], [99, 105], [106, 108], [109, 113], [114, 116], [117, 119], [120, 128], [129, 131], [132, 137], [137, 139], [140, 144], [144, 145]]}
{"doc_key": "ai-train-12", "ner": [[11, 12, "product"], [14, 17, "product"], [19, 19, "organisation"], [21, 21, "product"], [25, 25, "organisation"], [27, 27, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[21, 21, 14, 17, "usage", "uses_software", false, false], [21, 21, 19, 19, "artifact", "", false, false], [21, 21, 27, 27, "named", "", false, false], [27, 27, 25, 25, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Several", "of", "these", "programs", "can", "be", "found", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "powers", "AltaVista", "'s", "BabelFish", "(", "changed", "to", "Yahoo", "'s", "Babelfish", "as", "of", "May", "9", ",", "2008", ")", "."], "sentence-detokenized": "Several of these programs can be found online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (changed to Yahoo's Babelfish as of May 9, 2008).", "token2charspan": [[0, 7], [8, 10], [11, 16], [17, 25], [26, 29], [30, 32], [33, 38], [39, 45], [45, 46], [47, 51], [52, 54], [55, 61], [62, 71], [72, 75], [76, 79], [80, 87], [88, 94], [95, 99], [100, 106], [107, 116], [116, 118], [119, 128], [129, 130], [130, 137], [138, 140], [141, 146], [146, 148], [149, 158], [159, 161], [162, 164], [165, 168], [169, 170], [170, 171], [172, 176], [176, 177], [177, 178]]}
{"doc_key": "ai-train-13", "ner": [[3, 3, "researcher"], [7, 9, "researcher"], [11, 12, "researcher"], [21, 23, "field"], [27, 28, "misc"], [24, 33, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 3, 21, 23, "related-to", "", true, false], [3, 3, 27, 28, "related-to", "", true, false], [3, 3, 24, 33, "related-to", "", true, false], [7, 9, 21, 23, "related-to", "", true, false], [7, 9, 27, 28, "related-to", "", true, false], [7, 9, 24, 33, "related-to", "", true, false], [11, 12, 21, 23, "related-to", "", true, false], [11, 12, 27, 28, "related-to", "", true, false], [11, 12, 24, 33, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "Hut", ",", "together", "with", "J\u00fcrgen", "Schmidt", "Huber", "and", "Shane", "Leger", ",", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-motivated", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, Hut, together with J\u00fcrgen Schmidt Huber and Shane Leger, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 12], [12, 13], [14, 22], [23, 27], [28, 34], [35, 42], [43, 48], [49, 52], [53, 58], [59, 64], [64, 65], [66, 75], [76, 79], [80, 89], [90, 91], [92, 104], [105, 111], [112, 114], [115, 125], [126, 133], [134, 146], [147, 152], [153, 155], [156, 165], [166, 177], [178, 184], [185, 188], [189, 195], [195, 205], [206, 219], [220, 228], [228, 229]]}
{"doc_key": "ai-train-14", "ner": [[14, 14, "metrics"], [16, 22, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[14, 14, 16, 22, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "of", "doing", "this", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "measure", "."], "sentence-detokenized": "The most common way of doing this is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 28], [29, 33], [34, 36], [37, 39], [40, 43], [44, 47], [48, 50], [50, 51], [51, 57], [58, 63], [64, 65], [65, 71], [71, 72], [72, 80], [81, 91], [92, 95], [96, 103], [104, 114], [114, 115], [116, 123], [123, 124]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 13, "programlang"], [15, 15, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 13, "related-to", "", false, false], [0, 0, 15, 15, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "learning", "solutions", ",", "models", "and", "algorithms", "that", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013"], "sentence-detokenized": "RapidMiner provides learning solutions, models and algorithms that can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 38], [38, 39], [40, 46], [47, 50], [51, 61], [62, 66], [67, 70], [71, 73], [74, 82], [83, 88], [89, 90], [91, 94], [95, 101], [102, 109], [109, 110], [111, 116], [117, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 143], [144, 152], [153, 157]]}
{"doc_key": "ai-train-16", "ner": [[6, 8, "programlang"], [11, 12, "product"]], "ner_mapping_to_source": [4, 5], "relations": [[11, 12, 6, 8, "general-affiliation", "", true, false]], "relations_mapping_to_source": [4], "sentence": ["However", ",", "the", "most", "recent", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", "began", "development", "in", "1997", "and", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "particularly", "for", "education", "and", "research", "."], "sentence-detokenized": "However, the most recent fully Java-based version (Weka 3) began development in 1997 and is now used in many different application areas, particularly for education and research.", "token2charspan": [[0, 7], [7, 8], [9, 12], [13, 17], [18, 24], [25, 30], [31, 35], [35, 36], [36, 41], [42, 49], [50, 51], [51, 55], [56, 57], [57, 58], [59, 64], [65, 76], [77, 79], [80, 84], [85, 88], [89, 91], [92, 95], [96, 100], [101, 103], [104, 108], [109, 118], [119, 130], [131, 136], [136, 137], [138, 150], [151, 154], [155, 164], [165, 168], [169, 177], [177, 178]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [12, 22, "misc"], [25, 28, "misc"], [31, 38, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 22, 0, 0, "topic", "", false, false], [12, 22, 25, 28, "win-defeat", "", false, false], [25, 28, 31, 38, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Yurisko", "made", "many", "interesting", "discoveries", "and", "enjoyed", "great", "acclaim", "for", "his", "paper", "Heuretics", ".", "his", "paper", "\"", "Theory", "and", "Study", "of", "Heuristic", "Rules", "\"", "won", "the", "best", "paper", "award", "at", "the", "1982", "Society", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "."], "sentence-detokenized": "Yurisko made many interesting discoveries and enjoyed great acclaim for his paper Heuretics. his paper \"Theory and Study of Heuristic Rules\" won the best paper award at the 1982 Society for the Advancement of Artificial Intelligence.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 53], [54, 59], [60, 67], [68, 71], [72, 75], [76, 81], [82, 91], [91, 92], [93, 96], [97, 102], [103, 104], [104, 110], [111, 114], [115, 120], [121, 123], [124, 133], [134, 139], [139, 140], [141, 144], [145, 148], [149, 153], [154, 159], [160, 165], [166, 168], [169, 172], [173, 177], [178, 185], [186, 189], [190, 193], [194, 205], [206, 208], [209, 219], [220, 232], [232, 233]]}
{"doc_key": "ai-train-18", "ner": [[11, 13, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "order", "to", "take", "into", "account", "multiple", "entities", ",", "a", "separate", "Hinge", "loss", "is", "calculated", "for", "each", "capsule", "."], "sentence-detokenized": "In order to take into account multiple entities, a separate Hinge loss is calculated for each capsule.", "token2charspan": [[0, 2], [3, 8], [9, 11], [12, 16], [17, 21], [22, 29], [30, 38], [39, 47], [47, 48], [49, 50], [51, 59], [60, 65], [66, 70], [71, 73], [74, 84], [85, 88], [89, 93], [94, 101], [101, 102]]}
{"doc_key": "ai-train-19", "ner": [[8, 9, "product"], [11, 12, "product"], [14, 15, "product"], [17, 18, "product"], [6, 21, "product"], [23, 24, "product"], [33, 38, "product"], [41, 42, "product"], [44, 45, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 9, 23, 24, "type-of", "", false, false], [11, 12, 23, 24, "type-of", "", false, false], [14, 15, 23, 24, "type-of", "", false, false], [17, 18, 23, 24, "type-of", "", false, false], [6, 21, 23, 24, "type-of", "", false, false], [41, 42, 33, 38, "type-of", "", false, false], [44, 45, 33, 38, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "conversational", "assistants", "such", "as", "Apple", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "Bixby", ",", "voice", "portals", "can", "now", "be", "accessed", "via", "mobile", "devices", "and", "far", "-", "field", "voice", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of conversational assistants such as Apple Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung Bixby, voice portals can now be accessed via mobile devices and far-field voice smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 33], [34, 44], [45, 49], [50, 52], [53, 58], [59, 63], [63, 64], [65, 71], [72, 77], [77, 78], [79, 85], [86, 95], [95, 96], [97, 106], [107, 114], [115, 118], [119, 126], [127, 132], [132, 133], [134, 139], [140, 147], [148, 151], [152, 155], [156, 158], [159, 167], [168, 171], [172, 178], [179, 186], [187, 190], [191, 194], [194, 195], [195, 200], [201, 206], [207, 212], [213, 221], [222, 226], [227, 229], [230, 236], [237, 241], [242, 245], [246, 252], [253, 257], [257, 258]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 15, "algorithm"], [17, 17, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 3, "type-of", "", false, false], [9, 11, 2, 3, "type-of", "", false, false], [13, 15, 2, 3, "type-of", "", false, false], [17, 17, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "Naive", "Bayes", "classifiers", ",", "support", "vector", "machines", ",", "mixtures", "of", "Gaussians", "and", "networks", "."], "sentence-detokenized": "Examples of supervised learning are Naive Bayes classifiers, support vector machines, mixtures of Gaussians and networks.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 41], [42, 47], [48, 59], [59, 60], [61, 68], [69, 75], [76, 84], [84, 85], [86, 94], [95, 97], [98, 107], [108, 111], [112, 120], [120, 121]]}
{"doc_key": "ai-train-21", "ner": [[3, 5, "algorithm"], [25, 27, "algorithm"], [28, 28, "task"], [33, 34, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 5, 25, 27, "part-of", "", true, false], [33, 34, 28, 28, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["One", "can", "use", "the", "OSD", "algorithm", "to", "derive", "bounds", "on", "maths", "O", "(", "sqrt", "{", "T}", ")", "/", "math", "regret", "for", "the", "online", "version", "of", "support", "vector", "machine", "classification", ",", "which", "uses", "the", "hinge", "loss", "maths", "v", "_t", "(", "w", ")", "=", "\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "math"], "sentence-detokenized": "One can use the OSD algorithm to derive bounds on maths O(sqrt {T})/math regret for the online version of support vector machine classification, which uses the hinge loss maths v _t (w) = \\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / math", "token2charspan": [[0, 3], [4, 7], [8, 11], [12, 15], [16, 19], [20, 29], [30, 32], [33, 39], [40, 46], [47, 49], [50, 55], [56, 57], [57, 58], [58, 62], [63, 64], [64, 66], [66, 67], [67, 68], [68, 72], [73, 79], [80, 83], [84, 87], [88, 94], [95, 102], [103, 105], [106, 113], [114, 120], [121, 128], [129, 143], [143, 144], [145, 150], [151, 155], [156, 159], [160, 165], [166, 170], [171, 176], [177, 178], [179, 181], [182, 183], [183, 184], [184, 185], [186, 187], [188, 189], [190, 193], [193, 194], [195, 196], [196, 197], [197, 198], [199, 200], [201, 202], [203, 204], [205, 207], [208, 209], [209, 210], [210, 211], [212, 216], [217, 218], [219, 221], [221, 222], [222, 224], [225, 226], [227, 231]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [16, 17, "task"], [19, 20, "task"], [22, 25, "task"], [27, 28, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robot", "mapping", "and", "navigation", ",", "image", "stitching", ",", "3D", "modelling", ",", "gesture", "recognition", ",", "video", "tracking", ",", "individual", "identification", "of", "wildlife", "and", "matched", "movement", "."], "sentence-detokenized": "Applications include object recognition, robot mapping and navigation, image stitching, 3D modelling, gesture recognition, video tracking, individual identification of wildlife and matched movement.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 46], [47, 54], [55, 58], [59, 69], [69, 70], [71, 76], [77, 86], [86, 87], [88, 90], [91, 100], [100, 101], [102, 109], [110, 121], [121, 122], [123, 128], [129, 137], [137, 138], [139, 149], [150, 164], [165, 167], [168, 176], [177, 180], [181, 188], [189, 197], [197, 198]]}
{"doc_key": "ai-train-23", "ner": [[7, 8, "task"], [13, 14, "university"], [16, 18, "university"], [20, 21, "university"], [23, 24, "university"], [26, 31, "university"], [33, 35, "university"], [38, 39, "university"], [41, 42, "university"], [44, 49, "university"], [51, 51, "university"], [54, 58, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[7, 8, 13, 14, "related-to", "", true, false], [7, 8, 16, 18, "related-to", "", true, false], [7, 8, 20, 21, "related-to", "", true, false], [7, 8, 23, 24, "related-to", "", true, false], [7, 8, 26, 31, "related-to", "", true, false], [7, 8, 33, 35, "related-to", "", true, false], [7, 8, 38, 39, "related-to", "", true, false], [7, 8, 41, 42, "related-to", "", true, false], [7, 8, 44, 49, "related-to", "", true, false], [7, 8, 51, 51, "related-to", "", true, false], [7, 8, 54, 58, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Several", "groups", "and", "companies", "are", "working", "on", "posture", "estimates", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "Several groups and companies are working on posture estimates, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Science and Technology (NUST) and University of California, Irvine.", "token2charspan": [[0, 7], [8, 14], [15, 18], [19, 28], [29, 32], [33, 40], [41, 43], [44, 51], [52, 61], [61, 62], [63, 72], [73, 79], [80, 82], [83, 88], [89, 99], [99, 100], [101, 109], [110, 116], [117, 127], [127, 128], [129, 132], [133, 145], [145, 146], [147, 155], [156, 166], [166, 167], [168, 178], [179, 181], [182, 192], [192, 193], [194, 197], [198, 203], [203, 204], [205, 215], [216, 218], [219, 226], [226, 227], [228, 233], [234, 242], [243, 248], [248, 249], [250, 253], [254, 260], [260, 261], [262, 270], [271, 281], [282, 284], [285, 292], [293, 296], [297, 307], [308, 309], [309, 313], [313, 314], [315, 318], [319, 329], [330, 332], [333, 343], [343, 344], [345, 351], [351, 352]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "Sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "predict", "K", "independent", "probability", "values", "for", "maths", "0,1", "/", "math", "."], "sentence-detokenized": "The Sigmoid function Cross entropy loss is used to predict K independent probability values for maths 0,1/math.", "token2charspan": [[0, 3], [4, 11], [12, 20], [21, 26], [27, 34], [35, 39], [40, 42], [43, 47], [48, 50], [51, 58], [59, 60], [61, 72], [73, 84], [85, 91], [92, 95], [96, 101], [102, 105], [105, 106], [106, 110], [110, 111]]}
{"doc_key": "ai-train-25", "ner": [[11, 12, "misc"], [15, 15, "field"], [17, 17, "field"], [19, 22, "university"], [23, 25, "country"], [28, 30, "misc"], [31, 36, "university"], [37, 38, "country"], [4, 6, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[11, 12, 15, 15, "topic", "", false, false], [11, 12, 17, 17, "topic", "", false, false], [11, 12, 19, 22, "physical", "", true, false], [19, 22, 23, 25, "physical", "", false, false], [28, 30, 31, 36, "physical", "", true, false], [31, 36, 37, 38, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Before", "becoming", "a", "professor", "at", "Cambridge", "University", ",", "he", "held", "the", "John", "Bernoulli", "Chair", "in", "Mathematics", "and", "Informatics", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "."], "sentence-detokenized": "Before becoming a professor at Cambridge University, he held the John Bernoulli Chair in Mathematics and Informatics at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan.", "token2charspan": [[0, 6], [7, 15], [16, 17], [18, 27], [28, 30], [31, 40], [41, 51], [51, 52], [53, 55], [56, 60], [61, 64], [65, 69], [70, 79], [80, 85], [86, 88], [89, 100], [101, 104], [105, 116], [117, 119], [120, 123], [124, 134], [135, 137], [138, 147], [148, 150], [151, 154], [155, 166], [167, 170], [171, 174], [175, 182], [183, 190], [191, 196], [197, 199], [200, 203], [204, 209], [210, 219], [220, 222], [223, 233], [234, 236], [237, 242], [242, 243]]}
{"doc_key": "ai-train-26", "ner": [[5, 5, "algorithm"], [10, 13, "algorithm"], [15, 15, "algorithm"], [20, 21, "researcher"], [18, 24, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 5, 10, 13, "usage", "", true, false], [10, 13, 20, 21, "origin", "", false, false], [10, 13, 18, 24, "origin", "", false, false], [15, 15, 10, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "particularly", "used", "for", "recurrent", "neural", "networks", "is", "the", "Long", "Short", "Term", "Memory", "(", "LSTM", ")", "network", "proposed", "by", "Sepp", "Hochreiter", "and", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another technique particularly used for recurrent neural networks is the Long Short Term Memory (LSTM) network proposed by Sepp Hochreiter and J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 17], [18, 30], [31, 35], [36, 39], [40, 49], [50, 56], [57, 65], [66, 68], [69, 72], [73, 77], [78, 83], [84, 88], [89, 95], [96, 97], [97, 101], [101, 102], [103, 110], [111, 119], [120, 122], [123, 127], [128, 138], [139, 142], [143, 149], [150, 161], [162, 164], [165, 169], [169, 170]]}
{"doc_key": "ai-train-27", "ner": [[4, 6, "programlang"], [9, 10, "product"], [15, 15, "product"], [47, 47, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[9, 10, 4, 6, "general-affiliation", "", false, false], [9, 10, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "+", "+", "interpreter", "(", "CI", "NT", "until", "version", "5.34", ",", "Cling", "after", "version", "6", ")", "makes", "this", "package", "very", "versatile", "as", "it", "can", "be", "used", "in", "an", "interactive", ",", "scripted", "and", "compiled", "mode", "in", "a", "similar", "way", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C + + interpreter (CINT until version 5.34, Cling after version 6) makes this package very versatile as it can be used in an interactive, scripted and compiled mode in a similar way to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [21, 22], [23, 24], [25, 36], [37, 38], [38, 40], [40, 42], [43, 48], [49, 56], [57, 61], [61, 62], [63, 68], [69, 74], [75, 82], [83, 84], [84, 85], [86, 91], [92, 96], [97, 104], [105, 109], [110, 119], [120, 122], [123, 125], [126, 129], [130, 132], [133, 137], [138, 140], [141, 143], [144, 155], [155, 156], [157, 165], [166, 169], [170, 178], [179, 183], [184, 186], [187, 188], [189, 196], [197, 200], [201, 203], [204, 214], [215, 223], [224, 228], [229, 231], [232, 238], [238, 239]]}
{"doc_key": "ai-train-28", "ner": [[0, 3, "product"], [21, 23, "field"], [27, 29, "task"], [31, 33, "task"], [35, 36, "task"], [38, 39, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 3, 21, 23, "related-to", "", false, false], [27, 29, 21, 23, "part-of", "", false, false], [31, 33, 21, 23, "part-of", "", false, false], [35, 36, 21, 23, "part-of", "", false, false], [38, 39, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Speech", "user", "interfaces", "for", "interpreting", "and", "managing", "dialogue", "states", "are", "challenging", "to", "design", "because", "of", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "core", "reasoning", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "Speech user interfaces for interpreting and managing dialogue states are challenging to design because of the inherent difficulty of integrating complex natural language processing tasks such as core reasoning resolution, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 6], [7, 11], [12, 22], [23, 26], [27, 39], [40, 43], [44, 52], [53, 61], [62, 68], [69, 72], [73, 84], [85, 87], [88, 94], [95, 102], [103, 105], [106, 109], [110, 118], [119, 129], [130, 132], [133, 144], [145, 152], [153, 160], [161, 169], [170, 180], [181, 186], [187, 191], [192, 194], [195, 199], [200, 209], [210, 220], [220, 221], [222, 227], [228, 234], [235, 246], [246, 247], [248, 259], [260, 269], [270, 273], [274, 282], [283, 293], [293, 294]]}
{"doc_key": "ai-train-29", "ner": [[5, 5, "algorithm"], [9, 12, "algorithm"], [21, 24, "researcher"], [25, 30, "organisation"], [36, 37, "field"], [39, 40, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 5, 21, 24, "origin", "", false, false], [5, 5, 36, 37, "part-of", "", false, false], [5, 5, 39, 40, "part-of", "", false, false], [9, 12, 21, 24, "origin", "", false, false], [9, 12, 36, 37, "part-of", "", false, false], [9, 12, 39, 40, "part-of", "", false, false], [21, 24, 25, 30, "physical", "", false, false], [21, 24, 25, 30, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "feed", "-", "forward", "neural", "networks", "developed", "by", "the", "research", "group", "of", "J\u00fcrgen", "Schmidt", "Huber", "at", "the", "Swiss", "artificial", "intelligence", "laboratory", "IDSIA", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep feed-forward neural networks developed by the research group of J\u00fcrgen Schmidt Huber at the Swiss artificial intelligence laboratory IDSIA won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 62], [62, 63], [63, 70], [71, 77], [78, 86], [87, 96], [97, 99], [100, 103], [104, 112], [113, 118], [119, 121], [122, 128], [129, 136], [137, 142], [143, 145], [146, 149], [150, 155], [156, 166], [167, 179], [180, 190], [191, 196], [197, 200], [201, 206], [207, 220], [221, 233], [234, 236], [237, 244], [245, 256], [257, 260], [261, 268], [269, 277], [277, 278]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [9, 10, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 9, 10, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "voice", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and voice.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 105], [105, 106]]}
{"doc_key": "ai-train-31", "ner": [[11, 17, "misc"], [8, 9, "field"], [18, 21, "university"], [28, 31, "field"], [33, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[11, 17, 8, 9, "topic", "topic_of_award", false, false], [11, 17, 18, 21, "origin", "", true, false], [28, 31, 33, 36, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "a", "degree", "in", "psychology", "from", "S.", "V.", "della", "laurea", "ad", "honorem", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "doctorate", "in", "industrial", "design", "and", "engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, a degree in psychology from S. V. della laurea ad honorem from the University of Padua in 1995 and a doctorate in industrial design and engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 35], [36, 42], [43, 45], [46, 56], [57, 61], [62, 64], [65, 67], [68, 73], [74, 80], [81, 83], [84, 91], [92, 96], [97, 100], [101, 111], [112, 114], [115, 120], [121, 123], [124, 128], [129, 132], [133, 134], [135, 144], [145, 147], [148, 158], [159, 165], [166, 169], [170, 181], [182, 186], [187, 192], [193, 203], [204, 206], [207, 217], [217, 218]]}
{"doc_key": "ai-train-32", "ner": [[6, 7, "researcher"], [11, 16, "organisation"], [17, 18, "location"], [20, 20, "researcher"], [32, 32, "misc"], [48, 52, "misc"], [69, 70, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[6, 7, 11, 16, "physical", "", false, false], [6, 7, 11, 16, "role", "", false, false], [11, 16, 17, 18, "physical", "", false, false], [20, 20, 32, 32, "related-to", "works_with", true, false], [20, 20, 48, 52, "related-to", "works_with", true, false], [20, 20, 69, 70, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", "who", "had", "impaired", "multiplicative", "function", "but", "preserved", "subtractive", "function", "(", "associated", "with", "lesions", "in", "the", "inferior", "parietal", "lobe", ")", "and", "others", "who", "had", "impaired", "subtractive", "function", "but", "preserved", "multiplicative", "function", "(", "associated", "with", "lesions", "in", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Together with long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe who had impaired multiplicative function but preserved subtractive function (associated with lesions in the inferior parietal lobe) and others who had impaired subtractive function but preserved multiplicative function (associated with lesions in the intraparietal sulcus ).", "token2charspan": [[0, 8], [9, 13], [14, 18], [18, 19], [19, 23], [24, 36], [37, 44], [45, 50], [50, 51], [52, 53], [54, 65], [66, 68], [69, 72], [73, 78], [78, 79], [79, 90], [91, 99], [100, 102], [103, 108], [108, 109], [110, 117], [118, 122], [123, 133], [134, 142], [143, 147], [148, 155], [156, 158], [159, 168], [169, 176], [177, 179], [180, 183], [184, 192], [193, 197], [198, 201], [202, 205], [206, 214], [215, 229], [230, 238], [239, 242], [243, 252], [253, 264], [265, 273], [274, 275], [275, 285], [286, 290], [291, 298], [299, 301], [302, 305], [306, 314], [315, 323], [324, 328], [328, 329], [330, 333], [334, 340], [341, 344], [345, 348], [349, 357], [358, 369], [370, 378], [379, 382], [383, 392], [393, 407], [408, 416], [417, 418], [418, 428], [429, 433], [434, 441], [442, 444], [445, 448], [449, 462], [463, 469], [470, 471], [471, 472]]}
{"doc_key": "ai-train-33", "ner": [[6, 7, "product"], [12, 13, "misc"], [15, 16, "misc"], [26, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 13, 6, 7, "topic", "", false, false], [15, 16, 6, 7, "topic", "", false, false], [26, 28, 6, 7, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "AI", "robots", "in", "films", "such", "as", "Artificial", "Intelligence", "and", "Machine", "House", ",", "as", "well", "as", "the", "2016", "TV", "adaptation", "of", "Westworld", ",", "have", "made", "audiences", "sympathetic", "to", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of AI robots in films such as Artificial Intelligence and Machine House, as well as the 2016 TV adaptation of Westworld, have made audiences sympathetic to the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 46], [47, 53], [54, 56], [57, 62], [63, 67], [68, 70], [71, 81], [82, 94], [95, 98], [99, 106], [107, 112], [112, 113], [114, 116], [117, 121], [122, 124], [125, 128], [129, 133], [134, 136], [137, 147], [148, 150], [151, 160], [160, 161], [162, 166], [167, 171], [172, 181], [182, 193], [194, 196], [197, 200], [201, 207], [208, 218], [218, 219]]}
{"doc_key": "ai-train-34", "ner": [[6, 7, "field"], [9, 11, "algorithm"], [13, 14, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 6, 7, "part-of", "", false, false], [13, 14, 6, 7, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "two", "main", "methods", "used", "in", "unsupervised", "learning", "are", "Principal", "Component", "Analysis", "and", "Cluster", "Analysis", "."], "sentence-detokenized": "The two main methods used in unsupervised learning are Principal Component Analysis and Cluster Analysis.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 20], [21, 25], [26, 28], [29, 41], [42, 50], [51, 54], [55, 64], [65, 74], [75, 83], [84, 87], [88, 95], [96, 104], [104, 105]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [21, 23, "misc"], [28, 29, "misc"], [31, 33, "person"], [38, 39, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[21, 23, 0, 3, "artifact", "", false, false], [28, 29, 0, 3, "artifact", "", false, false], [28, 29, 31, 33, "role", "director_of", false, false], [28, 29, 38, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "more", "prominently", "in", "special", "venues", "to", "impress", "audiences", ",", "with", "The", "Magician", "'s", "Journey", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "being", "obvious", "examples", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films more prominently in special venues to impress audiences, with The Magician's Journey (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson) being obvious examples.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 55], [56, 67], [68, 70], [71, 78], [79, 85], [86, 88], [89, 96], [97, 106], [106, 107], [108, 112], [113, 116], [117, 125], [125, 127], [128, 135], [136, 137], [137, 141], [141, 142], [143, 146], [147, 154], [155, 157], [158, 159], [159, 166], [167, 171], [172, 179], [179, 180], [181, 185], [185, 186], [187, 195], [196, 203], [204, 211], [211, 212], [213, 218], [219, 226], [227, 235], [235, 236]]}
{"doc_key": "ai-train-36", "ner": [[12, 14, "field"], [19, 24, "task"], [17, 27, "task"], [29, 29, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[19, 24, 12, 14, "part-of", "", false, false], [17, 27, 12, 14, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "the", "field", "of", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "analysis", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in the field of natural language processing for tasks such as part-of-speech tagging and syntactic analysis (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 57], [58, 63], [64, 66], [67, 74], [75, 83], [84, 94], [95, 98], [99, 104], [105, 109], [110, 112], [113, 117], [117, 118], [118, 120], [120, 121], [121, 127], [128, 135], [136, 139], [140, 149], [150, 158], [159, 160], [160, 167], [167, 168], [169, 173], [173, 174], [174, 175]]}
{"doc_key": "ai-train-37", "ner": [[2, 4, "product"], [9, 12, "organisation"], [15, 16, "organisation"], [17, 18, "country"], [21, 26, "product"], [29, 30, "researcher"], [39, 39, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 12, 2, 4, "role", "introduces_to_market", true, false], [15, 16, 2, 4, "role", "introduces_to_market", true, false], [15, 16, 17, 18, "physical", "", false, false], [21, 26, 39, 39, "related-to", "sold_to", true, false], [29, 30, 21, 26, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", ".", "Invented", "by", "KUKA", "Robotics", "in", "Germany", ",", "the", "programmable", "universal", "machine", "for", "assembly", "was", "invented", "by", "Victor", "Scheinman", "in", "1976", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced in 1963 by Fuji Yusoki Kogyo. Invented by KUKA Robotics in Germany, the programmable universal machine for assembly was invented by Victor Scheinman in 1976 and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [71, 72], [73, 81], [82, 84], [85, 89], [90, 98], [99, 101], [102, 109], [109, 110], [111, 114], [115, 127], [128, 137], [138, 145], [146, 149], [150, 158], [159, 162], [163, 171], [172, 174], [175, 181], [182, 191], [192, 194], [195, 199], [200, 203], [204, 207], [208, 214], [215, 218], [219, 223], [224, 226], [227, 236], [236, 237]]}
{"doc_key": "ai-train-38", "ner": [[8, 9, "conference"], [11, 11, "researcher"], [20, 20, "field"], [35, 36, "researcher"], [43, 44, "researcher"], [54, 54, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[11, 11, 8, 9, "role", "president_of", false, false], [11, 11, 35, 36, "role", "colleagues", false, false], [20, 20, 54, 54, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "the", "mid-1990s", ",", "during", "his", "presidency", "of", "the", "AAAI", ",", "Hayes", "began", "a", "series", "of", "attacks", "on", "critics", "of", "AI", ",", "mostly", "delivered", "in", "a", "sarcastic", "tone", ",", "and", "(", "together", "with", "his", "colleague", "Kenneth", "Ford", ")", "invented", "a", "prize", "named", "after", "Simon", "Newcomb", "for", "the", "most", "absurd", "arguments", "against", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "In the mid-1990s, during his presidency of the AAAI, Hayes began a series of attacks on critics of AI, mostly delivered in a sarcastic tone, and (together with his colleague Kenneth Ford) invented a prize named after Simon Newcomb for the most absurd arguments against the possibility of AI.", "token2charspan": [[0, 2], [3, 6], [7, 16], [16, 17], [18, 24], [25, 28], [29, 39], [40, 42], [43, 46], [47, 51], [51, 52], [53, 58], [59, 64], [65, 66], [67, 73], [74, 76], [77, 84], [85, 87], [88, 95], [96, 98], [99, 101], [101, 102], [103, 109], [110, 119], [120, 122], [123, 124], [125, 134], [135, 139], [139, 140], [141, 144], [145, 146], [146, 154], [155, 159], [160, 163], [164, 173], [174, 181], [182, 186], [186, 187], [188, 196], [197, 198], [199, 204], [205, 210], [211, 216], [217, 222], [223, 230], [231, 234], [235, 238], [239, 243], [244, 250], [251, 260], [261, 268], [269, 272], [273, 284], [285, 287], [288, 290], [290, 291]]}
{"doc_key": "ai-train-39", "ner": [[17, 19, "algorithm"], [41, 42, "algorithm"], [54, 56, "algorithm"], [60, 63, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 19, 41, 42, "named", "same", false, false], [54, 56, 17, 19, "type-of", "", false, false], [60, 63, 17, 19, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["The", "optimal", "value", "of", "math", "\\", "alpha", "/", "math", "can", "be", "found", "by", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimises", "S", ",", "usually", "using", "a", "line", "search", "in", "the", "interval", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "backtracking", "line", "search", ",", "such", "as", "the", "Armijo", "line", "search", "."], "sentence-detokenized": "The optimal value of math \\ alpha / math can be found by using a line search algorithm, i.e. the size of math \\ alpha / math is determined by finding the value that minimises S, usually using a line search in the interval math0 \\ alpha 1 / math or a backtracking line search, such as the Armijo line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 20], [21, 25], [26, 27], [28, 33], [34, 35], [36, 40], [41, 44], [45, 47], [48, 53], [54, 56], [57, 62], [63, 64], [65, 69], [70, 76], [77, 86], [86, 87], [88, 92], [93, 96], [97, 101], [102, 104], [105, 109], [110, 111], [112, 117], [118, 119], [120, 124], [125, 127], [128, 138], [139, 141], [142, 149], [150, 153], [154, 159], [160, 164], [165, 174], [175, 176], [176, 177], [178, 185], [186, 191], [192, 193], [194, 198], [199, 205], [206, 208], [209, 212], [213, 221], [222, 227], [228, 229], [230, 235], [236, 237], [238, 239], [240, 244], [245, 247], [248, 249], [250, 262], [263, 267], [268, 274], [274, 275], [276, 280], [281, 283], [284, 287], [288, 294], [295, 299], [300, 306], [306, 307]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 10, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "breadth", "-", "first", "search", "and", "depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "these", "results", "represent", "expert", "systems", "that", "embody", "a", "great", "deal", "of", "technical", "knowledge", ",", "but", "do", "not", "shed", "much", "light", "on", "the", "mental", "processes", "humans", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses breadth-first search and depth-first search techniques, but ultimately concludes that these results represent expert systems that embody a great deal of technical knowledge, but do not shed much light on the mental processes humans use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 67], [67, 68], [69, 72], [73, 83], [84, 93], [94, 98], [99, 104], [105, 112], [113, 122], [123, 129], [130, 137], [138, 142], [143, 149], [150, 151], [152, 157], [158, 162], [163, 165], [166, 175], [176, 185], [185, 186], [187, 190], [191, 193], [194, 197], [198, 202], [203, 207], [208, 213], [214, 216], [217, 220], [221, 227], [228, 237], [238, 244], [245, 248], [249, 251], [252, 257], [258, 262], [263, 270], [270, 271]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [3, 4, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deal", "with", "how", "to", "use", "computers", "to", "understand", "or", "create", "spoken", "language", "."], "sentence-detokenized": "Speech recognition and speech synthesis deal with how to use computers to understand or create spoken language.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 44], [45, 49], [50, 53], [54, 56], [57, 60], [61, 70], [71, 73], [74, 84], [85, 87], [88, 94], [95, 101], [102, 110], [110, 111]]}
{"doc_key": "ai-train-42", "ner": [[55, 74, "algorithm"], [75, 75, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^", "{", "*}", "/", "math", "is", "usually", "estimated", "using", "either", "maximum", "likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "maximum", "a", "posteriori", "(", "math\\", "theta", "^", "{", "*}", "/", "math", ")", ".", "/", "maths", "is", "usually", "estimated", "using", "either", "the", "maximum", "likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "maximum", "a", "posteriori", "(", "math", "\\", "theta", "^", "{", "*}", "=", "theta", "^", "{", "MAP", "}", "/", "maths", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta ^ {*} / math is usually estimated using either maximum likelihood (math\\ theta ^ {*} = theta ^ {ML} / math) or maximum a posteriori (math\\ theta ^ {*} / math). / maths is usually estimated using either the maximum likelihood (math\\ theta ^ {*} = theta ^ {ML} / math) or maximum a posteriori (math\\ theta ^ {*} = theta ^ {MAP} / maths) procedure.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [17, 18], [19, 20], [20, 22], [23, 24], [25, 29], [30, 32], [33, 40], [41, 50], [51, 56], [57, 63], [64, 71], [72, 82], [83, 84], [84, 88], [88, 89], [90, 95], [96, 97], [98, 99], [99, 101], [102, 103], [104, 109], [110, 111], [112, 113], [113, 115], [115, 116], [117, 118], [119, 123], [123, 124], [125, 127], [128, 135], [136, 137], [138, 148], [149, 150], [150, 155], [156, 161], [162, 163], [164, 165], [165, 167], [168, 169], [170, 174], [174, 175], [175, 176], [177, 178], [179, 184], [185, 187], [188, 195], [196, 205], [206, 211], [212, 218], [219, 222], [223, 230], [231, 241], [242, 243], [243, 247], [247, 248], [249, 254], [255, 256], [257, 258], [258, 260], [261, 262], [263, 268], [269, 270], [271, 272], [272, 274], [274, 275], [276, 277], [278, 282], [282, 283], [284, 286], [287, 294], [295, 296], [297, 307], [308, 309], [309, 313], [313, 314], [315, 320], [321, 322], [323, 324], [324, 326], [327, 328], [329, 334], [335, 336], [337, 338], [338, 341], [341, 342], [343, 344], [345, 350], [350, 351], [352, 361], [361, 362]]}
{"doc_key": "ai-train-43", "ner": [[9, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "widely", "used", "languages", "use", "the", "open", "source", "eSpeak", "synthesiser", "for", "speech", ";", "producing", "a", "robotic", ",", "clunky", "sound", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less widely used languages use the open source eSpeak synthesiser for speech; producing a robotic, clunky sound that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 16], [17, 21], [22, 31], [32, 35], [36, 39], [40, 44], [45, 51], [52, 58], [59, 70], [71, 74], [75, 81], [81, 82], [83, 92], [93, 94], [95, 102], [102, 103], [104, 110], [111, 116], [117, 121], [122, 125], [126, 128], [129, 138], [139, 141], [142, 152], [152, 153]]}
{"doc_key": "ai-train-44", "ner": [[1, 20, "programlang"], [39, 40, "programlang"], [42, 42, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 20, 39, 40, "compare", "", false, false], [1, 20, 42, 42, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "R", "is", "primarily", "used", "by", "statisticians", "and", "other", "practitioners", "who", "require", "a", "statistical", "computing", "and", "software", "development", "environment", ",", "R", "can", "also", "be", "operated", "as", "a", "general", "matrix", "computing", "toolbox", "-", "with", "performance", "benchmarks", "comparable", "to", "those", "of", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although R is primarily used by statisticians and other practitioners who require a statistical computing and software development environment, R can also be operated as a general matrix computing toolbox - with performance benchmarks comparable to those of GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 10], [11, 13], [14, 23], [24, 28], [29, 31], [32, 45], [46, 49], [50, 55], [56, 69], [70, 73], [74, 81], [82, 83], [84, 95], [96, 105], [106, 109], [110, 118], [119, 130], [131, 142], [142, 143], [144, 145], [146, 149], [150, 154], [155, 157], [158, 166], [167, 169], [170, 171], [172, 179], [180, 186], [187, 196], [197, 204], [205, 206], [207, 211], [212, 223], [224, 234], [235, 245], [246, 248], [249, 254], [255, 257], [258, 261], [262, 268], [269, 271], [272, 278], [278, 279]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 3, 4, "part-of", "", false, false], [0, 0, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "-", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "mixing", "a", "combination", "of", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden that creates new frequencies by mixing a combination of two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [75, 76], [76, 84], [85, 93], [94, 103], [104, 108], [109, 116], [117, 120], [121, 132], [133, 135], [136, 142], [143, 144], [145, 156], [157, 159], [160, 163], [164, 175], [175, 176]]}
{"doc_key": "ai-train-46", "ner": [[16, 17, "person"], [19, 19, "misc"], [23, 25, "organisation"], [28, 28, "organisation"], [30, 32, "misc"], [33, 35, "person"], [37, 37, "organisation"], [39, 41, "misc"], [43, 44, "person"], [46, 47, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[16, 17, 19, 19, "role", "actor_in", false, false], [19, 19, 23, 25, "artifact", "", false, false], [30, 32, 28, 28, "artifact", "", false, false], [33, 35, 30, 32, "role", "actor_in", false, false], [39, 41, 37, 37, "artifact", "", false, false], [43, 44, 39, 41, "role", "actor_in", false, false], [46, 47, 39, 41, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["A", "few", "other", "films", "that", "helped", "put", "3D", "movies", "back", "on", "the", "map", "that", "month", "were", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "A few other films that helped put 3D movies back on the map that month were John Wayne's Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 1], [2, 5], [6, 11], [12, 17], [18, 22], [23, 29], [30, 33], [34, 36], [37, 43], [44, 48], [49, 51], [52, 55], [56, 59], [60, 64], [65, 70], [71, 75], [76, 80], [81, 86], [86, 88], [89, 94], [95, 96], [96, 107], [108, 110], [111, 117], [118, 122], [122, 123], [123, 124], [124, 125], [126, 134], [134, 136], [137, 141], [142, 147], [148, 156], [157, 161], [162, 166], [167, 175], [176, 179], [180, 189], [189, 191], [192, 197], [198, 202], [203, 207], [208, 212], [213, 217], [218, 224], [225, 228], [229, 234], [235, 240], [240, 241]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [3, 5, "field"], [6, 6, "task"], [16, 16, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 6, 6, "general-affiliation", "", false, false], [0, 0, 16, 16, "artifact", "", false, false], [6, 6, 3, 5, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "-", "learning", "facial", "recognition", "system", "created", "by", "a", "team", "of", "researchers", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep-learning facial recognition system created by a team of researchers at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [18, 19], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 71], [72, 74], [75, 86], [87, 89], [90, 98], [98, 99]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [15, 16, "field"], [24, 26, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 15, 16, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [24, 26, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "common", "research", "topic", "at", "SIGGRAPH", ",", "the", "premier", "academic", "conference", "on", "computer", "graphics", "and", "the", "main", "topic", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a common research topic at SIGGRAPH, the premier academic conference on computer graphics and the main topic of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 31], [32, 40], [41, 46], [47, 49], [50, 58], [58, 59], [60, 63], [64, 71], [72, 80], [81, 91], [92, 94], [95, 103], [104, 112], [113, 116], [117, 120], [121, 125], [126, 131], [132, 134], [135, 138], [139, 145], [146, 154], [155, 165], [166, 175], [175, 176]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [9, 11, "algorithm"], [13, 13, "algorithm"], [16, 18, "algorithm"], [20, 20, "algorithm"], [23, 25, "algorithm"], [27, 27, "algorithm"], [32, 32, "misc"], [39, 41, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[9, 11, 32, 32, "general-affiliation", "", false, false], [13, 13, 9, 11, "named", "", false, false], [16, 18, 32, 32, "general-affiliation", "", false, false], [20, 20, 16, 18, "named", "", false, false], [23, 25, 32, 32, "general-affiliation", "", false, false], [27, 27, 23, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "performed", "by", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "typical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "by", "k", "-", "NN", "on", "the", "feature", "vectors", "in", "the", "dimensionality", "reduction", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be performed by principal component analysis (PCA), linear discriminant analysis (LDA) or typical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k - NN on the feature vectors in the dimensionality reduction space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 64], [65, 67], [68, 77], [78, 87], [88, 96], [97, 98], [98, 101], [101, 102], [102, 103], [104, 110], [111, 123], [124, 132], [133, 134], [134, 137], [137, 138], [139, 141], [142, 149], [150, 161], [162, 170], [171, 172], [172, 175], [175, 176], [177, 187], [188, 190], [191, 192], [193, 207], [208, 212], [212, 213], [214, 222], [223, 225], [226, 236], [237, 239], [240, 241], [242, 243], [244, 246], [247, 249], [250, 253], [254, 261], [262, 269], [270, 272], [273, 276], [277, 291], [292, 301], [302, 307], [307, 308]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 2, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [35, 38, "algorithm"], [39, 40, "researcher"], [42, 44, "researcher"], [46, 52, "misc"], [54, 63, "conference"], [65, 65, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 2, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [46, 52, 35, 38, "topic", "", false, false], [46, 52, 39, 40, "artifact", "", false, false], [46, 52, 42, 44, "artifact", "", false, false], [46, 52, 54, 63, "temporal", "", false, false], [65, 65, 54, 63, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1", ":", "15", "-", "33", ",", "2000", "others", "uses", "local", "features", "like", "histogram", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others uses local features like histogram of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 130], [130, 131], [132, 134], [134, 135], [135, 137], [137, 138], [139, 143], [144, 150], [151, 155], [156, 161], [162, 170], [171, 175], [176, 185], [186, 188], [189, 197], [198, 207], [208, 210], [211, 216], [216, 217], [218, 219], [219, 220], [221, 227], [227, 228], [229, 239], [240, 242], [243, 251], [252, 261], [262, 265], [266, 271], [272, 281], [281, 282], [283, 287], [288, 296], [297, 304], [305, 315], [316, 318], [319, 327], [328, 334], [335, 338], [339, 346], [347, 358], [359, 360], [360, 364], [364, 365], [365, 366], [367, 372], [373, 374], [374, 375], [376, 383], [383, 384], [385, 389], [390, 401], [401, 402]]}
{"doc_key": "ai-train-52", "ner": [[0, 0, "algorithm"], [2, 6, "algorithm"], [8, 9, "task"], [12, 13, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 2, 6, "type-of", "", false, false], [8, 9, 0, 0, "usage", "", true, false], [8, 9, 12, 13, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Autoencoders", "are", "artificial", "neural", "networks", "used", "to", "learn", "feature", "learning", "in", "an", "unsupervised", "learning", "manner", "."], "sentence-detokenized": "Autoencoders are artificial neural networks used to learn feature learning in an unsupervised learning manner.", "token2charspan": [[0, 12], [13, 16], [17, 27], [28, 34], [35, 43], [44, 48], [49, 51], [52, 57], [58, 65], [66, 74], [75, 77], [78, 80], [81, 93], [94, 102], [103, 109], [109, 110]]}
{"doc_key": "ai-train-53", "ner": [[0, 1, "researcher"], [5, 7, "organisation"], [11, 12, "field"], [14, 15, "field"], [25, 44, "organisation"], [23, 24, "organisation"], [31, 32, "field"], [34, 35, "field"], [41, 42, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 1, 5, 7, "role", "fellow_of", false, false], [0, 1, 11, 12, "related-to", "contributes_to", false, false], [0, 1, 14, 15, "related-to", "contributes_to", false, false], [0, 1, 25, 44, "role", "fellow_of", false, false], [0, 1, 31, 32, "related-to", "contributes_to", false, false], [0, 1, 34, 35, "related-to", "contributes_to", false, false], [23, 24, 25, 44, "named", "", false, false], [41, 42, 25, 44, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "is", "a", "member", "of", "the", "IEEE", "for", "his", "contributions", "to", "computer", "vision", "and", "image", "processing", ",", "and", "a", "member", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "for", "his", "contributions", "to", "pattern", "recognition", ",", "image", "processing", "and", "service", "to", "the", "International", "Association", "for", "Pattern", "Recognition", "."], "sentence-detokenized": "Haralick is a member of the IEEE for his contributions to computer vision and image processing, and a member of the International Association for Pattern Recognition for his contributions to pattern recognition, image processing and service to the International Association for Pattern Recognition.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 40], [41, 54], [55, 57], [58, 66], [67, 73], [74, 77], [78, 83], [84, 94], [94, 95], [96, 99], [100, 101], [102, 108], [109, 111], [112, 115], [116, 129], [130, 141], [142, 145], [146, 153], [154, 165], [166, 169], [170, 173], [174, 187], [188, 190], [191, 198], [199, 210], [210, 211], [212, 217], [218, 228], [229, 232], [233, 240], [241, 243], [244, 247], [248, 261], [262, 273], [274, 277], [278, 285], [286, 297], [297, 298]]}
{"doc_key": "ai-train-54", "ner": [[31, 37, "task"], [21, 23, "algorithm"], [25, 25, "algorithm"], [3, 4, "researcher"], [6, 7, "organisation"], [9, 10, "researcher"], [12, 15, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[31, 37, 21, 23, "usage", "", false, false], [21, 23, 3, 4, "origin", "", true, false], [21, 23, 9, 10, "origin", "", true, false], [25, 25, 21, 23, "named", "", false, false], [3, 4, 6, 7, "physical", "", false, false], [3, 4, 6, 7, "role", "", false, false], [9, 10, 12, 15, "physical", "", false, false], [9, 10, 12, 15, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["In", "2014", ",", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "introduced", "a", "system", "based", "on", "connectionist", "temporal", "classification", "(", "CTC", ")", ",", "the", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "."], "sentence-detokenized": "In 2014, Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto introduced a system based on connectionist temporal classification (CTC), the first attempt at end-to-end ASR.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 13], [14, 20], [21, 23], [24, 30], [31, 39], [40, 43], [44, 51], [52, 58], [59, 61], [62, 65], [66, 76], [77, 79], [80, 87], [88, 98], [99, 100], [101, 107], [108, 113], [114, 116], [117, 130], [131, 139], [140, 154], [155, 156], [156, 159], [159, 160], [160, 161], [162, 165], [166, 171], [172, 179], [180, 182], [183, 186], [186, 187], [187, 189], [189, 190], [190, 193], [194, 197], [197, 198]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear", "Fractional", "Programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "Linear", "Programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear Fractional Programming (LFP) is a generalisation of Linear Programming (LP).", "token2charspan": [[0, 6], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 10, "misc"], [12, 21, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 10, "win-defeat", "", false, false], [8, 10, 12, 21, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Rafferty", "has", "won", "many", "awards", ",", "including", "two", "time", "testing", "awards", "at", "the", "International", "Conference", "on", "Machine", "Learning", "in", "2011", "and", "2012", "."], "sentence-detokenized": "Rafferty has won many awards, including two time testing awards at the International Conference on Machine Learning in 2011 and 2012.", "token2charspan": [[0, 8], [9, 12], [13, 16], [17, 21], [22, 28], [28, 29], [30, 39], [40, 43], [44, 48], [49, 56], [57, 63], [64, 66], [67, 70], [71, 84], [85, 95], [96, 98], [99, 106], [107, 115], [116, 118], [119, 123], [124, 127], [128, 132], [132, 133]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [24, 25, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "advent", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "enable", "the", "deployment", "of", "developed", "neural", "networks", "as", "inheritable", "components", "into", "these", "frameworks", "."], "sentence-detokenized": "With the advent of component-based frameworks such as .NET and Java, component-based development environments enable the deployment of developed neural networks as inheritable components into these frameworks.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 28], [28, 29], [29, 34], [35, 45], [46, 50], [51, 53], [54, 58], [59, 62], [63, 67], [67, 68], [69, 78], [78, 79], [79, 84], [85, 96], [97, 109], [110, 116], [117, 120], [121, 131], [132, 134], [135, 144], [145, 151], [152, 160], [161, 163], [164, 175], [176, 186], [187, 191], [192, 197], [198, 208], [208, 209]]}
{"doc_key": "ai-train-58", "ner": [[2, 4, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "with", "BLEU", ",", "the", "basic", "unit", "of", "evaluation", "is", "the", "sentence", ",", "and", "the", "algorithm", "first", "creates", "an", "alignment", "between", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "(", "see", "illustration", ")", "."], "sentence-detokenized": "As with BLEU, the basic unit of evaluation is the sentence, and the algorithm first creates an alignment between two sentences, the candidate translation string and the reference translation string (see illustration).", "token2charspan": [[0, 2], [3, 7], [8, 12], [12, 13], [14, 17], [18, 23], [24, 28], [29, 31], [32, 42], [43, 45], [46, 49], [50, 58], [58, 59], [60, 63], [64, 67], [68, 77], [78, 83], [84, 91], [92, 94], [95, 104], [105, 112], [113, 116], [117, 126], [126, 127], [128, 131], [132, 141], [142, 153], [154, 160], [161, 164], [165, 168], [169, 178], [179, 190], [191, 197], [198, 199], [199, 202], [203, 215], [215, 216], [216, 217]]}
{"doc_key": "ai-train-59", "ner": [[0, 6, "conference"], [15, 15, "task"], [17, 18, "task"], [25, 26, "metrics"], [28, 34, "metrics"], [39, 42, "conference"], [44, 44, "conference"], [47, 47, "location"], [49, 49, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 6, 15, 15, "related-to", "subject_at", false, false], [0, 6, 17, 18, "related-to", "subject_at", false, false], [25, 26, 0, 6, "temporal", "", false, false], [28, 34, 25, 26, "named", "", true, false], [44, 44, 39, 42, "named", "", false, false], [47, 47, 49, 49, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["At", "NIST", "'s", "annual", "document", "comprehension", "conference", ",", "the", "research", "team", "presented", "their", "system", "for", "summarization", "and", "translation", "tasks", ",", "one", "of", "which", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "At NIST's annual document comprehension conference, the research team presented their system for summarization and translation tasks, one of which is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 2], [3, 7], [7, 9], [10, 16], [17, 25], [26, 39], [40, 50], [50, 51], [52, 55], [56, 64], [65, 69], [70, 79], [80, 85], [86, 92], [93, 96], [97, 110], [111, 114], [115, 126], [127, 132], [132, 133], [134, 137], [138, 140], [141, 146], [147, 149], [150, 153], [154, 159], [160, 166], [167, 168], [168, 174], [174, 175], [175, 183], [184, 194], [195, 198], [199, 206], [207, 217], [217, 218], [219, 221], [222, 230], [231, 233], [234, 240], [241, 252], [253, 263], [264, 271], [272, 273], [273, 277], [277, 278], [278, 279], [280, 288], [288, 289], [290, 296], [296, 297], [298, 306], [307, 308], [309, 313], [313, 314]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [10, 11, "programlang"], [15, 15, "product"], [21, 21, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 10, 11, "type-of", "", false, false], [6, 6, 21, 21, "named", "", false, false], [8, 8, 10, 11, "part-of", "", false, false], [8, 8, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "same", "implementation", ",", "run", "in", "Java", "with", "JShell", "(", "Java", "9", "minimum", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "The same implementation, run in Java with JShell (Java 9 minimum): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 3], [4, 8], [9, 23], [23, 24], [25, 28], [29, 31], [32, 36], [37, 41], [42, 48], [49, 50], [50, 54], [55, 56], [57, 64], [64, 65], [65, 66], [67, 77], [78, 88], [89, 90], [91, 110], [111, 115], [116, 117], [118, 122]]}
{"doc_key": "ai-train-61", "ner": [[1, 3, "metrics"], [6, 10, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[1, 3, 6, 10, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metrics", "are", "based", "on", "the", "BLEU", "metrics", "with", "a", "few", "changes", "."], "sentence-detokenized": "The NIST metrics are based on the BLEU metrics with a few changes.", "token2charspan": [[0, 3], [4, 8], [9, 16], [17, 20], [21, 26], [27, 29], [30, 33], [34, 38], [39, 46], [47, 51], [52, 53], [54, 57], [58, 65], [65, 66]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [9, 12, "university"], [14, 17, "university"], [24, 25, "product"], [30, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 12, 6, 6, "physical", "", false, false], [14, 17, 6, 6, "physical", "", false, false], [24, 25, 9, 12, "origin", "", false, false], [24, 25, 14, 17, "origin", "", false, false], [24, 25, 30, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "started", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", "but", "with", "the", "added", "constraint", "that", "edges", "are", "restricted", "to", "come", "from", "a", "limited", "set", "of", "possible", "relations", "to", "facilitate", "algebra", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly started a project called Knowledge Graphs, which are semantic networks but with the added constraint that edges are restricted to come from a limited set of possible relations to facilitate algebra on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 116], [117, 118], [119, 126], [127, 133], [134, 143], [144, 150], [150, 151], [152, 157], [158, 161], [162, 170], [171, 179], [180, 183], [184, 188], [189, 192], [193, 198], [199, 209], [210, 214], [215, 220], [221, 224], [225, 235], [236, 238], [239, 243], [244, 248], [249, 250], [251, 258], [259, 262], [263, 265], [266, 274], [275, 284], [285, 287], [288, 298], [299, 306], [307, 309], [310, 313], [314, 319], [319, 320]]}
{"doc_key": "ai-train-63", "ner": [[0, 3, "product"], [18, 19, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 3, 18, 19, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "syntax", "checker", "is", "most", "often", "implemented", "as", "a", "function", "of", "a", "larger", "program", ",", "such", "as", "a", "word", "processor", ",", "but", "can", "also", "be", "activated", "as", "a", "stand", "-", "alone", "application", "from", "a", "program", "that", "handles", "editable", "text", "."], "sentence-detokenized": "The syntax checker is most often implemented as a function of a larger program, such as a word processor, but can also be activated as a stand-alone application from a program that handles editable text.", "token2charspan": [[0, 3], [4, 10], [11, 18], [19, 21], [22, 26], [27, 32], [33, 44], [45, 47], [48, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 78], [78, 79], [80, 84], [85, 87], [88, 89], [90, 94], [95, 104], [104, 105], [106, 109], [110, 113], [114, 118], [119, 121], [122, 131], [132, 134], [135, 136], [137, 142], [142, 143], [143, 148], [149, 160], [161, 165], [166, 167], [168, 175], [176, 180], [181, 188], [189, 197], [198, 202], [202, 203]]}
{"doc_key": "ai-train-64", "ner": [[5, 12, "organisation"], [14, 21, "conference"], [23, 26, "organisation"], [35, 36, "conference"], [38, 39, "conference"], [41, 42, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "and", "the", "Cognitive", "Science", "Society", ",", "and", "is", "an", "editor", "of", "the", "journals", "Automated", "Reasoning", ",", "Learning", "Science", "and", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement of Artificial Intelligence and the Cognitive Science Society, and is an editor of the journals Automated Reasoning, Learning Science and Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 113], [114, 124], [125, 137], [138, 141], [142, 145], [146, 155], [156, 163], [164, 171], [171, 172], [173, 176], [177, 179], [180, 182], [183, 189], [190, 192], [193, 196], [197, 205], [206, 215], [216, 225], [225, 226], [227, 235], [236, 243], [244, 247], [248, 255], [256, 264], [264, 265]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "task"], [23, 24, "researcher"], [26, 27, "university"], [29, 30, "researcher"], [32, 36, "organisation"], [38, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 23, 24, "origin", "", false, false], [0, 2, 29, 30, "origin", "", false, false], [4, 4, 0, 2, "named", "", false, false], [23, 24, 26, 27, "physical", "", false, false], [23, 24, 26, 27, "role", "", false, false], [29, 30, 32, 36, "role", "", false, false], [38, 38, 32, 36, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "Predictive", "Coding", "(", "LPC", ")", "is", "a", "form", "of", "speech", "coding", "that", "began", "to", "be", "developed", "in", "1966", "with", "the", "work", "of", "Fumitada", "Itakura", "at", "Nagoya", "University", "and", "Shuzo", "Saito", "at", "Nippon", "Telegraph", "and", "Telephone", "Corporation", "(", "NTT", ")", "."], "sentence-detokenized": "Linear Predictive Coding (LPC) is a form of speech coding that began to be developed in 1966 with the work of Fumitada Itakura at Nagoya University and Shuzo Saito at Nippon Telegraph and Telephone Corporation (NTT).", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [31, 33], [34, 35], [36, 40], [41, 43], [44, 50], [51, 57], [58, 62], [63, 68], [69, 71], [72, 74], [75, 84], [85, 87], [88, 92], [93, 97], [98, 101], [102, 106], [107, 109], [110, 118], [119, 126], [127, 129], [130, 136], [137, 147], [148, 151], [152, 157], [158, 163], [164, 166], [167, 173], [174, 183], [184, 187], [188, 197], [198, 209], [210, 211], [211, 214], [214, 215], [215, 216]]}
{"doc_key": "ai-train-66", "ner": [[21, 23, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "further", "traversed", ",", "all", "sampled", "paths", "exhibit", "the", "same", "time", "average", ",", "so", "in", "the", "sense", "of", "mean", "square", "error", ",", "maths", "R", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=", "\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "."], "sentence-detokenized": "If the signal is further traversed, all sampled paths exhibit the same time average, so in the sense of mean square error, maths R _ x ^ {n / T _ 0} (\\ tau) = \\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 24], [25, 34], [34, 35], [36, 39], [40, 47], [48, 53], [54, 61], [62, 65], [66, 70], [71, 75], [76, 83], [83, 84], [85, 87], [88, 90], [91, 94], [95, 100], [101, 103], [104, 108], [109, 115], [116, 121], [121, 122], [123, 128], [129, 130], [131, 132], [133, 134], [135, 136], [137, 138], [138, 139], [140, 141], [142, 143], [144, 145], [146, 147], [147, 148], [149, 150], [150, 151], [152, 155], [155, 156], [157, 158], [159, 160], [161, 168], [169, 170], [170, 171], [171, 172], [173, 174], [175, 176], [177, 178], [179, 180], [180, 181], [182, 183], [184, 185], [186, 187], [188, 189], [189, 190], [191, 192], [192, 193], [194, 197], [197, 198], [199, 200], [201, 205], [205, 206]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [4, 4, "task"], [9, 11, "algorithm"], [13, 13, "algorithm"], [16, 18, "algorithm"], [20, 20, "algorithm"], [23, 25, "algorithm"], [27, 27, "algorithm"], [29, 32, "algorithm"], [34, 34, "algorithm"], [39, 40, "misc"], [46, 48, "algorithm"], [51, 52, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[9, 11, 39, 40, "related-to", "", false, false], [13, 13, 9, 11, "named", "", false, false], [16, 18, 39, 40, "related-to", "", false, false], [20, 20, 16, 18, "named", "", false, false], [23, 25, 39, 40, "related-to", "", false, false], [27, 27, 23, 25, "named", "", false, false], [29, 32, 39, 40, "related-to", "", false, false], [34, 34, 29, 32, "named", "", false, false], [46, 48, 51, 52, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimensionality", "reduction", "can", "be", "performed", "by", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "typical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "decomposition", "(", "NMF", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "by", "K", "-", "NN", "on", "the", "feature", "vectors", "in", "the", "dimensionality", "reduction", "space", "."], "sentence-detokenized": "Feature extraction and dimensionality reduction can be performed by principal component analysis (PCA), linear discriminant analysis (LDA), typical correlation analysis (CCA) or non-negative matrix decomposition (NMF) techniques as a pre-processing step, followed by clustering by K-NN on the feature vectors in the dimensionality reduction space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 37], [38, 47], [48, 51], [52, 54], [55, 64], [65, 67], [68, 77], [78, 87], [88, 96], [97, 98], [98, 101], [101, 102], [102, 103], [104, 110], [111, 123], [124, 132], [133, 134], [134, 137], [137, 138], [138, 139], [140, 147], [148, 159], [160, 168], [169, 170], [170, 173], [173, 174], [175, 177], [178, 190], [191, 197], [198, 211], [212, 213], [213, 216], [216, 217], [218, 228], [229, 231], [232, 233], [234, 248], [249, 253], [253, 254], [255, 263], [264, 266], [267, 277], [278, 280], [281, 282], [282, 283], [283, 285], [286, 288], [289, 292], [293, 300], [301, 308], [309, 311], [312, 315], [316, 330], [331, 340], [341, 346], [346, 347]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [9, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 9, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", "."], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB.", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 3, "task"], [9, 11, "task"], [28, 29, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 3, 9, 11, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "identifying", "named", "entities", "in", "text", "is", "named", "entity", "identification", ",", "while", "the", "task", "of", "determining", "the", "identity", "of", "named", "entities", "mentioned", "in", "text", "is", "called", "entity", "linking", "."], "sentence-detokenized": "The task of identifying named entities in text is named entity identification, while the task of determining the identity of named entities mentioned in text is called entity linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 46], [47, 49], [50, 55], [56, 62], [63, 77], [77, 78], [79, 84], [85, 88], [89, 93], [94, 96], [97, 108], [109, 112], [113, 121], [122, 124], [125, 130], [131, 139], [140, 149], [150, 152], [153, 157], [158, 160], [161, 167], [168, 174], [175, 182], [182, 183]]}
{"doc_key": "ai-train-70", "ner": [[0, 1, "algorithm"], [26, 26, "programlang"], [28, 32, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 1, 28, 32, "part-of", "", true, false], [28, 32, 26, 26, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", "and", "from", "version", "0.8.0", "onwards", "they", "were", "released", "in", "a", "separate", "R", "package", "sigmoid", ",", "in", "order", "to", "enable", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package and from version 0.8.0 onwards they were released in a separate R package sigmoid, in order to enable more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [98, 101], [102, 106], [107, 114], [115, 120], [121, 128], [129, 133], [134, 138], [139, 147], [148, 150], [151, 152], [153, 161], [162, 163], [164, 171], [172, 179], [179, 180], [181, 183], [184, 189], [190, 192], [193, 199], [200, 204], [205, 212], [213, 216], [216, 217]]}
{"doc_key": "ai-train-71", "ner": [[1, 2, "programlang"], [7, 11, "organisation"], [13, 13, "organisation"], [22, 22, "location"], [26, 27, "researcher"], [29, 30, "researcher"], [24, 33, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 6, 7], "relations": [[1, 2, 26, 27, "artifact", "", true, false], [1, 2, 29, 30, "artifact", "", true, false], [1, 2, 24, 33, "artifact", "", true, false], [13, 13, 7, 11, "named", "", false, false], [26, 27, 7, 11, "role", "", false, false], [29, 30, 7, 11, "role", "", false, false], [24, 33, 7, 11, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "research", "firm", "in", "Cambridge", ",", "Massachusetts", ",", "headed", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "."], "sentence-detokenized": "The logo was created in 1967 at Bolt, Beranek and Newman (BBN), a research firm in Cambridge, Massachusetts, headed by Wally Feurzeig, Cynthia Solomon and Seymour Papert.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 36], [36, 37], [38, 45], [46, 49], [50, 56], [57, 58], [58, 61], [61, 62], [62, 63], [64, 65], [66, 74], [75, 79], [80, 82], [83, 92], [92, 93], [94, 107], [107, 108], [109, 115], [116, 118], [119, 124], [125, 133], [133, 134], [135, 142], [143, 150], [151, 154], [155, 162], [163, 169], [169, 170]]}
{"doc_key": "ai-train-72", "ner": [[0, 0, "misc"], [8, 9, "field"], [18, 19, "field"], [24, 24, "algorithm"], [27, 28, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 8, 9, "part-of", "", false, false], [0, 0, 18, 19, "compare", "", false, false], [24, 24, 18, 19, "part-of", "", false, false], [27, 28, 18, 19, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "often", "used", "as", "part", "of", "a", "reinforcement", "learning", "paradigm", ",", "which", "can", "be", "contrasted", "with", "traditional", "deep", "learning", "techniques", "that", "use", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is often used as part of a reinforcement learning paradigm, which can be contrasted with traditional deep learning techniques that use gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 23], [24, 28], [29, 31], [32, 36], [37, 39], [40, 41], [42, 55], [56, 64], [65, 73], [73, 74], [75, 80], [81, 84], [85, 87], [88, 98], [99, 103], [104, 115], [116, 120], [121, 129], [130, 140], [141, 145], [146, 149], [150, 158], [159, 166], [167, 169], [170, 171], [172, 178], [179, 186], [187, 191], [192, 193], [194, 199], [200, 208], [208, 209]]}
{"doc_key": "ai-train-73", "ner": [[42, 45, "algorithm"], [55, 57, "metrics"], [59, 59, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[59, 59, 55, 57, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "fit", "a", "function", "of", "hyperplane", "form", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "to", "the", "data", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264", "n", "/", "sub", "using", "the", "least", "squares", "method", ",", "we", "can", "evaluate", "the", "fit", "in", "terms", "of", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we fit a function of hyperplane form \u0177 = a + \u03b2 supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264 n / sub using the least squares method, we can evaluate the fit in terms of mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 11], [12, 20], [21, 23], [24, 34], [35, 39], [40, 41], [42, 43], [44, 45], [46, 47], [48, 49], [50, 54], [55, 56], [57, 60], [61, 62], [63, 65], [66, 69], [70, 74], [75, 76], [76, 77], [78, 81], [82, 83], [84, 85], [86, 89], [89, 90], [91, 92], [93, 96], [97, 98], [99, 100], [101, 104], [104, 105], [106, 109], [110, 111], [112, 113], [114, 115], [116, 117], [118, 119], [120, 121], [122, 125], [126, 131], [132, 135], [136, 141], [142, 149], [150, 156], [156, 157], [158, 160], [161, 164], [165, 173], [174, 177], [178, 181], [182, 184], [185, 190], [191, 193], [194, 198], [199, 206], [207, 212], [213, 214], [214, 217], [217, 218], [218, 219]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [41, 41, "country"], [43, 43, "country"], [45, 45, "country"], [47, 48, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "offices", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "UK", "."], "sentence-detokenized": "The company has international offices in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the UK.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 37], [38, 40], [41, 50], [50, 51], [52, 58], [58, 59], [60, 66], [66, 67], [68, 73], [73, 74], [75, 82], [82, 83], [84, 89], [89, 90], [91, 96], [96, 97], [98, 103], [103, 104], [105, 110], [110, 111], [112, 121], [121, 122], [123, 129], [129, 130], [131, 139], [139, 140], [141, 152], [152, 153], [154, 160], [160, 161], [162, 171], [171, 172], [173, 178], [179, 185], [185, 186], [187, 192], [192, 193], [194, 200], [200, 201], [202, 210], [210, 211], [212, 218], [219, 222], [223, 226], [227, 229], [229, 230]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 9, "field"], [10, 10, "organisation"], [13, 17, "university"], [27, 29, "organisation"], [31, 36, "university"], [40, 41, "university"], [43, 44, "university"], [46, 49, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 9, "topic", "", false, false], [3, 3, 10, 10, "origin", "", false, false], [3, 3, 13, 17, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", "(", "2000", ")", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "the", "Paris", "High", "Tech", "School", "and", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering from Inria and the University of Nice Sophia Antipolis (2000) and has held permanent positions at Siemens Corporate Technology, the Paris High Tech School and visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 58], [59, 64], [65, 68], [69, 72], [73, 83], [84, 86], [87, 91], [92, 98], [99, 108], [109, 110], [110, 114], [114, 115], [116, 119], [120, 123], [124, 128], [129, 138], [139, 148], [149, 151], [152, 159], [160, 169], [170, 180], [180, 181], [182, 185], [186, 191], [192, 196], [197, 201], [202, 208], [209, 212], [213, 221], [222, 231], [232, 234], [235, 242], [243, 253], [253, 254], [255, 259], [260, 270], [271, 274], [275, 278], [279, 289], [290, 292], [293, 300], [300, 301]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [0, 0, "researcher"], [13, 14, "product"], [15, 17, "country"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 8, "role", "licensing_patent_to", false, false], [0, 0, 15, 17, "physical", "", false, false], [20, 20, 0, 0, "artifact", "", false, false], [20, 20, 13, 14, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberg", "licensed", "the", "original", "patent", "to", "inventor", "George", "DeVore", "and", "developed", "the", "first", "industrial", "robot", "in", "the", "USA", ",", "the", "Unimate", ",", "in", "the", "1950s", "."], "sentence-detokenized": "Engelberg licensed the original patent to inventor George DeVore and developed the first industrial robot in the USA, the Unimate, in the 1950s.", "token2charspan": [[0, 9], [10, 18], [19, 22], [23, 31], [32, 38], [39, 41], [42, 50], [51, 57], [58, 64], [65, 68], [69, 78], [79, 82], [83, 88], [89, 99], [100, 105], [106, 108], [109, 112], [113, 116], [116, 117], [118, 121], [122, 129], [129, 130], [131, 133], [134, 137], [138, 143], [143, 144]]}
{"doc_key": "ai-train-77", "ner": [[5, 6, "task"], [13, 14, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "known", "as", "speech", "recognition", "and", "the", "output", "is", "known", "as", "speech", "synthesis", "."], "sentence-detokenized": "The input is known as speech recognition and the output is known as speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 18], [19, 21], [22, 28], [29, 40], [41, 44], [45, 48], [49, 55], [56, 58], [59, 64], [65, 67], [68, 74], [75, 84], [84, 85]]}
{"doc_key": "ai-train-78", "ner": [[3, 3, "programlang"], [14, 15, "programlang"], [18, 18, "programlang"], [6, 6, "programlang"]], "ner_mapping_to_source": [0, 2, 3, 4], "relations": [[3, 3, 14, 15, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Descendants", "of", "the", "CLIPS", "language", "include", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "was", "rewritten", "in", "Java", "and", "later", "developed", "in", "a", "different", "direction", ")", ",", "which", "was", "originally", "inspired", "by"], "sentence-detokenized": "Descendants of the CLIPS language include Jess (the rule-based part of CLIPS was rewritten in Java and later developed in a different direction), which was originally inspired by", "token2charspan": [[0, 11], [12, 14], [15, 18], [19, 24], [25, 33], [34, 41], [42, 46], [47, 48], [48, 51], [52, 56], [56, 57], [57, 62], [63, 67], [68, 70], [71, 76], [77, 80], [81, 90], [91, 93], [94, 98], [99, 102], [103, 108], [109, 118], [119, 121], [122, 123], [124, 133], [134, 143], [143, 144], [144, 145], [146, 151], [152, 155], [156, 166], [167, 175], [176, 178]]}
{"doc_key": "ai-train-79", "ner": [[6, 6, "product"], [10, 15, "product"], [17, 21, "organisation"], [24, 25, "product"], [43, 44, "product"], [46, 48, "product"], [64, 65, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[10, 15, 6, 6, "type-of", "", false, false], [17, 21, 10, 15, "usage", "", false, false], [24, 25, 17, 21, "artifact", "", false, false], [43, 44, 17, 21, "origin", "", true, false], [43, 44, 64, 65, "related-to", "", true, false], [46, 48, 17, 21, "origin", "", true, false], [46, 48, 64, 65, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["It", "has", "also", "created", "flexible", "intelligent", "AGV", "applications", ",", "designing", "the", "Motivity", "control", "system", ",", "which", "was", "used", "by", "RMT", "Robotics", "to", "develop", "the", "ADAM", "iAGV", "(", "self", "-", "guided", "vehicle", ")", "for", "complex", "pick", "and", "place", "operations", ",", "used", "in", "combination", "with", "gantry", "systems", "and", "industrial", "robot", "arms", "for", "front", "-", "line", "automotive", "supply", "plants", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "."], "sentence-detokenized": "It has also created flexible intelligent AGV applications, designing the Motivity control system, which was used by RMT Robotics to develop the ADAM iAGV (self-guided vehicle) for complex pick and place operations, used in combination with gantry systems and industrial robot arms for front-line automotive supply plants to move products from process to process in non-linear layouts.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 19], [20, 28], [29, 40], [41, 44], [45, 57], [57, 58], [59, 68], [69, 72], [73, 81], [82, 89], [90, 96], [96, 97], [98, 103], [104, 107], [108, 112], [113, 115], [116, 119], [120, 128], [129, 131], [132, 139], [140, 143], [144, 148], [149, 153], [154, 155], [155, 159], [159, 160], [160, 166], [167, 174], [174, 175], [176, 179], [180, 187], [188, 192], [193, 196], [197, 202], [203, 213], [213, 214], [215, 219], [220, 222], [223, 234], [235, 239], [240, 246], [247, 254], [255, 258], [259, 269], [270, 275], [276, 280], [281, 284], [285, 290], [290, 291], [291, 295], [296, 306], [307, 313], [314, 320], [321, 323], [324, 328], [329, 337], [338, 342], [343, 350], [351, 353], [354, 361], [362, 364], [365, 375], [376, 383], [383, 384]]}
{"doc_key": "ai-train-80", "ner": [[7, 10, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "parameter", "\u03b2", "is", "usually", "estimated", "by", "the", "maximum", "likelihood", "method", "."], "sentence-detokenized": "The parameter \u03b2 is usually estimated by the maximum likelihood method.", "token2charspan": [[0, 3], [4, 13], [14, 15], [16, 18], [19, 26], [27, 36], [37, 39], [40, 43], [44, 51], [52, 62], [63, 69], [69, 70]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 12, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 0, 1, "part-of", "", false, false], [8, 8, 0, 1, "part-of", "", false, false], [10, 12, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", ",", "such", "as", "precision", "and", "recall", "or", "DCG", ",", "are", "very", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics, such as precision and recall or DCG, are very useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [29, 30], [31, 35], [36, 38], [39, 48], [49, 52], [53, 59], [60, 62], [63, 66], [66, 67], [68, 71], [72, 76], [77, 83], [84, 87], [88, 97], [98, 101], [102, 109], [110, 112], [113, 114], [115, 129], [130, 136], [136, 137]]}
{"doc_key": "ai-train-82", "ner": [[6, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "typical", "factory", "contains", "hundreds", "of", "industrial", "robots", "working", "on", "fully", "automated", "production", "lines", ",", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "A typical factory contains hundreds of industrial robots working on fully automated production lines, one robot for every ten human workers.", "token2charspan": [[0, 1], [2, 9], [10, 17], [18, 26], [27, 35], [36, 38], [39, 49], [50, 56], [57, 64], [65, 67], [68, 73], [74, 83], [84, 94], [95, 100], [100, 101], [102, 105], [106, 111], [112, 115], [116, 121], [122, 125], [126, 131], [132, 139], [139, 140]]}
{"doc_key": "ai-train-83", "ner": [[4, 4, "product"], [12, 13, "field"], [17, 18, "task"], [20, 21, "task"], [23, 24, "task"], [26, 27, "task"], [29, 30, "task"], [32, 33, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[12, 13, 4, 4, "usage", "", false, true], [17, 18, 12, 13, "part-of", "", false, false], [20, 21, 12, 13, "part-of", "", false, false], [23, 24, 12, 13, "part-of", "", false, false], [26, 27, 12, 13, "part-of", "", false, false], [29, 30, 12, 13, "part-of", "", false, false], [32, 33, 12, 13, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Over", "the", "last", "decade", "PCNN", "has", "been", "used", "in", "a", "variety", "of", "image", "processing", "applications", "including", ":", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growing", "and", "noise", "reduction", "."], "sentence-detokenized": "Over the last decade PCNN has been used in a variety of image processing applications including: image segmentation, feature generation, face extraction, motion detection, region growing and noise reduction.", "token2charspan": [[0, 4], [5, 8], [9, 13], [14, 20], [21, 25], [26, 29], [30, 34], [35, 39], [40, 42], [43, 44], [45, 52], [53, 55], [56, 61], [62, 72], [73, 85], [86, 95], [95, 96], [97, 102], [103, 115], [115, 116], [117, 124], [125, 135], [135, 136], [137, 141], [142, 152], [152, 153], [154, 160], [161, 170], [170, 171], [172, 178], [179, 186], [187, 190], [191, 196], [197, 206], [206, 207]]}
{"doc_key": "ai-train-84", "ner": [[16, 17, "field"], [19, 23, "misc"], [27, 33, "conference"], [35, 35, "conference"], [38, 41, "misc"], [45, 49, "conference"], [44, 51, "conference"], [56, 60, "conference"], [62, 62, "conference"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[19, 23, 27, 33, "temporal", "", false, false], [35, 35, 27, 33, "named", "", false, false], [38, 41, 45, 49, "temporal", "", false, false], [38, 41, 56, 60, "temporal", "", false, false], [44, 51, 45, 49, "named", "", false, false], [62, 62, 56, 60, "named", "", false, false]], "relations_mapping_to_source": [3, 4, 5, 6, 7, 8], "sentence": ["He", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "2012", "International", "Conference", "on", "Non-Realistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "and", "the", "Best", "Reviewer", "Award", "at", "the", "2012", "Asian", "Conference", "on", "Computer", "Vision", "(", "ACCV", ")", "and", "the", "2015", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "."], "sentence-detokenized": "He has published more than 50 papers in international conferences and journals in the field of computer vision and won the Best Paper Award at the 2012 International Conference on Non-Realistic Rendering and Animation (NPAR) and the Best Reviewer Award at the 2012 Asian Conference on Computer Vision (ACCV) and the 2015 International Conference on Computer Vision (ICCV).", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 151], [152, 165], [166, 176], [177, 179], [180, 193], [194, 203], [204, 207], [208, 217], [218, 219], [219, 223], [223, 224], [225, 228], [229, 232], [233, 237], [238, 246], [247, 252], [253, 255], [256, 259], [260, 264], [265, 270], [271, 281], [282, 284], [285, 293], [294, 300], [301, 302], [302, 306], [306, 307], [308, 311], [312, 315], [316, 320], [321, 334], [335, 345], [346, 348], [349, 357], [358, 364], [365, 366], [366, 370], [370, 371], [371, 372]]}
{"doc_key": "ai-train-85", "ner": [[0, 0, "programlang"], [2, 3, "field"], [5, 6, "field"], [9, 10, "misc"], [13, 14, "researcher"], [16, 18, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 2, 3, "part-of", "", false, false], [0, 0, 5, 6, "part-of", "", false, false], [0, 0, 9, 10, "type-of", "", false, false], [16, 18, 0, 0, "usage", "", false, false], [16, 18, 13, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "in", "Computer", "Science", "and", "Artificial", "Intelligence", "is", "an", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "Artificial", "Project", "."], "sentence-detokenized": "CycL in Computer Science and Artificial Intelligence is an ontology language used by Doug Lenat's Cyc Artificial Project.", "token2charspan": [[0, 4], [5, 7], [8, 16], [17, 24], [25, 28], [29, 39], [40, 52], [53, 55], [56, 58], [59, 67], [68, 76], [77, 81], [82, 84], [85, 89], [90, 95], [95, 97], [98, 101], [102, 112], [113, 120], [120, 121]]}
{"doc_key": "ai-train-86", "ner": [[2, 3, "task"], [6, 8, "metrics"], [15, 17, "metrics"], [24, 26, "metrics"], [35, 36, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 8, 2, 3, "part-of", "", false, false], [15, 17, 6, 8, "named", "", false, false], [24, 26, 6, 8, "named", "", false, false], [35, 36, 6, 8, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "the", "mean", "squared", "error", ",", "often", "referred", "to", "as", "the", "mean", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "of", "the", "squared", "deviations", "from", "the", "true", "values", "of", "the", "predicted", "values", "generated", "by", "a", "model", "estimated", "over", "a", "given", "sample", "space", "on", "an", "out", "-", "of", "-", "sample", "test", "space", "."], "sentence-detokenized": "Also in regression analysis, the mean squared error, often referred to as the mean prediction error or out-of-sample mean squared error, can refer to the average of the squared deviations from the true values of the predicted values generated by a model estimated over a given sample space on an out-of-sample test space.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 32], [33, 37], [38, 45], [46, 51], [51, 52], [53, 58], [59, 67], [68, 70], [71, 73], [74, 77], [78, 82], [83, 93], [94, 99], [100, 102], [103, 106], [106, 107], [107, 109], [109, 110], [110, 116], [117, 121], [122, 129], [130, 135], [135, 136], [137, 140], [141, 146], [147, 149], [150, 153], [154, 161], [162, 164], [165, 168], [169, 176], [177, 187], [188, 192], [193, 196], [197, 201], [202, 208], [209, 211], [212, 215], [216, 225], [226, 232], [233, 242], [243, 245], [246, 247], [248, 253], [254, 263], [264, 268], [269, 270], [271, 276], [277, 283], [284, 289], [290, 292], [293, 295], [296, 299], [299, 300], [300, 302], [302, 303], [303, 309], [310, 314], [315, 320], [320, 321]]}
{"doc_key": "ai-train-87", "ner": [[6, 8, "algorithm"], [10, 11, "algorithm"], [19, 22, "algorithm"], [36, 38, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 8, 10, 11, "compare", "", false, false], [6, 8, 19, 22, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "for", "the", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "performed", "comparably", ",", "with", "the", "C", "-", "HOG", "descriptor", "maintaining", "a", "slight", "advantage", "in", "terms", "of", "detection", "miss", "rate", "for", "a", "fixed", "FALSE", "positive", "rate", "in", "both", "data", "sets", "."], "sentence-detokenized": "As for the results, the C-HOG and R-HOG block descriptors performed comparably, with the C-HOG descriptor maintaining a slight advantage in terms of detection miss rate for a fixed FALSE positive rate in both data sets.", "token2charspan": [[0, 2], [3, 6], [7, 10], [11, 18], [18, 19], [20, 23], [24, 25], [25, 26], [26, 29], [30, 33], [34, 36], [36, 39], [40, 45], [46, 57], [58, 67], [68, 78], [78, 79], [80, 84], [85, 88], [89, 90], [90, 91], [91, 94], [95, 105], [106, 117], [118, 119], [120, 126], [127, 136], [137, 139], [140, 145], [146, 148], [149, 158], [159, 163], [164, 168], [169, 172], [173, 174], [175, 180], [181, 186], [187, 195], [196, 200], [201, 203], [204, 208], [209, 213], [214, 218], [218, 219]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 15, "algorithm"], [17, 19, "algorithm"], [21, 23, "algorithm"], [25, 27, "algorithm"], [29, 32, "misc"], [36, 38, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 29, 32, "usage", "", false, false], [14, 15, 29, 32, "usage", "", false, false], [17, 19, 29, 32, "usage", "", false, false], [21, 23, 29, 32, "usage", "", false, false], [25, 27, 29, 32, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "elastic", "matching", "using", "the", "Fisherface", "algorithm", ",", "Hidden", "Markov", "Models", ",", "multilinear", "subspace", "learning", "using", "tensor", "representations", ",", "and", "neuron", "-", "driven", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic matching using the Fisherface algorithm, Hidden Markov Models, multilinear subspace learning using tensor representations, and neuron-driven dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 142], [143, 153], [154, 163], [163, 164], [165, 171], [172, 178], [179, 185], [185, 186], [187, 198], [199, 207], [208, 216], [217, 222], [223, 229], [230, 245], [245, 246], [247, 250], [251, 257], [257, 258], [258, 264], [265, 272], [273, 277], [278, 286], [286, 287]]}
{"doc_key": "ai-train-89", "ner": [[2, 7, "misc"], [17, 21, "location"], [36, 39, "location"], [52, 52, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[17, 21, 2, 7, "temporal", "", false, false], [36, 39, 2, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "may", "now", "be", "restricted", "to", "screenings", "at", "the", "Scotiabank", "Theatre", "in", "Toronto", "-", "one", "of", "the", "festival", "'s", "main", "venues", "-", "or", "elsewhere", "(", "such", "as", "the", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "distributed", "by", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films may now be restricted to screenings at the Scotiabank Theatre in Toronto - one of the festival's main venues - or elsewhere (such as the TIFF Bell Lightbox and other local cinemas) if distributed by a service such as Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 69], [70, 73], [74, 76], [77, 87], [88, 90], [91, 101], [102, 104], [105, 108], [109, 119], [120, 127], [128, 130], [131, 138], [139, 140], [141, 144], [145, 147], [148, 151], [152, 160], [160, 162], [163, 167], [168, 174], [175, 176], [177, 179], [180, 189], [190, 191], [191, 195], [196, 198], [199, 202], [203, 207], [208, 212], [213, 221], [222, 225], [226, 231], [232, 237], [238, 245], [245, 246], [247, 249], [250, 261], [262, 264], [265, 266], [267, 274], [275, 279], [280, 282], [283, 290], [290, 291]]}
{"doc_key": "ai-train-90", "ner": [[2, 2, "organisation"], [4, 5, "researcher"], [7, 7, "organisation"], [21, 25, "product"], [11, 35, "researcher"], [39, 41, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 6], "relations": [[2, 2, 7, 7, "related-to", "purchases", false, false], [4, 5, 11, 35, "named", "same", false, false], [7, 7, 4, 5, "origin", "founded_by", false, false], [21, 25, 2, 2, "artifact", "", false, false], [39, 41, 11, 35, "artifact", "", true, false]], "relations_mapping_to_source": [0, 2, 3, 4, 5], "sentence": ["In", "1977", "Unimation", "acquired", "Victor", "Scheinman", "'s", "Vicarm", "company", "and", "with", "Scheinman", "'s", "help", "the", "company", "created", "and", "began", "producing", "the", "Universal", "Machine", "for", "Programmable", "Assembly", ",", "a", "new", "type", "of", "robotic", "arm", "and", "using", "Scheinman", "'s", "cutting", "edge", "VAL", "programming", "language", "."], "sentence-detokenized": "In 1977 Unimation acquired Victor Scheinman's Vicarm company and with Scheinman's help the company created and began producing the Universal Machine for Programmable Assembly, a new type of robotic arm and using Scheinman's cutting edge VAL programming language.", "token2charspan": [[0, 2], [3, 7], [8, 17], [18, 26], [27, 33], [34, 43], [43, 45], [46, 52], [53, 60], [61, 64], [65, 69], [70, 79], [79, 81], [82, 86], [87, 90], [91, 98], [99, 106], [107, 110], [111, 116], [117, 126], [127, 130], [131, 140], [141, 148], [149, 152], [153, 165], [166, 174], [174, 175], [176, 177], [178, 181], [182, 186], [187, 189], [190, 197], [198, 201], [202, 205], [206, 211], [212, 221], [221, 223], [224, 231], [232, 236], [237, 240], [241, 252], [253, 261], [261, 262]]}
{"doc_key": "ai-train-91", "ner": [[0, 1, "product"], [6, 6, "programlang"], [9, 11, "algorithm"], [14, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 6, 6, "general-affiliation", "", false, false], [0, 1, 9, 11, "origin", "implementation_of", false, false], [0, 1, 14, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[7, 7, "metrics"], [0, 3, "product"], [23, 30, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[7, 7, 0, 3, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["According", "to", "Google", "Scholar", ",", "the", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", ",", "and", "it", "also", "won", "the", "2016", "IEEE", "Signal", "Processing", "Society", "'s", "Sustained", "Impact", "Award", ",", "indicating", "that", "a", "paper", "has", "an", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "According to Google Scholar, the 2004 SSIM paper has been cited more than 20,000 times, and it also won the 2016 IEEE Signal Processing Society's Sustained Impact Award, indicating that a paper has an unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 9], [10, 12], [13, 19], [20, 27], [27, 28], [29, 32], [33, 37], [38, 42], [43, 48], [49, 52], [53, 57], [58, 63], [64, 68], [69, 73], [74, 80], [81, 86], [86, 87], [88, 91], [92, 94], [95, 99], [100, 103], [104, 107], [108, 112], [113, 117], [118, 124], [125, 135], [136, 143], [143, 145], [146, 155], [156, 162], [163, 168], [168, 169], [170, 180], [181, 185], [186, 187], [188, 193], [194, 197], [198, 200], [201, 210], [211, 215], [216, 222], [223, 226], [227, 229], [230, 235], [236, 238], [239, 244], [245, 250], [251, 262], [262, 263]]}
{"doc_key": "ai-train-93", "ner": [[1, 1, "task"], [24, 25, "product"], [36, 40, "product"], [43, 43, "organisation"], [44, 44, "product"], [47, 47, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[1, 1, 43, 43, "artifact", "", false, false], [24, 25, 1, 1, "related-to", "performs", false, false], [24, 25, 36, 40, "part-of", "", false, false], [43, 43, 47, 47, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Speech", "synthesis", "has", "come", "close", "to", "being", "completely", "indistinguishable", "from", "the", "real", "human", "voice", "with", "the", "2016", "launch", "of", "voice", "editing", "and", "generation", "software", "Adobe", "Voco", ",", "a", "prototype", "of", "which", "is", "expected", "to", "be", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "Google", "prototype", "."], "sentence-detokenized": "Speech synthesis has come close to being completely indistinguishable from the real human voice with the 2016 launch of voice editing and generation software Adobe Voco, a prototype of which is expected to be part of the Adobe Creative Suite, and DeepMind WaveNet, a Google prototype.", "token2charspan": [[0, 6], [7, 16], [17, 20], [21, 25], [26, 31], [32, 34], [35, 40], [41, 51], [52, 69], [70, 74], [75, 78], [79, 83], [84, 89], [90, 95], [96, 100], [101, 104], [105, 109], [110, 116], [117, 119], [120, 125], [126, 133], [134, 137], [138, 148], [149, 157], [158, 163], [164, 168], [168, 169], [170, 171], [172, 181], [182, 184], [185, 190], [191, 193], [194, 202], [203, 205], [206, 208], [209, 213], [214, 216], [217, 220], [221, 226], [227, 235], [236, 241], [241, 242], [243, 246], [247, 255], [256, 263], [263, 264], [265, 266], [267, 273], [274, 283], [283, 284]]}
{"doc_key": "ai-train-94", "ner": [[0, 0, "researcher"], [6, 9, "organisation"], [14, 20, "organisation"], [27, 27, "conference"], [33, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 6, 9, "role", "", false, false], [0, 0, 14, 20, "role", "", false, false], [0, 0, 27, 27, "role", "", false, false], [0, 0, 33, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "member", "of", "the", "Neuroscience", "Research", "Program", ",", "a", "member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "and", "a", "founding", "fellow", "of", "AAAI", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary member of the Neuroscience Research Program, a member of the American Academy of Arts and Sciences, and a founding fellow of AAAI and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 65], [65, 66], [67, 68], [69, 75], [76, 78], [79, 82], [83, 91], [92, 99], [100, 102], [103, 107], [108, 111], [112, 120], [120, 121], [122, 125], [126, 127], [128, 136], [137, 143], [144, 146], [147, 151], [152, 155], [156, 157], [158, 166], [167, 173], [174, 176], [177, 180], [181, 189], [190, 199], [200, 203], [204, 209], [210, 218], [218, 219]]}
{"doc_key": "ai-train-95", "ner": [[9, 10, "task"], [12, 13, "task"], [17, 18, "task"], [25, 25, "misc"], [21, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 10, 17, 18, "cause-effect", "", false, false], [12, 13, 17, 18, "cause-effect", "", false, false], [21, 27, 17, 18, "topic", "", false, false], [21, 27, 25, 25, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "encouraged", "by", "the", "success", "of", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "into", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, encouraged by the success of speech recognition and speech synthesis, research into speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 31], [32, 39], [40, 42], [43, 49], [50, 61], [62, 65], [66, 72], [73, 82], [82, 83], [84, 92], [93, 97], [98, 104], [105, 116], [117, 122], [123, 127], [128, 131], [132, 143], [144, 146], [147, 150], [151, 157], [158, 167], [168, 175], [175, 176]]}
{"doc_key": "ai-train-96", "ner": [[3, 4, "researcher"], [8, 9, "researcher"], [11, 12, "researcher"], [14, 15, "algorithm"], [19, 20, "algorithm"], [24, 24, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[3, 4, 8, 9, "role", "", false, false], [14, 15, 3, 4, "origin", "", false, false], [14, 15, 8, 9, "origin", "", false, false], [14, 15, 11, 12, "origin", "", false, false], [14, 15, 24, 24, "part-of", "", false, false], [19, 20, 14, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", ",", "Felix", "Gers", "and", "his", "advisors", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "oblivion", "gates", "(", "also", "called", "hold", "gates", ")", "to", "the", "LSTM", "architecture", "."], "sentence-detokenized": "In 1999, Felix Gers and his advisors J\u00fcrgen Schmidhuber and Fred Cummins introduced oblivion gates (also called hold gates) to the LSTM architecture.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 14], [15, 19], [20, 23], [24, 27], [28, 36], [37, 43], [44, 55], [56, 59], [60, 64], [65, 72], [73, 83], [84, 92], [93, 98], [99, 100], [100, 104], [105, 111], [112, 116], [117, 122], [122, 123], [124, 126], [127, 130], [131, 135], [136, 148], [148, 149]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [5, 6, "field"], [9, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 1, 3, "part-of", "", false, false], [9, 11, 5, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalized", "sinc", "function", "is", "usually", "defined", "as"], "sentence-detokenized": "In digital signal processing and information theory, the normalized sinc function is usually defined as", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103]]}
{"doc_key": "ai-train-98", "ner": [[2, 3, "field"], [9, 10, "researcher"], [16, 20, "conference"], [23, 27, "organisation"], [29, 29, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 3, 9, 10, "origin", "coined_term", false, false], [9, 10, 16, 20, "role", "", false, false], [9, 10, 23, 27, "role", "", false, false], [29, 29, 23, 27, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "itself", "was", "first", "coined", "by", "David", "Hayes", ",", "a", "founding", "member", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics itself was first coined by David Hayes, a founding member of the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 41], [42, 45], [46, 51], [52, 58], [59, 61], [62, 67], [68, 73], [73, 74], [75, 76], [77, 85], [86, 92], [93, 95], [96, 99], [100, 111], [112, 115], [116, 129], [130, 141], [142, 145], [146, 149], [150, 163], [164, 173], [174, 176], [177, 190], [191, 202], [203, 204], [204, 208], [208, 209], [209, 210]]}
{"doc_key": "ai-train-99", "ner": [[8, 13, "misc"], [18, 20, "misc"], [33, 35, "metrics"], [37, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[37, 37, 33, 35, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one", "-dimensional", "polynomial", "-", "based", "memory", "(", "or", "memoryless", ")", "DPDs", ",", "in", "order", "to", "solve", "for", "the", "digital", "predistorter", "polynomial", "coefficients", "and", "minimise", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distortion", "output", "of", "the", "non-linear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "captures", "the", "non-linear", "products", "of", "the", "digital", "predistorter", "order", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional polynomial-based memory (or memoryless) DPDs, in order to solve for the digital predistorter polynomial coefficients and minimise the mean squared error (MSE), the distortion output of the non-linear system must be oversampled at a rate that captures the non-linear products of the digital predistorter order.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 35], [35, 47], [48, 58], [58, 59], [59, 64], [65, 71], [72, 73], [73, 75], [76, 86], [86, 87], [88, 92], [92, 93], [94, 96], [97, 102], [103, 105], [106, 111], [112, 115], [116, 119], [120, 127], [128, 140], [141, 151], [152, 164], [165, 168], [169, 177], [178, 181], [182, 186], [187, 194], [195, 200], [201, 202], [202, 205], [205, 206], [206, 207], [208, 211], [212, 222], [223, 229], [230, 232], [233, 236], [237, 247], [248, 254], [255, 259], [260, 262], [263, 274], [275, 277], [278, 279], [280, 284], [285, 289], [290, 298], [299, 302], [303, 313], [314, 322], [323, 325], [326, 329], [330, 337], [338, 350], [351, 356], [356, 357]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 14, "location"], [16, 16, "country"], [20, 20, "location"], [21, 22, "country"], [42, 47, "organisation"], [37, 40, "organisation"], [49, 49, "location"], [54, 57, "organisation"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 37, 40, "physical", "", false, false], [0, 1, 54, 57, "role", "", false, false], [9, 14, 16, 16, "physical", "", false, false], [42, 47, 37, 40, "part-of", "", false, false], [37, 40, 49, 49, "physical", "", false, false], [54, 57, 42, 47, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "born", "5", "October", "1947", "in", "Kiev", ",", "Moldavian", "Soviet", "Socialist", "Republic", ",", "USSR", ",", "(", "now", "Kiev", ",", "Moldova", ")", ")", "is", "a", "Principal", "American", "Research", "Scientist", "(", "computer", "scientist", ")", "at", "the", "Massachusetts", "Institute", "of", "Technology", "'s", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "in", "Cambridge", "and", "head", "of", "the", "Information", "Laboratory", "group", "at", "the", "lab", "."], "sentence-detokenized": "Boris Katz, (born 5 October 1947 in Kiev, Moldavian Soviet Socialist Republic, USSR, (now Kiev, Moldova)) is a Principal American Research Scientist (computer scientist) at the Massachusetts Institute of Technology's Computer Science and Artificial Intelligence Laboratory in Cambridge and head of the Information Laboratory group at the lab.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 17], [18, 19], [20, 27], [28, 32], [33, 35], [36, 40], [40, 41], [42, 51], [52, 58], [59, 68], [69, 77], [77, 78], [79, 83], [83, 84], [85, 86], [86, 89], [90, 94], [94, 95], [96, 103], [103, 104], [104, 105], [106, 108], [109, 110], [111, 120], [121, 129], [130, 138], [139, 148], [149, 150], [150, 158], [159, 168], [168, 169], [170, 172], [173, 176], [177, 190], [191, 200], [201, 203], [204, 214], [214, 216], [217, 225], [226, 233], [234, 237], [238, 248], [249, 261], [262, 272], [273, 275], [276, 285], [286, 289], [290, 294], [295, 297], [298, 301], [302, 313], [314, 324], [325, 330], [331, 333], [334, 337], [338, 341], [341, 342]]}
