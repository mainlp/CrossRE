{"doc_key": "ai-train-1", "ner": [[3, 7, "product"], [13, 14, "field"], [16, 17, "task"], [19, 20, "task"], [24, 26, "task"], [11, 30, "field"], [31, 33, "researcher"], [37, 37, "researcher"], [41, 42, "researcher"], [44, 48, "researcher"], [52, 52, "researcher"], [56, 57, "researcher"], [59, 61, "researcher"], [63, 64, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 7, 13, 14, "part-of", "", false, false], [3, 7, 13, 14, "usage", "", false, false], [3, 7, 16, 17, "part-of", "", false, false], [3, 7, 16, 17, "usage", "", false, false], [3, 7, 19, 20, "part-of", "", false, false], [3, 7, 19, 20, "usage", "", false, false], [3, 7, 11, 30, "part-of", "", false, false], [3, 7, 11, 30, "usage", "", false, false], [24, 26, 19, 20, "part-of", "", false, false], [24, 26, 19, 20, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Popular", "approaches", "of", "opinion", "-", "based", "recommender", "system", "use", "various", "techniques", "such", "as", "text", "mining", ",", "information", "retrieval", ",", "sentiment", "analysis", "(", "see", "also", "Multimodal", "sentiment", "analysis", ")", "and", "deep", "learning", "X.Y", ".", "Feng", ",", "H", ".", "Zhang", ",", "Y", ".", "J.", "Ren", ",", "P", ".", "H", ".", "Shang", ",", "Y", ".", "Zhu", ",", "Y", ".", "C.", "Liang", ",", "R.", "C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Popular approaches of opinion-based recommender system use various techniques such as text mining, information retrieval, sentiment analysis (see also Multimodal sentiment analysis) and deep learning X.Y. Feng, H. Zhang, Y. J. Ren, P. H. Shang, Y. Zhu, Y. C. Liang, R. C. Guan, D. Xu, (2019), 21(5): e12957.", "token2charspan": [[0, 7], [8, 18], [19, 21], [22, 29], [29, 30], [30, 35], [36, 47], [48, 54], [55, 58], [59, 66], [67, 77], [78, 82], [83, 85], [86, 90], [91, 97], [97, 98], [99, 110], [111, 120], [120, 121], [122, 131], [132, 140], [141, 142], [142, 145], [146, 150], [151, 161], [162, 171], [172, 180], [180, 181], [182, 185], [186, 190], [191, 199], [200, 203], [203, 204], [205, 209], [209, 210], [211, 212], [212, 213], [214, 219], [219, 220], [221, 222], [222, 223], [224, 226], [227, 230], [230, 231], [232, 233], [233, 234], [235, 236], [236, 237], [238, 243], [243, 244], [245, 246], [246, 247], [248, 251], [251, 252], [253, 254], [254, 255], [256, 258], [259, 264], [264, 265], [266, 268], [269, 271], [272, 276], [276, 277], [278, 280], [281, 283], [283, 284], [285, 286], [286, 290], [290, 291], [291, 292], [293, 295], [295, 296], [296, 297], [297, 298], [298, 299], [300, 306], [306, 307]]}
{"doc_key": "ai-train-2", "ner": [[7, 7, "university"], [11, 12, "researcher"], [14, 15, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[11, 12, 7, 7, "physical", "", false, false], [11, 12, 7, 7, "role", "", false, false], [14, 15, 7, 7, "physical", "", false, false], [14, 15, 7, 7, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Proponents", "of", "procedural", "representations", "are", "concentrated", "at", "MIT", ",", "led", "by", "Marvin", "Minsky", "and", "Seymour", "Papert", "."], "sentence-detokenized": "Proponents of procedural representations are concentrated at MIT, led by Marvin Minsky and Seymour Papert.", "token2charspan": [[0, 10], [11, 13], [14, 24], [25, 40], [41, 44], [45, 57], [58, 60], [61, 64], [64, 65], [66, 69], [70, 72], [73, 79], [80, 86], [87, 90], [91, 98], [99, 105], [105, 106]]}
{"doc_key": "ai-train-3", "ner": [[9, 10, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "standard", "interface", "and", "the", "calculator", "interface", "are", "written", "in", "Java", "."], "sentence-detokenized": "The standard interface and the calculator interface are written in Java.", "token2charspan": [[0, 3], [4, 12], [13, 22], [23, 26], [27, 30], [31, 41], [42, 51], [52, 55], [56, 63], [64, 66], [67, 71], [71, 72]]}
{"doc_key": "ai-train-4", "ner": [[0, 1, "product"], [22, 22, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 1, 22, 22, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "helps", "to", "numerically", "solve", "linear", "and", "nonlinear", "problems", "and", "perform", "other", "numerical", "experiments", "using", "a", "programme", "that", "is", "mostly", "compatible", "with", "MATLAB", "."], "sentence-detokenized": "Octave helps to numerically solve linear and nonlinear problems and perform other numerical experiments using a programme that is mostly compatible with MATLAB.", "token2charspan": [[0, 6], [7, 12], [13, 15], [16, 27], [28, 33], [34, 40], [41, 44], [45, 54], [55, 63], [64, 67], [68, 75], [76, 81], [82, 91], [92, 103], [104, 109], [110, 111], [112, 121], [122, 126], [127, 129], [130, 136], [137, 147], [148, 152], [153, 159], [159, 160]]}
{"doc_key": "ai-train-5", "ner": [[2, 6, "algorithm"], [11, 12, "misc"], [14, 15, "researcher"], [19, 22, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[2, 6, 14, 15, "origin", "", false, false], [11, 12, 14, 15, "origin", "", false, false], [14, 15, 19, 22, "physical", "", false, false], [14, 15, 19, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Variants", "of", "the", "back", "-", "propagation", "algorithm", ",", "as", "well", "as", "unsupervised", "methods", "by", "Geoff", "Hinton", "and", "colleagues", "at", "the", "University", "of", "Toronto", ",", "can", "be", "used", "to", "train", "deep", ",", "highly", "nonlinear", "neural", "architectures", ",", "{", "{{", "cite", "journal"], "sentence-detokenized": "Variants of the back-propagation algorithm, as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto, can be used to train deep, highly nonlinear neural architectures, {{{cite journal", "token2charspan": [[0, 8], [9, 11], [12, 15], [16, 20], [20, 21], [21, 32], [33, 42], [42, 43], [44, 46], [47, 51], [52, 54], [55, 67], [68, 75], [76, 78], [79, 84], [85, 91], [92, 95], [96, 106], [107, 109], [110, 113], [114, 124], [125, 127], [128, 135], [135, 136], [137, 140], [141, 143], [144, 148], [149, 151], [152, 157], [158, 162], [162, 163], [164, 170], [171, 180], [181, 187], [188, 201], [201, 202], [203, 204], [204, 206], [206, 210], [211, 218]]}
{"doc_key": "ai-train-6", "ner": [[3, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["or", "equivalently", "using", "DCG", "notation", ":"], "sentence-detokenized": "or equivalently using DCG notation:", "token2charspan": [[0, 2], [3, 15], [16, 21], [22, 25], [26, 34], [34, 35]]}
{"doc_key": "ai-train-7", "ner": [[0, 3, "algorithm"], [7, 9, "algorithm"], [14, 16, "algorithm"], [19, 22, "algorithm"], [27, 27, "algorithm"], [25, 26, "algorithm"], [36, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 3, 7, 9, "type-of", "", false, false], [0, 3, 14, 16, "usage", "part-of?", true, false], [14, 16, 19, 22, "compare", "", false, false], [27, 27, 19, 22, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Self", "-", "organising", "maps", "differ", "from", "other", "artificial", "neural", "networks", "in", "that", "they", "apply", "competitive", "learning", "as", "opposed", "to", "error", "-", "corrected", "learning", "such", "as", "gradient", "descent", "backpropagation", "and", "use", "a", "neighbourhood", "function", "to", "preserve", "the", "topological", "properties", "of", "the", "input", "space", "."], "sentence-detokenized": "Self-organising maps differ from other artificial neural networks in that they apply competitive learning as opposed to error-corrected learning such as gradient descent backpropagation and use a neighbourhood function to preserve the topological properties of the input space.", "token2charspan": [[0, 4], [4, 5], [5, 15], [16, 20], [21, 27], [28, 32], [33, 38], [39, 49], [50, 56], [57, 65], [66, 68], [69, 73], [74, 78], [79, 84], [85, 96], [97, 105], [106, 108], [109, 116], [117, 119], [120, 125], [125, 126], [126, 135], [136, 144], [145, 149], [150, 152], [153, 161], [162, 169], [170, 185], [186, 189], [190, 193], [194, 195], [196, 209], [210, 218], [219, 221], [222, 230], [231, 234], [235, 246], [247, 257], [258, 260], [261, 264], [265, 270], [271, 276], [276, 277]]}
{"doc_key": "ai-train-8", "ner": [[14, 19, "organisation"], [27, 28, "misc"], [33, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Since", "the", "early", "1990s", ",", "it", "has", "been", "recommended", "by", "various", "authorities", ",", "including", "the", "Audio", "Engineering", "Society", ",", "that", "dynamic", "range", "measurements", "be", "made", "with", "an", "audio", "signal", "present", "and", "then", "filtered", "in", "the", "noise", "floor", "measurement", "used", "to", "determine", "the", "dynamic", "range", ".", "This", "avoids", "questionable", "measurements", "based", "on", "the", "use", "of", "blank", "media", "or", "silent", "circuits", "."], "sentence-detokenized": "Since the early 1990s, it has been recommended by various authorities, including the Audio Engineering Society, that dynamic range measurements be made with an audio signal present and then filtered in the noise floor measurement used to determine the dynamic range. This avoids questionable measurements based on the use of blank media or silent circuits.", "token2charspan": [[0, 5], [6, 9], [10, 15], [16, 21], [21, 22], [23, 25], [26, 29], [30, 34], [35, 46], [47, 49], [50, 57], [58, 69], [69, 70], [71, 80], [81, 84], [85, 90], [91, 102], [103, 110], [110, 111], [112, 116], [117, 124], [125, 130], [131, 143], [144, 146], [147, 151], [152, 156], [157, 159], [160, 165], [166, 172], [173, 180], [181, 184], [185, 189], [190, 198], [199, 201], [202, 205], [206, 211], [212, 217], [218, 229], [230, 234], [235, 237], [238, 247], [248, 251], [252, 259], [260, 265], [265, 266], [267, 271], [272, 278], [279, 291], [292, 304], [305, 310], [311, 313], [314, 317], [318, 321], [322, 324], [325, 330], [331, 336], [337, 339], [340, 346], [347, 355], [355, 356]]}
{"doc_key": "ai-train-9", "ner": [[5, 5, "misc"], [17, 17, "task"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [28, 28, "task"], [29, 33, "task"], [35, 37, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[5, 5, 17, 17, "part-of", "concept_used_in", true, false], [5, 5, 19, 20, "part-of", "concept_used_in", false, false], [5, 5, 22, 23, "part-of", "concept_used_in", false, false], [5, 5, 25, 26, "part-of", "concept_used_in", false, false], [5, 5, 28, 28, "part-of", "concept_used_in", false, false], [5, 5, 29, 33, "part-of", "concept_used_in", false, false], [5, 5, 35, 37, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["The", "technique", "used", "to", "generate", "eigenfaces", "and", "use", "them", "for", "recognition", "is", "also", "used", "outside", "of", "face", "recognition", ":", "handwriting", "recognition", ",", "lip", "reading", ",", "voice", "recognition", ",", "sign", "language", "/", "hand", "gesture", "interpretation", "and", "medical", "imaging", "analysis", "."], "sentence-detokenized": "The technique used to generate eigenfaces and use them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gesture interpretation and medical imaging analysis.", "token2charspan": [[0, 3], [4, 13], [14, 18], [19, 21], [22, 30], [31, 41], [42, 45], [46, 49], [50, 54], [55, 58], [59, 70], [71, 73], [74, 78], [79, 83], [84, 91], [92, 94], [95, 99], [100, 111], [111, 112], [113, 124], [125, 136], [136, 137], [138, 141], [142, 149], [149, 150], [151, 156], [157, 168], [168, 169], [170, 174], [175, 183], [183, 184], [184, 188], [189, 196], [197, 211], [212, 215], [216, 223], [224, 231], [232, 240], [240, 241]]}
{"doc_key": "ai-train-10", "ner": [[0, 3, "organisation"], [12, 16, "organisation"], [18, 18, "organisation"], [22, 28, "organisation"], [29, 32, "organisation"], [35, 40, "organisation"], [41, 47, "organisation"], [48, 48, "organisation"], [51, 54, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[12, 16, 0, 3, "part-of", "", false, false], [18, 18, 12, 16, "named", "", false, false], [22, 28, 0, 3, "part-of", "", false, false], [29, 32, 0, 3, "part-of", "", false, false], [35, 40, 0, 3, "part-of", "", false, false], [41, 47, 0, 3, "part-of", "", false, false], [48, 48, 41, 47, "named", "", false, false], [51, 54, 0, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "National", "Science", "Foundation", "was", "an", "umbrella", "for", "work", "coordinated", "by", "the", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "the", "US", "Department", "of", "Energy", ",", "the", "US", "Department", "of", "Commerce", "NIST", ",", "the", "US", "Department", "of", "Defence", ",", "the", "Defence", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "and", "the", "Office", "of", "Naval", "Research", "to", "inform", "strategic", "planners", "in", "their", "deliberations", "."], "sentence-detokenized": "The National Science Foundation was an umbrella for work coordinated by the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defence, the Defence Advanced Research Projects Agency (DARPA) and the Office of Naval Research to inform strategic planners in their deliberations.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 38], [39, 47], [48, 51], [52, 56], [57, 68], [69, 71], [72, 75], [76, 84], [85, 96], [97, 100], [101, 106], [107, 121], [122, 123], [123, 127], [127, 128], [128, 129], [130, 133], [134, 136], [137, 147], [148, 150], [151, 157], [157, 158], [159, 162], [163, 165], [166, 176], [177, 179], [180, 188], [189, 193], [193, 194], [195, 198], [199, 201], [202, 212], [213, 215], [216, 223], [223, 224], [225, 228], [229, 236], [237, 245], [246, 254], [255, 263], [264, 270], [271, 272], [272, 277], [277, 278], [279, 282], [283, 286], [287, 293], [294, 296], [297, 302], [303, 311], [312, 314], [315, 321], [322, 331], [332, 340], [341, 343], [344, 349], [350, 363], [363, 364]]}
{"doc_key": "ai-train-11", "ner": [[5, 6, "metrics"], [9, 11, "algorithm"], [15, 17, "researcher"], [23, 23, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 6, 9, 11, "part-of", "", false, false], [15, 17, 23, 23, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["A", "fast", "method", "for", "calculating", "maximum", "likelihood", "estimates", "for", "the", "probit", "model", "was", "proposed", "by", "Ronald", "Fisher", "in", "1935", "as", "a", "supplement", "to", "Bliss", "'", "work", "."], "sentence-detokenized": "A fast method for calculating maximum likelihood estimates for the probit model was proposed by Ronald Fisher in 1935 as a supplement to Bliss' work.", "token2charspan": [[0, 1], [2, 6], [7, 13], [14, 17], [18, 29], [30, 37], [38, 48], [49, 58], [59, 62], [63, 66], [67, 73], [74, 79], [80, 83], [84, 92], [93, 95], [96, 102], [103, 109], [110, 112], [113, 117], [118, 120], [121, 122], [123, 133], [134, 136], [137, 142], [142, 143], [144, 148], [148, 149]]}
{"doc_key": "ai-train-12", "ner": [[10, 11, "product"], [14, 15, "product"], [18, 18, "organisation"], [20, 20, "product"], [22, 22, "organisation"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[20, 20, 14, 15, "usage", "uses_software", false, false], [20, 20, 18, 18, "artifact", "", false, false], [20, 20, 24, 24, "named", "", false, false], [24, 24, 22, 22, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Many", "of", "these", "programs", "are", "available", "online", ",", "such", "as", "Google", "Translate", "and", "the", "SYSTRAN", "system", "that", "powers", "AltaVista", "'s", "BabelFish", "(", "Yahoo", "'s", "Babelfish", "as", "of", "9", "May", "2008", ")", "."], "sentence-detokenized": "Many of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (Yahoo's Babelfish as of 9 May 2008).", "token2charspan": [[0, 4], [5, 7], [8, 13], [14, 22], [23, 26], [27, 36], [37, 43], [43, 44], [45, 49], [50, 52], [53, 59], [60, 69], [70, 73], [74, 77], [78, 85], [86, 92], [93, 97], [98, 104], [105, 114], [114, 116], [117, 126], [127, 128], [128, 133], [133, 135], [136, 145], [146, 148], [149, 151], [152, 153], [154, 157], [158, 162], [162, 163], [163, 164]]}
{"doc_key": "ai-train-13", "ner": [[11, 11, "researcher"], [5, 6, "researcher"], [3, 9, "researcher"], [19, 21, "field"], [25, 26, "misc"], [32, 32, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[11, 11, 19, 21, "related-to", "", true, false], [11, 11, 25, 26, "related-to", "", true, false], [11, 11, 32, 32, "related-to", "", true, false], [5, 6, 19, 21, "related-to", "", true, false], [5, 6, 25, 26, "related-to", "", true, false], [5, 6, 32, 32, "related-to", "", true, false], [3, 9, 19, 21, "related-to", "", true, false], [3, 9, 25, 26, "related-to", "", true, false], [3, 9, 32, 32, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["In", "2002", ",", "together", "with", "J\u00fcrgen", "Schmidhuber", "and", "Shane", "Legg", ",", "Hutter", "developed", "and", "published", "a", "mathematical", "theory", "of", "artificial", "general", "intelligence", "based", "on", "idealised", "intelligent", "agents", "and", "reward", "-", "driven", "reinforcement", "learning", "."], "sentence-detokenized": "In 2002, together with J\u00fcrgen Schmidhuber and Shane Legg, Hutter developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-driven reinforcement learning.", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 17], [18, 22], [23, 29], [30, 41], [42, 45], [46, 51], [52, 56], [56, 57], [58, 64], [65, 74], [75, 78], [79, 88], [89, 90], [91, 103], [104, 110], [111, 113], [114, 124], [125, 132], [133, 145], [146, 151], [152, 154], [155, 164], [165, 176], [177, 183], [184, 187], [188, 194], [194, 195], [195, 201], [202, 215], [216, 224], [224, 225]]}
{"doc_key": "ai-train-14", "ner": [[11, 11, "metrics"], [13, 19, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[11, 11, 13, 19, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "most", "common", "way", "is", "to", "use", "the", "so", "-", "called", "ROUGE", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "."], "sentence-detokenized": "The most common way is to use the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 19], [20, 22], [23, 25], [26, 29], [30, 33], [34, 36], [36, 37], [37, 43], [44, 49], [50, 51], [51, 57], [57, 58], [58, 66], [67, 77], [78, 81], [82, 89], [90, 100], [100, 101], [101, 102]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [13, 14, "programlang"], [15, 15, "programlang"], [18, 19, "researcher"], [21, 22, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 13, 14, "related-to", "", false, false], [0, 0, 15, 15, "related-to", "", false, false], [18, 19, 21, 22, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["RapidMiner", "provides", "learning", "schemes", ",", "models", "and", "algorithms", "and", "can", "be", "extended", "using", "R", "and", "Python", "scripts", ".", "David", "Norris", ",", "Bloor", "Research", ",", "13", "November", "2013", "."], "sentence-detokenized": "RapidMiner provides learning schemes, models and algorithms and can be extended using R and Python scripts. David Norris, Bloor Research, 13 November 2013.", "token2charspan": [[0, 10], [11, 19], [20, 28], [29, 36], [36, 37], [38, 44], [45, 48], [49, 59], [60, 63], [64, 67], [68, 70], [71, 79], [80, 85], [86, 87], [88, 91], [92, 98], [99, 106], [106, 107], [108, 113], [114, 120], [120, 121], [122, 127], [128, 136], [136, 137], [138, 140], [141, 149], [150, 154], [154, 155]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 11, "field"], [14, 14, "task"], [18, 20, "misc"], [38, 39, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 5], "relations": [[0, 0, 10, 11, "related-to", "", false, false], [0, 0, 14, 14, "related-to", "", false, false], [0, 0, 38, 39, "related-to", "", true, false], [18, 20, 0, 0, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["tity", "includes", "a", "collection", "of", "visualisation", "tools", "and", "algorithms", "for", "data", "analysis", "and", "predictive", "modelling", ",", "together", "with", "graphical", "user", "interfaces", "for", "easy", "access", "to", "these", "functions", ".", "however", ",", "the", "newer", "fully", "Java", "-", "based", "version", "(", "Weka", "3", ")", ",", "which", "began", "development", "in", "1997", ",", "is", "now", "used", "in", "many", "different", "application", "areas", ",", "especially", "for", "educational", "purposes", "and", "research", "."], "sentence-detokenized": "tity includes a collection of visualisation tools and algorithms for data analysis and predictive modelling, together with graphical user interfaces for easy access to these functions. however, the newer fully Java-based version (Weka 3), which began development in 1997, is now used in many different application areas, especially for educational purposes and research.", "token2charspan": [[0, 4], [5, 13], [14, 15], [16, 26], [27, 29], [30, 43], [44, 49], [50, 53], [54, 64], [65, 68], [69, 73], [74, 82], [83, 86], [87, 97], [98, 107], [107, 108], [109, 117], [118, 122], [123, 132], [133, 137], [138, 148], [149, 152], [153, 157], [158, 164], [165, 167], [168, 173], [174, 183], [183, 184], [185, 192], [192, 193], [194, 197], [198, 203], [204, 209], [210, 214], [214, 215], [215, 220], [221, 228], [229, 230], [230, 234], [235, 236], [236, 237], [237, 238], [239, 244], [245, 250], [251, 262], [263, 265], [266, 270], [270, 271], [272, 274], [275, 278], [279, 283], [284, 286], [287, 291], [292, 301], [302, 313], [314, 319], [319, 320], [321, 331], [332, 335], [336, 347], [348, 356], [357, 360], [361, 369], [369, 370]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [6, 23, "misc"], [25, 28, "misc"], [31, 38, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 23, 0, 0, "topic", "", false, false], [6, 23, 25, 28, "win-defeat", "", false, false], [25, 28, 31, 38, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "made", "many", "interesting", "discoveries", "and", "was", "highly", "acclaimed", "for", "his", "work", "Heuretics", ":", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", ",", "which", "won", "the", "Best", "Paper", "award", "at", "the", "1982", "Association", "for", "the", "Development", "of", "Artificial", "Intelligence", "."], "sentence-detokenized": "Eurisko made many interesting discoveries and was highly acclaimed for his work Heuretics: Heuretics: Theoretical and Study of Heuristic Rules, which won the Best Paper award at the 1982 Association for the Development of Artificial Intelligence.", "token2charspan": [[0, 7], [8, 12], [13, 17], [18, 29], [30, 41], [42, 45], [46, 49], [50, 56], [57, 66], [67, 70], [71, 74], [75, 79], [80, 89], [89, 90], [91, 100], [100, 101], [102, 113], [114, 117], [118, 123], [124, 126], [127, 136], [137, 142], [142, 143], [144, 149], [150, 153], [154, 157], [158, 162], [163, 168], [169, 174], [175, 177], [178, 181], [182, 186], [187, 198], [199, 202], [203, 206], [207, 218], [219, 221], [222, 232], [233, 245], [245, 246]]}
{"doc_key": "ai-train-18", "ner": [[2, 3, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["A", "separate", "Hinge", "loss", "is", "calculated", "for", "each", "capsule", "to", "allow", "multiple", "entities", "."], "sentence-detokenized": "A separate Hinge loss is calculated for each capsule to allow multiple entities.", "token2charspan": [[0, 1], [2, 10], [11, 16], [17, 21], [22, 24], [25, 35], [36, 39], [40, 44], [45, 52], [53, 55], [56, 61], [62, 70], [71, 79], [79, 80]]}
{"doc_key": "ai-train-19", "ner": [[8, 10, "product"], [12, 13, "product"], [15, 16, "product"], [18, 19, "product"], [4, 23, "product"], [25, 27, "product"], [35, 39, "product"], [42, 43, "product"], [45, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[8, 10, 25, 27, "type-of", "", false, false], [12, 13, 25, 27, "type-of", "", false, false], [15, 16, 25, 27, "type-of", "", false, false], [18, 19, 25, 27, "type-of", "", false, false], [4, 23, 25, 27, "type-of", "", false, false], [42, 43, 35, 39, "type-of", "", false, false], [45, 46, 35, 39, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["With", "the", "advent", "of", "voice", "assistants", "such", "as", "Apple", "'s", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "and", "Samsung", "'s", "Bixby", ",", "Voice", "Portals", "can", "now", "be", "accessed", "through", "mobile", "devices", "and", "Remote", "Area", "voice", "smart", "speakers", "such", "as", "Amazon", "Echo", "and", "Google", "Home", "."], "sentence-detokenized": "With the advent of voice assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana and Samsung's Bixby, Voice Portals can now be accessed through mobile devices and Remote Area voice smart speakers such as Amazon Echo and Google Home.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 18], [19, 24], [25, 35], [36, 40], [41, 43], [44, 49], [49, 51], [52, 56], [56, 57], [58, 64], [65, 70], [70, 71], [72, 78], [79, 88], [88, 89], [90, 99], [100, 107], [108, 111], [112, 119], [119, 121], [122, 127], [127, 128], [129, 134], [135, 142], [143, 146], [147, 150], [151, 153], [154, 162], [163, 170], [171, 177], [178, 185], [186, 189], [190, 196], [197, 201], [202, 207], [208, 213], [214, 222], [223, 227], [228, 230], [231, 237], [238, 242], [243, 246], [247, 253], [254, 258], [258, 259]]}
{"doc_key": "ai-train-20", "ner": [[2, 4, "field"], [5, 7, "algorithm"], [9, 11, "algorithm"], [13, 14, "algorithm"], [16, 16, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 7, 2, 4, "type-of", "", false, false], [9, 11, 2, 4, "type-of", "", false, false], [13, 14, 2, 4, "type-of", "", false, false], [16, 16, 2, 4, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Examples", "of", "supervised", "learning", "are", "Naive", "Bayes", "classifier", ",", "Support", "vector", "machine", ",", "Gaussian", "mixtures", "and", "network", "."], "sentence-detokenized": "Examples of supervised learning are Naive Bayes classifier, Support vector machine, Gaussian mixtures and network.", "token2charspan": [[0, 8], [9, 11], [12, 22], [23, 31], [32, 35], [36, 41], [42, 47], [48, 58], [58, 59], [60, 67], [68, 74], [75, 82], [82, 83], [84, 92], [93, 101], [102, 105], [106, 113], [113, 114]]}
{"doc_key": "ai-train-21", "ner": [[0, 2, "algorithm"], [27, 30, "algorithm"], [31, 32, "task"], [37, 40, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 27, 30, "part-of", "", true, false], [37, 40, 31, 32, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "OSD", "algorithm", "can", "be", "used", "to", "derive", "the", "regret", "bounds", "maths", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "maths", "for", "the", "online", "version", "of", "the", "support", "vector", "machine", "for", "classification", ";", "these", "bounds", "use", "the", "hinge", "loss", "maths", "v", "_t", "(", "w", ")", "=\\", "max", "\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w", "\\", "cdot", "x", "_t", ")", "\\}", "/", "maths"], "sentence-detokenized": "The OSD algorithm can be used to derive the regret bounds maths O(\\ sqrt {T}) / maths for the online version of the support vector machine for classification; these bounds use the hinge loss maths v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} / maths", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 24], [25, 29], [30, 32], [33, 39], [40, 43], [44, 50], [51, 57], [58, 63], [64, 65], [65, 66], [66, 67], [68, 72], [73, 74], [74, 75], [75, 76], [76, 77], [78, 79], [80, 85], [86, 89], [90, 93], [94, 100], [101, 108], [109, 111], [112, 115], [116, 123], [124, 130], [131, 138], [139, 142], [143, 157], [157, 158], [159, 164], [165, 171], [172, 175], [176, 179], [180, 185], [186, 190], [191, 196], [197, 198], [199, 201], [202, 203], [203, 204], [204, 205], [206, 208], [209, 212], [212, 213], [214, 215], [215, 216], [216, 217], [218, 219], [220, 221], [222, 223], [224, 226], [227, 228], [228, 229], [229, 230], [231, 235], [236, 237], [238, 240], [240, 241], [241, 243], [244, 245], [246, 251]]}
{"doc_key": "ai-train-22", "ner": [[2, 3, "task"], [5, 6, "task"], [8, 8, "task"], [10, 11, "task"], [13, 14, "task"], [17, 17, "task"], [19, 20, "task"], [22, 25, "task"], [16, 28, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Applications", "include", "object", "recognition", ",", "robotic", "mapping", "and", "navigation", ",", "image", "fusion", ",", "3D", "modelling", ",", "motion", "recognition", ",", "video", "tracking", ",", "individual", "identification", "of", "wildlife", "and", "motion", "matching", "."], "sentence-detokenized": "Applications include object recognition, robotic mapping and navigation, image fusion, 3D modelling, motion recognition, video tracking, individual identification of wildlife and motion matching.", "token2charspan": [[0, 12], [13, 20], [21, 27], [28, 39], [39, 40], [41, 48], [49, 56], [57, 60], [61, 71], [71, 72], [73, 78], [79, 85], [85, 86], [87, 89], [90, 99], [99, 100], [101, 107], [108, 119], [119, 120], [121, 126], [127, 135], [135, 136], [137, 147], [148, 162], [163, 165], [166, 174], [175, 178], [179, 185], [186, 194], [194, 195]]}
{"doc_key": "ai-train-23", "ner": [[8, 11, "task"], [14, 15, "university"], [17, 19, "university"], [21, 22, "university"], [24, 25, "university"], [27, 32, "university"], [34, 36, "university"], [38, 40, "university"], [42, 43, "university"], [45, 50, "university"], [52, 52, "university"], [56, 60, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[8, 11, 14, 15, "related-to", "", true, false], [8, 11, 17, 19, "related-to", "", true, false], [8, 11, 21, 22, "related-to", "", true, false], [8, 11, 24, 25, "related-to", "", true, false], [8, 11, 27, 32, "related-to", "", true, false], [8, 11, 34, 36, "related-to", "", true, false], [8, 11, 38, 40, "related-to", "", true, false], [8, 11, 42, 43, "related-to", "", true, false], [8, 11, 45, 50, "related-to", "", true, false], [8, 11, 52, 52, "related-to", "", true, false], [8, 11, 56, 60, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["A", "number", "of", "groups", "and", "companies", "are", "investigating", "pose", "estimation", ",", "including", "groups", "at", "Brown", "University", ",", "Carnegie", "Mellon", "University", ",", "MPI", "Saarbruecken", ",", "Stanford", "University", ",", "University", "of", "California", ",", "San", "Diego", ",", "University", "of", "Toronto", ",", "\u00c9cole", "Centrale", "Paris", ",", "ETH", "Zurich", ",", "National", "University", "of", "Science", "and", "Technology", "(", "NUST", ")", ",", "and", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "A number of groups and companies are investigating pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, University of California, San Diego, University of Toronto, \u00c9cole Centrale Paris, ETH Zurich, National University of Science and Technology (NUST), and University of California, Irvine.", "token2charspan": [[0, 1], [2, 8], [9, 11], [12, 18], [19, 22], [23, 32], [33, 36], [37, 50], [51, 55], [56, 66], [66, 67], [68, 77], [78, 84], [85, 87], [88, 93], [94, 104], [104, 105], [106, 114], [115, 121], [122, 132], [132, 133], [134, 137], [138, 150], [150, 151], [152, 160], [161, 171], [171, 172], [173, 183], [184, 186], [187, 197], [197, 198], [199, 202], [203, 208], [208, 209], [210, 220], [221, 223], [224, 231], [231, 232], [233, 238], [239, 247], [248, 253], [253, 254], [255, 258], [259, 265], [265, 266], [267, 275], [276, 286], [287, 289], [290, 297], [298, 301], [302, 312], [313, 314], [314, 318], [318, 319], [319, 320], [321, 324], [325, 335], [336, 338], [339, 349], [349, 350], [351, 357], [357, 358]]}
{"doc_key": "ai-train-24", "ner": [[0, 5, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Sigmoid", "function", "Cross", "entropy", "loss", "is", "used", "to", "estimate", "K", "independent", "probability", "values", "in", "maths", "0,1", "/", "maths", "."], "sentence-detokenized": "Sigmoid function Cross entropy loss is used to estimate K independent probability values in maths 0,1 / maths.", "token2charspan": [[0, 7], [8, 16], [17, 22], [23, 30], [31, 35], [36, 38], [39, 43], [44, 46], [47, 55], [56, 57], [58, 69], [70, 81], [82, 88], [89, 91], [92, 97], [98, 101], [102, 103], [104, 109], [109, 110]]}
{"doc_key": "ai-train-25", "ner": [[10, 11, "misc"], [14, 14, "field"], [16, 16, "field"], [17, 21, "university"], [23, 24, "country"], [27, 29, "misc"], [30, 35, "university"], [37, 37, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[10, 11, 14, 14, "topic", "", false, false], [10, 11, 16, 16, "topic", "", false, false], [10, 11, 17, 21, "physical", "", true, false], [17, 21, 23, 24, "physical", "", false, false], [27, 29, 30, 35, "physical", "", true, false], [30, 35, 37, 37, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Before", "becoming", "a", "Professor", "at", "Cambridge", ",", "he", "held", "the", "Johann", "Bernoulli", "Chair", "in", "Mathematics", "and", "Informatics", "at", "the", "University", "of", "Groningen", "in", "the", "Netherlands", "and", "the", "Toshiba", "Endowed", "Chair", "at", "the", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "."], "sentence-detokenized": "Before becoming a Professor at Cambridge, he held the Johann Bernoulli Chair in Mathematics and Informatics at the University of Groningen in the Netherlands and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan.", "token2charspan": [[0, 6], [7, 15], [16, 17], [18, 27], [28, 30], [31, 40], [40, 41], [42, 44], [45, 49], [50, 53], [54, 60], [61, 70], [71, 76], [77, 79], [80, 91], [92, 95], [96, 107], [108, 110], [111, 114], [115, 125], [126, 128], [129, 138], [139, 141], [142, 145], [146, 157], [158, 161], [162, 165], [166, 173], [174, 181], [182, 187], [188, 190], [191, 194], [195, 200], [201, 210], [211, 213], [214, 224], [225, 227], [228, 233], [233, 234]]}
{"doc_key": "ai-train-26", "ner": [[5, 6, "algorithm"], [10, 14, "algorithm"], [16, 16, "algorithm"], [21, 22, "researcher"], [24, 25, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 10, 14, "usage", "", true, false], [10, 14, 21, 22, "origin", "", false, false], [10, 14, 24, 25, "origin", "", false, false], [16, 16, 10, 14, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Another", "technique", "used", "especially", "for", "recurrent", "neural", "networks", "is", "the", "long", "short", "-", "term", "memory", "(", "LSTM", ")", "network", "developed", "by", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "in", "1997", "."], "sentence-detokenized": "Another technique used especially for recurrent neural networks is the long short-term memory (LSTM) network developed by Sepp Hochreiter & J\u00fcrgen Schmidhuber in 1997.", "token2charspan": [[0, 7], [8, 17], [18, 22], [23, 33], [34, 37], [38, 47], [48, 54], [55, 63], [64, 66], [67, 70], [71, 75], [76, 81], [81, 82], [82, 86], [87, 93], [94, 95], [95, 99], [99, 100], [101, 108], [109, 118], [119, 121], [122, 126], [127, 137], [138, 139], [140, 146], [147, 158], [159, 161], [162, 166], [166, 167]]}
{"doc_key": "ai-train-27", "ner": [[4, 5, "programlang"], [8, 13, "product"], [15, 15, "product"], [46, 46, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[8, 13, 4, 5, "general-affiliation", "", false, false], [8, 13, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "inclusion", "of", "a", "C", "++", "interpreter", "(", "CI", "NT", "up", "to", "version", "5.34", ",", "Cling", "from", "version", "6", ")", "makes", "this", "package", "very", "versatile", "as", "it", "can", "be", "used", "in", "interactive", ",", "scripted", "and", "compiled", "modes", "in", "a", "similar", "way", "to", "commercial", "products", "such", "as", "MATLAB", "."], "sentence-detokenized": "The inclusion of a C++ interpreter (CINT up to version 5.34, Cling from version 6) makes this package very versatile as it can be used in interactive, scripted and compiled modes in a similar way to commercial products such as MATLAB.", "token2charspan": [[0, 3], [4, 13], [14, 16], [17, 18], [19, 20], [20, 22], [23, 34], [35, 36], [36, 38], [38, 40], [41, 43], [44, 46], [47, 54], [55, 59], [59, 60], [61, 66], [67, 71], [72, 79], [80, 81], [81, 82], [83, 88], [89, 93], [94, 101], [102, 106], [107, 116], [117, 119], [120, 122], [123, 126], [127, 129], [130, 134], [135, 137], [138, 149], [149, 150], [151, 159], [160, 163], [164, 172], [173, 178], [179, 181], [182, 183], [184, 191], [192, 195], [196, 198], [199, 209], [210, 218], [219, 223], [224, 226], [227, 233], [233, 234]]}
{"doc_key": "ai-train-28", "ner": [[2, 5, "product"], [22, 24, "field"], [28, 29, "task"], [31, 33, "task"], [35, 36, "task"], [38, 39, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[2, 5, 22, 24, "related-to", "", false, false], [28, 29, 22, 24, "part-of", "", false, false], [31, 33, 22, 24, "part-of", "", false, false], [35, 36, 22, 24, "part-of", "", false, false], [38, 39, 22, 24, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "design", "of", "voice", "user", "interfaces", "that", "interpret", "and", "manage", "speech", "state", "is", "challenging", "due", "to", "the", "inherent", "difficulty", "of", "integrating", "complex", "natural", "language", "processing", "tasks", "such", "as", "coreference", "resolution", ",", "named", "entity", "recognition", ",", "information", "retrieval", "and", "dialogue", "management", "."], "sentence-detokenized": "The design of voice user interfaces that interpret and manage speech state is challenging due to the inherent difficulty of integrating complex natural language processing tasks such as coreference resolution, named entity recognition, information retrieval and dialogue management.", "token2charspan": [[0, 3], [4, 10], [11, 13], [14, 19], [20, 24], [25, 35], [36, 40], [41, 50], [51, 54], [55, 61], [62, 68], [69, 74], [75, 77], [78, 89], [90, 93], [94, 96], [97, 100], [101, 109], [110, 120], [121, 123], [124, 135], [136, 143], [144, 151], [152, 160], [161, 171], [172, 177], [178, 182], [183, 185], [186, 197], [198, 208], [208, 209], [210, 215], [216, 222], [223, 234], [234, 235], [236, 247], [248, 257], [258, 261], [262, 270], [271, 281], [281, 282]]}
{"doc_key": "ai-train-29", "ner": [[5, 6, "algorithm"], [9, 13, "algorithm"], [20, 22, "researcher"], [23, 29, "organisation"], [35, 36, "field"], [38, 39, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[5, 6, 20, 22, "origin", "", false, false], [5, 6, 35, 36, "part-of", "", false, false], [5, 6, 38, 39, "part-of", "", false, false], [9, 13, 20, 22, "origin", "", false, false], [9, 13, 35, 36, "part-of", "", false, false], [9, 13, 38, 39, "part-of", "", false, false], [20, 22, 23, 29, "physical", "", false, false], [20, 22, 23, 29, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Between", "2009", "and", "2012", ",", "recurrent", "neural", "networks", "and", "deep", "feed", "-", "forward", "neural", "networks", "developed", "in", "the", "research", "group", "of", "J\u00fcrgen", "Schmidhuber", "at", "the", "Swiss", "Artificial", "Intelligence", "Laboratory", "IDSIA", "won", "eight", "international", "competitions", "in", "pattern", "recognition", "and", "machine", "learning", "."], "sentence-detokenized": "Between 2009 and 2012, recurrent neural networks and deep feed-forward neural networks developed in the research group of J\u00fcrgen Schmidhuber at the Swiss Artificial Intelligence Laboratory IDSIA won eight international competitions in pattern recognition and machine learning.", "token2charspan": [[0, 7], [8, 12], [13, 16], [17, 21], [21, 22], [23, 32], [33, 39], [40, 48], [49, 52], [53, 57], [58, 62], [62, 63], [63, 70], [71, 77], [78, 86], [87, 96], [97, 99], [100, 103], [104, 112], [113, 118], [119, 121], [122, 128], [129, 140], [141, 143], [144, 147], [148, 153], [154, 164], [165, 177], [178, 188], [189, 194], [195, 198], [199, 204], [205, 218], [219, 231], [232, 234], [235, 242], [243, 254], [255, 258], [259, 266], [267, 275], [275, 276]]}
{"doc_key": "ai-train-30", "ner": [[1, 3, "product"], [6, 7, "product"], [10, 10, "product"], [14, 15, "task"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 3, 6, 7, "usage", "", false, false], [1, 3, 10, 10, "usage", "", false, false], [1, 3, 14, 15, "usage", "", true, false], [1, 3, 17, 17, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Modern", "Windows", "desktop", "systems", "can", "use", "SAPI", "4", "and", "SAPI", "5", "components", "to", "support", "speech", "synthesis", "and", "speech", "."], "sentence-detokenized": "Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.", "token2charspan": [[0, 6], [7, 14], [15, 22], [23, 30], [31, 34], [35, 38], [39, 43], [44, 45], [46, 49], [50, 54], [55, 56], [57, 67], [68, 70], [71, 78], [79, 85], [86, 95], [96, 99], [100, 106], [106, 107]]}
{"doc_key": "ai-train-31", "ner": [[7, 23, "misc"], [14, 14, "field"], [16, 19, "university"], [26, 29, "field"], [31, 34, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 23, 14, 14, "topic", "topic_of_award", false, false], [7, 23, 16, 19, "origin", "", true, false], [26, 29, 31, 34, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "received", "two", "honorary", "degrees", ",", "the", "S.", "V.", "della", "laurea", "ad", "honem", "in", "Psychology", "from", "the", "University", "of", "Padua", "in", "1995", "and", "a", "PhD", "in", "Industrial", "Design", "and", "Engineering", "from", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "He received two honorary degrees, the S. V. della laurea ad honem in Psychology from the University of Padua in 1995 and a PhD in Industrial Design and Engineering from Delft University of Technology.", "token2charspan": [[0, 2], [3, 11], [12, 15], [16, 24], [25, 32], [32, 33], [34, 37], [38, 40], [41, 43], [44, 49], [50, 56], [57, 59], [60, 65], [66, 68], [69, 79], [80, 84], [85, 88], [89, 99], [100, 102], [103, 108], [109, 111], [112, 116], [117, 120], [121, 122], [123, 126], [127, 129], [130, 140], [141, 147], [148, 151], [152, 163], [164, 168], [169, 174], [175, 185], [186, 188], [189, 199], [199, 200]]}
{"doc_key": "ai-train-32", "ner": [[0, 8, "researcher"], [12, 18, "organisation"], [19, 19, "location"], [21, 21, "researcher"], [31, 32, "misc"], [31, 52, "misc"], [65, 71, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 8, 12, 18, "physical", "", false, false], [0, 8, 12, 18, "role", "", false, false], [12, 18, 19, 19, "physical", "", false, false], [21, 21, 31, 32, "related-to", "works_with", true, false], [21, 21, 31, 52, "related-to", "works_with", true, false], [21, 21, 65, 71, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Together", "with", "his", "long", "-", "time", "collaborator", "Laurent", "Cohen", ",", "a", "neurologist", "at", "the", "Piti\u00e9", "-", "Salp\u00eatri\u00e8re", "Hospital", "in", "Paris", ",", "Dehaene", "also", "identified", "patients", "with", "lesions", "in", "different", "regions", "of", "the", "parietal", "lobe", ",", "in", "whom", "multiplication", "was", "impaired", "but", "subtraction", "was", "preserved", "(", "associated", "with", "lesions", "of", "the", "inferior", "parietal", "lobule", ")", "and", "in", "whom", "subtraction", "was", "impaired", "but", "multiplication", "was", "preserved", "(", "associated", "with", "lesions", "of", "the", "intraparietal", "sulcus", ")", "."], "sentence-detokenized": "Together with his long-time collaborator Laurent Cohen, a neurologist at the Piti\u00e9-Salp\u00eatri\u00e8re Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe, in whom multiplication was impaired but subtraction was preserved (associated with lesions of the inferior parietal lobule) and in whom subtraction was impaired but multiplication was preserved (associated with lesions of the intraparietal sulcus).", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [22, 23], [23, 27], [28, 40], [41, 48], [49, 54], [54, 55], [56, 57], [58, 69], [70, 72], [73, 76], [77, 82], [82, 83], [83, 94], [95, 103], [104, 106], [107, 112], [112, 113], [114, 121], [122, 126], [127, 137], [138, 146], [147, 151], [152, 159], [160, 162], [163, 172], [173, 180], [181, 183], [184, 187], [188, 196], [197, 201], [201, 202], [203, 205], [206, 210], [211, 225], [226, 229], [230, 238], [239, 242], [243, 254], [255, 258], [259, 268], [269, 270], [270, 280], [281, 285], [286, 293], [294, 296], [297, 300], [301, 309], [310, 318], [319, 325], [325, 326], [327, 330], [331, 333], [334, 338], [339, 350], [351, 354], [355, 363], [364, 367], [368, 382], [383, 386], [387, 396], [397, 398], [398, 408], [409, 413], [414, 421], [422, 424], [425, 428], [429, 442], [443, 449], [449, 450], [450, 451]]}
{"doc_key": "ai-train-33", "ner": [[5, 7, "product"], [12, 15, "misc"], [17, 18, "misc"], [26, 26, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 15, 5, 7, "topic", "", false, false], [17, 18, 5, 7, "topic", "", false, false], [26, 26, 5, 7, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["More", "recently", ",", "fictional", "representations", "of", "AI", "robots", "in", "films", "such", "as", "A.I", ".", "Artificial", "Intelligence", "and", "Ex", "Machina", ",", "and", "the", "2016", "TV", "adaptation", "of", "Westworld", ",", "have", "made", "audiences", "sympathise", "with", "the", "robots", "themselves", "."], "sentence-detokenized": "More recently, fictional representations of AI robots in films such as A.I. Artificial Intelligence and Ex Machina, and the 2016 TV adaptation of Westworld, have made audiences sympathise with the robots themselves.", "token2charspan": [[0, 4], [5, 13], [13, 14], [15, 24], [25, 40], [41, 43], [44, 46], [47, 53], [54, 56], [57, 62], [63, 67], [68, 70], [71, 74], [74, 75], [76, 86], [87, 99], [100, 103], [104, 106], [107, 114], [114, 115], [116, 119], [120, 123], [124, 128], [129, 131], [132, 142], [143, 145], [146, 155], [155, 156], [157, 161], [162, 166], [167, 176], [177, 187], [188, 192], [193, 196], [197, 203], [204, 214], [214, 215]]}
{"doc_key": "ai-train-34", "ner": [[6, 9, "field"], [10, 15, "algorithm"], [14, 14, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[10, 15, 6, 9, "part-of", "", false, false], [14, 14, 6, 9, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Two", "of", "the", "main", "methods", "used", "in", "unsupervised", "learning", "are", "principal", "component", "analysis", "and", "cluster", "analysis", "."], "sentence-detokenized": "Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 15], [16, 23], [24, 28], [29, 31], [32, 44], [45, 53], [54, 57], [58, 67], [68, 77], [78, 86], [87, 90], [91, 98], [99, 107], [107, 108]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [24, 25, "misc"], [30, 31, "misc"], [33, 35, "person"], [40, 41, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[24, 25, 0, 3, "artifact", "", false, false], [30, 31, 0, 3, "artifact", "", false, false], [30, 31, 33, 35, "role", "director_of", false, false], [30, 31, 40, 41, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "Walt", "Disney", "Company", "also", "began", "to", "use", "3D", "films", "more", "prominently", "in", "special", "locations", "to", "impress", "audiences", ",", "with", "notable", "examples", "such", "as", "Magic", "Journeys", "(", "1982", ")", "and", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "starring", "Michael", "Jackson", ")", "."], "sentence-detokenized": "The Walt Disney Company also began to use 3D films more prominently in special locations to impress audiences, with notable examples such as Magic Journeys (1982) and Captain EO (Francis Ford Coppola, 1986, starring Michael Jackson).", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 28], [29, 34], [35, 37], [38, 41], [42, 44], [45, 50], [51, 55], [56, 67], [68, 70], [71, 78], [79, 88], [89, 91], [92, 99], [100, 109], [109, 110], [111, 115], [116, 123], [124, 132], [133, 137], [138, 140], [141, 146], [147, 155], [156, 157], [157, 161], [161, 162], [163, 166], [167, 174], [175, 177], [178, 179], [179, 186], [187, 191], [192, 199], [199, 200], [201, 205], [205, 206], [207, 215], [216, 223], [224, 231], [231, 232], [232, 233]]}
{"doc_key": "ai-train-36", "ner": [[9, 12, "field"], [16, 21, "task"], [23, 24, "task"], [26, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[16, 21, 9, 12, "part-of", "", false, false], [23, 24, 9, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Since", "2002", ",", "perceptron", "training", "has", "become", "popular", "in", "natural", "language", "processing", "for", "tasks", "such", "as", "part", "-", "of", "-", "speech", "tagging", "and", "syntactic", "parsing", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Since 2002, perceptron training has become popular in natural language processing for tasks such as part-of-speech tagging and syntactic parsing (Collins, 2002).", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 22], [23, 31], [32, 35], [36, 42], [43, 50], [51, 53], [54, 61], [62, 70], [71, 81], [82, 85], [86, 91], [92, 96], [97, 99], [100, 104], [104, 105], [105, 107], [107, 108], [108, 114], [115, 122], [123, 126], [127, 136], [137, 144], [145, 146], [146, 153], [153, 154], [155, 159], [159, 160], [160, 161]]}
{"doc_key": "ai-train-37", "ner": [[2, 3, "product"], [9, 12, "organisation"], [11, 21, "organisation"], [17, 18, "country"], [24, 29, "product"], [34, 35, "researcher"], [41, 42, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[9, 12, 2, 3, "role", "introduces_to_market", true, false], [11, 21, 2, 3, "role", "introduces_to_market", true, false], [11, 21, 17, 18, "physical", "", false, false], [24, 29, 41, 42, "related-to", "sold_to", true, false], [34, 35, 24, 29, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["The", "first", "palletising", "robot", "was", "introduced", "in", "1963", "by", "Fuji", "Yusoki", "Kogyo", "Company", ".", "It", "was", "invented", "in", "Germany", "by", "KUKA", "robotics", "and", "the", "Programmable", "Universal", "Machine", "for", "Assembly", "was", "invented", "in", "1976", "by", "Victor", "Scheinman", "and", "the", "design", "was", "sold", "to", "Unimation", "."], "sentence-detokenized": "The first palletising robot was introduced in 1963 by Fuji Yusoki Kogyo Company. It was invented in Germany by KUKA robotics and the Programmable Universal Machine for Assembly was invented in 1976 by Victor Scheinman and the design was sold to Unimation.", "token2charspan": [[0, 3], [4, 9], [10, 21], [22, 27], [28, 31], [32, 42], [43, 45], [46, 50], [51, 53], [54, 58], [59, 65], [66, 71], [72, 79], [79, 80], [81, 83], [84, 87], [88, 96], [97, 99], [100, 107], [108, 110], [111, 115], [116, 124], [125, 128], [129, 132], [133, 145], [146, 155], [156, 163], [164, 167], [168, 176], [177, 180], [181, 189], [190, 192], [193, 197], [198, 200], [201, 207], [208, 217], [218, 221], [222, 225], [226, 232], [233, 236], [237, 241], [242, 244], [245, 254], [254, 255]]}
{"doc_key": "ai-train-38", "ner": [[5, 7, "conference"], [11, 11, "researcher"], [22, 23, "field"], [29, 30, "researcher"], [37, 40, "researcher"], [50, 51, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[11, 11, 5, 7, "role", "president_of", false, false], [11, 11, 29, 30, "role", "colleagues", false, false], [22, 23, 50, 51, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["While", "serving", "as", "president", "of", "the", "AAAI", "in", "the", "mid-1990s", ",", "Hayes", "launched", "a", "series", "of", "mostly", "ironically", "worded", "attacks", "against", "critics", "of", "AI", ",", "and", "(", "with", "colleague", "Kenneth", "Ford", ")", "invented", "a", "prize", "named", "after", "Simon", "Newcomb", "to", "be", "awarded", "to", "the", "most", "absurd", "argument", "refuting", "the", "possibility", "of", "AI", "."], "sentence-detokenized": "While serving as president of the AAAI in the mid-1990s, Hayes launched a series of mostly ironically worded attacks against critics of AI, and (with colleague Kenneth Ford) invented a prize named after Simon Newcomb to be awarded to the most absurd argument refuting the possibility of AI.", "token2charspan": [[0, 5], [6, 13], [14, 16], [17, 26], [27, 29], [30, 33], [34, 38], [39, 41], [42, 45], [46, 55], [55, 56], [57, 62], [63, 71], [72, 73], [74, 80], [81, 83], [84, 90], [91, 101], [102, 108], [109, 116], [117, 124], [125, 132], [133, 135], [136, 138], [138, 139], [140, 143], [144, 145], [145, 149], [150, 159], [160, 167], [168, 172], [172, 173], [174, 182], [183, 184], [185, 190], [191, 196], [197, 202], [203, 208], [209, 216], [217, 219], [220, 222], [223, 230], [231, 233], [234, 237], [238, 242], [243, 249], [250, 258], [259, 267], [268, 271], [272, 283], [284, 286], [287, 289], [289, 290]]}
{"doc_key": "ai-train-39", "ner": [[9, 27, "algorithm"], [53, 57, "algorithm"], [61, 63, "algorithm"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[53, 57, 9, 27, "type-of", "", false, false], [61, 63, 9, 27, "type-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["The", "optimal", "value", "for", "math", "\\", "alpha", "/", "math", "can", "be", "found", "using", "a", "line", "search", "algorithm", ",", "i.e.", "the", "size", "of", "math", "\\", "alpha", "/", "math", "is", "determined", "by", "finding", "the", "value", "that", "minimises", "S", ",", "usually", "using", "a", "line", "search", "in", "the", "range", "math0", "\\", "alpha", "1", "/", "math", "or", "a", "trace", "-", "back", "line", "search", "such", "as", "the", "Armijo", "line", "search", "."], "sentence-detokenized": "The optimal value for math\\ alpha / math can be found using a line search algorithm, i.e. the size of math\\ alpha / math is determined by finding the value that minimises S, usually using a line search in the range math0\\ alpha 1 / math or a trace-back line search such as the Armijo line search.", "token2charspan": [[0, 3], [4, 11], [12, 17], [18, 21], [22, 26], [26, 27], [28, 33], [34, 35], [36, 40], [41, 44], [45, 47], [48, 53], [54, 59], [60, 61], [62, 66], [67, 73], [74, 83], [83, 84], [85, 89], [90, 93], [94, 98], [99, 101], [102, 106], [106, 107], [108, 113], [114, 115], [116, 120], [121, 123], [124, 134], [135, 137], [138, 145], [146, 149], [150, 155], [156, 160], [161, 170], [171, 172], [172, 173], [174, 181], [182, 187], [188, 189], [190, 194], [195, 201], [202, 204], [205, 208], [209, 214], [215, 220], [220, 221], [222, 227], [228, 229], [230, 231], [232, 236], [237, 239], [240, 241], [242, 247], [247, 248], [248, 252], [253, 257], [258, 264], [265, 269], [270, 272], [273, 276], [277, 283], [284, 288], [289, 295], [295, 296]]}
{"doc_key": "ai-train-40", "ner": [[2, 5, "algorithm"], [7, 10, "algorithm"], [20, 20, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "discusses", "breadth", "-", "first", "search", "and", "depth", "-", "first", "search", "techniques", ",", "but", "ultimately", "concludes", "that", "the", "results", "represent", "expert", "systems", "with", "a", "lot", "of", "technical", "knowledge", ",", "but", "do", "not", "shed", "much", "light", "on", "the", "mental", "processes", "people", "use", "to", "solve", "such", "puzzles", "."], "sentence-detokenized": "He discusses breadth-first search and depth-first search techniques, but ultimately concludes that the results represent expert systems with a lot of technical knowledge, but do not shed much light on the mental processes people use to solve such puzzles.", "token2charspan": [[0, 2], [3, 12], [13, 20], [20, 21], [21, 26], [27, 33], [34, 37], [38, 43], [43, 44], [44, 49], [50, 56], [57, 67], [67, 68], [69, 72], [73, 83], [84, 93], [94, 98], [99, 102], [103, 110], [111, 120], [121, 127], [128, 135], [136, 140], [141, 142], [143, 146], [147, 149], [150, 159], [160, 169], [169, 170], [171, 174], [175, 177], [178, 181], [182, 186], [187, 191], [192, 197], [198, 200], [201, 204], [205, 211], [212, 221], [222, 228], [229, 232], [233, 235], [236, 241], [242, 246], [247, 254], [254, 255]]}
{"doc_key": "ai-train-41", "ner": [[0, 1, "task"], [4, 8, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Speech", "recognition", "and", "speech", "synthesis", "deals", "with", "how", "spoken", "language", "can", "be", "understood", "or", "generated", "using", "computers", "."], "sentence-detokenized": "Speech recognition and speech synthesis deals with how spoken language can be understood or generated using computers.", "token2charspan": [[0, 6], [7, 18], [19, 22], [23, 29], [30, 39], [40, 45], [46, 50], [51, 54], [55, 61], [62, 70], [71, 74], [75, 77], [78, 88], [89, 91], [92, 101], [102, 107], [108, 117], [117, 118]]}
{"doc_key": "ai-train-42", "ner": [[14, 15, "algorithm"], [33, 35, "algorithm"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["This", "math", "\\", "theta", "^", "{", "*}", "/", "math", "is", "normally", "estimated", "using", "the", "Maximum", "Likelihood", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "or", "Maximum", "A", "Posteriori", "(", "math", "\\", "theta", "^", "{", "*}", "=\\", "theta", "^", "{", "MAP", "}", "/", "math", ")", "procedure", "."], "sentence-detokenized": "This math\\ theta ^ {*} / math is normally estimated using the Maximum Likelihood (math\\ theta ^ {*} =\\ theta ^ {ML} / math) or Maximum A Posteriori (math\\ theta ^ {*} =\\ theta ^ {MAP} / math) procedure.", "token2charspan": [[0, 4], [5, 9], [9, 10], [11, 16], [17, 18], [19, 20], [20, 22], [23, 24], [25, 29], [30, 32], [33, 41], [42, 51], [52, 57], [58, 61], [62, 69], [70, 80], [81, 82], [82, 86], [86, 87], [88, 93], [94, 95], [96, 97], [97, 99], [100, 102], [103, 108], [109, 110], [111, 112], [112, 114], [114, 115], [116, 117], [118, 122], [122, 123], [124, 126], [127, 134], [135, 136], [137, 147], [148, 149], [149, 153], [153, 154], [155, 160], [161, 162], [163, 164], [164, 166], [167, 169], [170, 175], [176, 177], [178, 179], [179, 182], [182, 183], [184, 185], [186, 190], [190, 191], [192, 201], [201, 202]]}
{"doc_key": "ai-train-43", "ner": [[6, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Some", "less", "commonly", "spoken", "languages", "use", "the", "open", "source", "eSpeak", "synthesiser", "for", "their", "speech", ",", "producing", "a", "robotic", ",", "strange", "sound", "that", "can", "be", "difficult", "to", "understand", "."], "sentence-detokenized": "Some less commonly spoken languages use the open source eSpeak synthesiser for their speech, producing a robotic, strange sound that can be difficult to understand.", "token2charspan": [[0, 4], [5, 9], [10, 18], [19, 25], [26, 35], [36, 39], [40, 43], [44, 48], [49, 55], [56, 62], [63, 74], [75, 78], [79, 84], [85, 91], [91, 92], [93, 102], [103, 104], [105, 112], [112, 113], [114, 121], [122, 127], [128, 132], [133, 136], [137, 139], [140, 149], [150, 152], [153, 163], [163, 164]]}
{"doc_key": "ai-train-44", "ner": [[19, 19, "programlang"], [34, 35, "programlang"], [37, 37, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[19, 19, 34, 35, "compare", "", false, false], [19, 19, 37, 37, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Although", "mainly", "used", "by", "statisticians", "and", "other", "practitioners", "who", "need", "an", "environment", "for", "statistical", "computing", "and", "software", "development", ",", "R", "can", "also", "work", "as", "a", "general", "matrix", "computing", "toolbox", "with", "performance", "metrics", "comparable", "to", "GNU", "Octave", "or", "MATLAB", "."], "sentence-detokenized": "Although mainly used by statisticians and other practitioners who need an environment for statistical computing and software development, R can also work as a general matrix computing toolbox with performance metrics comparable to GNU Octave or MATLAB.", "token2charspan": [[0, 8], [9, 15], [16, 20], [21, 23], [24, 37], [38, 41], [42, 47], [48, 61], [62, 65], [66, 70], [71, 73], [74, 85], [86, 89], [90, 101], [102, 111], [112, 115], [116, 124], [125, 136], [136, 137], [138, 139], [140, 143], [144, 148], [149, 153], [154, 156], [157, 158], [159, 166], [167, 173], [174, 183], [184, 191], [192, 196], [197, 208], [209, 216], [217, 227], [228, 230], [231, 234], [235, 241], [242, 244], [245, 251], [251, 252]]}
{"doc_key": "ai-train-45", "ner": [[0, 1, "algorithm"], [3, 4, "field"], [8, 11, "misc"], [12, 13, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 3, 4, "part-of", "", false, false], [0, 1, 12, 13, "origin", "", false, false], [8, 11, 12, 13, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Heterodyning", "is", "a", "signal", "processing", "technique", "invented", "by", "Canadian", "inventor", "-", "engineer", "Reginald", "Fessenden", "that", "creates", "new", "frequencies", "by", "mixing", "two", "frequencies", "."], "sentence-detokenized": "Heterodyning is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden that creates new frequencies by mixing two frequencies.", "token2charspan": [[0, 12], [13, 15], [16, 17], [18, 24], [25, 35], [36, 45], [46, 54], [55, 57], [58, 66], [67, 75], [75, 76], [76, 84], [85, 93], [94, 103], [104, 108], [109, 116], [117, 120], [121, 132], [133, 135], [136, 142], [143, 146], [147, 158], [158, 159]]}
{"doc_key": "ai-train-46", "ner": [[13, 14, "person"], [16, 16, "misc"], [20, 22, "organisation"], [25, 26, "organisation"], [27, 29, "misc"], [30, 32, "person"], [35, 35, "organisation"], [37, 39, "misc"], [41, 42, "person"], [44, 45, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 14, 16, 16, "role", "actor_in", false, false], [16, 16, 20, 22, "artifact", "", false, false], [27, 29, 25, 26, "artifact", "", false, false], [30, 32, 27, 29, "role", "actor_in", false, false], [37, 39, 35, 35, "artifact", "", false, false], [41, 42, 37, 39, "role", "actor_in", false, false], [44, 45, 37, 39, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Other", "films", "that", "helped", "put", "3D", "back", "on", "the", "agenda", "that", "month", "included", "John", "Wayne", "'s", "Hondo", "(", "distributed", "by", "Warner", "Bros", ".", ")", ",", "Columbia", "'s", "Miss", "Sadie", "Thompson", "with", "Rita", "Hayworth", ",", "and", "Paramount", "'s", "Money", "From", "Home", "with", "Dean", "Martin", "and", "Jerry", "Lewis", "."], "sentence-detokenized": "Other films that helped put 3D back on the agenda that month included John Wayne's Hondo (distributed by Warner Bros.), Columbia's Miss Sadie Thompson with Rita Hayworth, and Paramount's Money From Home with Dean Martin and Jerry Lewis.", "token2charspan": [[0, 5], [6, 11], [12, 16], [17, 23], [24, 27], [28, 30], [31, 35], [36, 38], [39, 42], [43, 49], [50, 54], [55, 60], [61, 69], [70, 74], [75, 80], [80, 82], [83, 88], [89, 90], [90, 101], [102, 104], [105, 111], [112, 116], [116, 117], [117, 118], [118, 119], [120, 128], [128, 130], [131, 135], [136, 141], [142, 150], [151, 155], [156, 160], [161, 169], [169, 170], [171, 174], [175, 184], [184, 186], [187, 192], [193, 197], [198, 202], [203, 207], [208, 212], [213, 219], [220, 223], [224, 229], [230, 235], [235, 236]]}
{"doc_key": "ai-train-47", "ner": [[0, 1, "product"], [3, 4, "field"], [5, 6, "task"], [10, 14, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 5, 6, "general-affiliation", "", false, false], [0, 1, 10, 14, "artifact", "", false, false], [5, 6, 3, 4, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["DeepFace", "is", "a", "deep", "learning", "facial", "recognition", "system", "created", "by", "a", "research", "group", "at", "Facebook", "."], "sentence-detokenized": "DeepFace is a deep learning facial recognition system created by a research group at Facebook.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 18], [19, 27], [28, 34], [35, 46], [47, 53], [54, 61], [62, 64], [65, 66], [67, 75], [76, 81], [82, 84], [85, 93], [93, 94]]}
{"doc_key": "ai-train-48", "ner": [[0, 1, "field"], [8, 8, "conference"], [12, 13, "field"], [26, 27, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 1, 12, 13, "part-of", "subfield", false, false], [8, 8, 0, 1, "topic", "", false, false], [26, 27, 0, 1, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometry", "processing", "is", "a", "prevalent", "research", "topic", "at", "SIGGRAPH", ",", "the", "leading", "computer", "graphics", "academic", "conference", ",", "and", "is", "the", "main", "topic", "of", "the", "annual", "Geometry", "Processing", "Symposium", "."], "sentence-detokenized": "Geometry processing is a prevalent research topic at SIGGRAPH, the leading computer graphics academic conference, and is the main topic of the annual Geometry Processing Symposium.", "token2charspan": [[0, 8], [9, 19], [20, 22], [23, 24], [25, 34], [35, 43], [44, 49], [50, 52], [53, 61], [61, 62], [63, 66], [67, 74], [75, 83], [84, 92], [93, 101], [102, 112], [112, 113], [114, 117], [118, 120], [121, 124], [125, 129], [130, 135], [136, 138], [139, 142], [143, 149], [150, 158], [159, 169], [170, 179], [179, 180]]}
{"doc_key": "ai-train-49", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [36, 36, "misc"], [43, 45, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[13, 15, 36, 36, "general-affiliation", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 36, 36, "general-affiliation", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 36, 36, "general-affiliation", "", false, false], [31, 31, 27, 29, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "Principal", "Component", "Analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", "or", "canonical", "correlation", "analysis", "(", "CCA", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "with", "k", "-", "NN", "on", "the", "feature", "vectors", "in", "the", "reduced", "dimension", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in a single step using Principal Component Analysis (PCA), linear discriminant analysis (LDA) or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering with k -NN on the feature vectors in the reduced dimension space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [153, 155], [156, 165], [166, 177], [178, 186], [187, 188], [188, 191], [191, 192], [193, 203], [204, 206], [207, 208], [209, 223], [224, 228], [228, 229], [230, 238], [239, 241], [242, 252], [253, 257], [258, 259], [260, 261], [261, 263], [264, 266], [267, 270], [271, 278], [279, 286], [287, 289], [290, 293], [294, 301], [302, 311], [312, 317], [317, 318]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [8, 13, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 8, 13, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Artificial", "neural", "networks", "are", "computational", "models", "that", "excel", "in", "machine", "learning", "and", "pattern", "recognition", "."], "sentence-detokenized": "Artificial neural networks are computational models that excel in machine learning and pattern recognition.", "token2charspan": [[0, 10], [11, 17], [18, 26], [27, 30], [31, 44], [45, 51], [52, 56], [57, 62], [63, 65], [66, 73], [74, 82], [83, 86], [87, 94], [95, 106], [106, 107]]}
{"doc_key": "ai-train-51", "ner": [[1, 3, "researcher"], [4, 5, "researcher"], [7, 11, "misc"], [13, 17, "conference"], [19, 19, "conference"], [34, 37, "algorithm"], [38, 39, "researcher"], [41, 43, "researcher"], [45, 51, "misc"], [53, 62, "conference"], [64, 64, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[7, 11, 1, 3, "artifact", "", false, false], [7, 11, 4, 5, "artifact", "", false, false], [7, 11, 13, 17, "temporal", "", false, false], [19, 19, 13, 17, "named", "", false, false], [45, 51, 34, 37, "topic", "", false, false], [45, 51, 38, 39, "artifact", "", false, false], [45, 51, 41, 43, "artifact", "", false, false], [45, 51, 53, 62, "temporal", "", false, false], [64, 64, 53, 62, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": [",", "C.", "Papageorgiou", "and", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "pages", "1:15", "-", "33", ",", "2000", "others", "use", "local", "features", "such", "as", "histograms", "of", "oriented", "gradients", "N.", "Dalal", ",", "B", ".", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "pages", "1:886", "-", "893", ",", "2005", "descriptors", "."], "sentence-detokenized": ", C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1:15-33, 2000 others use local features such as histograms of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1:886-893, 2005 descriptors.", "token2charspan": [[0, 1], [2, 4], [5, 17], [18, 21], [22, 24], [25, 31], [31, 32], [33, 34], [35, 44], [45, 55], [56, 65], [66, 72], [72, 73], [74, 87], [88, 95], [96, 98], [99, 107], [108, 114], [115, 116], [116, 120], [120, 121], [121, 122], [123, 128], [129, 133], [133, 134], [134, 136], [136, 137], [138, 142], [143, 149], [150, 153], [154, 159], [160, 168], [169, 173], [174, 176], [177, 187], [188, 190], [191, 199], [200, 209], [210, 212], [213, 218], [218, 219], [220, 221], [221, 222], [223, 229], [229, 230], [231, 241], [242, 244], [245, 253], [254, 263], [264, 267], [268, 273], [274, 283], [283, 284], [285, 289], [290, 298], [299, 306], [307, 317], [318, 320], [321, 329], [330, 336], [337, 340], [341, 348], [349, 360], [361, 362], [362, 366], [366, 367], [367, 368], [369, 374], [375, 380], [380, 381], [381, 384], [384, 385], [386, 390], [391, 402], [402, 403]]}
{"doc_key": "ai-train-52", "ner": [[1, 3, "algorithm"], [6, 8, "algorithm"], [12, 14, "task"], [16, 16, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 3, 6, 8, "type-of", "", false, false], [12, 14, 1, 3, "usage", "", true, false], [12, 14, 16, 16, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["An", "autoencoder", "is", "a", "type", "of", "artificial", "neural", "network", "used", "to", "learn", "feature", "learning", "in", "an", "unsupervised", "manner", "."], "sentence-detokenized": "An autoencoder is a type of artificial neural network used to learn feature learning in an unsupervised manner.", "token2charspan": [[0, 2], [3, 14], [15, 17], [18, 19], [20, 24], [25, 27], [28, 38], [39, 45], [46, 53], [54, 58], [59, 61], [62, 67], [68, 75], [76, 84], [85, 87], [88, 90], [91, 103], [104, 110], [110, 111]]}
{"doc_key": "ai-train-53", "ner": [[0, 2, "researcher"], [6, 6, "organisation"], [10, 11, "field"], [9, 14, "field"], [20, 28, "organisation"], [22, 32, "field"], [34, 35, "field"], [39, 41, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7, 8], "relations": [[0, 2, 6, 6, "role", "fellow_of", false, false], [0, 2, 10, 11, "related-to", "contributes_to", false, false], [0, 2, 9, 14, "related-to", "contributes_to", false, false], [0, 2, 20, 28, "role", "fellow_of", false, false], [0, 2, 22, 32, "related-to", "contributes_to", false, false], [0, 2, 34, 35, "related-to", "contributes_to", false, false], [39, 41, 20, 28, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 7], "sentence": ["Haralick", "is", "a", "Fellow", "of", "the", "IEEE", "for", "contributions", "in", "computer", "vision", "and", "image", "processing", "and", "a", "Fellow", "of", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "for", "contributions", "in", "pattern", "recognition", ",", "image", "processing", ",", "and", "service", "to", "the", "IAPR", "."], "sentence-detokenized": "Haralick is a Fellow of the IEEE for contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for contributions in pattern recognition, image processing, and service to the IAPR.", "token2charspan": [[0, 8], [9, 11], [12, 13], [14, 20], [21, 23], [24, 27], [28, 32], [33, 36], [37, 50], [51, 53], [54, 62], [63, 69], [70, 73], [74, 79], [80, 90], [91, 94], [95, 96], [97, 103], [104, 106], [107, 110], [111, 124], [125, 136], [137, 140], [141, 148], [149, 160], [161, 162], [162, 166], [166, 167], [168, 171], [172, 185], [186, 188], [189, 196], [197, 208], [208, 209], [210, 215], [216, 226], [226, 227], [228, 231], [232, 239], [240, 242], [243, 246], [247, 251], [251, 252]]}
{"doc_key": "ai-train-54", "ner": [[4, 11, "task"], [12, 14, "algorithm"], [16, 16, "algorithm"], [24, 25, "researcher"], [27, 28, "organisation"], [30, 31, "researcher"], [33, 36, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 11, 12, 14, "usage", "", false, false], [12, 14, 24, 25, "origin", "", true, false], [12, 14, 30, 31, "origin", "", true, false], [16, 16, 12, 14, "named", "", false, false], [24, 25, 27, 28, "physical", "", false, false], [24, 25, 27, 28, "role", "", false, false], [30, 31, 33, 36, "physical", "", false, false], [30, 31, 33, 36, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["The", "first", "attempt", "at", "end", "-", "to", "-", "end", "ASR", "was", "the", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "based", "systems", "introduced", "in", "2014", "by", "Alex", "Graves", "of", "Google", "DeepMind", "and", "Navdeep", "Jaitly", "of", "the", "University", "of", "Toronto", "."], "sentence-detokenized": "The first attempt at end-to-end ASR was the Connectionist Temporal Classification (CTC) based systems introduced in 2014 by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto.", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 20], [21, 24], [24, 25], [25, 27], [27, 28], [28, 31], [32, 35], [36, 39], [40, 43], [44, 57], [58, 66], [67, 81], [82, 83], [83, 86], [86, 87], [88, 93], [94, 101], [102, 112], [113, 115], [116, 120], [121, 123], [124, 128], [129, 135], [136, 138], [139, 145], [146, 154], [155, 158], [159, 166], [167, 173], [174, 176], [177, 180], [181, 191], [192, 194], [195, 202], [202, 203]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 10, "algorithm"], [11, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 10, 0, 2, "named", "", false, false], [11, 11, 0, 2, "type-of", "", false, false], [13, 13, 11, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Linear-", "fractional", "programming", "(", "LFP", ")", "is", "a", "generalisation", "of", "linear", "programming", "(", "LP", ")", "."], "sentence-detokenized": "Linear-fractional programming (LFP) is a generalisation of linear programming (LP).", "token2charspan": [[0, 7], [7, 17], [18, 29], [30, 31], [31, 34], [34, 35], [36, 38], [39, 40], [41, 55], [56, 58], [59, 65], [66, 77], [78, 79], [79, 81], [81, 82], [82, 83]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [8, 13, "misc"], [15, 22, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 8, 13, "win-defeat", "", false, false], [8, 13, 15, 22, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "has", "received", "numerous", "awards", ",", "including", "two", "Test", "-", "of", "-", "Time", "awards", "at", "the", "International", "Machine", "Learning", "Conference", "2011", "and", "2012", ","], "sentence-detokenized": "Lafferty has received numerous awards, including two Test-of-Time awards at the International Machine Learning Conference 2011 and 2012,", "token2charspan": [[0, 8], [9, 12], [13, 21], [22, 30], [31, 37], [37, 38], [39, 48], [49, 52], [53, 57], [57, 58], [58, 60], [60, 61], [61, 65], [66, 72], [73, 75], [76, 79], [80, 93], [94, 101], [102, 110], [111, 121], [122, 126], [127, 130], [131, 135], [135, 136]]}
{"doc_key": "ai-train-57", "ner": [[10, 10, "product"], [12, 12, "programlang"], [23, 24, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["With", "the", "emergence", "of", "component", "-", "based", "frameworks", "such", "as", ".NET", "and", "Java", ",", "component", "-", "based", "development", "environments", "can", "deploy", "the", "developed", "neural", "network", "as", "inheritable", "components", "to", "these", "frameworks", "."], "sentence-detokenized": "With the emergence of component-based frameworks such as .NET and Java, component-based development environments can deploy the developed neural network as inheritable components to these frameworks.", "token2charspan": [[0, 4], [5, 8], [9, 18], [19, 21], [22, 31], [31, 32], [32, 37], [38, 48], [49, 53], [54, 56], [57, 61], [62, 65], [66, 70], [70, 71], [72, 81], [81, 82], [82, 87], [88, 99], [100, 112], [113, 116], [117, 123], [124, 127], [128, 137], [138, 144], [145, 152], [153, 155], [156, 167], [168, 178], [179, 181], [182, 187], [188, 198], [198, 199]]}
{"doc_key": "ai-train-58", "ner": [[0, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["As", "in", "BLEU", ",", "the", "basic", "evaluation", "unit", "is", "the", "sentence", ",", "the", "algorithm", "first", "creates", "an", "alignment", "(", "see", "pictures", ")", "between", "the", "two", "sentences", ",", "the", "candidate", "translation", "string", "and", "the", "reference", "translation", "string", "."], "sentence-detokenized": "As in BLEU, the basic evaluation unit is the sentence, the algorithm first creates an alignment (see pictures) between the two sentences, the candidate translation string and the reference translation string.", "token2charspan": [[0, 2], [3, 5], [6, 10], [10, 11], [12, 15], [16, 21], [22, 32], [33, 37], [38, 40], [41, 44], [45, 53], [53, 54], [55, 58], [59, 68], [69, 74], [75, 82], [83, 85], [86, 95], [96, 97], [97, 100], [101, 109], [109, 110], [111, 118], [119, 122], [123, 126], [127, 136], [136, 137], [138, 141], [142, 151], [152, 163], [164, 170], [171, 174], [175, 178], [179, 188], [189, 200], [201, 207], [207, 208]]}
{"doc_key": "ai-train-59", "ner": [[5, 11, "conference"], [21, 21, "task"], [23, 26, "task"], [3, 29, "metrics"], [31, 37, "metrics"], [42, 45, "conference"], [47, 47, "conference"], [50, 50, "location"], [52, 52, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[5, 11, 21, 21, "related-to", "subject_at", false, false], [5, 11, 23, 26, "related-to", "subject_at", false, false], [3, 29, 5, 11, "temporal", "", false, false], [31, 37, 3, 29, "named", "", true, false], [47, 47, 42, 45, "named", "", false, false], [50, 50, 52, 52, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["One", "of", "the", "metrics", "used", "at", "NIST", "'s", "annual", "Document", "Understanding", "Conferences", ",", "where", "research", "groups", "present", "their", "systems", "for", "both", "summarisation", "and", "translation", "tasks", ",", "is", "the", "ROUGE", "metric", "(", "Recall", "-", "Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "in", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Canada", ",", "December", "-", "2014", "."], "sentence-detokenized": "One of the metrics used at NIST's annual Document Understanding Conferences, where research groups present their systems for both summarisation and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation, in Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.", "token2charspan": [[0, 3], [4, 6], [7, 10], [11, 18], [19, 23], [24, 26], [27, 31], [31, 33], [34, 40], [41, 49], [50, 63], [64, 75], [75, 76], [77, 82], [83, 91], [92, 98], [99, 106], [107, 112], [113, 120], [121, 124], [125, 129], [130, 143], [144, 147], [148, 159], [160, 165], [165, 166], [167, 169], [170, 173], [174, 179], [180, 186], [187, 188], [188, 194], [194, 195], [195, 203], [204, 214], [215, 218], [219, 226], [227, 237], [237, 238], [239, 241], [242, 250], [251, 253], [254, 260], [261, 272], [273, 283], [284, 291], [292, 293], [293, 297], [297, 298], [298, 299], [300, 308], [308, 309], [310, 316], [316, 317], [318, 326], [327, 328], [329, 333], [333, 334]]}
{"doc_key": "ai-train-60", "ner": [[7, 7, "programlang"], [9, 9, "product"], [12, 12, "programlang"], [16, 16, "product"], [22, 22, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 7, 12, 12, "type-of", "", false, false], [7, 7, 22, 22, "named", "", false, false], [9, 9, 12, 12, "part-of", "", false, false], [9, 9, 16, 16, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "same", "application", ",", "to", "run", "in", "Java", "with", "JShell", "(", "Java", "9", "minimum", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "The same application, to run in Java with JShell (Java 9 minimum): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 3], [4, 8], [9, 20], [20, 21], [22, 24], [25, 28], [29, 31], [32, 36], [37, 41], [42, 48], [49, 50], [50, 54], [55, 56], [57, 64], [64, 65], [65, 66], [67, 77], [78, 88], [89, 90], [91, 110], [111, 115], [116, 117], [118, 122]]}
{"doc_key": "ai-train-61", "ner": [[0, 2, "metrics"], [6, 8, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 6, 8, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "NIST", "metric", "is", "based", "on", "the", "BLEU", "metric", ",", "but", "with", "some", "modifications", "."], "sentence-detokenized": "The NIST metric is based on the BLEU metric, but with some modifications.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 18], [19, 24], [25, 27], [28, 31], [32, 36], [37, 43], [43, 44], [45, 48], [49, 53], [54, 58], [59, 72], [72, 73]]}
{"doc_key": "ai-train-62", "ner": [[6, 6, "country"], [9, 12, "university"], [14, 17, "university"], [24, 25, "product"], [28, 30, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[9, 12, 6, 6, "physical", "", false, false], [14, 17, 6, 6, "physical", "", false, false], [24, 25, 9, 12, "origin", "", false, false], [24, 25, 14, 17, "origin", "", false, false], [24, 25, 28, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "the", "late", "1980s", ",", "two", "Dutch", "universities", ",", "the", "University", "of", "Groningen", "and", "the", "University", "of", "Twente", ",", "jointly", "initiated", "a", "project", "called", "Knowledge", "Graphs", ",", "which", "are", "semantic", "networks", ",", "but", "with", "the", "added", "constraint", "that", "edges", "must", "be", "from", "a", "finite", "set", "of", "possible", "relations", "to", "facilitate", "algebra", "on", "the", "graph", "."], "sentence-detokenized": "In the late 1980s, two Dutch universities, the University of Groningen and the University of Twente, jointly initiated a project called Knowledge Graphs, which are semantic networks, but with the added constraint that edges must be from a finite set of possible relations to facilitate algebra on the graph.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 17], [17, 18], [19, 22], [23, 28], [29, 41], [41, 42], [43, 46], [47, 57], [58, 60], [61, 70], [71, 74], [75, 78], [79, 89], [90, 92], [93, 99], [99, 100], [101, 108], [109, 118], [119, 120], [121, 128], [129, 135], [136, 145], [146, 152], [152, 153], [154, 159], [160, 163], [164, 172], [173, 181], [181, 182], [183, 186], [187, 191], [192, 195], [196, 201], [202, 212], [213, 217], [218, 223], [224, 228], [229, 231], [232, 236], [237, 238], [239, 245], [246, 249], [250, 252], [253, 261], [262, 271], [272, 274], [275, 285], [286, 293], [294, 296], [297, 300], [301, 306], [306, 307]]}
{"doc_key": "ai-train-63", "ner": [[0, 2, "product"], [13, 17, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 2, 13, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammar", "checkers", "are", "most", "often", "implemented", "as", "a", "feature", "of", "a", "larger", "program", "such", "as", "a", "word", "processor", ",", "but", "are", "also", "available", "as", "a", "stand", "-", "alone", "application", "that", "can", "be", "activated", "from", "within", "programs", "that", "work", "with", "editable", "text", "."], "sentence-detokenized": "Grammar checkers are most often implemented as a feature of a larger program such as a word processor, but are also available as a stand-alone application that can be activated from within programs that work with editable text.", "token2charspan": [[0, 7], [8, 16], [17, 20], [21, 25], [26, 31], [32, 43], [44, 46], [47, 48], [49, 56], [57, 59], [60, 61], [62, 68], [69, 76], [77, 81], [82, 84], [85, 86], [87, 91], [92, 101], [101, 102], [103, 106], [107, 110], [111, 115], [116, 125], [126, 128], [129, 130], [131, 136], [136, 137], [137, 142], [143, 154], [155, 159], [160, 163], [164, 166], [167, 176], [177, 181], [182, 188], [189, 197], [198, 202], [203, 207], [208, 212], [213, 221], [222, 226], [226, 227]]}
{"doc_key": "ai-train-64", "ner": [[6, 12, "organisation"], [15, 23, "conference"], [24, 26, "organisation"], [34, 35, "conference"], [38, 39, "conference"], [43, 44, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["He", "is", "a", "member", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "the", "Association", "for", "the", "Advancement", "Artificial", "Intelligence", ",", "and", "the", "Cognitive", "Science", "Society", ",", "and", "is", "an", "editor", "of", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", ",", "and", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "He is a member of the American Association for the Advancement of Science, the Association for the Advancement Artificial Intelligence, and the Cognitive Science Society, and is an editor of J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.", "token2charspan": [[0, 2], [3, 5], [6, 7], [8, 14], [15, 17], [18, 21], [22, 30], [31, 42], [43, 46], [47, 50], [51, 62], [63, 65], [66, 73], [73, 74], [75, 78], [79, 90], [91, 94], [95, 98], [99, 110], [111, 121], [122, 134], [134, 135], [136, 139], [140, 143], [144, 153], [154, 161], [162, 169], [169, 170], [171, 174], [175, 177], [178, 180], [181, 187], [188, 190], [191, 193], [194, 203], [204, 213], [213, 214], [215, 217], [218, 226], [227, 235], [235, 236], [237, 240], [241, 243], [244, 251], [252, 260], [260, 261]]}
{"doc_key": "ai-train-65", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "task"], [18, 19, "researcher"], [20, 22, "university"], [24, 25, "researcher"], [26, 30, "organisation"], [32, 32, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 2, 10, 11, "type-of", "", false, false], [0, 2, 18, 19, "origin", "", false, false], [0, 2, 24, 25, "origin", "", false, false], [4, 4, 0, 2, "named", "", false, false], [18, 19, 20, 22, "physical", "", false, false], [18, 19, 20, 22, "role", "", false, false], [24, 25, 26, 30, "role", "", false, false], [32, 32, 26, 30, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Linear", "predictive", "coding", "(", "LPC", ")", ",", "a", "form", "of", "speech", "coding", ",", "was", "developed", "in", "1966", "by", "Fumitada", "Itakura", "of", "Nagoya", "University", "and", "Shuzo", "Saito", "of", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "."], "sentence-detokenized": "Linear predictive coding (LPC), a form of speech coding, was developed in 1966 by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT).", "token2charspan": [[0, 6], [7, 17], [18, 24], [25, 26], [26, 29], [29, 30], [30, 31], [32, 33], [34, 38], [39, 41], [42, 48], [49, 55], [55, 56], [57, 60], [61, 70], [71, 73], [74, 78], [79, 81], [82, 90], [91, 98], [99, 101], [102, 108], [109, 119], [120, 123], [124, 129], [130, 135], [136, 138], [139, 145], [146, 155], [156, 159], [160, 169], [170, 171], [171, 174], [174, 175], [175, 176]]}
{"doc_key": "ai-train-66", "ner": [[14, 59, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["If", "the", "signal", "is", "more", "ergodic", ",", "all", "sample", "paths", "exhibit", "the", "same", "time", "averaging", "and", "thus", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "in", "the", "sense", "of", "mean", "square", "error", "."], "sentence-detokenized": "If the signal is more ergodic, all sample paths exhibit the same time averaging and thus mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math in the sense of mean square error.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 16], [17, 21], [22, 29], [29, 30], [31, 34], [35, 41], [42, 47], [48, 55], [56, 59], [60, 64], [65, 69], [70, 79], [80, 83], [84, 88], [89, 94], [95, 96], [97, 98], [99, 100], [101, 102], [102, 103], [104, 105], [106, 107], [108, 109], [110, 111], [111, 112], [113, 114], [114, 115], [116, 119], [119, 120], [121, 123], [124, 131], [132, 133], [133, 134], [134, 135], [136, 137], [138, 139], [140, 141], [142, 143], [143, 144], [145, 146], [147, 148], [149, 150], [151, 152], [152, 153], [154, 155], [155, 156], [157, 160], [160, 161], [162, 163], [164, 168], [169, 171], [172, 175], [176, 181], [182, 184], [185, 189], [190, 196], [197, 202], [202, 203]]}
{"doc_key": "ai-train-67", "ner": [[0, 1, "task"], [3, 4, "task"], [13, 15, "algorithm"], [17, 17, "algorithm"], [20, 22, "algorithm"], [24, 24, "algorithm"], [27, 29, "algorithm"], [31, 31, "algorithm"], [34, 36, "algorithm"], [38, 38, "algorithm"], [43, 44, "misc"], [50, 52, "algorithm"], [54, 55, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[13, 15, 43, 44, "related-to", "", false, false], [17, 17, 13, 15, "named", "", false, false], [20, 22, 43, 44, "related-to", "", false, false], [24, 24, 20, 22, "named", "", false, false], [27, 29, 43, 44, "related-to", "", false, false], [31, 31, 27, 29, "named", "", false, false], [34, 36, 43, 44, "related-to", "", false, false], [38, 38, 34, 36, "named", "", false, false], [50, 52, 54, 55, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Feature", "extraction", "and", "dimension", "reduction", "can", "be", "combined", "in", "a", "single", "step", "using", "principal", "component", "analysis", "(", "PCA", ")", ",", "linear", "discriminant", "analysis", "(", "LDA", ")", ",", "canonical", "correlation", "analysis", "(", "CCA", ")", "or", "non-negative", "matrix", "factorisation", "(", "NMF", ")", "techniques", "as", "a", "pre-processing", "step", ",", "followed", "by", "clustering", "with", "K", "-", "NN", "on", "feature", "vectors", "in", "the", "reduced", "dimension", "space", "."], "sentence-detokenized": "Feature extraction and dimension reduction can be combined in a single step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA) or non-negative matrix factorisation (NMF) techniques as a pre-processing step, followed by clustering with K-NN on feature vectors in the reduced dimension space.", "token2charspan": [[0, 7], [8, 18], [19, 22], [23, 32], [33, 42], [43, 46], [47, 49], [50, 58], [59, 61], [62, 63], [64, 70], [71, 75], [76, 81], [82, 91], [92, 101], [102, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 124], [125, 137], [138, 146], [147, 148], [148, 151], [151, 152], [152, 153], [154, 163], [164, 175], [176, 184], [185, 186], [186, 189], [189, 190], [191, 193], [194, 206], [207, 213], [214, 227], [228, 229], [229, 232], [232, 233], [234, 244], [245, 247], [248, 249], [250, 264], [265, 269], [269, 270], [271, 279], [280, 282], [283, 293], [294, 298], [299, 300], [300, 301], [301, 303], [304, 306], [307, 314], [315, 322], [323, 325], [326, 329], [330, 337], [338, 347], [348, 353], [353, 354]]}
{"doc_key": "ai-train-68", "ner": [[3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [2, 9, "programlang"], [15, 15, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[15, 15, 3, 3, "related-to", "program_type_compatible_with", false, false], [15, 15, 5, 5, "related-to", "program_type_compatible_with", false, false], [15, 15, 7, 7, "related-to", "program_type_compatible_with", false, false], [15, 15, 2, 9, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Libraries", "written", "in", "Perl", ",", "Java", ",", "ActiveX", "or", ".NET", "can", "be", "called", "directly", "from", "MATLAB", ","], "sentence-detokenized": "Libraries written in Perl, Java, ActiveX or .NET can be called directly from MATLAB,", "token2charspan": [[0, 9], [10, 17], [18, 20], [21, 25], [25, 26], [27, 31], [31, 32], [33, 40], [41, 43], [44, 48], [49, 52], [53, 55], [56, 62], [63, 71], [72, 76], [77, 83], [83, 84]]}
{"doc_key": "ai-train-69", "ner": [[3, 9, "task"], [11, 16, "task"], [28, 30, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 9, 11, 16, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["The", "task", "of", "recognising", "named", "entities", "in", "the", "text", "is", "called", "Named", "Entity", "Recognition", ",", "while", "the", "task", "of", "identifying", "the", "named", "entities", "mentioned", "in", "the", "text", "is", "called", "Entity", "Linking", "."], "sentence-detokenized": "The task of recognising named entities in the text is called Named Entity Recognition, while the task of identifying the named entities mentioned in the text is called Entity Linking.", "token2charspan": [[0, 3], [4, 8], [9, 11], [12, 23], [24, 29], [30, 38], [39, 41], [42, 45], [46, 50], [51, 53], [54, 60], [61, 66], [67, 73], [74, 85], [85, 86], [87, 92], [93, 96], [97, 101], [102, 104], [105, 116], [117, 120], [121, 126], [127, 135], [136, 145], [146, 148], [149, 152], [153, 157], [158, 160], [161, 167], [168, 174], [175, 182], [182, 183]]}
{"doc_key": "ai-train-70", "ner": [[1, 1, "algorithm"], [27, 27, "programlang"], [29, 29, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 29, 29, "part-of", "", true, false], [29, 29, 27, 27, "part-of", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["The", "sigmoid", "functions", "and", "derivatives", "used", "in", "the", "package", "were", "originally", "included", "in", "the", "package", ",", "as", "of", "version", "0.8.0", "they", "have", "been", "released", "as", "a", "separate", "R", "package", "sigmoid", "for", "more", "general", "use", "."], "sentence-detokenized": "The sigmoid functions and derivatives used in the package were originally included in the package, as of version 0.8.0 they have been released as a separate R package sigmoid for more general use.", "token2charspan": [[0, 3], [4, 11], [12, 21], [22, 25], [26, 37], [38, 42], [43, 45], [46, 49], [50, 57], [58, 62], [63, 73], [74, 82], [83, 85], [86, 89], [90, 97], [97, 98], [99, 101], [102, 104], [105, 112], [113, 118], [119, 123], [124, 128], [129, 133], [134, 142], [143, 145], [146, 147], [148, 156], [157, 158], [159, 166], [167, 174], [175, 178], [179, 183], [184, 191], [192, 195], [195, 196]]}
{"doc_key": "ai-train-71", "ner": [[0, 1, "programlang"], [16, 20, "organisation"], [22, 22, "organisation"], [26, 26, "location"], [28, 28, "location"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 14, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 1, 7, 8, "artifact", "", true, false], [0, 1, 10, 11, "artifact", "", true, false], [0, 1, 13, 14, "artifact", "", true, false], [22, 22, 16, 20, "named", "", false, false], [22, 22, 26, 26, "physical", "", false, false], [26, 26, 28, 28, "physical", "", false, false], [7, 8, 16, 20, "role", "", false, false], [10, 11, 16, 20, "role", "", false, false], [13, 14, 16, 20, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["The", "logo", "was", "created", "in", "1967", "by", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "and", "Seymour", "Papert", "at", "Bolt", ",", "Beranek", "and", "Newman", "(", "BBN", ")", ",", "a", "Cambridge", ",", "Massachusetts", "research", "firm", "."], "sentence-detokenized": "The logo was created in 1967 by Wally Feurzeig, Cynthia Solomon and Seymour Papert at Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts research firm.", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 20], [21, 23], [24, 28], [29, 31], [32, 37], [38, 46], [46, 47], [48, 55], [56, 63], [64, 67], [68, 75], [76, 82], [83, 85], [86, 90], [90, 91], [92, 99], [100, 103], [104, 110], [111, 112], [112, 115], [115, 116], [116, 117], [118, 119], [120, 129], [129, 130], [131, 144], [145, 153], [154, 158], [158, 159]]}
{"doc_key": "ai-train-72", "ner": [[0, 0, "misc"], [8, 9, "field"], [16, 17, "field"], [20, 21, "algorithm"], [24, 25, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 8, 9, "part-of", "", false, false], [0, 0, 16, 17, "compare", "", false, false], [20, 21, 16, 17, "part-of", "", false, false], [24, 25, 16, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Neuroevolution", "is", "often", "used", "as", "part", "of", "a", "reinforcement", "learning", "paradigm", "and", "is", "comparable", "to", "traditional", "deep", "learning", "techniques", "using", "gradient", "descent", "on", "a", "neural", "network", "with", "a", "fixed", "topology", "."], "sentence-detokenized": "Neuroevolution is often used as part of a reinforcement learning paradigm and is comparable to traditional deep learning techniques using gradient descent on a neural network with a fixed topology.", "token2charspan": [[0, 14], [15, 17], [18, 23], [24, 28], [29, 31], [32, 36], [37, 39], [40, 41], [42, 55], [56, 64], [65, 73], [74, 77], [78, 80], [81, 91], [92, 94], [95, 106], [107, 111], [112, 120], [121, 131], [132, 137], [138, 146], [147, 154], [155, 157], [158, 159], [160, 166], [167, 174], [175, 179], [180, 181], [182, 187], [188, 196], [196, 197]]}
{"doc_key": "ai-train-73", "ner": [[42, 45, "algorithm"], [54, 56, "metrics"], [58, 58, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[58, 58, 54, 56, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["If", "we", "fit", "a", "function", "of", "the", "form", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "hyperplane", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "to", "sub", "1", "\u2264", "i", "\u2264", "n", "/", "sub", "data", "using", "the", "least", "squares", "method", ",", "we", "can", "evaluate", "the", "fit", "using", "the", "mean", "squared", "error", "(", "MSE", ")", "."], "sentence-detokenized": "If we fit a function of the form \u0177 = a + \u03b2 supT / sup x hyperplane (x sub i / sub, y sub i / sub) to sub 1 \u2264 i \u2264 n / sub data using the least squares method, we can evaluate the fit using the mean squared error (MSE).", "token2charspan": [[0, 2], [3, 5], [6, 9], [10, 11], [12, 20], [21, 23], [24, 27], [28, 32], [33, 34], [35, 36], [37, 38], [39, 40], [41, 42], [43, 47], [48, 49], [50, 53], [54, 55], [56, 66], [67, 68], [68, 69], [70, 73], [74, 75], [76, 77], [78, 81], [81, 82], [83, 84], [85, 88], [89, 90], [91, 92], [93, 96], [96, 97], [98, 100], [101, 104], [105, 106], [107, 108], [109, 110], [111, 112], [113, 114], [115, 116], [117, 120], [121, 125], [126, 131], [132, 135], [136, 141], [142, 149], [150, 156], [156, 157], [158, 160], [161, 164], [165, 173], [174, 177], [178, 181], [182, 187], [188, 191], [192, 196], [197, 204], [205, 210], [211, 212], [212, 215], [215, 216], [216, 217]]}
{"doc_key": "ai-train-74", "ner": [[6, 6, "country"], [8, 8, "country"], [10, 10, "country"], [12, 12, "country"], [14, 14, "country"], [16, 16, "country"], [18, 18, "country"], [20, 20, "country"], [22, 22, "country"], [24, 24, "country"], [26, 26, "country"], [28, 28, "country"], [30, 30, "country"], [32, 32, "country"], [34, 34, "country"], [36, 37, "country"], [39, 39, "country"], [40, 41, "country"], [42, 43, "country"], [44, 45, "country"], [47, 49, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "company", "has", "international", "locations", "in", "Australia", ",", "Brazil", ",", "Canada", ",", "China", ",", "Germany", ",", "India", ",", "Italy", ",", "Japan", ",", "Korea", ",", "Lithuania", ",", "Poland", ",", "Malaysia", ",", "Philippines", ",", "Russia", ",", "Singapore", ",", "South", "Africa", ",", "Spain", ",", "Taiwan", ",", "Thailand", ",", "Turkey", "and", "the", "United", "Kingdom", "."], "sentence-detokenized": "The company has international locations in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 29], [30, 39], [40, 42], [43, 52], [52, 53], [54, 60], [60, 61], [62, 68], [68, 69], [70, 75], [75, 76], [77, 84], [84, 85], [86, 91], [91, 92], [93, 98], [98, 99], [100, 105], [105, 106], [107, 112], [112, 113], [114, 123], [123, 124], [125, 131], [131, 132], [133, 141], [141, 142], [143, 154], [154, 155], [156, 162], [162, 163], [164, 173], [173, 174], [175, 180], [181, 187], [187, 188], [189, 194], [194, 195], [196, 202], [202, 203], [204, 212], [212, 213], [214, 220], [221, 224], [225, 228], [229, 235], [236, 243], [243, 244]]}
{"doc_key": "ai-train-75", "ner": [[0, 3, "misc"], [5, 8, "field"], [13, 13, "organisation"], [12, 22, "university"], [27, 29, "organisation"], [26, 38, "university"], [42, 43, "university"], [45, 46, "university"], [48, 51, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 3, 5, 8, "topic", "", false, false], [0, 3, 13, 13, "origin", "", false, false], [0, 3, 12, 22, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["He", "holds", "a", "PhD", "in", "electrical", "and", "computer", "engineering", "(", "2000", ")", "from", "Inria", "and", "the", "University", "of", "Nice", "Sophia", "Antipolis", "and", "has", "held", "permanent", "positions", "at", "Siemens", "Corporate", "Technology", ",", "\u00c9cole", "des", "ponts", "ParisTech", ",", "as", "well", "as", "visiting", "positions", "at", "Rutgers", "University", ",", "Yale", "University", "and", "the", "University", "of", "Houston", "."], "sentence-detokenized": "He holds a PhD in electrical and computer engineering (2000) from Inria and the University of Nice Sophia Antipolis and has held permanent positions at Siemens Corporate Technology, \u00c9cole des ponts ParisTech, as well as visiting positions at Rutgers University, Yale University and the University of Houston.", "token2charspan": [[0, 2], [3, 8], [9, 10], [11, 14], [15, 17], [18, 28], [29, 32], [33, 41], [42, 53], [54, 55], [55, 59], [59, 60], [61, 65], [66, 71], [72, 75], [76, 79], [80, 90], [91, 93], [94, 98], [99, 105], [106, 115], [116, 119], [120, 123], [124, 128], [129, 138], [139, 148], [149, 151], [152, 159], [160, 169], [170, 180], [180, 181], [182, 187], [188, 191], [192, 197], [198, 207], [207, 208], [209, 211], [212, 216], [217, 219], [220, 228], [229, 238], [239, 241], [242, 249], [250, 260], [260, 261], [262, 266], [267, 277], [278, 281], [282, 285], [286, 296], [297, 299], [300, 307], [307, 308]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [10, 10, "researcher"], [16, 17, "product"], [20, 21, "country"], [12, 12, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 7, 8, "role", "licensing_patent_to", false, false], [10, 10, 20, 21, "physical", "", false, false], [12, 12, 10, 10, "artifact", "", false, false], [12, 12, 16, 17, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Licensing", "the", "original", "patent", "granted", "to", "inventor", "George", "Devol", ",", "Engelberger", "developed", "Unimate", ",", "the", "first", "industrial", "robot", "in", "the", "United", "States", "in", "the", "1950s", "."], "sentence-detokenized": "Licensing the original patent granted to inventor George Devol, Engelberger developed Unimate, the first industrial robot in the United States in the 1950s.", "token2charspan": [[0, 9], [10, 13], [14, 22], [23, 29], [30, 37], [38, 40], [41, 49], [50, 56], [57, 62], [62, 63], [64, 75], [76, 85], [86, 93], [93, 94], [95, 98], [99, 104], [105, 115], [116, 121], [122, 124], [125, 128], [129, 135], [136, 142], [143, 145], [146, 149], [150, 155], [155, 156]]}
{"doc_key": "ai-train-77", "ner": [[4, 5, "task"], [3, 12, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "input", "is", "called", "speech", "recognition", "and", "the", "output", "is", "called", "speech", "synthesis", "."], "sentence-detokenized": "The input is called speech recognition and the output is called speech synthesis.", "token2charspan": [[0, 3], [4, 9], [10, 12], [13, 19], [20, 26], [27, 38], [39, 42], [43, 46], [47, 53], [54, 56], [57, 63], [64, 70], [71, 80], [80, 81]]}
{"doc_key": "ai-train-78", "ner": [[5, 5, "programlang"], [8, 8, "programlang"], [16, 16, "programlang"], [30, 30, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4], "relations": [[5, 5, 16, 16, "named", "", false, false], [8, 8, 5, 5, "origin", "descendant_of", false, false], [8, 8, 30, 30, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 3], "sentence": ["Among", "the", "descendants", "of", "the", "CLIPS", "language", ",", "Jess", "(", "the", "rule", "-", "based", "part", "of", "CLIPS", "was", "rewritten", "in", "Java", ",", "which", "later", "grew", "in", "different", "direction", ")", ",", "JESS", "was", "initially", "inspired", "by"], "sentence-detokenized": "Among the descendants of the CLIPS language, Jess (the rule-based part of CLIPS was rewritten in Java, which later grew in different direction), JESS was initially inspired by", "token2charspan": [[0, 5], [6, 9], [10, 21], [22, 24], [25, 28], [29, 34], [35, 43], [43, 44], [45, 49], [50, 51], [51, 54], [55, 59], [59, 60], [60, 65], [66, 70], [71, 73], [74, 79], [80, 83], [84, 93], [94, 96], [97, 101], [101, 102], [103, 108], [109, 114], [115, 119], [120, 122], [123, 132], [133, 142], [142, 143], [143, 144], [145, 149], [150, 153], [154, 163], [164, 172], [173, 175]]}
{"doc_key": "ai-train-79", "ner": [[33, 33, "product"], [4, 6, "product"], [21, 22, "organisation"], [10, 12, "product"], [38, 39, "product"], [36, 43, "product"], [53, 54, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 6, 33, 33, "type-of", "", false, false], [21, 22, 4, 6, "usage", "", false, false], [10, 12, 21, 22, "artifact", "", false, false], [38, 39, 21, 22, "origin", "", true, false], [38, 39, 53, 54, "related-to", "", true, false], [36, 43, 21, 22, "origin", "", true, false], [36, 43, 53, 54, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["He", "also", "designed", "the", "Motivity", "control", "system", "used", "to", "develop", "the", "ADAM", "iAGV", "(", "Self", "-", "Guided", "Vehicle", ")", "used", "by", "RMT", "Robotics", "for", "complex", "pick", "and", "place", "operations", ",", "creating", "flexible", "intelligent", "AGV", "applications", "in", "combination", "with", "gantry", "systems", "and", "industrial", "robot", "arms", "used", "to", "move", "products", "from", "process", "to", "process", "in", "non-linear", "layouts", "in", "first", "-", "tier", "car", "supply", "factories", "."], "sentence-detokenized": "He also designed the Motivity control system used to develop the ADAM iAGV (Self-Guided Vehicle) used by RMT Robotics for complex pick and place operations, creating flexible intelligent AGV applications in combination with gantry systems and industrial robot arms used to move products from process to process in non-linear layouts in first-tier car supply factories.", "token2charspan": [[0, 2], [3, 7], [8, 16], [17, 20], [21, 29], [30, 37], [38, 44], [45, 49], [50, 52], [53, 60], [61, 64], [65, 69], [70, 74], [75, 76], [76, 80], [80, 81], [81, 87], [88, 95], [95, 96], [97, 101], [102, 104], [105, 108], [109, 117], [118, 121], [122, 129], [130, 134], [135, 138], [139, 144], [145, 155], [155, 156], [157, 165], [166, 174], [175, 186], [187, 190], [191, 203], [204, 206], [207, 218], [219, 223], [224, 230], [231, 238], [239, 242], [243, 253], [254, 259], [260, 264], [265, 269], [270, 272], [273, 277], [278, 286], [287, 291], [292, 299], [300, 302], [303, 310], [311, 313], [314, 324], [325, 332], [333, 335], [336, 341], [341, 342], [342, 346], [347, 350], [351, 357], [358, 367], [367, 368]]}
{"doc_key": "ai-train-80", "ner": [[7, 8, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["The", "\u03b2", "parameters", "are", "typically", "estimated", "by", "maximum", "likelihood", "."], "sentence-detokenized": "The \u03b2 parameters are typically estimated by maximum likelihood.", "token2charspan": [[0, 3], [4, 5], [6, 16], [17, 20], [21, 30], [31, 40], [41, 43], [44, 51], [52, 62], [62, 63]]}
{"doc_key": "ai-train-81", "ner": [[0, 1, "task"], [5, 5, "metrics"], [7, 7, "metrics"], [9, 9, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[5, 5, 0, 1, "part-of", "", false, false], [7, 7, 0, 1, "part-of", "", false, false], [9, 9, 0, 1, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Information", "retrieval", "metrics", "such", "as", "precision", "and", "recall", "or", "DCG", "are", "useful", "for", "assessing", "the", "quality", "of", "a", "recommendation", "method", "."], "sentence-detokenized": "Information retrieval metrics such as precision and recall or DCG are useful for assessing the quality of a recommendation method.", "token2charspan": [[0, 11], [12, 21], [22, 29], [30, 34], [35, 37], [38, 47], [48, 51], [52, 58], [59, 61], [62, 65], [66, 69], [70, 76], [77, 80], [81, 90], [91, 94], [95, 102], [103, 105], [106, 107], [108, 122], [123, 129], [129, 130]]}
{"doc_key": "ai-train-82", "ner": [[5, 10, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "a", "typical", "factory", ",", "there", "are", "hundreds", "of", "industrial", "robots", "operating", "on", "fully", "automated", "production", "lines", ",", "with", "one", "robot", "for", "every", "ten", "human", "workers", "."], "sentence-detokenized": "In a typical factory, there are hundreds of industrial robots operating on fully automated production lines, with one robot for every ten human workers.", "token2charspan": [[0, 2], [3, 4], [5, 12], [13, 20], [20, 21], [22, 27], [28, 31], [32, 40], [41, 43], [44, 54], [55, 61], [62, 71], [72, 74], [75, 80], [81, 90], [91, 101], [102, 107], [107, 108], [109, 113], [114, 117], [118, 123], [124, 127], [128, 133], [134, 137], [138, 143], [144, 151], [151, 152]]}
{"doc_key": "ai-train-83", "ner": [[5, 5, "product"], [11, 12, "field"], [16, 17, "task"], [19, 20, "task"], [22, 23, "task"], [25, 26, "task"], [28, 29, "task"], [14, 32, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[11, 12, 5, 5, "usage", "", false, true], [16, 17, 11, 12, "part-of", "", false, false], [19, 20, 11, 12, "part-of", "", false, false], [22, 23, 11, 12, "part-of", "", false, false], [25, 26, 11, 12, "part-of", "", false, false], [28, 29, 11, 12, "part-of", "", false, false], [14, 32, 11, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["In", "the", "last", "decade", ",", "PCNNs", "have", "been", "used", "in", "various", "image", "processing", "applications", "such", "as", "image", "segmentation", ",", "feature", "generation", ",", "face", "extraction", ",", "motion", "detection", ",", "region", "growing", "and", "noise", "reduction", "."], "sentence-detokenized": "In the last decade, PCNNs have been used in various image processing applications such as image segmentation, feature generation, face extraction, motion detection, region growing and noise reduction.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 18], [18, 19], [20, 25], [26, 30], [31, 35], [36, 40], [41, 43], [44, 51], [52, 57], [58, 68], [69, 81], [82, 86], [87, 89], [90, 95], [96, 108], [108, 109], [110, 117], [118, 128], [128, 129], [130, 134], [135, 145], [145, 146], [147, 153], [154, 163], [163, 164], [165, 171], [172, 179], [180, 183], [184, 189], [190, 199], [199, 200]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [16, 17, "field"], [20, 37, "misc"], [28, 31, "conference"], [33, 33, "conference"], [38, 40, "misc"], [42, 49, "conference"], [50, 51, "conference"], [53, 57, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 0, 16, 17, "related-to", "contributes_to", false, false], [0, 0, 20, 37, "win-defeat", "", false, false], [0, 0, 38, 40, "win-defeat", "", false, false], [20, 37, 28, 31, "temporal", "", false, false], [33, 33, 28, 31, "named", "", false, false], [38, 40, 42, 49, "temporal", "", false, false], [38, 40, 53, 57, "temporal", "", false, false], [50, 51, 42, 49, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Xu", "has", "published", "more", "than", "50", "papers", "in", "international", "conferences", "and", "journals", "in", "the", "field", "of", "computer", "vision", "and", "won", "the", "Best", "Paper", "Award", "at", "the", "2012", "international", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "conference", "and", "the", "Best", "Referee", "Award", "at", "the", "international", "conferences", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "and", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "."], "sentence-detokenized": "Xu has published more than 50 papers in international conferences and journals in the field of computer vision and won the Best Paper Award at the 2012 international Non-Photorealistic Rendering and Animation (NPAR) conference and the Best Referee Award at the international conferences Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision (ICCV) 2015.", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 21], [22, 26], [27, 29], [30, 36], [37, 39], [40, 53], [54, 65], [66, 69], [70, 78], [79, 81], [82, 85], [86, 91], [92, 94], [95, 103], [104, 110], [111, 114], [115, 118], [119, 122], [123, 127], [128, 133], [134, 139], [140, 142], [143, 146], [147, 151], [152, 165], [166, 184], [185, 194], [195, 198], [199, 208], [209, 210], [210, 214], [214, 215], [216, 226], [227, 230], [231, 234], [235, 239], [240, 247], [248, 253], [254, 256], [257, 260], [261, 274], [275, 286], [287, 292], [293, 303], [304, 306], [307, 315], [316, 322], [323, 327], [328, 332], [333, 336], [337, 350], [351, 361], [362, 364], [365, 373], [374, 380], [381, 382], [382, 386], [386, 387], [388, 392], [392, 393]]}
{"doc_key": "ai-train-85", "ner": [[7, 8, "programlang"], [1, 2, "field"], [4, 5, "field"], [10, 11, "misc"], [14, 15, "researcher"], [17, 19, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[7, 8, 1, 2, "part-of", "", false, false], [7, 8, 4, 5, "part-of", "", false, false], [7, 8, 10, 11, "type-of", "", false, false], [17, 19, 7, 8, "usage", "", false, false], [17, 19, 14, 15, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "computer", "science", "and", "artificial", "intelligence", ",", "CycL", "is", "an", "ontology", "language", "used", "by", "Doug", "Lenat", "'s", "Cyc", "artificial", "project", "."], "sentence-detokenized": "In computer science and artificial intelligence, CycL is an ontology language used by Doug Lenat's Cyc artificial project.", "token2charspan": [[0, 2], [3, 11], [12, 19], [20, 23], [24, 34], [35, 47], [47, 48], [49, 53], [54, 56], [57, 59], [60, 68], [69, 77], [78, 82], [83, 85], [86, 90], [91, 96], [96, 98], [99, 102], [103, 113], [114, 121], [121, 122]]}
{"doc_key": "ai-train-86", "ner": [[1, 3, "task"], [25, 25, "metrics"], [15, 16, "metrics"], [23, 24, "metrics"], [31, 37, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[25, 25, 1, 3, "part-of", "", false, false], [15, 16, 25, 25, "named", "", false, false], [23, 24, 25, 25, "named", "", false, false], [31, 37, 25, 25, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Also", "in", "regression", "analysis", ",", "mean", "squared", "error", ",", "often", "referred", "to", "as", "mean", "squared", "prediction", "error", "or", "out", "-", "of", "-", "sample", "mean", "squared", "error", ",", "can", "refer", "to", "the", "average", "value", "of", "the", "squared", "deviations", "of", "the", "predictions", "from", "the", "TRUE", "values", "on", "an", "out", "-", "of", "-", "sample", "test", "space", "generated", "by", "a", "model", "estimated", "on", "a", "given", "sample", "space", "."], "sentence-detokenized": "Also in regression analysis, mean squared error, often referred to as mean squared prediction error or out-of-sample mean squared error, can refer to the average value of the squared deviations of the predictions from the TRUE values on an out-of-sample test space generated by a model estimated on a given sample space.", "token2charspan": [[0, 4], [5, 7], [8, 18], [19, 27], [27, 28], [29, 33], [34, 41], [42, 47], [47, 48], [49, 54], [55, 63], [64, 66], [67, 69], [70, 74], [75, 82], [83, 93], [94, 99], [100, 102], [103, 106], [106, 107], [107, 109], [109, 110], [110, 116], [117, 121], [122, 129], [130, 135], [135, 136], [137, 140], [141, 146], [147, 149], [150, 153], [154, 161], [162, 167], [168, 170], [171, 174], [175, 182], [183, 193], [194, 196], [197, 200], [201, 212], [213, 217], [218, 221], [222, 226], [227, 233], [234, 236], [237, 239], [240, 243], [243, 244], [244, 246], [246, 247], [247, 253], [254, 258], [259, 264], [265, 274], [275, 277], [278, 279], [280, 285], [286, 295], [296, 298], [299, 300], [301, 306], [307, 313], [314, 319], [319, 320]]}
{"doc_key": "ai-train-87", "ner": [[6, 8, "algorithm"], [10, 11, "algorithm"], [21, 24, "algorithm"], [36, 37, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 8, 10, 11, "compare", "", false, false], [6, 8, 21, 24, "named", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["As", "for", "the", "results", ",", "the", "C", "-", "HOG", "and", "R-", "HOG", "block", "descriptors", "show", "a", "comparable", "performance", ",", "with", "the", "C", "-", "HOG", "descriptors", "showing", "a", "slight", "advantage", "in", "detection", "miss", "rate", "at", "constant", "FALSE", "positive", "rates", "in", "both", "data", "sets", "."], "sentence-detokenized": "As for the results, the C-HOG and R-HOG block descriptors show a comparable performance, with the C-HOG descriptors showing a slight advantage in detection miss rate at constant FALSE positive rates in both data sets.", "token2charspan": [[0, 2], [3, 6], [7, 10], [11, 18], [18, 19], [20, 23], [24, 25], [25, 26], [26, 29], [30, 33], [34, 36], [36, 39], [40, 45], [46, 57], [58, 62], [63, 64], [65, 75], [76, 87], [87, 88], [89, 93], [94, 97], [98, 99], [99, 100], [100, 103], [104, 115], [116, 123], [124, 125], [126, 132], [133, 142], [143, 145], [146, 155], [156, 160], [161, 165], [166, 168], [169, 177], [178, 183], [184, 192], [193, 198], [199, 201], [202, 206], [207, 211], [212, 216], [216, 217]]}
{"doc_key": "ai-train-88", "ner": [[4, 6, "algorithm"], [8, 8, "misc"], [10, 12, "algorithm"], [14, 18, "algorithm"], [17, 18, "algorithm"], [20, 22, "algorithm"], [24, 26, "algorithm"], [27, 29, "misc"], [33, 35, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 6, 8, 8, "usage", "", false, false], [10, 12, 27, 29, "usage", "", false, false], [14, 18, 27, 29, "usage", "", false, false], [17, 18, 27, 29, "usage", "", false, false], [20, 22, 27, 29, "usage", "", false, false], [24, 26, 27, 29, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Popular", "recognition", "algorithms", "include", "principal", "component", "analysis", "using", "eigenfaces", ",", "linear", "discriminant", "analysis", ",", "Elastic", "matching", "using", "Fisherface", "algorithm", ",", "hidden", "Markov", "model", ",", "multilinear", "subspace", "learning", "using", "tensor", "representation", "and", "neuronally", "motivated", "dynamic", "link", "matching", "."], "sentence-detokenized": "Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, Elastic matching using Fisherface algorithm, hidden Markov model, multilinear subspace learning using tensor representation and neuronally motivated dynamic link matching.", "token2charspan": [[0, 7], [8, 19], [20, 30], [31, 38], [39, 48], [49, 58], [59, 67], [68, 73], [74, 84], [84, 85], [86, 92], [93, 105], [106, 114], [114, 115], [116, 123], [124, 132], [133, 138], [139, 149], [150, 159], [159, 160], [161, 167], [168, 174], [175, 180], [180, 181], [182, 193], [194, 202], [203, 211], [212, 217], [218, 224], [225, 239], [240, 243], [244, 254], [255, 264], [265, 272], [273, 277], [278, 286], [286, 287]]}
{"doc_key": "ai-train-89", "ner": [[1, 7, "misc"], [14, 19, "location"], [37, 40, "location"], [52, 52, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 19, 1, 7, "temporal", "", false, false], [37, 40, 1, 7, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Starting", "with", "the", "2019", "Toronto", "International", "Film", "Festival", ",", "films", "may", "be", "restricted", "from", "screening", "at", "the", "Scotiabank", "Theatre", "Toronto", ",", "one", "of", "the", "festival", "'s", "main", "venues", ",", "and", "may", "be", "shown", "elsewhere", "(", "such", "as", "TIFF", "Bell", "Lightbox", "and", "other", "local", "cinemas", ")", "if", "distributed", "by", "a", "service", "such", "as", "Netflix", "."], "sentence-detokenized": "Starting with the 2019 Toronto International Film Festival, films may be restricted from screening at the Scotiabank Theatre Toronto, one of the festival's main venues, and may be shown elsewhere (such as TIFF Bell Lightbox and other local cinemas) if distributed by a service such as Netflix.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 30], [31, 44], [45, 49], [50, 58], [58, 59], [60, 65], [66, 69], [70, 72], [73, 83], [84, 88], [89, 98], [99, 101], [102, 105], [106, 116], [117, 124], [125, 132], [132, 133], [134, 137], [138, 140], [141, 144], [145, 153], [153, 155], [156, 160], [161, 167], [167, 168], [169, 172], [173, 176], [177, 179], [180, 185], [186, 195], [196, 197], [197, 201], [202, 204], [205, 209], [210, 214], [215, 223], [224, 227], [228, 233], [234, 239], [240, 247], [247, 248], [249, 251], [252, 263], [264, 266], [267, 268], [269, 276], [277, 281], [282, 284], [285, 292], [292, 293]]}
{"doc_key": "ai-train-90", "ner": [[0, 1, "organisation"], [2, 3, "researcher"], [5, 6, "organisation"], [30, 36, "product"], [12, 39, "researcher"], [47, 49, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 6], "relations": [[0, 1, 5, 6, "related-to", "purchases", false, false], [2, 3, 12, 39, "named", "same", false, false], [5, 6, 2, 3, "origin", "founded_by", false, false], [30, 36, 0, 1, "artifact", "", false, false], [47, 49, 12, 39, "artifact", "", true, false]], "relations_mapping_to_source": [0, 2, 3, 4, 5], "sentence": ["Unimation", "acquired", "Victor", "Scheinman", "'s", "Vicarm", "Inc.", "in", "1977", ",", "and", "with", "Scheinman", "'s", "help", ",", "the", "company", "created", "and", "began", "manufacturing", "a", "new", "model", "of", "robotic", "arm", ",", "the", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "which", "used", "Scheinman", "'s", "state", "-", "of", "-", "the", "-", "art", "VAL", "programming", "language", "."], "sentence-detokenized": "Unimation acquired Victor Scheinman's Vicarm Inc. in 1977, and with Scheinman's help, the company created and began manufacturing a new model of robotic arm, the Programmable Universal Machine for Assembly, which used Scheinman's state-of-the-art VAL programming language.", "token2charspan": [[0, 9], [10, 18], [19, 25], [26, 35], [35, 37], [38, 44], [45, 49], [50, 52], [53, 57], [57, 58], [59, 62], [63, 67], [68, 77], [77, 79], [80, 84], [84, 85], [86, 89], [90, 97], [98, 105], [106, 109], [110, 115], [116, 129], [130, 131], [132, 135], [136, 141], [142, 144], [145, 152], [153, 156], [156, 157], [158, 161], [162, 174], [175, 184], [185, 192], [193, 196], [197, 205], [205, 206], [207, 212], [213, 217], [218, 227], [227, 229], [230, 235], [235, 236], [236, 238], [238, 239], [239, 242], [242, 243], [243, 246], [247, 250], [251, 262], [263, 271], [271, 272]]}
{"doc_key": "ai-train-91", "ner": [[0, 2, "product"], [6, 6, "programlang"], [8, 11, "algorithm"], [12, 17, "product"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 2, 6, 6, "general-affiliation", "", false, false], [0, 2, 8, 11, "origin", "implementation_of", false, false], [0, 2, 12, 17, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["J", "48", "is", "an", "open", "source", "Java", "implementation", "of", "the", "C4.5", "algorithm", "in", "the", "Weka", "data", "mining", "tool", "."], "sentence-detokenized": "J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool.", "token2charspan": [[0, 1], [1, 3], [4, 6], [7, 9], [10, 14], [15, 21], [22, 26], [27, 41], [42, 44], [45, 48], [49, 53], [54, 63], [64, 66], [67, 70], [71, 75], [76, 80], [81, 87], [88, 92], [92, 93]]}
{"doc_key": "ai-train-92", "ner": [[7, 7, "metrics"], [1, 3, "product"], [22, 28, "misc"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[7, 7, 1, 3, "win-defeat", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["According", "to", "Google", "Scholar", ",", "the", "2004", "SSIM", "paper", "has", "been", "cited", "more", "than", "20,000", "times", ".", "It", "also", "received", "the", "2016", "IEEE", "Signal", "Processing", "Society", "Sustained", "Impact", "Award", ",", "which", "recognises", "a", "paper", "with", "unusually", "high", "impact", "for", "at", "least", "10", "years", "after", "publication", "."], "sentence-detokenized": "According to Google Scholar, the 2004 SSIM paper has been cited more than 20,000 times. It also received the 2016 IEEE Signal Processing Society Sustained Impact Award, which recognises a paper with unusually high impact for at least 10 years after publication.", "token2charspan": [[0, 9], [10, 12], [13, 19], [20, 27], [27, 28], [29, 32], [33, 37], [38, 42], [43, 48], [49, 52], [53, 57], [58, 63], [64, 68], [69, 73], [74, 80], [81, 86], [86, 87], [88, 90], [91, 95], [96, 104], [105, 108], [109, 113], [114, 118], [119, 125], [126, 136], [137, 144], [145, 154], [155, 161], [162, 167], [167, 168], [169, 174], [175, 185], [186, 187], [188, 193], [194, 198], [199, 208], [209, 213], [214, 220], [221, 224], [225, 227], [228, 233], [234, 236], [237, 242], [243, 248], [249, 260], [260, 261]]}
{"doc_key": "ai-train-93", "ner": [[35, 36, "task"], [12, 12, "product"], [11, 24, "product"], [27, 27, "organisation"], [28, 28, "product"], [32, 33, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[35, 36, 27, 27, "artifact", "", false, false], [12, 12, 35, 36, "related-to", "performs", false, false], [12, 12, 11, 24, "part-of", "", false, false], [27, 27, 32, 33, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["With", "the", "introduction", "in", "2016", "of", "voice", "editing", "and", "creation", "software", "Adobe", "Voco", ",", "a", "prototype", "planned", "to", "become", "part", "of", "the", "Adobe", "Creative", "Suite", ",", "and", "DeepMind", "WaveNet", ",", "a", "prototype", "from", "Google", ",", "speech", "synthesis", "is", "becoming", "completely", "indistinguishable", "from", "a", "real", "human", "voice", "."], "sentence-detokenized": "With the introduction in 2016 of voice editing and creation software Adobe Voco, a prototype planned to become part of the Adobe Creative Suite, and DeepMind WaveNet, a prototype from Google, speech synthesis is becoming completely indistinguishable from a real human voice.", "token2charspan": [[0, 4], [5, 8], [9, 21], [22, 24], [25, 29], [30, 32], [33, 38], [39, 46], [47, 50], [51, 59], [60, 68], [69, 74], [75, 79], [79, 80], [81, 82], [83, 92], [93, 100], [101, 103], [104, 110], [111, 115], [116, 118], [119, 122], [123, 128], [129, 137], [138, 143], [143, 144], [145, 148], [149, 157], [158, 165], [165, 166], [167, 168], [169, 178], [179, 183], [184, 190], [190, 191], [192, 198], [199, 208], [209, 211], [212, 220], [221, 231], [232, 249], [250, 254], [255, 256], [257, 261], [262, 267], [268, 273], [273, 274]]}
{"doc_key": "ai-train-94", "ner": [[0, 2, "researcher"], [5, 9, "organisation"], [14, 20, "organisation"], [25, 26, "conference"], [33, 38, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 2, 5, 9, "role", "", false, false], [0, 2, 14, 20, "role", "", false, false], [0, 2, 25, 26, "role", "", false, false], [0, 2, 33, 38, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "is", "an", "honorary", "member", "of", "the", "Neuroscience", "Research", "Programme", ",", "a", "fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "a", "founding", "member", "of", "AAAI", ",", "and", "a", "founding", "member", "of", "the", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio is an honorary member of the Neuroscience Research Programme, a fellow of the American Academy of Arts and Sciences, a founding member of AAAI, and a founding member of the McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 9], [10, 12], [13, 21], [22, 28], [29, 31], [32, 35], [36, 48], [49, 57], [58, 67], [67, 68], [69, 70], [71, 77], [78, 80], [81, 84], [85, 93], [94, 101], [102, 104], [105, 109], [110, 113], [114, 122], [122, 123], [124, 125], [126, 134], [135, 141], [142, 144], [145, 149], [149, 150], [151, 154], [155, 156], [157, 165], [166, 172], [173, 175], [176, 179], [180, 188], [189, 198], [199, 202], [203, 208], [209, 217], [217, 218]]}
{"doc_key": "ai-train-95", "ner": [[10, 10, "task"], [8, 13, "task"], [16, 18, "task"], [25, 25, "misc"], [24, 27, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[10, 10, 16, 18, "cause-effect", "", false, false], [8, 13, 16, 18, "cause-effect", "", false, false], [24, 27, 16, 18, "topic", "", false, false], [24, 27, 25, 25, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "the", "1990s", ",", "encouraged", "by", "the", "successes", "in", "speech", "recognition", "and", "speech", "synthesis", ",", "research", "on", "speech", "translation", "began", "with", "the", "development", "of", "the", "German", "Verbmobil", "project", "."], "sentence-detokenized": "In the 1990s, encouraged by the successes in speech recognition and speech synthesis, research on speech translation began with the development of the German Verbmobil project.", "token2charspan": [[0, 2], [3, 6], [7, 12], [12, 13], [14, 24], [25, 27], [28, 31], [32, 41], [42, 44], [45, 51], [52, 63], [64, 67], [68, 74], [75, 84], [84, 85], [86, 94], [95, 97], [98, 104], [105, 116], [117, 122], [123, 127], [128, 131], [132, 143], [144, 146], [147, 150], [151, 157], [158, 167], [168, 175], [175, 176]]}
{"doc_key": "ai-train-96", "ner": [[2, 4, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"], [13, 15, "algorithm"], [19, 20, "algorithm"], [24, 24, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[2, 4, 7, 8, "role", "", false, false], [13, 15, 2, 4, "origin", "", false, false], [13, 15, 7, 8, "origin", "", false, false], [13, 15, 10, 11, "origin", "", false, false], [13, 15, 24, 24, "part-of", "", false, false], [19, 20, 13, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["In", "1999", "Felix", "Gers", "and", "his", "consultant", "J\u00fcrgen", "Schmidhuber", "and", "Fred", "Cummins", "introduced", "the", "forget", "gate", "(", "also", "called", "keep", "gate", ")", "into", "the", "LSTM", "architecture", ","], "sentence-detokenized": "In 1999 Felix Gers and his consultant J\u00fcrgen Schmidhuber and Fred Cummins introduced the forget gate (also called keep gate) into the LSTM architecture,", "token2charspan": [[0, 2], [3, 7], [8, 13], [14, 18], [19, 22], [23, 26], [27, 37], [38, 44], [45, 56], [57, 60], [61, 65], [66, 73], [74, 84], [85, 88], [89, 95], [96, 100], [101, 102], [102, 106], [107, 113], [114, 118], [119, 123], [123, 124], [125, 129], [130, 133], [134, 138], [139, 151], [151, 152]]}
{"doc_key": "ai-train-97", "ner": [[1, 3, "field"], [0, 6, "field"], [9, 11, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 11, 1, 3, "part-of", "", false, false], [9, 11, 0, 6, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "digital", "signal", "processing", "and", "information", "theory", ",", "the", "normalised", "sinc", "function", "is", "usually", "defined", "as"], "sentence-detokenized": "In digital signal processing and information theory, the normalised sinc function is usually defined as", "token2charspan": [[0, 2], [3, 10], [11, 17], [18, 28], [29, 32], [33, 44], [45, 51], [51, 52], [53, 56], [57, 67], [68, 72], [73, 81], [82, 84], [85, 92], [93, 100], [101, 103]]}
{"doc_key": "ai-train-98", "ner": [[2, 4, "field"], [8, 9, "researcher"], [16, 18, "conference"], [23, 27, "organisation"], [29, 29, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 4, 8, 9, "origin", "coined_term", false, false], [8, 9, 16, 18, "role", "", false, false], [8, 9, 23, 27, "role", "", false, false], [29, 29, 23, 27, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["The", "term", "computational", "linguistics", "was", "first", "coined", "by", "David", "Hays", ",", "a", "founding", "member", "of", "both", "the", "Association", "for", "Computational", "Linguistics", "and", "the", "International", "Committee", "for", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "The term computational linguistics was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee for Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 8], [9, 22], [23, 34], [35, 38], [39, 44], [45, 51], [52, 54], [55, 60], [61, 65], [65, 66], [67, 68], [69, 77], [78, 84], [85, 87], [88, 92], [93, 96], [97, 108], [109, 112], [113, 126], [127, 138], [139, 142], [143, 146], [147, 160], [161, 170], [171, 174], [175, 188], [189, 200], [201, 202], [202, 206], [206, 207], [207, 208]]}
{"doc_key": "ai-train-99", "ner": [[8, 20, "misc"], [7, 13, "misc"], [31, 34, "metrics"], [36, 36, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[36, 36, 31, 34, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct.", "2011", "In", "one-dimensional", "polynomial", "-", "based", "DPD", "with", "(", "or", "without", ")", "memory", ",", "in", "order", "to", "resolve", "the", "digital", "pre-deflector", "polynomial", "coefficients", "and", "minimise", "the", "mean", "squared", "error", "(", "MSE", ")", ",", "the", "distorted", "output", "of", "the", "nonlinear", "system", "must", "be", "oversampled", "at", "a", "rate", "that", "ensures", "that", "the", "nonlinear", "products", "of", "digital", "pre-deflector", "order", "are", "captured", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In one-dimensional polynomial-based DPD with (or without) memory, in order to resolve the digital pre-deflector polynomial coefficients and minimise the mean squared error (MSE), the distorted output of the nonlinear system must be oversampled at a rate that ensures that the nonlinear products of digital pre-deflector order are captured.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 23], [24, 28], [29, 31], [32, 47], [48, 58], [58, 59], [59, 64], [65, 68], [69, 73], [74, 75], [75, 77], [78, 85], [85, 86], [87, 93], [93, 94], [95, 97], [98, 103], [104, 106], [107, 114], [115, 118], [119, 126], [127, 140], [141, 151], [152, 164], [165, 168], [169, 177], [178, 181], [182, 186], [187, 194], [195, 200], [201, 202], [202, 205], [205, 206], [206, 207], [208, 211], [212, 221], [222, 228], [229, 231], [232, 235], [236, 245], [246, 252], [253, 257], [258, 260], [261, 272], [273, 275], [276, 277], [278, 282], [283, 287], [288, 295], [296, 300], [301, 304], [305, 314], [315, 323], [324, 326], [327, 334], [335, 348], [349, 354], [355, 358], [359, 367], [367, 368]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [9, 12, "location"], [14, 15, "country"], [19, 19, "location"], [21, 21, "country"], [37, 45, "organisation"], [46, 50, "organisation"], [51, 53, "location"], [61, 62, "organisation"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 46, 50, "physical", "", false, false], [0, 1, 61, 62, "role", "", false, false], [9, 12, 14, 15, "physical", "", false, false], [37, 45, 46, 50, "part-of", "", false, false], [46, 50, 51, 53, "physical", "", false, false], [61, 62, 37, 45, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "b.", "5", "October", "1947", ",", "Chisinau", ",", "Moldavian", "SSR", ",", "Soviet", "Union", ",", "(", "now", "Chisinau", ",", "Moldova", ")", ")", "He", "is", "a", "principal", "American", "research", "scientist", "(", "computer", "scientist", ")", "at", "the", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "at", "the", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", ",", "MA", ",", "and", "head", "of", "the", "Laboratory", "'s", "InfoLab", "Group", "."], "sentence-detokenized": "Boris Katz, (b. 5 October 1947, Chisinau, Moldavian SSR, Soviet Union, (now Chisinau, Moldova)) He is a principal American research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge, MA, and head of the Laboratory's InfoLab Group.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 15], [16, 17], [18, 25], [26, 30], [30, 31], [32, 40], [40, 41], [42, 51], [52, 55], [55, 56], [57, 63], [64, 69], [69, 70], [71, 72], [72, 75], [76, 84], [84, 85], [86, 93], [93, 94], [94, 95], [96, 98], [99, 101], [102, 103], [104, 113], [114, 122], [123, 131], [132, 141], [142, 143], [143, 151], [152, 161], [161, 162], [163, 165], [166, 169], [170, 173], [174, 182], [183, 190], [191, 194], [195, 205], [206, 218], [219, 229], [230, 232], [233, 236], [237, 250], [251, 260], [261, 263], [264, 274], [275, 277], [278, 287], [287, 288], [289, 291], [291, 292], [293, 296], [297, 301], [302, 304], [305, 308], [309, 319], [319, 321], [322, 329], [330, 335], [335, 336]]}
