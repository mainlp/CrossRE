{"doc_key": "ai-train-1", "ner": [[3, 4, "product"], [9, 10, "field"], [12, 13, "task"], [15, 16, "task"], [20, 22, "task"], [25, 26, "field"], [27, 28, "researcher"], [30, 31, "researcher"], [33, 34, "researcher"], [36, 37, "researcher"], [39, 40, "researcher"], [42, 43, "researcher"], [45, 46, "researcher"], [48, 49, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], "relations": [[3, 4, 9, 10, "part-of", "", false, false], [3, 4, 9, 10, "usage", "", false, false], [3, 4, 12, 13, "part-of", "", false, false], [3, 4, 12, 13, "usage", "", false, false], [3, 4, 15, 16, "part-of", "", false, false], [3, 4, 15, 16, "usage", "", false, false], [3, 4, 25, 26, "part-of", "", false, false], [3, 4, 25, 26, "usage", "", false, false], [20, 22, 15, 16, "part-of", "", false, false], [20, 22, 15, 16, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "sentence": ["Beliebte", "Ans\u00e4tze", "f\u00fcr", "meinungsbasierte", "Empfehlungssysteme", "nutzen", "verschiedene", "Techniken", "wie", "Text", "Mining", ",", "Information", "Retrieval", ",", "Sentiment", "Analysis", "(", "siehe", "auch", "Multimodal", "Sentiment", "Analysis", ")", "und", "Deep", "Learning", "X.Y.", "Feng", ",", "H.", "Zhang", ",", "Y.J.", "Ren", ",", "P.H.", "Shang", ",", "Y.", "Zhu", ",", "Y.C.", "Liang", ",", "R.C.", "Guan", ",", "D.", "Xu", ",", "(", "2019", ")", ",", "21", "(", "5", ")", ":", "e12957", "."], "sentence-detokenized": "Beliebte Ans\u00e4tze f\u00fcr meinungsbasierte Empfehlungssysteme nutzen verschiedene Techniken wie Text Mining, Information Retrieval, Sentiment Analysis (siehe auch Multimodal Sentiment Analysis) und Deep Learning X.Y. Feng, H. Zhang, Y.J. Ren, P.H. Shang, Y. Zhu, Y.C. Liang, R.C. Guan, D. Xu, (2019), 21 (5): e12957.", "token2charspan": [[0, 8], [9, 16], [17, 20], [21, 37], [38, 56], [57, 63], [64, 76], [77, 86], [87, 90], [91, 95], [96, 102], [102, 103], [104, 115], [116, 125], [125, 126], [127, 136], [137, 145], [146, 147], [147, 152], [153, 157], [158, 168], [169, 178], [179, 187], [187, 188], [189, 192], [193, 197], [198, 206], [207, 211], [212, 216], [216, 217], [218, 220], [221, 226], [226, 227], [228, 232], [233, 236], [236, 237], [238, 242], [243, 248], [248, 249], [250, 252], [253, 256], [256, 257], [258, 262], [263, 268], [268, 269], [270, 274], [275, 279], [279, 280], [281, 283], [284, 286], [286, 287], [288, 289], [289, 293], [293, 294], [294, 295], [296, 298], [299, 300], [300, 301], [301, 302], [302, 303], [304, 310], [310, 311]]}
{"doc_key": "ai-train-2", "ner": [[9, 9, "university"], [14, 15, "researcher"], [17, 20, "researcher"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[14, 15, 9, 9, "physical", "", false, false], [14, 15, 9, 9, "role", "", false, false], [17, 20, 9, 9, "physical", "", false, false], [17, 20, 9, 9, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Die", "Bef\u00fcrworter", "der", "prozeduralen", "Repr\u00e4sentation", "waren", "vor", "allem", "am", "MIT", "unter", "der", "Leitung", "von", "Marvin", "Minsky", "und", "Seymour", "Papert", "zu", "finden", "."], "sentence-detokenized": "Die Bef\u00fcrworter der prozeduralen Repr\u00e4sentation waren vor allem am MIT unter der Leitung von Marvin Minsky und Seymour Papert zu finden.", "token2charspan": [[0, 3], [4, 15], [16, 19], [20, 32], [33, 47], [48, 53], [54, 57], [58, 63], [64, 66], [67, 70], [71, 76], [77, 80], [81, 88], [89, 92], [93, 99], [100, 106], [107, 110], [111, 118], [119, 125], [126, 128], [129, 135], [135, 136]]}
{"doc_key": "ai-train-3", "ner": [[7, 7, "programlang"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "Standardschnittstelle", "und", "die", "Taschenrechnerschnittstelle", "sind", "in", "Java", "geschrieben", "."], "sentence-detokenized": "Die Standardschnittstelle und die Taschenrechnerschnittstelle sind in Java geschrieben.", "token2charspan": [[0, 3], [4, 25], [26, 29], [30, 33], [34, 61], [62, 66], [67, 69], [70, 74], [75, 86], [86, 87]]}
{"doc_key": "ai-train-4", "ner": [[0, 0, "product"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 24, 24, "related-to", "compatible_with", false, false]], "relations_mapping_to_source": [0], "sentence": ["Octave", "hilft", "bei", "der", "numerischen", "L\u00f6sung", "linearer", "und", "nichtlinearer", "Probleme", "und", "bei", "der", "Durchf\u00fchrung", "anderer", "numerischer", "Experimente", "mit", "einem", "Programm", ",", "das", "gr\u00f6\u00dftenteils", "mit", "MATLAB", "kompatibel", "ist", "."], "sentence-detokenized": "Octave hilft bei der numerischen L\u00f6sung linearer und nichtlinearer Probleme und bei der Durchf\u00fchrung anderer numerischer Experimente mit einem Programm, das gr\u00f6\u00dftenteils mit MATLAB kompatibel ist.", "token2charspan": [[0, 6], [7, 12], [13, 16], [17, 20], [21, 32], [33, 39], [40, 48], [49, 52], [53, 66], [67, 75], [76, 79], [80, 83], [84, 87], [88, 100], [101, 108], [109, 120], [121, 132], [133, 136], [137, 142], [143, 151], [151, 152], [153, 156], [157, 169], [170, 173], [174, 180], [181, 191], [192, 195], [195, 196]]}
{"doc_key": "ai-train-5", "ner": [[2, 2, "algorithm"], [4, 5, "misc"], [7, 8, "researcher"], [13, 14, "university"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[2, 2, 7, 8, "origin", "", false, false], [4, 5, 7, 8, "origin", "", false, false], [7, 8, 13, 14, "physical", "", false, false], [7, 8, 13, 14, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Varianten", "des", "Backpropagation-Algorithmus", "sowie", "un\u00fcberwachte", "Methoden", "von", "Geoff", "Hinton", "und", "Kollegen", "an", "der", "Universit\u00e4t", "Toronto", "k\u00f6nnen", "zum", "Training", "tiefer", ",", "hochgradig", "nichtlinearer", "neuronaler", "Architekturen", "verwendet", "werden", ",", "{", "{", "cite", "journal"], "sentence-detokenized": "Varianten des Backpropagation-Algorithmus sowie un\u00fcberwachte Methoden von Geoff Hinton und Kollegen an der Universit\u00e4t Toronto k\u00f6nnen zum Training tiefer, hochgradig nichtlinearer neuronaler Architekturen verwendet werden, {{cite journal", "token2charspan": [[0, 9], [10, 13], [14, 41], [42, 47], [48, 60], [61, 69], [70, 73], [74, 79], [80, 86], [87, 90], [91, 99], [100, 102], [103, 106], [107, 118], [119, 126], [127, 133], [134, 137], [138, 146], [147, 153], [153, 154], [155, 165], [166, 179], [180, 190], [191, 204], [205, 214], [215, 221], [221, 222], [223, 224], [224, 225], [225, 229], [230, 237]]}
{"doc_key": "ai-train-6", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["oder", "\u00e4quivalent", "in", "DCG-Notation", ":"], "sentence-detokenized": "oder \u00e4quivalent in DCG-Notation:", "token2charspan": [[0, 4], [5, 15], [16, 18], [19, 31], [31, 32]]}
{"doc_key": "ai-train-7", "ner": [[0, 1, "algorithm"], [6, 8, "algorithm"], [13, 14, "algorithm"], [20, 20, "algorithm"], [22, 22, "algorithm"], [24, 24, "algorithm"], [35, 36, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[0, 1, 6, 8, "type-of", "", false, false], [0, 1, 13, 14, "usage", "part-of?", true, false], [13, 14, 20, 20, "compare", "", false, false], [22, 22, 20, 20, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Selbstorganisierende", "Karten", "unterscheiden", "sich", "von", "anderen", "k\u00fcnstlichen", "neuronalen", "Netzen", "dadurch", ",", "dass", "sie", "kompetitives", "Lernen", "anwenden", "(", "im", "Gegensatz", "zu", "Fehlerkorrektur-Lernen", "wie", "Backpropagation", "mit", "Gradientenabstieg", ")", "und", "dass", "sie", "eine", "Nachbarschaftsfunktion", "verwenden", ",", "um", "die", "topologischen", "Eigenschaften", "des", "Eingaberaums", "zu", "erhalten", "."], "sentence-detokenized": "Selbstorganisierende Karten unterscheiden sich von anderen k\u00fcnstlichen neuronalen Netzen dadurch, dass sie kompetitives Lernen anwenden (im Gegensatz zu Fehlerkorrektur-Lernen wie Backpropagation mit Gradientenabstieg) und dass sie eine Nachbarschaftsfunktion verwenden, um die topologischen Eigenschaften des Eingaberaums zu erhalten.", "token2charspan": [[0, 20], [21, 27], [28, 41], [42, 46], [47, 50], [51, 58], [59, 70], [71, 81], [82, 88], [89, 96], [96, 97], [98, 102], [103, 106], [107, 119], [120, 126], [127, 135], [136, 137], [137, 139], [140, 149], [150, 152], [153, 175], [176, 179], [180, 195], [196, 199], [200, 217], [217, 218], [219, 222], [223, 227], [228, 231], [232, 236], [237, 259], [260, 269], [269, 270], [271, 273], [274, 277], [278, 291], [292, 305], [306, 309], [310, 322], [323, 325], [326, 334], [334, 335]]}
{"doc_key": "ai-train-8", "ner": [[12, 14, "organisation"], [23, 24, "misc"], [30, 32, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Seit", "Anfang", "der", "1990er", "Jahre", "wird", "von", "verschiedenen", "Stellen", ",", "darunter", "die", "Audio", "Engineering", "Society", ",", "empfohlen", ",", "Messungen", "des", "Dynamikbereichs", "bei", "vorhandenem", "Audiosignal", "vorzunehmen", ",", "das", "dann", "bei", "der", "Messung", "des", "Rauschpegels", ",", "die", "zur", "Bestimmung", "des", "Dynamikbereichs", "verwendet", "wird", ",", "herausgefiltert", "wird", ".", "Dadurch", "werden", "fragw\u00fcrdige", "Messungen", "vermieden", ",", "die", "auf", "der", "Verwendung", "von", "Leermedien", "oder", "Stummschaltungen", "beruhen", "."], "sentence-detokenized": "Seit Anfang der 1990er Jahre wird von verschiedenen Stellen, darunter die Audio Engineering Society, empfohlen, Messungen des Dynamikbereichs bei vorhandenem Audiosignal vorzunehmen, das dann bei der Messung des Rauschpegels, die zur Bestimmung des Dynamikbereichs verwendet wird, herausgefiltert wird. Dadurch werden fragw\u00fcrdige Messungen vermieden, die auf der Verwendung von Leermedien oder Stummschaltungen beruhen.", "token2charspan": [[0, 4], [5, 11], [12, 15], [16, 22], [23, 28], [29, 33], [34, 37], [38, 51], [52, 59], [59, 60], [61, 69], [70, 73], [74, 79], [80, 91], [92, 99], [99, 100], [101, 110], [110, 111], [112, 121], [122, 125], [126, 141], [142, 145], [146, 157], [158, 169], [170, 181], [181, 182], [183, 186], [187, 191], [192, 195], [196, 199], [200, 207], [208, 211], [212, 224], [224, 225], [226, 229], [230, 233], [234, 244], [245, 248], [249, 264], [265, 274], [275, 279], [279, 280], [281, 296], [297, 301], [301, 302], [303, 310], [311, 317], [318, 329], [330, 339], [340, 349], [349, 350], [351, 354], [355, 358], [359, 362], [363, 373], [374, 377], [378, 388], [389, 393], [394, 410], [411, 418], [418, 419]]}
{"doc_key": "ai-train-9", "ner": [[5, 5, "misc"], [16, 16, "task"], [19, 19, "task"], [21, 21, "task"], [23, 23, "task"], [25, 27, "task"], [29, 31, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 6, 7], "relations": [[5, 5, 16, 16, "part-of", "concept_used_in", true, false], [5, 5, 19, 19, "part-of", "concept_used_in", false, false], [5, 5, 21, 21, "part-of", "concept_used_in", false, false], [5, 5, 23, 23, "part-of", "concept_used_in", false, false], [5, 5, 25, 27, "part-of", "concept_used_in", false, false], [5, 5, 29, 31, "part-of", "concept_used_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5, 6], "sentence": ["Die", "Technik", "zur", "Erstellung", "von", "Eigengesichtern", "und", "deren", "Verwendung", "f\u00fcr", "die", "Erkennung", "wird", "auch", "au\u00dferhalb", "der", "Gesichtserkennung", "eingesetzt", ":", "Handschrifterkennung", ",", "Lippenlesen", ",", "Stimmerkennung", ",", "Interpretation", "von", "Geb\u00e4rdensprache/Handgesten", "und", "Analyse", "medizinischer", "Bilder", "."], "sentence-detokenized": "Die Technik zur Erstellung von Eigengesichtern und deren Verwendung f\u00fcr die Erkennung wird auch au\u00dferhalb der Gesichtserkennung eingesetzt: Handschrifterkennung, Lippenlesen, Stimmerkennung, Interpretation von Geb\u00e4rdensprache/Handgesten und Analyse medizinischer Bilder.", "token2charspan": [[0, 3], [4, 11], [12, 15], [16, 26], [27, 30], [31, 46], [47, 50], [51, 56], [57, 67], [68, 71], [72, 75], [76, 85], [86, 90], [91, 95], [96, 105], [106, 109], [110, 127], [128, 138], [138, 139], [140, 160], [160, 161], [162, 173], [173, 174], [175, 189], [189, 190], [191, 205], [206, 209], [210, 236], [237, 240], [241, 248], [249, 262], [263, 269], [269, 270]]}
{"doc_key": "ai-train-10", "ner": [[1, 3, "organisation"], [9, 13, "organisation"], [15, 15, "organisation"], [19, 19, "organisation"], [22, 23, "organisation"], [26, 26, "organisation"], [29, 33, "organisation"], [35, 35, "organisation"], [39, 42, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[9, 13, 1, 3, "part-of", "", false, false], [15, 15, 9, 13, "named", "", false, false], [19, 19, 1, 3, "part-of", "", false, false], [22, 23, 1, 3, "part-of", "", false, false], [26, 26, 1, 3, "part-of", "", false, false], [29, 33, 1, 3, "part-of", "", false, false], [35, 35, 29, 33, "named", "", false, false], [39, 42, 1, 3, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Die", "National", "Science", "Foundation", "war", "ein", "Dach", "f\u00fcr", "die", "National", "Aeronautics", "and", "Space", "Administration", "(", "NASA", ")", ",", "das", "US-Energieministerium", ",", "das", "US-Handelsministerium", "NIST", ",", "das", "US-Verteidigungsministerium", ",", "die", "Defense", "Advanced", "Research", "Projects", "Agency", "(", "DARPA", ")", "und", "das", "Office", "of", "Naval", "Research", "koordinierten", "Studien", ",", "um", "strategische", "Planer", "bei", "ihren", "\u00dcberlegungen", "zu", "unterst\u00fctzen", "."], "sentence-detokenized": "Die National Science Foundation war ein Dach f\u00fcr die National Aeronautics and Space Administration (NASA), das US-Energieministerium, das US-Handelsministerium NIST, das US-Verteidigungsministerium, die Defense Advanced Research Projects Agency (DARPA) und das Office of Naval Research koordinierten Studien, um strategische Planer bei ihren \u00dcberlegungen zu unterst\u00fctzen.", "token2charspan": [[0, 3], [4, 12], [13, 20], [21, 31], [32, 35], [36, 39], [40, 44], [45, 48], [49, 52], [53, 61], [62, 73], [74, 77], [78, 83], [84, 98], [99, 100], [100, 104], [104, 105], [105, 106], [107, 110], [111, 132], [132, 133], [134, 137], [138, 159], [160, 164], [164, 165], [166, 169], [170, 197], [197, 198], [199, 202], [203, 210], [211, 219], [220, 228], [229, 237], [238, 244], [245, 246], [246, 251], [251, 252], [253, 256], [257, 260], [261, 267], [268, 270], [271, 276], [277, 285], [286, 299], [300, 307], [307, 308], [309, 311], [312, 324], [325, 331], [332, 335], [336, 341], [342, 354], [355, 357], [358, 370], [370, 371]]}
{"doc_key": "ai-train-11", "ner": [[9, 9, "algorithm"], [13, 14, "researcher"], [18, 18, "researcher"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[13, 14, 18, 18, "related-to", "added_appendix_to_work_of", false, false]], "relations_mapping_to_source": [1], "sentence": ["Eine", "schnelle", "Methode", "zur", "Berechnung", "von", "Maximum-Likelihood-Sch\u00e4tzungen", "f\u00fcr", "das", "Probit-Modell", "wurde", "1935", "von", "Ronald", "Fisher", "als", "Anhang", "zu", "Bliss'", "Arbeit", "vorgeschlagen", "."], "sentence-detokenized": "Eine schnelle Methode zur Berechnung von Maximum-Likelihood-Sch\u00e4tzungen f\u00fcr das Probit-Modell wurde 1935 von Ronald Fisher als Anhang zu Bliss' Arbeit vorgeschlagen.", "token2charspan": [[0, 4], [5, 13], [14, 21], [22, 25], [26, 36], [37, 40], [41, 71], [72, 75], [76, 79], [80, 93], [94, 99], [100, 104], [105, 108], [109, 115], [116, 122], [123, 126], [127, 133], [134, 136], [137, 143], [144, 150], [151, 164], [164, 165]]}
{"doc_key": "ai-train-12", "ner": [[9, 10, "product"], [13, 13, "product"], [16, 16, "organisation"], [17, 17, "product"], [26, 26, "organisation"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[17, 17, 13, 13, "usage", "uses_software", false, false], [17, 17, 16, 16, "artifact", "", false, false], [17, 17, 24, 24, "named", "", false, false], [24, 24, 26, 26, "artifact", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Mehrere", "dieser", "Programme", "sind", "online", "verf\u00fcgbar", ",", "z.", "B.", "Google", "Translate", "und", "das", "SYSTRAN-System", ",", "das", "AltaVistas", "BabelFish", "(", "seit", "dem", "9.", "Mai", "2008", "Babelfish", "von", "Yahoo", ")", "antreibt", "."], "sentence-detokenized": "Mehrere dieser Programme sind online verf\u00fcgbar, z. B. Google Translate und das SYSTRAN-System, das AltaVistas BabelFish (seit dem 9. Mai 2008 Babelfish von Yahoo) antreibt.", "token2charspan": [[0, 7], [8, 14], [15, 24], [25, 29], [30, 36], [37, 46], [46, 47], [48, 50], [51, 53], [54, 60], [61, 70], [71, 74], [75, 78], [79, 93], [93, 94], [95, 98], [99, 109], [110, 119], [120, 121], [121, 125], [126, 129], [130, 132], [133, 136], [137, 141], [142, 151], [152, 155], [156, 161], [161, 162], [163, 171], [171, 172]]}
{"doc_key": "ai-train-13", "ner": [[6, 6, "researcher"], [9, 10, "researcher"], [12, 13, "researcher"], [18, 20, "field"], [25, 26, "misc"], [29, 30, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[6, 6, 18, 20, "related-to", "", true, false], [6, 6, 25, 26, "related-to", "", true, false], [6, 6, 29, 30, "related-to", "", true, false], [9, 10, 18, 20, "related-to", "", true, false], [9, 10, 25, 26, "related-to", "", true, false], [9, 10, 29, 30, "related-to", "", true, false], [12, 13, 18, 20, "related-to", "", true, false], [12, 13, 25, 26, "related-to", "", true, false], [12, 13, 29, 30, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Im", "Jahr", "2002", "entwickelte", "und", "ver\u00f6ffentlichte", "Hutter", "zusammen", "mit", "J\u00fcrgen", "Schmidhuber", "und", "Shane", "Legg", "eine", "mathematische", "Theorie", "der", "k\u00fcnstlichen", "allgemeinen", "Intelligenz", ",", "die", "auf", "idealisierten", "intelligenten", "Agenten", "und", "belohnungsmotiviertem", "Verst\u00e4rkungslernen", "basiert", "."], "sentence-detokenized": "Im Jahr 2002 entwickelte und ver\u00f6ffentlichte Hutter zusammen mit J\u00fcrgen Schmidhuber und Shane Legg eine mathematische Theorie der k\u00fcnstlichen allgemeinen Intelligenz, die auf idealisierten intelligenten Agenten und belohnungsmotiviertem Verst\u00e4rkungslernen basiert.", "token2charspan": [[0, 2], [3, 7], [8, 12], [13, 24], [25, 28], [29, 44], [45, 51], [52, 60], [61, 64], [65, 71], [72, 83], [84, 87], [88, 93], [94, 98], [99, 103], [104, 117], [118, 125], [126, 129], [130, 141], [142, 153], [154, 165], [165, 166], [167, 170], [171, 174], [175, 188], [189, 202], [203, 210], [211, 214], [215, 236], [237, 255], [256, 263], [263, 264]]}
{"doc_key": "ai-train-14", "ner": [[11, 15, "metrics"]], "ner_mapping_to_source": [1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "gebr\u00e4uchlichste", "Methode", "ist", "die", "Verwendung", "des", "so", "genannten", "ROUGE-Ma\u00dfes", "(", "Recall-Oriented", "Understudy", "for", "Gisting", "Evaluation", ")", "."], "sentence-detokenized": "Die gebr\u00e4uchlichste Methode ist die Verwendung des so genannten ROUGE-Ma\u00dfes (Recall-Oriented Understudy for Gisting Evaluation).", "token2charspan": [[0, 3], [4, 19], [20, 27], [28, 31], [32, 35], [36, 46], [47, 50], [51, 53], [54, 63], [64, 75], [76, 77], [77, 92], [93, 103], [104, 107], [108, 115], [116, 126], [126, 127], [127, 128]]}
{"doc_key": "ai-train-15", "ner": [[0, 0, "product"], [16, 17, "researcher"], [19, 20, "organisation"]], "ner_mapping_to_source": [0, 3, 4], "relations": [[16, 17, 19, 20, "role", "", false, false]], "relations_mapping_to_source": [2], "sentence": ["RapidMiner", "bietet", "Lernschemata", ",", "Modelle", "und", "Algorithmen", "und", "kann", "durch", "R-", "und", "Python-Skripte", "erweitert", "werden", ".", "David", "Norris", ",", "Bloor", "Research", ",", "November", "13", ",", "2013", "."], "sentence-detokenized": "RapidMiner bietet Lernschemata, Modelle und Algorithmen und kann durch R- und Python-Skripte erweitert werden. David Norris, Bloor Research, November 13, 2013.", "token2charspan": [[0, 10], [11, 17], [18, 30], [30, 31], [32, 39], [40, 43], [44, 55], [56, 59], [60, 64], [65, 70], [71, 73], [74, 77], [78, 92], [93, 102], [103, 109], [109, 110], [111, 116], [117, 123], [123, 124], [125, 130], [131, 139], [139, 140], [141, 149], [150, 152], [152, 153], [154, 158], [158, 159]]}
{"doc_key": "ai-train-16", "ner": [[0, 0, "product"], [10, 10, "field"], [12, 13, "task"], [15, 16, "misc"], [30, 30, "programlang"], [34, 35, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 0, 10, 10, "related-to", "", false, false], [0, 0, 12, 13, "related-to", "", false, false], [0, 0, 34, 35, "related-to", "", true, false], [15, 16, 0, 0, "part-of", "", false, false], [34, 35, 30, 30, "general-affiliation", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["tity", "enth\u00e4lt", "eine", "Sammlung", "von", "Visualisierungswerkzeugen", "und", "Algorithmen", "f\u00fcr", "die", "Datenanalyse", "und", "pr\u00e4diktive", "Modellierung", "sowie", "grafische", "Benutzeroberfl\u00e4chen", "f\u00fcr", "den", "einfachen", "Zugriff", "auf", "diese", "Funktionen", ".", "Die", "neuere", ",", "vollst\u00e4ndig", "auf", "Java", "basierende", "Version", "(", "Weka", "3", ")", ",", "mit", "deren", "Entwicklung", "1997", "begonnen", "wurde", ",", "wird", "inzwischen", "in", "vielen", "verschiedenen", "Anwendungsbereichen", "eingesetzt", ",", "insbesondere", "in", "der", "Lehre", "und", "Forschung", "."], "sentence-detokenized": "tity enth\u00e4lt eine Sammlung von Visualisierungswerkzeugen und Algorithmen f\u00fcr die Datenanalyse und pr\u00e4diktive Modellierung sowie grafische Benutzeroberfl\u00e4chen f\u00fcr den einfachen Zugriff auf diese Funktionen. Die neuere, vollst\u00e4ndig auf Java basierende Version (Weka 3), mit deren Entwicklung 1997 begonnen wurde, wird inzwischen in vielen verschiedenen Anwendungsbereichen eingesetzt, insbesondere in der Lehre und Forschung.", "token2charspan": [[0, 4], [5, 12], [13, 17], [18, 26], [27, 30], [31, 56], [57, 60], [61, 72], [73, 76], [77, 80], [81, 93], [94, 97], [98, 108], [109, 121], [122, 127], [128, 137], [138, 157], [158, 161], [162, 165], [166, 175], [176, 183], [184, 187], [188, 193], [194, 204], [204, 205], [206, 209], [210, 216], [216, 217], [218, 229], [230, 233], [234, 238], [239, 249], [250, 257], [258, 259], [259, 263], [264, 265], [265, 266], [266, 267], [268, 271], [272, 277], [278, 289], [290, 294], [295, 303], [304, 309], [309, 310], [311, 315], [316, 326], [327, 329], [330, 336], [337, 350], [351, 370], [371, 381], [381, 382], [383, 395], [396, 398], [399, 402], [403, 408], [409, 412], [413, 422], [422, 423]]}
{"doc_key": "ai-train-17", "ner": [[0, 0, "product"], [13, 21, "misc"], [34, 37, "misc"], [22, 31, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 21, 0, 0, "topic", "", false, false], [13, 21, 34, 37, "win-defeat", "", false, false], [34, 37, 22, 31, "temporal", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eurisko", "machte", "viele", "interessante", "Entdeckungen", "und", "erntete", "gro\u00dfen", "Beifall", ",", "als", "seine", "Arbeit", "Heuretics", ":", "Theoretical", "and", "Study", "of", "Heuristic", "Rules", "wurde", "1982", "von", "der", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "mit", "dem", "Best", "Paper", "Award", "ausgezeichnet", "."], "sentence-detokenized": "Eurisko machte viele interessante Entdeckungen und erntete gro\u00dfen Beifall, als seine Arbeit Heuretics: Theoretical and Study of Heuristic Rules wurde 1982 von der Association for the Advancement of Artificial Intelligence mit dem Best Paper Award ausgezeichnet.", "token2charspan": [[0, 7], [8, 14], [15, 20], [21, 33], [34, 46], [47, 50], [51, 58], [59, 65], [66, 73], [73, 74], [75, 78], [79, 84], [85, 91], [92, 101], [101, 102], [103, 114], [115, 118], [119, 124], [125, 127], [128, 137], [138, 143], [144, 149], [150, 154], [155, 158], [159, 162], [163, 174], [175, 178], [179, 182], [183, 194], [195, 197], [198, 208], [209, 221], [222, 225], [226, 229], [230, 234], [235, 240], [241, 246], [247, 260], [260, 261]]}
{"doc_key": "ai-train-18", "ner": [[12, 12, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Um", "mehrere", "Einheiten", "zu", "ber\u00fccksichtigen", ",", "wird", "f\u00fcr", "jede", "Kapsel", "ein", "separater", "Scharnierverlust", "berechnet", "."], "sentence-detokenized": "Um mehrere Einheiten zu ber\u00fccksichtigen, wird f\u00fcr jede Kapsel ein separater Scharnierverlust berechnet.", "token2charspan": [[0, 2], [3, 10], [11, 20], [21, 23], [24, 39], [39, 40], [41, 45], [46, 49], [50, 54], [55, 61], [62, 65], [66, 75], [76, 92], [93, 102], [102, 103]]}
{"doc_key": "ai-train-19", "ner": [[6, 7, "product"], [9, 10, "product"], [12, 13, "product"], [15, 16, "product"], [18, 19, "product"], [22, 22, "product"], [29, 30, "product"], [32, 33, "product"], [35, 36, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 7, 22, 22, "type-of", "", false, false], [9, 10, 22, 22, "type-of", "", false, false], [12, 13, 22, 22, "type-of", "", false, false], [15, 16, 22, 22, "type-of", "", false, false], [18, 19, 22, 22, "type-of", "", false, false], [32, 33, 29, 30, "type-of", "", false, false], [35, 36, 29, 30, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "sentence": ["Mit", "dem", "Aufkommen", "von", "Sprachassistenten", "wie", "Apples", "Siri", ",", "Amazon", "Alexa", ",", "Google", "Assistant", ",", "Microsoft", "Cortana", "und", "Samsungs", "Bixby", "kann", "auf", "Sprachportale", "nun", "auch", "\u00fcber", "mobile", "Ger\u00e4te", "und", "intelligente", "Fernfeld-Sprachlautsprecher", "wie", "Amazon", "Echo", "und", "Google", "Home", "zugegriffen", "werden", "."], "sentence-detokenized": "Mit dem Aufkommen von Sprachassistenten wie Apples Siri, Amazon Alexa, Google Assistant, Microsoft Cortana und Samsungs Bixby kann auf Sprachportale nun auch \u00fcber mobile Ger\u00e4te und intelligente Fernfeld-Sprachlautsprecher wie Amazon Echo und Google Home zugegriffen werden.", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 39], [40, 43], [44, 50], [51, 55], [55, 56], [57, 63], [64, 69], [69, 70], [71, 77], [78, 87], [87, 88], [89, 98], [99, 106], [107, 110], [111, 119], [120, 125], [126, 130], [131, 134], [135, 148], [149, 152], [153, 157], [158, 162], [163, 169], [170, 176], [177, 180], [181, 193], [194, 221], [222, 225], [226, 232], [233, 237], [238, 241], [242, 248], [249, 253], [254, 265], [266, 272], [272, 273]]}
{"doc_key": "ai-train-20", "ner": [[2, 3, "field"], [5, 6, "algorithm"], [8, 10, "algorithm"], [12, 13, "algorithm"], [15, 15, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[5, 6, 2, 3, "type-of", "", false, false], [8, 10, 2, 3, "type-of", "", false, false], [12, 13, 2, 3, "type-of", "", false, false], [15, 15, 2, 3, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Beispiele", "f\u00fcr", "\u00fcberwachtes", "Lernen", "sind", "Naive", "Bayes-Klassifikator", ",", "Support", "Vector", "Machine", ",", "Gau\u00dfsche", "Mischungen", "und", "Netzwerk", "."], "sentence-detokenized": "Beispiele f\u00fcr \u00fcberwachtes Lernen sind Naive Bayes-Klassifikator, Support Vector Machine, Gau\u00dfsche Mischungen und Netzwerk.", "token2charspan": [[0, 9], [10, 13], [14, 25], [26, 32], [33, 37], [38, 43], [44, 63], [63, 64], [65, 72], [73, 79], [80, 87], [87, 88], [89, 97], [98, 108], [109, 112], [113, 121], [121, 122]]}
{"doc_key": "ai-train-21", "ner": [[3, 3, "algorithm"], [23, 23, "algorithm"], [26, 26, "task"], [31, 31, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[3, 3, 23, 23, "part-of", "", true, false], [31, 31, 26, 26, "usage", "", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Man", "kann", "den", "OSD-Algorithmus", "verwenden", ",", "um", "math", "O", "(", "\\", "sqrt", "{", "T", "}", ")", "/", "math", "regret-Grenzen", "f\u00fcr", "die", "Online-Version", "der", "Support-Vektor-Maschine", "f\u00fcr", "die", "Klassifizierung", "abzuleiten", ",", "die", "den", "Scharnierverlust", "math", "v", "_t", "(", "w", ")", "=\\", "max\\", "{", "0", ",", "1", "-", "y", "_t", "(", "w\\", "cdot", "x", "_t)\\", "}", "verwenden", "/", "math"], "sentence-detokenized": "Man kann den OSD-Algorithmus verwenden, um math O (\\ sqrt {T}) / math regret-Grenzen f\u00fcr die Online-Version der Support-Vektor-Maschine f\u00fcr die Klassifizierung abzuleiten, die den Scharnierverlust math v _t (w) =\\ max\\ {0, 1 - y _t (w\\ cdot x _t)\\} verwenden / math", "token2charspan": [[0, 3], [4, 8], [9, 12], [13, 28], [29, 38], [38, 39], [40, 42], [43, 47], [48, 49], [50, 51], [51, 52], [53, 57], [58, 59], [59, 60], [60, 61], [61, 62], [63, 64], [65, 69], [70, 84], [85, 88], [89, 92], [93, 107], [108, 111], [112, 135], [136, 139], [140, 143], [144, 159], [160, 170], [170, 171], [172, 175], [176, 179], [180, 196], [197, 201], [202, 203], [204, 206], [207, 208], [208, 209], [209, 210], [211, 213], [214, 218], [219, 220], [220, 221], [221, 222], [223, 224], [225, 226], [227, 228], [229, 231], [232, 233], [233, 235], [236, 240], [241, 242], [243, 247], [247, 248], [249, 258], [259, 260], [261, 265]]}
{"doc_key": "ai-train-22", "ner": [[4, 4, "task"], [6, 7, "task"], [9, 9, "task"], [11, 11, "task"], [13, 13, "task"], [15, 15, "task"], [17, 17, "task"], [19, 22, "task"], [24, 24, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [], "relations_mapping_to_source": [], "sentence": ["Zu", "den", "Anwendungen", "geh\u00f6ren", "Objekterkennung", ",", "robotergest\u00fctzte", "Kartierung", "und", "Navigation", ",", "Bildstitching", ",", "3D-Modellierung", ",", "Gestenerkennung", ",", "Videoverfolgung", ",", "individuelle", "Identifizierung", "von", "Wildtieren", "und", "Matchmaking", "."], "sentence-detokenized": "Zu den Anwendungen geh\u00f6ren Objekterkennung, robotergest\u00fctzte Kartierung und Navigation, Bildstitching, 3D-Modellierung, Gestenerkennung, Videoverfolgung, individuelle Identifizierung von Wildtieren und Matchmaking.", "token2charspan": [[0, 2], [3, 6], [7, 18], [19, 26], [27, 42], [42, 43], [44, 60], [61, 71], [72, 75], [76, 86], [86, 87], [88, 101], [101, 102], [103, 118], [118, 119], [120, 135], [135, 136], [137, 152], [152, 153], [154, 166], [167, 182], [183, 186], [187, 197], [198, 201], [202, 213], [213, 214]]}
{"doc_key": "ai-train-23", "ner": [[9, 9, "task"], [14, 15, "university"], [18, 20, "university"], [23, 24, "university"], [27, 28, "university"], [31, 36, "university"], [39, 41, "university"], [44, 46, "university"], [49, 50, "university"], [53, 58, "university"], [60, 60, "university"], [64, 68, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "relations": [[9, 9, 14, 15, "related-to", "", true, false], [9, 9, 18, 20, "related-to", "", true, false], [9, 9, 23, 24, "related-to", "", true, false], [9, 9, 27, 28, "related-to", "", true, false], [9, 9, 31, 36, "related-to", "", true, false], [9, 9, 39, 41, "related-to", "", true, false], [9, 9, 44, 46, "related-to", "", true, false], [9, 9, 49, 50, "related-to", "", true, false], [9, 9, 53, 58, "related-to", "", true, false], [9, 9, 60, 60, "related-to", "", true, false], [9, 9, 64, 68, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "sentence": ["Eine", "Reihe", "von", "Gruppen", "und", "Unternehmen", "forschen", "an", "der", "Posensch\u00e4tzung", ",", "darunter", "Gruppen", "der", "Brown", "University", ",", "der", "Carnegie", "Mellon", "University", ",", "des", "MPI", "Saarbr\u00fccken", ",", "der", "Stanford", "University", ",", "der", "University", "of", "California", ",", "San", "Diego", ",", "der", "University", "of", "Toronto", ",", "der", "\u00c9cole", "Centrale", "Paris", ",", "der", "ETH", "Z\u00fcrich", ",", "der", "National", "University", "of", "Sciences", "and", "Technology", "(", "NUST", ")", "und", "der", "University", "of", "California", ",", "Irvine", "."], "sentence-detokenized": "Eine Reihe von Gruppen und Unternehmen forschen an der Posensch\u00e4tzung, darunter Gruppen der Brown University, der Carnegie Mellon University, des MPI Saarbr\u00fccken, der Stanford University, der University of California, San Diego, der University of Toronto, der \u00c9cole Centrale Paris, der ETH Z\u00fcrich, der National University of Sciences and Technology (NUST) und der University of California, Irvine.", "token2charspan": [[0, 4], [5, 10], [11, 14], [15, 22], [23, 26], [27, 38], [39, 47], [48, 50], [51, 54], [55, 69], [69, 70], [71, 79], [80, 87], [88, 91], [92, 97], [98, 108], [108, 109], [110, 113], [114, 122], [123, 129], [130, 140], [140, 141], [142, 145], [146, 149], [150, 161], [161, 162], [163, 166], [167, 175], [176, 186], [186, 187], [188, 191], [192, 202], [203, 205], [206, 216], [216, 217], [218, 221], [222, 227], [227, 228], [229, 232], [233, 243], [244, 246], [247, 254], [254, 255], [256, 259], [260, 265], [266, 274], [275, 280], [280, 281], [282, 285], [286, 289], [290, 296], [296, 297], [298, 301], [302, 310], [311, 321], [322, 324], [325, 333], [334, 337], [338, 348], [349, 350], [350, 354], [354, 355], [356, 359], [360, 363], [364, 374], [375, 377], [378, 388], [388, 389], [390, 396], [396, 397]]}
{"doc_key": "ai-train-24", "ner": [[0, 1, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Sigmoidfunktion", "Kreuzentropieverlust", "wird", "f\u00fcr", "die", "Vorhersage", "von", "K", "unabh\u00e4ngigen", "Wahrscheinlichkeitswerten", "in", "math", "0,1", "/", "math", "verwendet", "."], "sentence-detokenized": "Sigmoidfunktion Kreuzentropieverlust wird f\u00fcr die Vorhersage von K unabh\u00e4ngigen Wahrscheinlichkeitswerten in math 0,1 / math verwendet.", "token2charspan": [[0, 15], [16, 36], [37, 41], [42, 45], [46, 49], [50, 60], [61, 64], [65, 66], [67, 79], [80, 105], [106, 108], [109, 113], [114, 117], [118, 119], [120, 124], [125, 134], [134, 135]]}
{"doc_key": "ai-train-25", "ner": [[3, 3, "misc"], [5, 5, "field"], [7, 7, "field"], [10, 11, "university"], [14, 14, "country"], [17, 17, "misc"], [19, 22, "university"], [24, 24, "country"], [31, 31, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 5, "topic", "", false, false], [3, 3, 7, 7, "topic", "", false, false], [3, 3, 10, 11, "physical", "", true, false], [10, 11, 14, 14, "physical", "", false, false], [17, 17, 19, 22, "physical", "", true, false], [19, 22, 24, 24, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Er", "hatte", "den", "Johann-Bernoulli-Lehrstuhl", "f\u00fcr", "Mathematik", "und", "Informatik", "an", "der", "Universit\u00e4t", "Groningen", "in", "den", "Niederlanden", "und", "den", "Toshiba-Stiftungslehrstuhl", "am", "Tokyo", "Institute", "of", "Technology", "in", "Japan", "inne", ",", "bevor", "er", "Professor", "in", "Cambridge", "wurde", "."], "sentence-detokenized": "Er hatte den Johann-Bernoulli-Lehrstuhl f\u00fcr Mathematik und Informatik an der Universit\u00e4t Groningen in den Niederlanden und den Toshiba-Stiftungslehrstuhl am Tokyo Institute of Technology in Japan inne, bevor er Professor in Cambridge wurde.", "token2charspan": [[0, 2], [3, 8], [9, 12], [13, 39], [40, 43], [44, 54], [55, 58], [59, 69], [70, 72], [73, 76], [77, 88], [89, 98], [99, 101], [102, 105], [106, 118], [119, 122], [123, 126], [127, 153], [154, 156], [157, 162], [163, 172], [173, 175], [176, 186], [187, 189], [190, 195], [196, 200], [200, 201], [202, 207], [208, 210], [211, 220], [221, 223], [224, 233], [234, 239], [239, 240]]}
{"doc_key": "ai-train-26", "ner": [[7, 8, "algorithm"], [17, 19, "algorithm"], [22, 23, "researcher"], [25, 26, "researcher"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[7, 8, 17, 19, "usage", "", true, false], [17, 19, 22, 23, "origin", "", false, false], [17, 19, 25, 26, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Eine", "weitere", "Technik", ",", "die", "insbesondere", "f\u00fcr", "rekurrente", "neuronale", "Netze", "verwendet", "wird", ",", "ist", "das", "LSTM-Netz", "(", "Long", "Short", "Memory", ")", "von", "Sepp", "Hochreiter", "&", "J\u00fcrgen", "Schmidhuber", "aus", "dem", "Jahr", "1997", "."], "sentence-detokenized": "Eine weitere Technik, die insbesondere f\u00fcr rekurrente neuronale Netze verwendet wird, ist das LSTM-Netz (Long Short Memory) von Sepp Hochreiter & J\u00fcrgen Schmidhuber aus dem Jahr 1997.", "token2charspan": [[0, 4], [5, 12], [13, 20], [20, 21], [22, 25], [26, 38], [39, 42], [43, 53], [54, 63], [64, 69], [70, 79], [80, 84], [84, 85], [86, 89], [90, 93], [94, 103], [104, 105], [105, 109], [110, 115], [116, 122], [122, 123], [124, 127], [128, 132], [133, 143], [144, 145], [146, 152], [153, 164], [165, 168], [169, 172], [173, 177], [178, 182], [182, 183]]}
{"doc_key": "ai-train-27", "ner": [[5, 5, "product"], [10, 10, "product"], [28, 28, "product"]], "ner_mapping_to_source": [1, 2, 3], "relations": [[5, 5, 10, 10, "named", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["Die", "Integration", "eines", "C++-Interpreters", "(", "CINT", "bis", "Version", "5.34", ",", "Cling", "ab", "Version", "6", ")", "macht", "dieses", "Paket", "sehr", "vielseitig", ",", "da", "es", "\u00e4hnlich", "wie", "kommerzielle", "Produkte", "wie", "MATLAB", "im", "interaktiven", ",", "skriptgesteuerten", "und", "kompilierten", "Modus", "verwendet", "werden", "kann", "."], "sentence-detokenized": "Die Integration eines C++-Interpreters (CINT bis Version 5.34, Cling ab Version 6) macht dieses Paket sehr vielseitig, da es \u00e4hnlich wie kommerzielle Produkte wie MATLAB im interaktiven, skriptgesteuerten und kompilierten Modus verwendet werden kann.", "token2charspan": [[0, 3], [4, 15], [16, 21], [22, 38], [39, 40], [40, 44], [45, 48], [49, 56], [57, 61], [61, 62], [63, 68], [69, 71], [72, 79], [80, 81], [81, 82], [83, 88], [89, 95], [96, 101], [102, 106], [107, 117], [117, 118], [119, 121], [122, 124], [125, 132], [133, 136], [137, 149], [150, 158], [159, 162], [163, 169], [170, 172], [173, 185], [185, 186], [187, 204], [205, 208], [209, 221], [222, 227], [228, 237], [238, 244], [245, 249], [249, 250]]}
{"doc_key": "ai-train-28", "ner": [[0, 1, "product"], [21, 23, "field"], [26, 28, "task"], [31, 33, "task"], [36, 36, "task"], [39, 40, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 21, 23, "related-to", "", false, false], [26, 28, 21, 23, "part-of", "", false, false], [31, 33, 21, 23, "part-of", "", false, false], [36, 36, 21, 23, "part-of", "", false, false], [39, 40, 21, 23, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["Sprachgesteuerte", "Benutzeroberfl\u00e4chen", ",", "die", "den", "Gespr\u00e4chszustand", "interpretieren", "und", "verwalten", ",", "sind", "eine", "Herausforderung", ",", "da", "es", "schwierig", "ist", ",", "komplexe", "Aufgaben", "der", "nat\u00fcrlichen", "Sprachverarbeitung", "wie", "die", "Aufl\u00f6sung", "von", "Koreferenzen", ",", "die", "Erkennung", "von", "Namen", ",", "die", "Informationsgewinnung", "und", "das", "Dialogmanagement", "zu", "integrieren", "."], "sentence-detokenized": "Sprachgesteuerte Benutzeroberfl\u00e4chen, die den Gespr\u00e4chszustand interpretieren und verwalten, sind eine Herausforderung, da es schwierig ist, komplexe Aufgaben der nat\u00fcrlichen Sprachverarbeitung wie die Aufl\u00f6sung von Koreferenzen, die Erkennung von Namen, die Informationsgewinnung und das Dialogmanagement zu integrieren.", "token2charspan": [[0, 16], [17, 36], [36, 37], [38, 41], [42, 45], [46, 62], [63, 77], [78, 81], [82, 91], [91, 92], [93, 97], [98, 102], [103, 118], [118, 119], [120, 122], [123, 125], [126, 135], [136, 139], [139, 140], [141, 149], [150, 158], [159, 162], [163, 174], [175, 193], [194, 197], [198, 201], [202, 211], [212, 215], [216, 228], [228, 229], [230, 233], [234, 243], [244, 247], [248, 253], [253, 254], [255, 258], [259, 280], [281, 284], [285, 288], [289, 305], [306, 308], [309, 320], [320, 321]]}
{"doc_key": "ai-train-29", "ner": [[6, 7, "algorithm"], [11, 13, "algorithm"], [21, 22, "researcher"], [24, 26, "organisation"], [35, 35, "field"], [37, 38, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[6, 7, 21, 22, "origin", "", false, false], [6, 7, 35, 35, "part-of", "", false, false], [6, 7, 37, 38, "part-of", "", false, false], [11, 13, 21, 22, "origin", "", false, false], [11, 13, 35, 35, "part-of", "", false, false], [11, 13, 37, 38, "part-of", "", false, false], [21, 22, 24, 26, "physical", "", false, false], [21, 22, 24, 26, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Zwischen", "2009", "und", "2012", "haben", "die", "rekurrenten", "neuronalen", "Netze", "und", "die", "tiefen", "vorw\u00e4rtsgerichteten", "neuronalen", "Netze", ",", "die", "in", "der", "Forschungsgruppe", "von", "J\u00fcrgen", "Schmidhuber", "am", "Schweizer", "KI-Labor", "IDSIA", "entwickelt", "wurden", ",", "acht", "internationale", "Wettbewerbe", "im", "Bereich", "Mustererkennung", "und", "maschinelles", "Lernen", "gewonnen", "."], "sentence-detokenized": "Zwischen 2009 und 2012 haben die rekurrenten neuronalen Netze und die tiefen vorw\u00e4rtsgerichteten neuronalen Netze, die in der Forschungsgruppe von J\u00fcrgen Schmidhuber am Schweizer KI-Labor IDSIA entwickelt wurden, acht internationale Wettbewerbe im Bereich Mustererkennung und maschinelles Lernen gewonnen.", "token2charspan": [[0, 8], [9, 13], [14, 17], [18, 22], [23, 28], [29, 32], [33, 44], [45, 55], [56, 61], [62, 65], [66, 69], [70, 76], [77, 96], [97, 107], [108, 113], [113, 114], [115, 118], [119, 121], [122, 125], [126, 142], [143, 146], [147, 153], [154, 165], [166, 168], [169, 178], [179, 187], [188, 193], [194, 204], [205, 211], [211, 212], [213, 217], [218, 232], [233, 244], [245, 247], [248, 255], [256, 271], [272, 275], [276, 288], [289, 295], [296, 304], [304, 305]]}
{"doc_key": "ai-train-30", "ner": [[1, 1, "product"], [3, 4, "product"], [6, 6, "product"], [11, 11, "task"], [13, 13, "task"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[1, 1, 3, 4, "usage", "", false, false], [1, 1, 6, 6, "usage", "", false, false], [1, 1, 11, 11, "usage", "", true, false], [1, 1, 13, 13, "usage", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Moderne", "Windows-Desktop-Systeme", "k\u00f6nnen", "SAPI", "4-", "und", "SAPI", "5-Komponenten", "zur", "Unterst\u00fctzung", "von", "Sprachsynthese", "und", "Sprache", "verwenden", "."], "sentence-detokenized": "Moderne Windows-Desktop-Systeme k\u00f6nnen SAPI 4- und SAPI 5-Komponenten zur Unterst\u00fctzung von Sprachsynthese und Sprache verwenden.", "token2charspan": [[0, 7], [8, 31], [32, 38], [39, 43], [44, 46], [47, 50], [51, 55], [56, 69], [70, 73], [74, 87], [88, 91], [92, 106], [107, 110], [111, 118], [119, 128], [128, 129]]}
{"doc_key": "ai-train-31", "ner": [[6, 11, "misc"], [13, 13, "field"], [16, 17, "university"], [25, 27, "field"], [30, 33, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 11, 13, 13, "topic", "topic_of_award", false, false], [6, 11, 16, 17, "origin", "", true, false], [25, 27, 30, 33, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Er", "erhielt", "zwei", "Ehrentitel", ",", "einen", "S.", "V.", "della", "laurea", "ad", "honorem", "in", "Psychologie", "von", "der", "Universit\u00e4t", "Padua", "im", "Jahr", "1995", "und", "einen", "Doktortitel", "in", "Industriedesign", "und", "Ingenieurwesen", "von", "der", "Delft", "University", "of", "Technology", "."], "sentence-detokenized": "Er erhielt zwei Ehrentitel, einen S. V. della laurea ad honorem in Psychologie von der Universit\u00e4t Padua im Jahr 1995 und einen Doktortitel in Industriedesign und Ingenieurwesen von der Delft University of Technology.", "token2charspan": [[0, 2], [3, 10], [11, 15], [16, 26], [26, 27], [28, 33], [34, 36], [37, 39], [40, 45], [46, 52], [53, 55], [56, 63], [64, 66], [67, 78], [79, 82], [83, 86], [87, 98], [99, 104], [105, 107], [108, 112], [113, 117], [118, 121], [122, 127], [128, 139], [140, 142], [143, 158], [159, 162], [163, 177], [178, 181], [182, 185], [186, 191], [192, 202], [203, 205], [206, 216], [216, 217]]}
{"doc_key": "ai-train-32", "ner": [[5, 6, "researcher"], [12, 13, "organisation"], [11, 11, "location"], [16, 16, "researcher"], [25, 25, "misc"], [44, 45, "misc"], [68, 69, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[5, 6, 12, 13, "physical", "", false, false], [5, 6, 12, 13, "role", "", false, false], [12, 13, 11, 11, "physical", "", false, false], [16, 16, 25, 25, "related-to", "works_with", true, false], [16, 16, 44, 45, "related-to", "works_with", true, false], [16, 16, 68, 69, "related-to", "works_with", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Gemeinsam", "mit", "seinem", "langj\u00e4hrigen", "Mitarbeiter", "Laurent", "Cohen", ",", "einem", "Neurologen", "am", "Pariser", "Krankenhaus", "Piti\u00e9-Salp\u00eatri\u00e8re", ",", "identifizierte", "Dehaene", "auch", "Patienten", "mit", "L\u00e4sionen", "in", "verschiedenen", "Regionen", "des", "Scheitellappens", ",", "bei", "denen", "die", "Multiplikation", "beeintr\u00e4chtigt", ",", "die", "Subtraktion", "jedoch", "erhalten", "war", "(", "in", "Verbindung", "mit", "L\u00e4sionen", "des", "unteren", "Scheitellappens", ")", ",", "und", "andere", ",", "bei", "denen", "die", "Subtraktion", "beeintr\u00e4chtigt", ",", "die", "Multiplikation", "jedoch", "erhalten", "war", "(", "in", "Verbindung", "mit", "L\u00e4sionen", "im", "intraparietalen", "Sulcus", ")", "."], "sentence-detokenized": "Gemeinsam mit seinem langj\u00e4hrigen Mitarbeiter Laurent Cohen, einem Neurologen am Pariser Krankenhaus Piti\u00e9-Salp\u00eatri\u00e8re, identifizierte Dehaene auch Patienten mit L\u00e4sionen in verschiedenen Regionen des Scheitellappens, bei denen die Multiplikation beeintr\u00e4chtigt, die Subtraktion jedoch erhalten war (in Verbindung mit L\u00e4sionen des unteren Scheitellappens), und andere, bei denen die Subtraktion beeintr\u00e4chtigt, die Multiplikation jedoch erhalten war (in Verbindung mit L\u00e4sionen im intraparietalen Sulcus).", "token2charspan": [[0, 9], [10, 13], [14, 20], [21, 33], [34, 45], [46, 53], [54, 59], [59, 60], [61, 66], [67, 77], [78, 80], [81, 88], [89, 100], [101, 118], [118, 119], [120, 134], [135, 142], [143, 147], [148, 157], [158, 161], [162, 170], [171, 173], [174, 187], [188, 196], [197, 200], [201, 216], [216, 217], [218, 221], [222, 227], [228, 231], [232, 246], [247, 261], [261, 262], [263, 266], [267, 278], [279, 285], [286, 294], [295, 298], [299, 300], [300, 302], [303, 313], [314, 317], [318, 326], [327, 330], [331, 338], [339, 354], [354, 355], [355, 356], [357, 360], [361, 367], [367, 368], [369, 372], [373, 378], [379, 382], [383, 394], [395, 409], [409, 410], [411, 414], [415, 429], [430, 436], [437, 445], [446, 449], [450, 451], [451, 453], [454, 464], [465, 468], [469, 477], [478, 480], [481, 496], [497, 503], [503, 504], [504, 505]]}
{"doc_key": "ai-train-33", "ner": [[7, 9, "product"], [13, 15, "misc"], [17, 18, "misc"], [24, 24, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[13, 15, 7, 9, "topic", "", false, false], [17, 18, 7, 9, "topic", "", false, false], [24, 24, 7, 9, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["In", "j\u00fcngerer", "Zeit", "haben", "fiktionale", "Darstellungen", "von", "k\u00fcnstlich", "intelligenten", "Robotern", "in", "Filmen", "wie", "A.I.", "Artificial", "Intelligence", "und", "Ex", "Machina", "sowie", "in", "der", "Fernsehadaption", "von", "Westworld", "(", "2016", ")", "die", "Sympathie", "der", "Zuschauer", "f\u00fcr", "die", "Roboter", "selbst", "geweckt", "."], "sentence-detokenized": "In j\u00fcngerer Zeit haben fiktionale Darstellungen von k\u00fcnstlich intelligenten Robotern in Filmen wie A.I. Artificial Intelligence und Ex Machina sowie in der Fernsehadaption von Westworld (2016) die Sympathie der Zuschauer f\u00fcr die Roboter selbst geweckt.", "token2charspan": [[0, 2], [3, 11], [12, 16], [17, 22], [23, 33], [34, 47], [48, 51], [52, 61], [62, 75], [76, 84], [85, 87], [88, 94], [95, 98], [99, 103], [104, 114], [115, 127], [128, 131], [132, 134], [135, 142], [143, 148], [149, 151], [152, 155], [156, 171], [172, 175], [176, 185], [186, 187], [187, 191], [191, 192], [193, 196], [197, 206], [207, 210], [211, 220], [221, 224], [225, 228], [229, 236], [237, 243], [244, 251], [251, 252]]}
{"doc_key": "ai-train-34", "ner": [[7, 8, "field"], [14, 14, "algorithm"], [17, 17, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[14, 14, 7, 8, "part-of", "", false, false], [17, 17, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Zwei", "der", "wichtigsten", "Methoden", ",", "die", "beim", "un\u00fcberwachten", "Lernen", "eingesetzt", "werden", ",", "sind", "die", "Hauptkomponentenanalyse", "und", "die", "Clusteranalyse", "."], "sentence-detokenized": "Zwei der wichtigsten Methoden, die beim un\u00fcberwachten Lernen eingesetzt werden, sind die Hauptkomponentenanalyse und die Clusteranalyse.", "token2charspan": [[0, 4], [5, 8], [9, 20], [21, 29], [29, 30], [31, 34], [35, 39], [40, 53], [54, 60], [61, 71], [72, 78], [78, 79], [80, 84], [85, 88], [89, 112], [113, 116], [117, 120], [121, 135], [135, 136]]}
{"doc_key": "ai-train-35", "ner": [[0, 3, "organisation"], [19, 20, "misc"], [25, 26, "misc"], [28, 30, "person"], [35, 36, "person"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[19, 20, 0, 3, "artifact", "", false, false], [25, 26, 0, 3, "artifact", "", false, false], [25, 26, 28, 30, "role", "director_of", false, false], [25, 26, 35, 36, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Die", "Walt", "Disney", "Company", "begann", "auch", ",", "3D-Filme", "an", "besonderen", "Orten", "einzusetzen", ",", "um", "das", "Publikum", "zu", "beeindrucken", ".", "Magic", "Journeys", "(", "1982", ")", "und", "Captain", "EO", "(", "Francis", "Ford", "Coppola", ",", "1986", ",", "mit", "Michael", "Jackson", "in", "der", "Hauptrolle", ")", "sind", "bemerkenswerte", "Beispiele", "."], "sentence-detokenized": "Die Walt Disney Company begann auch, 3D-Filme an besonderen Orten einzusetzen, um das Publikum zu beeindrucken. Magic Journeys (1982) und Captain EO (Francis Ford Coppola, 1986, mit Michael Jackson in der Hauptrolle) sind bemerkenswerte Beispiele.", "token2charspan": [[0, 3], [4, 8], [9, 15], [16, 23], [24, 30], [31, 35], [35, 36], [37, 45], [46, 48], [49, 59], [60, 65], [66, 77], [77, 78], [79, 81], [82, 85], [86, 94], [95, 97], [98, 110], [110, 111], [112, 117], [118, 126], [127, 128], [128, 132], [132, 133], [134, 137], [138, 145], [146, 148], [149, 150], [150, 157], [158, 162], [163, 170], [170, 171], [172, 176], [176, 177], [178, 181], [182, 189], [190, 197], [198, 200], [201, 204], [205, 215], [215, 216], [217, 221], [222, 236], [237, 246], [246, 247]]}
{"doc_key": "ai-train-36", "ner": [[8, 10, "field"], [14, 14, "task"], [16, 17, "task"], [21, 21, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[14, 14, 8, 10, "part-of", "", false, false], [16, 17, 8, 10, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Seit", "2002", "ist", "das", "Perceptron-Training", "im", "Bereich", "der", "Verarbeitung", "nat\u00fcrlicher", "Sprache", "f\u00fcr", "Aufgaben", "wie", "Part-of-Speech-Tagging", "und", "syntaktisches", "Parsing", "popul\u00e4r", "geworden", "(", "Collins", ",", "2002", ")", "."], "sentence-detokenized": "Seit 2002 ist das Perceptron-Training im Bereich der Verarbeitung nat\u00fcrlicher Sprache f\u00fcr Aufgaben wie Part-of-Speech-Tagging und syntaktisches Parsing popul\u00e4r geworden (Collins, 2002).", "token2charspan": [[0, 4], [5, 9], [10, 13], [14, 17], [18, 37], [38, 40], [41, 48], [49, 52], [53, 65], [66, 77], [78, 85], [86, 89], [90, 98], [99, 102], [103, 125], [126, 129], [130, 143], [144, 151], [152, 159], [160, 168], [169, 170], [170, 177], [177, 178], [179, 183], [183, 184], [184, 185]]}
{"doc_key": "ai-train-37", "ner": [[2, 2, "product"], [7, 13, "organisation"], [14, 18, "product"], [22, 23, "researcher"], [30, 30, "organisation"]], "ner_mapping_to_source": [0, 1, 4, 5, 6], "relations": [[7, 13, 2, 2, "role", "introduces_to_market", true, false], [14, 18, 30, 30, "related-to", "sold_to", true, false], [22, 23, 14, 18, "origin", "", false, false]], "relations_mapping_to_source": [0, 3, 4], "sentence": ["Der", "erste", "Palettierroboter", "wurde", "1963", "von", "der", "Fuji", "Yusoki", "Kogyo", "Company", "vorgestellt", ".", "Die", "programmierbare", "Universalmaschine", "f\u00fcr", "die", "Montage", "wurde", "1976", "von", "Victor", "Scheinman", "erfunden", "und", "das", "Design", "wurde", "an", "Unimation", "verkauft", "."], "sentence-detokenized": "Der erste Palettierroboter wurde 1963 von der Fuji Yusoki Kogyo Company vorgestellt. Die programmierbare Universalmaschine f\u00fcr die Montage wurde 1976 von Victor Scheinman erfunden und das Design wurde an Unimation verkauft.", "token2charspan": [[0, 3], [4, 9], [10, 26], [27, 32], [33, 37], [38, 41], [42, 45], [46, 50], [51, 57], [58, 63], [64, 71], [72, 83], [83, 84], [85, 88], [89, 104], [105, 122], [123, 126], [127, 130], [131, 138], [139, 144], [145, 149], [150, 153], [154, 160], [161, 170], [171, 179], [180, 183], [184, 187], [188, 194], [195, 200], [201, 203], [204, 213], [214, 222], [222, 223]]}
{"doc_key": "ai-train-38", "ner": [[11, 11, "conference"], [14, 14, "researcher"], [23, 23, "field"], [38, 39, "researcher"], [43, 44, "researcher"], [58, 58, "field"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[14, 14, 11, 11, "role", "president_of", false, false], [14, 14, 38, 39, "role", "colleagues", false, false], [23, 23, 58, 58, "named", "same", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Mitte", "der", "1990er", "Jahre", ",", "w\u00e4hrend", "seiner", "Amtszeit", "als", "Pr\u00e4sident", "der", "AAAI", ",", "begann", "Hayes", "mit", "einer", "Reihe", "von", "Angriffen", "auf", "Kritiker", "der", "KI", ",", "die", "meist", "ironisch", "formuliert", "waren", ",", "und", "erfand", "(", "zusammen", "mit", "seinem", "Kollegen", "Kenneth", "Ford", ")", "einen", "nach", "Simon", "Newcomb", "benannten", "Preis", ",", "der", "f\u00fcr", "das", "l\u00e4cherlichste", "Argument", "zur", "Widerlegung", "der", "M\u00f6glichkeit", "von", "KI", "verliehen", "werden", "sollte", "."], "sentence-detokenized": "Mitte der 1990er Jahre, w\u00e4hrend seiner Amtszeit als Pr\u00e4sident der AAAI, begann Hayes mit einer Reihe von Angriffen auf Kritiker der KI, die meist ironisch formuliert waren, und erfand (zusammen mit seinem Kollegen Kenneth Ford) einen nach Simon Newcomb benannten Preis, der f\u00fcr das l\u00e4cherlichste Argument zur Widerlegung der M\u00f6glichkeit von KI verliehen werden sollte.", "token2charspan": [[0, 5], [6, 9], [10, 16], [17, 22], [22, 23], [24, 31], [32, 38], [39, 47], [48, 51], [52, 61], [62, 65], [66, 70], [70, 71], [72, 78], [79, 84], [85, 88], [89, 94], [95, 100], [101, 104], [105, 114], [115, 118], [119, 127], [128, 131], [132, 134], [134, 135], [136, 139], [140, 145], [146, 154], [155, 165], [166, 171], [171, 172], [173, 176], [177, 183], [184, 185], [185, 193], [194, 197], [198, 204], [205, 213], [214, 221], [222, 226], [226, 227], [228, 233], [234, 238], [239, 244], [245, 252], [253, 262], [263, 268], [268, 269], [270, 273], [274, 277], [278, 281], [282, 295], [296, 304], [305, 308], [309, 320], [321, 324], [325, 336], [337, 340], [341, 343], [344, 353], [354, 360], [361, 367], [367, 368]]}
{"doc_key": "ai-train-39", "ner": [[12, 12, "algorithm"], [43, 43, "algorithm"], [53, 53, "algorithm"], [56, 56, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 12, 43, 43, "named", "same", false, false], [53, 53, 12, 12, "type-of", "", false, false], [56, 56, 12, 12, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Ein", "optimaler", "Wert", "f\u00fcr", "math\\", "alpha", "/", "math", "kann", "mit", "Hilfe", "eines", "Zeilensuchalgorithmus", "gefunden", "werden", ",", "d.", "h.", "die", "Gr\u00f6\u00dfe", "von", "math\\", "alpha", "/", "math", "wird", "bestimmt", ",", "indem", "der", "Wert", "gefunden", "wird", ",", "der", "S", "minimiert", ",", "wobei", "in", "der", "Regel", "eine", "Zeilensuche", "im", "Intervall", "math0\\", "alpha", "1", "/", "math", "oder", "eine", "Backtracking-Zeilensuche", "wie", "die", "Armijo-Zeilensuche", "verwendet", "wird", "."], "sentence-detokenized": "Ein optimaler Wert f\u00fcr math\\ alpha / math kann mit Hilfe eines Zeilensuchalgorithmus gefunden werden, d. h. die Gr\u00f6\u00dfe von math\\ alpha / math wird bestimmt, indem der Wert gefunden wird, der S minimiert, wobei in der Regel eine Zeilensuche im Intervall math0\\ alpha 1 / math oder eine Backtracking-Zeilensuche wie die Armijo-Zeilensuche verwendet wird.", "token2charspan": [[0, 3], [4, 13], [14, 18], [19, 22], [23, 28], [29, 34], [35, 36], [37, 41], [42, 46], [47, 50], [51, 56], [57, 62], [63, 84], [85, 93], [94, 100], [100, 101], [102, 104], [105, 107], [108, 111], [112, 117], [118, 121], [122, 127], [128, 133], [134, 135], [136, 140], [141, 145], [146, 154], [154, 155], [156, 161], [162, 165], [166, 170], [171, 179], [180, 184], [184, 185], [186, 189], [190, 191], [192, 201], [201, 202], [203, 208], [209, 211], [212, 215], [216, 221], [222, 226], [227, 238], [239, 241], [242, 251], [252, 258], [259, 264], [265, 266], [267, 268], [269, 273], [274, 278], [279, 283], [284, 308], [309, 312], [313, 316], [317, 335], [336, 345], [346, 350], [350, 351]]}
{"doc_key": "ai-train-40", "ner": [], "ner_mapping_to_source": [], "relations": [], "relations_mapping_to_source": [], "sentence": ["Er", "er\u00f6rtert", "Breadth-first-", "und", "Depth-first-Suchtechniken", ",", "kommt", "aber", "schlie\u00dflich", "zu", "dem", "Schluss", ",", "dass", "die", "Ergebnisse", "Expertensysteme", "darstellen", ",", "die", "viel", "technisches", "Wissen", "verk\u00f6rpern", ",", "aber", "nicht", "viel", "Licht", "auf", "die", "mentalen", "Prozesse", "werfen", ",", "die", "Menschen", "zur", "L\u00f6sung", "solcher", "R\u00e4tsel", "verwenden", "."], "sentence-detokenized": "Er er\u00f6rtert Breadth-first- und Depth-first-Suchtechniken, kommt aber schlie\u00dflich zu dem Schluss, dass die Ergebnisse Expertensysteme darstellen, die viel technisches Wissen verk\u00f6rpern, aber nicht viel Licht auf die mentalen Prozesse werfen, die Menschen zur L\u00f6sung solcher R\u00e4tsel verwenden.", "token2charspan": [[0, 2], [3, 11], [12, 26], [27, 30], [31, 56], [56, 57], [58, 63], [64, 68], [69, 80], [81, 83], [84, 87], [88, 95], [95, 96], [97, 101], [102, 105], [106, 116], [117, 132], [133, 143], [143, 144], [145, 148], [149, 153], [154, 165], [166, 172], [173, 183], [183, 184], [185, 189], [190, 195], [196, 200], [201, 206], [207, 210], [211, 214], [215, 223], [224, 232], [233, 239], [239, 240], [241, 244], [245, 253], [254, 257], [258, 264], [265, 272], [273, 279], [280, 289], [289, 290]]}
{"doc_key": "ai-train-41", "ner": [[0, 0, "task"], [2, 2, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Spracherkennung", "und", "Sprachsynthese", "befassen", "sich", "damit", ",", "wie", "gesprochene", "Sprache", "mit", "Hilfe", "von", "Computern", "verstanden", "oder", "erzeugt", "werden", "kann", "."], "sentence-detokenized": "Spracherkennung und Sprachsynthese befassen sich damit, wie gesprochene Sprache mit Hilfe von Computern verstanden oder erzeugt werden kann.", "token2charspan": [[0, 15], [16, 19], [20, 34], [35, 43], [44, 48], [49, 54], [54, 55], [56, 59], [60, 71], [72, 79], [80, 83], [84, 89], [90, 93], [94, 103], [104, 114], [115, 119], [120, 127], [128, 134], [135, 139], [139, 140]]}
{"doc_key": "ai-train-42", "ner": [[13, 13, "algorithm"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Dieses", "math\\", "theta", "^", "{", "*", "}", "/", "math", "wird", "normalerweise", "mit", "einem", "Maximum-Likelihood-", "(", "math\\", "theta", "^", "{", "*", "}", "=\\", "theta", "^", "{", "ML", "}", "/", "math", ")", "oder", "Maximum-A-Posteriori-Verfahren", "(", "math\\", "theta", "^", "{", "*", "}", "=\\", "theta", "^", "{", "MAP", "}", "/", "math", ")", "gesch\u00e4tzt", "."], "sentence-detokenized": "Dieses math\\ theta ^ {*} / math wird normalerweise mit einem Maximum-Likelihood- (math\\ theta ^ {*} =\\ theta ^ {ML} / math) oder Maximum-A-Posteriori-Verfahren (math\\ theta ^ {*} =\\ theta ^ {MAP} / math) gesch\u00e4tzt.", "token2charspan": [[0, 6], [7, 12], [13, 18], [19, 20], [21, 22], [22, 23], [23, 24], [25, 26], [27, 31], [32, 36], [37, 50], [51, 54], [55, 60], [61, 80], [81, 82], [82, 87], [88, 93], [94, 95], [96, 97], [97, 98], [98, 99], [100, 102], [103, 108], [109, 110], [111, 112], [112, 114], [114, 115], [116, 117], [118, 122], [122, 123], [124, 128], [129, 159], [160, 161], [161, 166], [167, 172], [173, 174], [175, 176], [176, 177], [177, 178], [179, 181], [182, 187], [188, 189], [190, 191], [191, 194], [194, 195], [196, 197], [198, 202], [202, 203], [204, 213], [213, 214]]}
{"doc_key": "ai-train-43", "ner": [[6, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Einige", "weniger", "verbreitete", "Sprachen", "verwenden", "den", "Open-Source-Synthesizer", "eSpeak", "f\u00fcr", "ihre", "Sprache", ",", "der", "eine", "roboterhafte", ",", "unbeholfene", "Stimme", "erzeugt", ",", "die", "schwer", "zu", "verstehen", "sein", "kann", "."], "sentence-detokenized": "Einige weniger verbreitete Sprachen verwenden den Open-Source-Synthesizer eSpeak f\u00fcr ihre Sprache, der eine roboterhafte, unbeholfene Stimme erzeugt, die schwer zu verstehen sein kann.", "token2charspan": [[0, 6], [7, 14], [15, 26], [27, 35], [36, 45], [46, 49], [50, 73], [74, 80], [81, 84], [85, 89], [90, 97], [97, 98], [99, 102], [103, 107], [108, 120], [120, 121], [122, 133], [134, 140], [141, 148], [148, 149], [150, 153], [154, 160], [161, 163], [164, 173], [174, 178], [179, 183], [183, 184]]}
{"doc_key": "ai-train-44", "ner": [[1, 1, "programlang"], [35, 36, "programlang"], [38, 38, "product"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[1, 1, 35, 36, "compare", "", false, false], [1, 1, 38, 38, "compare", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Obwohl", "R", "haupts\u00e4chlich", "von", "Statistikern", "und", "anderen", "Praktikern", "verwendet", "wird", ",", "die", "eine", "Umgebung", "f\u00fcr", "statistische", "Berechnungen", "und", "Softwareentwicklung", "ben\u00f6tigen", ",", "kann", "es", "auch", "als", "allgemeiner", "Werkzeugkasten", "f\u00fcr", "Matrixberechnungen", "eingesetzt", "werden", "-", "mit", "Leistungsvergleichen", "zu", "GNU", "Octave", "oder", "MATLAB", "."], "sentence-detokenized": "Obwohl R haupts\u00e4chlich von Statistikern und anderen Praktikern verwendet wird, die eine Umgebung f\u00fcr statistische Berechnungen und Softwareentwicklung ben\u00f6tigen, kann es auch als allgemeiner Werkzeugkasten f\u00fcr Matrixberechnungen eingesetzt werden - mit Leistungsvergleichen zu GNU Octave oder MATLAB.", "token2charspan": [[0, 6], [7, 8], [9, 22], [23, 26], [27, 39], [40, 43], [44, 51], [52, 62], [63, 72], [73, 77], [77, 78], [79, 82], [83, 87], [88, 96], [97, 100], [101, 113], [114, 126], [127, 130], [131, 150], [151, 160], [160, 161], [162, 166], [167, 169], [170, 174], [175, 178], [179, 190], [191, 205], [206, 209], [210, 228], [229, 239], [240, 246], [247, 248], [249, 252], [253, 273], [274, 276], [277, 280], [281, 287], [288, 292], [293, 299], [299, 300]]}
{"doc_key": "ai-train-45", "ner": [[0, 0, "algorithm"], [5, 8, "misc"], [9, 10, "researcher"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 0, 9, 10, "origin", "", false, false], [5, 8, 9, 10, "named", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["Heterodyning", "ist", "ein", "von", "dem", "kanadischen", "Erfinder", "und", "Ingenieur", "Reginald", "Fessenden", "erfundenes", "Signalverarbeitungsverfahren", ",", "bei", "dem", "neue", "Frequenzen", "durch", "die", "Kombination", "zweier", "Frequenzen", "erzeugt", "werden", "."], "sentence-detokenized": "Heterodyning ist ein von dem kanadischen Erfinder und Ingenieur Reginald Fessenden erfundenes Signalverarbeitungsverfahren, bei dem neue Frequenzen durch die Kombination zweier Frequenzen erzeugt werden.", "token2charspan": [[0, 12], [13, 16], [17, 20], [21, 24], [25, 28], [29, 40], [41, 49], [50, 53], [54, 63], [64, 72], [73, 82], [83, 93], [94, 122], [122, 123], [124, 127], [128, 131], [132, 136], [137, 147], [148, 153], [154, 157], [158, 169], [170, 176], [177, 187], [188, 195], [196, 202], [202, 203]]}
{"doc_key": "ai-train-46", "ner": [[21, 21, "person"], [23, 23, "misc"], [27, 28, "organisation"], [32, 34, "misc"], [36, 37, "person"], [40, 42, "misc"], [44, 45, "person"], [47, 48, "person"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 7, 8, 9], "relations": [[21, 21, 23, 23, "role", "actor_in", false, false], [23, 23, 27, 28, "artifact", "", false, false], [36, 37, 32, 34, "role", "actor_in", false, false], [44, 45, 40, 42, "role", "actor_in", false, false], [47, 48, 40, 42, "role", "actor_in", false, false]], "relations_mapping_to_source": [0, 1, 3, 5, 6], "sentence": ["Einige", "andere", "Filme", ",", "die", "dazu", "beitrugen", ",", "dass", "3D", "in", "diesem", "Monat", "wieder", "auf", "der", "Karte", "erschien", ",", "waren", "der", "John", "Wayne-Film", "Hondo", "(", "vertrieben", "von", "Warner", "Bros.", ")", ",", "Columbias", "Miss", "Sadie", "Thompson", "mit", "Rita", "Hayworth", "und", "Paramounts", "Money", "From", "Home", "mit", "Dean", "Martin", "und", "Jerry", "Lewis", "."], "sentence-detokenized": "Einige andere Filme, die dazu beitrugen, dass 3D in diesem Monat wieder auf der Karte erschien, waren der John Wayne-Film Hondo (vertrieben von Warner Bros.), Columbias Miss Sadie Thompson mit Rita Hayworth und Paramounts Money From Home mit Dean Martin und Jerry Lewis.", "token2charspan": [[0, 6], [7, 13], [14, 19], [19, 20], [21, 24], [25, 29], [30, 39], [39, 40], [41, 45], [46, 48], [49, 51], [52, 58], [59, 64], [65, 71], [72, 75], [76, 79], [80, 85], [86, 94], [94, 95], [96, 101], [102, 105], [106, 110], [111, 121], [122, 127], [128, 129], [129, 139], [140, 143], [144, 150], [151, 156], [156, 157], [157, 158], [159, 168], [169, 173], [174, 179], [180, 188], [189, 192], [193, 197], [198, 206], [207, 210], [211, 221], [222, 227], [228, 232], [233, 237], [238, 241], [242, 246], [247, 253], [254, 257], [258, 263], [264, 269], [269, 270]]}
{"doc_key": "ai-train-47", "ner": [[0, 0, "product"], [10, 10, "organisation"]], "ner_mapping_to_source": [0, 3], "relations": [[0, 0, 10, 10, "artifact", "", false, false]], "relations_mapping_to_source": [1], "sentence": ["DeepFace", "ist", "ein", "Deep-Learning-Gesichtserkennungssystem", ",", "das", "von", "einer", "Forschungsgruppe", "bei", "Facebook", "entwickelt", "wurde", "."], "sentence-detokenized": "DeepFace ist ein Deep-Learning-Gesichtserkennungssystem, das von einer Forschungsgruppe bei Facebook entwickelt wurde.", "token2charspan": [[0, 8], [9, 12], [13, 16], [17, 55], [55, 56], [57, 60], [61, 64], [65, 70], [71, 87], [88, 91], [92, 100], [101, 111], [112, 117], [117, 118]]}
{"doc_key": "ai-train-48", "ner": [[0, 0, "field"], [7, 7, "conference"], [14, 14, "field"], [21, 23, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[0, 0, 14, 14, "part-of", "subfield", false, false], [7, 7, 0, 0, "topic", "", false, false], [21, 23, 0, 0, "topic", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Geometrieverarbeitung", "ist", "ein", "h\u00e4ufiges", "Forschungsthema", "auf", "der", "SIGGRAPH", ",", "der", "f\u00fchrenden", "akademischen", "Konferenz", "f\u00fcr", "Computergrafik", ",", "und", "das", "Hauptthema", "des", "j\u00e4hrlichen", "Symposiums", "f\u00fcr", "Geometrieverarbeitung", "."], "sentence-detokenized": "Geometrieverarbeitung ist ein h\u00e4ufiges Forschungsthema auf der SIGGRAPH, der f\u00fchrenden akademischen Konferenz f\u00fcr Computergrafik, und das Hauptthema des j\u00e4hrlichen Symposiums f\u00fcr Geometrieverarbeitung.", "token2charspan": [[0, 21], [22, 25], [26, 29], [30, 38], [39, 54], [55, 58], [59, 62], [63, 71], [71, 72], [73, 76], [77, 86], [87, 99], [100, 109], [110, 113], [114, 128], [128, 129], [130, 133], [134, 137], [138, 148], [149, 152], [153, 163], [164, 174], [175, 178], [179, 200], [200, 201]]}
{"doc_key": "ai-train-49", "ner": [[0, 0, "task"], [2, 2, "task"], [12, 12, "algorithm"], [14, 14, "algorithm"], [18, 19, "algorithm"], [21, 21, "algorithm"], [25, 26, "algorithm"], [28, 28, "algorithm"], [31, 31, "misc"], [40, 42, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[12, 12, 31, 31, "general-affiliation", "", false, false], [14, 14, 12, 12, "named", "", false, false], [18, 19, 31, 31, "general-affiliation", "", false, false], [21, 21, 18, 19, "named", "", false, false], [25, 26, 31, 31, "general-affiliation", "", false, false], [28, 28, 25, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Merkmalsextraktion", "und", "Dimensionsreduktion", "k\u00f6nnen", "in", "einem", "Schritt", "kombiniert", "werden", ",", "indem", "die", "Hauptkomponentenanalyse", "(", "PCA", ")", ",", "die", "lineare", "Diskriminanzanalyse", "(", "LDA", ")", "oder", "die", "kanonische", "Korrelationsanalyse", "(", "CCA", ")", "als", "Vorverarbeitungsschritt", "eingesetzt", "werden", ",", "gefolgt", "von", "einer", "Clusterbildung", "durch", "k", "-", "NN", "auf", "den", "Merkmalsvektoren", "im", "dimensionsreduzierten", "Raum", "."], "sentence-detokenized": "Merkmalsextraktion und Dimensionsreduktion k\u00f6nnen in einem Schritt kombiniert werden, indem die Hauptkomponentenanalyse (PCA), die lineare Diskriminanzanalyse (LDA) oder die kanonische Korrelationsanalyse (CCA) als Vorverarbeitungsschritt eingesetzt werden, gefolgt von einer Clusterbildung durch k -NN auf den Merkmalsvektoren im dimensionsreduzierten Raum.", "token2charspan": [[0, 18], [19, 22], [23, 42], [43, 49], [50, 52], [53, 58], [59, 66], [67, 77], [78, 84], [84, 85], [86, 91], [92, 95], [96, 119], [120, 121], [121, 124], [124, 125], [125, 126], [127, 130], [131, 138], [139, 158], [159, 160], [160, 163], [163, 164], [165, 169], [170, 173], [174, 184], [185, 204], [205, 206], [206, 209], [209, 210], [211, 214], [215, 238], [239, 249], [250, 256], [256, 257], [258, 265], [266, 269], [270, 275], [276, 290], [291, 296], [297, 298], [299, 300], [300, 302], [303, 306], [307, 310], [311, 327], [328, 330], [331, 352], [353, 357], [357, 358]]}
{"doc_key": "ai-train-50", "ner": [[0, 2, "algorithm"], [9, 10, "field"], [12, 12, "field"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 2, 9, 10, "related-to", "good_at", true, false], [0, 2, 12, 12, "related-to", "good_at", true, false]], "relations_mapping_to_source": [0, 1], "sentence": ["K\u00fcnstliche", "neuronale", "Netze", "sind", "Berechnungsmodelle", ",", "die", "sich", "durch", "maschinelles", "Lernen", "und", "Mustererkennung", "auszeichnen", "."], "sentence-detokenized": "K\u00fcnstliche neuronale Netze sind Berechnungsmodelle, die sich durch maschinelles Lernen und Mustererkennung auszeichnen.", "token2charspan": [[0, 10], [11, 20], [21, 26], [27, 31], [32, 50], [50, 51], [52, 55], [56, 60], [61, 66], [67, 79], [80, 86], [87, 90], [91, 106], [107, 118], [118, 119]]}
{"doc_key": "ai-train-51", "ner": [[0, 1, "researcher"], [3, 4, "researcher"], [6, 10, "misc"], [12, 16, "conference"], [18, 18, "conference"], [32, 35, "algorithm"], [36, 37, "researcher"], [39, 40, "researcher"], [42, 48, "misc"], [50, 59, "conference"], [61, 61, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "relations": [[6, 10, 0, 1, "artifact", "", false, false], [6, 10, 3, 4, "artifact", "", false, false], [6, 10, 12, 16, "temporal", "", false, false], [18, 18, 12, 16, "named", "", false, false], [42, 48, 32, 35, "topic", "", false, false], [42, 48, 36, 37, "artifact", "", false, false], [42, 48, 39, 40, "artifact", "", false, false], [42, 48, 50, 59, "temporal", "", false, false], [61, 61, 50, 59, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["C.", "Papageorgiou", "und", "T.", "Poggio", ",", "A", "Trainable", "Pedestrian", "Detection", "system", ",", "International", "Journal", "of", "Computer", "Vision", "(", "IJCV", ")", ",", "Seiten", "1", ":", "15-33", ",", "2000", "andere", "verwenden", "lokale", "Merkmale", "wie", "Histogram", "of", "oriented", "gradients", "N.", "Dalal", ",", "B.", "Triggs", ",", "Histograms", "of", "oriented", "gradients", "for", "human", "detection", ",", "IEEE", "Computer", "Society", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", ",", "Seiten", "1", ":", "886-893", ",", "2005", "descriptors", "."], "sentence-detokenized": "C. Papageorgiou und T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), Seiten 1: 15-33, 2000 andere verwenden lokale Merkmale wie Histogram of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Seiten 1: 886-893, 2005 descriptors.", "token2charspan": [[0, 2], [3, 15], [16, 19], [20, 22], [23, 29], [29, 30], [31, 32], [33, 42], [43, 53], [54, 63], [64, 70], [70, 71], [72, 85], [86, 93], [94, 96], [97, 105], [106, 112], [113, 114], [114, 118], [118, 119], [119, 120], [121, 127], [128, 129], [129, 130], [131, 136], [136, 137], [138, 142], [143, 149], [150, 159], [160, 166], [167, 175], [176, 179], [180, 189], [190, 192], [193, 201], [202, 211], [212, 214], [215, 220], [220, 221], [222, 224], [225, 231], [231, 232], [233, 243], [244, 246], [247, 255], [256, 265], [266, 269], [270, 275], [276, 285], [285, 286], [287, 291], [292, 300], [301, 308], [309, 319], [320, 322], [323, 331], [332, 338], [339, 342], [343, 350], [351, 362], [363, 364], [364, 368], [368, 369], [369, 370], [371, 377], [378, 379], [379, 380], [381, 388], [388, 389], [390, 394], [395, 406], [406, 407]]}
{"doc_key": "ai-train-52", "ner": [[1, 1, "algorithm"], [5, 7, "algorithm"], [14, 15, "task"], [12, 12, "field"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[1, 1, 5, 7, "type-of", "", false, false], [14, 15, 1, 1, "usage", "", true, false], [14, 15, 12, 12, "part-of", "task_part_of_field", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Ein", "Autoencoder", "ist", "eine", "Art", "k\u00fcnstliches", "neuronales", "Netz", ",", "das", "f\u00fcr", "das", "un\u00fcberwachte", "Lernen", "von", "Merkmalen", "verwendet", "wird", "."], "sentence-detokenized": "Ein Autoencoder ist eine Art k\u00fcnstliches neuronales Netz, das f\u00fcr das un\u00fcberwachte Lernen von Merkmalen verwendet wird.", "token2charspan": [[0, 3], [4, 15], [16, 19], [20, 24], [25, 28], [29, 40], [41, 51], [52, 56], [56, 57], [58, 61], [62, 65], [66, 69], [70, 82], [83, 89], [90, 93], [94, 103], [104, 113], [114, 118], [118, 119]]}
{"doc_key": "ai-train-53", "ner": [[0, 0, "researcher"], [4, 4, "organisation"], [10, 11, "field"], [13, 13, "field"], [17, 21, "organisation"], [23, 23, "organisation"], [30, 30, "field"], [32, 32, "field"], [39, 39, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[0, 0, 4, 4, "role", "fellow_of", false, false], [0, 0, 10, 11, "related-to", "contributes_to", false, false], [0, 0, 13, 13, "related-to", "contributes_to", false, false], [0, 0, 17, 21, "role", "fellow_of", false, false], [0, 0, 30, 30, "related-to", "contributes_to", false, false], [0, 0, 32, 32, "related-to", "contributes_to", false, false], [23, 23, 17, 21, "named", "", false, false], [39, 39, 17, 21, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Haralick", "ist", "Fellow", "des", "IEEE", "f\u00fcr", "seine", "Beitr\u00e4ge", "im", "Bereich", "Computer", "Vision", "und", "Bildverarbeitung", "und", "Fellow", "der", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", "f\u00fcr", "seine", "Beitr\u00e4ge", "im", "Bereich", "Mustererkennung", "und", "Bildverarbeitung", "sowie", "f\u00fcr", "seine", "Verdienste", "um", "die", "IAPR", "."], "sentence-detokenized": "Haralick ist Fellow des IEEE f\u00fcr seine Beitr\u00e4ge im Bereich Computer Vision und Bildverarbeitung und Fellow der International Association for Pattern Recognition (IAPR) f\u00fcr seine Beitr\u00e4ge im Bereich Mustererkennung und Bildverarbeitung sowie f\u00fcr seine Verdienste um die IAPR.", "token2charspan": [[0, 8], [9, 12], [13, 19], [20, 23], [24, 28], [29, 32], [33, 38], [39, 47], [48, 50], [51, 58], [59, 67], [68, 74], [75, 78], [79, 95], [96, 99], [100, 106], [107, 110], [111, 124], [125, 136], [137, 140], [141, 148], [149, 160], [161, 162], [162, 166], [166, 167], [168, 171], [172, 177], [178, 186], [187, 189], [190, 197], [198, 213], [214, 217], [218, 234], [235, 240], [241, 244], [245, 250], [251, 261], [262, 264], [265, 268], [269, 273], [273, 274]]}
{"doc_key": "ai-train-54", "ner": [[4, 4, "task"], [28, 30, "algorithm"], [32, 32, "algorithm"], [8, 9, "researcher"], [11, 12, "organisation"], [14, 15, "researcher"], [18, 21, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6], "relations": [[4, 4, 28, 30, "usage", "", false, false], [28, 30, 8, 9, "origin", "", true, false], [28, 30, 14, 15, "origin", "", true, false], [32, 32, 28, 30, "named", "", false, false], [8, 9, 11, 12, "physical", "", false, false], [8, 9, 11, 12, "role", "", false, false], [14, 15, 18, 21, "physical", "", false, false], [14, 15, 18, 21, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Der", "erste", "Versuch", "einer", "End-to-End-ASR", "war", "das", "von", "Alex", "Graves", "von", "Google", "DeepMind", "und", "Navdeep", "Jaitly", "von", "der", "University", "of", "Toronto", "2014", "vorgestellte", "System", "auf", "der", "Grundlage", "der", "Connectionist", "Temporal", "Classification", "(", "CTC", ")", "."], "sentence-detokenized": "Der erste Versuch einer End-to-End-ASR war das von Alex Graves von Google DeepMind und Navdeep Jaitly von der University of Toronto 2014 vorgestellte System auf der Grundlage der Connectionist Temporal Classification (CTC).", "token2charspan": [[0, 3], [4, 9], [10, 17], [18, 23], [24, 38], [39, 42], [43, 46], [47, 50], [51, 55], [56, 62], [63, 66], [67, 73], [74, 82], [83, 86], [87, 94], [95, 101], [102, 105], [106, 109], [110, 120], [121, 123], [124, 131], [132, 136], [137, 149], [150, 156], [157, 160], [161, 164], [165, 174], [175, 178], [179, 192], [193, 201], [202, 216], [217, 218], [218, 221], [221, 222], [222, 223]]}
{"doc_key": "ai-train-55", "ner": [[0, 2, "algorithm"], [4, 4, "algorithm"], [10, 11, "algorithm"], [13, 13, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[4, 4, 0, 2, "named", "", false, false], [10, 11, 0, 2, "type-of", "", false, false], [13, 13, 10, 11, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Die", "linear-fraktionelle", "Programmierung", "(", "LFP", ")", "ist", "eine", "Verallgemeinerung", "der", "linearen", "Programmierung", "(", "LP", ")", "."], "sentence-detokenized": "Die linear-fraktionelle Programmierung (LFP) ist eine Verallgemeinerung der linearen Programmierung (LP).", "token2charspan": [[0, 3], [4, 23], [24, 38], [39, 40], [40, 43], [43, 44], [45, 48], [49, 53], [54, 71], [72, 75], [76, 84], [85, 99], [100, 101], [101, 103], [103, 104], [104, 105]]}
{"doc_key": "ai-train-56", "ner": [[0, 0, "researcher"], [7, 7, "misc"], [10, 17, "conference"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[0, 0, 7, 7, "win-defeat", "", false, false], [7, 7, 10, 17, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Lafferty", "erhielt", "zahlreiche", "Auszeichnungen", ",", "darunter", "zwei", "Test-of-Time-Preise", "auf", "der", "International", "Conference", "on", "Machine", "Learning", "2011", "und", "2012", ","], "sentence-detokenized": "Lafferty erhielt zahlreiche Auszeichnungen, darunter zwei Test-of-Time-Preise auf der International Conference on Machine Learning 2011 und 2012,", "token2charspan": [[0, 8], [9, 16], [17, 27], [28, 42], [42, 43], [44, 52], [53, 57], [58, 77], [78, 81], [82, 85], [86, 99], [100, 110], [111, 113], [114, 121], [122, 130], [131, 135], [136, 139], [140, 144], [144, 145]]}
{"doc_key": "ai-train-57", "ner": [[7, 7, "product"], [9, 9, "programlang"], [19, 20, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Mit", "dem", "Aufkommen", "von", "komponentenbasierten", "Frameworks", "wie", ".NET", "und", "Java", "sind", "komponentenbasierte", "Entwicklungsumgebungen", "in", "der", "Lage", ",", "das", "entwickelte", "neuronale", "Netz", "in", "diesen", "Frameworks", "als", "vererbbare", "Komponenten", "einzusetzen", "."], "sentence-detokenized": "Mit dem Aufkommen von komponentenbasierten Frameworks wie .NET und Java sind komponentenbasierte Entwicklungsumgebungen in der Lage, das entwickelte neuronale Netz in diesen Frameworks als vererbbare Komponenten einzusetzen.", "token2charspan": [[0, 3], [4, 7], [8, 17], [18, 21], [22, 42], [43, 53], [54, 57], [58, 62], [63, 66], [67, 71], [72, 76], [77, 96], [97, 119], [120, 122], [123, 126], [127, 131], [131, 132], [133, 136], [137, 148], [149, 158], [159, 163], [164, 166], [167, 173], [174, 184], [185, 188], [189, 199], [200, 211], [212, 223], [223, 224]]}
{"doc_key": "ai-train-58", "ner": [[2, 2, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Wie", "bei", "BLEU", "ist", "die", "Grundeinheit", "der", "Bewertung", "der", "Satz", ".", "Der", "Algorithmus", "erstellt", "zun\u00e4chst", "ein", "Alignment", "(", "siehe", "Abbildungen", ")", "zwischen", "zwei", "S\u00e4tzen", ",", "der", "Kandidaten\u00fcbersetzung", "und", "der", "Referenz\u00fcbersetzung", "."], "sentence-detokenized": "Wie bei BLEU ist die Grundeinheit der Bewertung der Satz. Der Algorithmus erstellt zun\u00e4chst ein Alignment (siehe Abbildungen) zwischen zwei S\u00e4tzen, der Kandidaten\u00fcbersetzung und der Referenz\u00fcbersetzung.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 16], [17, 20], [21, 33], [34, 37], [38, 47], [48, 51], [52, 56], [56, 57], [58, 61], [62, 73], [74, 82], [83, 91], [92, 95], [96, 105], [106, 107], [107, 112], [113, 124], [124, 125], [126, 134], [135, 139], [140, 146], [146, 147], [148, 151], [152, 173], [174, 177], [178, 181], [182, 201], [201, 202]]}
{"doc_key": "ai-train-59", "ner": [[6, 12, "conference"], [23, 23, "task"], [28, 28, "task"], [33, 33, "metrics"], [35, 39, "metrics"], [44, 47, "conference"], [49, 49, "conference"], [52, 52, "location"], [54, 54, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[6, 12, 23, 23, "related-to", "subject_at", false, false], [6, 12, 28, 28, "related-to", "subject_at", false, false], [33, 33, 6, 12, "temporal", "", false, false], [35, 39, 33, 33, "named", "", true, false], [49, 49, 44, 47, "named", "", false, false], [52, 52, 54, 54, "physical", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Eine", "der", "Metriken", ",", "die", "bei", "den", "j\u00e4hrlichen", "Document", "Understanding", "Conferences", "des", "NIST", "verwendet", "werden", ",", "bei", "denen", "Forschungsgruppen", "ihre", "Systeme", "sowohl", "f\u00fcr", "Zusammenfassungs", "-", "als", "auch", "f\u00fcr", "\u00dcbersetzungsaufgaben", "einreichen", ",", "ist", "die", "ROUGE-Metrik", "(", "Recall-Oriented", "Understudy", "for", "Gisting", "Evaluation", ",", "In", "Advances", "of", "Neural", "Information", "Processing", "Systems", "(", "NIPS", ")", ",", "Montreal", ",", "Kanada", ",", "Dezember", "2014", "."], "sentence-detokenized": "Eine der Metriken, die bei den j\u00e4hrlichen Document Understanding Conferences des NIST verwendet werden, bei denen Forschungsgruppen ihre Systeme sowohl f\u00fcr Zusammenfassungs- als auch f\u00fcr \u00dcbersetzungsaufgaben einreichen, ist die ROUGE-Metrik (Recall-Oriented Understudy for Gisting Evaluation, In Advances of Neural Information Processing Systems (NIPS), Montreal, Kanada, Dezember 2014.", "token2charspan": [[0, 4], [5, 8], [9, 17], [17, 18], [19, 22], [23, 26], [27, 30], [31, 41], [42, 50], [51, 64], [65, 76], [77, 80], [81, 85], [86, 95], [96, 102], [102, 103], [104, 107], [108, 113], [114, 131], [132, 136], [137, 144], [145, 151], [152, 155], [156, 172], [172, 173], [174, 177], [178, 182], [183, 186], [187, 207], [208, 218], [218, 219], [220, 223], [224, 227], [228, 240], [241, 242], [242, 257], [258, 268], [269, 272], [273, 280], [281, 291], [291, 292], [293, 295], [296, 304], [305, 307], [308, 314], [315, 326], [327, 337], [338, 345], [346, 347], [347, 351], [351, 352], [352, 353], [354, 362], [362, 363], [364, 370], [370, 371], [372, 380], [381, 385], [385, 386]]}
{"doc_key": "ai-train-60", "ner": [[6, 6, "programlang"], [8, 8, "product"], [11, 12, "programlang"], [15, 15, "product"], [21, 21, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[6, 6, 11, 12, "type-of", "", false, false], [6, 6, 21, 21, "named", "", false, false], [8, 8, 11, 12, "part-of", "", false, false], [8, 8, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Gleiche", "Implementierung", ",", "zur", "Ausf\u00fchrung", "in", "Java", "mit", "JShell", "(", "mindestens", "Java", "9", ")", ":", "codejshell", "scriptfile", "/", "codesyntaxhighlight", "lang", "=", "java"], "sentence-detokenized": "Gleiche Implementierung, zur Ausf\u00fchrung in Java mit JShell (mindestens Java 9): codejshell scriptfile / codesyntaxhighlight lang = java", "token2charspan": [[0, 7], [8, 23], [23, 24], [25, 28], [29, 39], [40, 42], [43, 47], [48, 51], [52, 58], [59, 60], [60, 70], [71, 75], [76, 77], [77, 78], [78, 79], [80, 90], [91, 101], [102, 103], [104, 123], [124, 128], [129, 130], [131, 135]]}
{"doc_key": "ai-train-61", "ner": [[1, 1, "metrics"], [5, 5, "metrics"]], "ner_mapping_to_source": [0, 1], "relations": [[1, 1, 5, 5, "origin", "based_on", false, false]], "relations_mapping_to_source": [0], "sentence": ["Die", "NIST-Methode", "basiert", "auf", "der", "BLEU-Methode", ",", "allerdings", "mit", "einigen", "\u00c4nderungen", "."], "sentence-detokenized": "Die NIST-Methode basiert auf der BLEU-Methode, allerdings mit einigen \u00c4nderungen.", "token2charspan": [[0, 3], [4, 16], [17, 24], [25, 28], [29, 32], [33, 45], [45, 46], [47, 57], [58, 61], [62, 69], [70, 80], [80, 81]]}
{"doc_key": "ai-train-62", "ner": [[7, 7, "country"], [11, 12, "university"], [15, 16, "university"], [24, 24, "product"], [31, 33, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[11, 12, 7, 7, "physical", "", false, false], [15, 16, 7, 7, "physical", "", false, false], [24, 24, 11, 12, "origin", "", false, false], [24, 24, 15, 16, "origin", "", false, false], [24, 24, 31, 33, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["In", "den", "sp\u00e4ten", "1980er", "Jahren", "begannen", "zwei", "niederl\u00e4ndische", "Universit\u00e4ten", ",", "die", "Universit\u00e4t", "Groningen", "und", "die", "Universit\u00e4t", "Twente", ",", "gemeinsam", "ein", "Projekt", "mit", "dem", "Namen", "Wissensgraphen", ",", "bei", "dem", "es", "sich", "um", "semantische", "Netze", "handelt", ",", "bei", "denen", "jedoch", "die", "Kanten", "auf", "eine", "begrenzte", "Anzahl", "m\u00f6glicher", "Beziehungen", "beschr\u00e4nkt", "sind", ",", "um", "Algebren", "auf", "dem", "Graphen", "zu", "erleichtern", "."], "sentence-detokenized": "In den sp\u00e4ten 1980er Jahren begannen zwei niederl\u00e4ndische Universit\u00e4ten, die Universit\u00e4t Groningen und die Universit\u00e4t Twente, gemeinsam ein Projekt mit dem Namen Wissensgraphen, bei dem es sich um semantische Netze handelt, bei denen jedoch die Kanten auf eine begrenzte Anzahl m\u00f6glicher Beziehungen beschr\u00e4nkt sind, um Algebren auf dem Graphen zu erleichtern.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 20], [21, 27], [28, 36], [37, 41], [42, 57], [58, 71], [71, 72], [73, 76], [77, 88], [89, 98], [99, 102], [103, 106], [107, 118], [119, 125], [125, 126], [127, 136], [137, 140], [141, 148], [149, 152], [153, 156], [157, 162], [163, 177], [177, 178], [179, 182], [183, 186], [187, 189], [190, 194], [195, 197], [198, 209], [210, 215], [216, 223], [223, 224], [225, 228], [229, 234], [235, 241], [242, 245], [246, 252], [253, 256], [257, 261], [262, 271], [272, 278], [279, 288], [289, 300], [301, 311], [312, 316], [316, 317], [318, 320], [321, 329], [330, 333], [334, 337], [338, 345], [346, 348], [349, 360], [360, 361]]}
{"doc_key": "ai-train-63", "ner": [[0, 0, "product"], [12, 12, "product"]], "ner_mapping_to_source": [0, 1], "relations": [[0, 0, 12, 12, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Grammatikpr\u00fcfprogramme", "werden", "meist", "als", "Teil", "eines", "gr\u00f6\u00dferen", "Programms", ",", "z.", "B.", "eines", "Textverarbeitungsprogramms", ",", "implementiert", ",", "sind", "aber", "auch", "als", "eigenst\u00e4ndige", "Anwendung", "erh\u00e4ltlich", ",", "die", "aus", "Programmen", "heraus", "aktiviert", "werden", "kann", ",", "die", "mit", "editierbarem", "Text", "arbeiten", "."], "sentence-detokenized": "Grammatikpr\u00fcfprogramme werden meist als Teil eines gr\u00f6\u00dferen Programms, z. B. eines Textverarbeitungsprogramms, implementiert, sind aber auch als eigenst\u00e4ndige Anwendung erh\u00e4ltlich, die aus Programmen heraus aktiviert werden kann, die mit editierbarem Text arbeiten.", "token2charspan": [[0, 22], [23, 29], [30, 35], [36, 39], [40, 44], [45, 50], [51, 59], [60, 69], [69, 70], [71, 73], [74, 76], [77, 82], [83, 109], [109, 110], [111, 124], [124, 125], [126, 130], [131, 135], [136, 140], [141, 144], [145, 158], [159, 168], [169, 179], [179, 180], [181, 184], [185, 188], [189, 199], [200, 206], [207, 216], [217, 223], [224, 228], [228, 229], [230, 233], [234, 237], [238, 250], [251, 255], [256, 264], [264, 265]]}
{"doc_key": "ai-train-64", "ner": [[4, 10, "organisation"], [13, 18, "conference"], [21, 23, "organisation"], [28, 30, "conference"], [32, 34, "conference"], [36, 38, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [], "relations_mapping_to_source": [], "sentence": ["Er", "ist", "Fellow", "der", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "der", "Association", "for", "the", "Advancement", "Artificial", "Intelligence", "und", "der", "Cognitive", "Science", "Society", "und", "Redakteur", "der", "Zeitschriften", "J.", "Automated", "Reasoning", ",", "J.", "Learning", "Sciences", "und", "J.", "Applied", "Ontology", "."], "sentence-detokenized": "Er ist Fellow der American Association for the Advancement of Science, der Association for the Advancement Artificial Intelligence und der Cognitive Science Society und Redakteur der Zeitschriften J. Automated Reasoning, J. Learning Sciences und J. Applied Ontology.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 17], [18, 26], [27, 38], [39, 42], [43, 46], [47, 58], [59, 61], [62, 69], [69, 70], [71, 74], [75, 86], [87, 90], [91, 94], [95, 106], [107, 117], [118, 130], [131, 134], [135, 138], [139, 148], [149, 156], [157, 164], [165, 168], [169, 178], [179, 182], [183, 196], [197, 199], [200, 209], [210, 219], [219, 220], [221, 223], [224, 232], [233, 241], [242, 245], [246, 248], [249, 256], [257, 265], [265, 266]]}
{"doc_key": "ai-train-65", "ner": [[3, 5, "algorithm"], [7, 7, "algorithm"], [13, 13, "task"], [21, 22, "researcher"], [25, 26, "university"], [28, 29, "researcher"], [31, 34, "organisation"], [36, 36, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[3, 5, 13, 13, "type-of", "", false, false], [3, 5, 21, 22, "origin", "", false, false], [3, 5, 28, 29, "origin", "", false, false], [7, 7, 3, 5, "named", "", false, false], [21, 22, 25, 26, "physical", "", false, false], [21, 22, 25, 26, "role", "", false, false], [28, 29, 31, 34, "role", "", false, false], [36, 36, 31, 34, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Die", "Entwicklung", "der", "linearen", "pr\u00e4diktiven", "Kodierung", "(", "LPC", ")", ",", "einer", "Form", "der", "Sprachkodierung", ",", "begann", "1966", "mit", "der", "Arbeit", "von", "Fumitada", "Itakura", "von", "der", "Universit\u00e4t", "Nagoya", "und", "Shuzo", "Saito", "von", "Nippon", "Telegraph", "and", "Telephone", "(", "NTT", ")", "."], "sentence-detokenized": "Die Entwicklung der linearen pr\u00e4diktiven Kodierung (LPC), einer Form der Sprachkodierung, begann 1966 mit der Arbeit von Fumitada Itakura von der Universit\u00e4t Nagoya und Shuzo Saito von Nippon Telegraph and Telephone (NTT).", "token2charspan": [[0, 3], [4, 15], [16, 19], [20, 28], [29, 40], [41, 50], [51, 52], [52, 55], [55, 56], [56, 57], [58, 63], [64, 68], [69, 72], [73, 88], [88, 89], [90, 96], [97, 101], [102, 105], [106, 109], [110, 116], [117, 120], [121, 129], [130, 137], [138, 141], [142, 145], [146, 157], [158, 164], [165, 168], [169, 174], [175, 180], [181, 184], [185, 191], [192, 201], [202, 205], [206, 215], [216, 217], [217, 220], [220, 221], [221, 222]]}
{"doc_key": "ai-train-66", "ner": [[54, 57, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Wenn", "das", "Signal", "weiterhin", "ergodisch", "ist", ",", "weisen", "alle", "Abtastpfade", "den", "gleichen", "Zeitmittelwert", "auf", "und", "somit", "mathR", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "=\\", "widehat", "{", "R", "}", "_", "x", "^", "{", "n", "/", "T", "_", "0", "}", "(", "\\", "tau", ")", "/", "math", "im", "Sinne", "des", "mittleren", "quadratischen", "Fehlers", "."], "sentence-detokenized": "Wenn das Signal weiterhin ergodisch ist, weisen alle Abtastpfade den gleichen Zeitmittelwert auf und somit mathR _ x ^ {n / T _ 0} (\\ tau) =\\ widehat {R} _ x ^ {n / T _ 0} (\\ tau) / math im Sinne des mittleren quadratischen Fehlers.", "token2charspan": [[0, 4], [5, 8], [9, 15], [16, 25], [26, 35], [36, 39], [39, 40], [41, 47], [48, 52], [53, 64], [65, 68], [69, 77], [78, 92], [93, 96], [97, 100], [101, 106], [107, 112], [113, 114], [115, 116], [117, 118], [119, 120], [120, 121], [122, 123], [124, 125], [126, 127], [128, 129], [129, 130], [131, 132], [132, 133], [134, 137], [137, 138], [139, 141], [142, 149], [150, 151], [151, 152], [152, 153], [154, 155], [156, 157], [158, 159], [160, 161], [161, 162], [163, 164], [165, 166], [167, 168], [169, 170], [170, 171], [172, 173], [173, 174], [175, 178], [178, 179], [180, 181], [182, 186], [187, 189], [190, 195], [196, 199], [200, 209], [210, 223], [224, 231], [231, 232]]}
{"doc_key": "ai-train-67", "ner": [[0, 0, "task"], [2, 2, "task"], [12, 12, "algorithm"], [14, 14, "algorithm"], [18, 19, "algorithm"], [21, 21, "algorithm"], [25, 26, "algorithm"], [28, 28, "algorithm"], [32, 33, "algorithm"], [35, 35, "algorithm"], [38, 38, "misc"], [47, 47, "algorithm"], [49, 49, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], "relations": [[12, 12, 38, 38, "related-to", "", false, false], [14, 14, 12, 12, "named", "", false, false], [18, 19, 38, 38, "related-to", "", false, false], [21, 21, 18, 19, "named", "", false, false], [25, 26, 38, 38, "related-to", "", false, false], [28, 28, 25, 26, "named", "", false, false], [32, 33, 38, 38, "related-to", "", false, false], [35, 35, 32, 33, "named", "", false, false], [47, 47, 49, 49, "related-to", "", true, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Merkmalsextraktion", "und", "Dimensionsreduktion", "k\u00f6nnen", "in", "einem", "Schritt", "kombiniert", "werden", ",", "indem", "die", "Hauptkomponentenanalyse", "(", "PCA", ")", ",", "die", "lineare", "Diskriminanzanalyse", "(", "LDA", ")", ",", "die", "kanonische", "Korrelationsanalyse", "(", "CCA", ")", "oder", "die", "nicht-negative", "Matrixfaktorisierung", "(", "NMF", ")", "als", "Vorverarbeitungsschritt", "verwendet", "werden", ",", "gefolgt", "von", "einer", "Clusterbildung", "durch", "K-NN", "auf", "Merkmalsvektoren", "im", "reduzierten", "Dimensionsraum", "."], "sentence-detokenized": "Merkmalsextraktion und Dimensionsreduktion k\u00f6nnen in einem Schritt kombiniert werden, indem die Hauptkomponentenanalyse (PCA), die lineare Diskriminanzanalyse (LDA), die kanonische Korrelationsanalyse (CCA) oder die nicht-negative Matrixfaktorisierung (NMF) als Vorverarbeitungsschritt verwendet werden, gefolgt von einer Clusterbildung durch K-NN auf Merkmalsvektoren im reduzierten Dimensionsraum.", "token2charspan": [[0, 18], [19, 22], [23, 42], [43, 49], [50, 52], [53, 58], [59, 66], [67, 77], [78, 84], [84, 85], [86, 91], [92, 95], [96, 119], [120, 121], [121, 124], [124, 125], [125, 126], [127, 130], [131, 138], [139, 158], [159, 160], [160, 163], [163, 164], [164, 165], [166, 169], [170, 180], [181, 200], [201, 202], [202, 205], [205, 206], [207, 211], [212, 215], [216, 230], [231, 251], [252, 253], [253, 256], [256, 257], [258, 261], [262, 285], [286, 295], [296, 302], [302, 303], [304, 311], [312, 315], [316, 321], [322, 336], [337, 342], [343, 347], [348, 351], [352, 368], [369, 371], [372, 383], [384, 398], [398, 399]]}
{"doc_key": "ai-train-68", "ner": [[1, 1, "programlang"], [3, 3, "programlang"], [5, 5, "programlang"], [7, 7, "programlang"], [13, 13, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[13, 13, 1, 1, "related-to", "program_type_compatible_with", false, false], [13, 13, 3, 3, "related-to", "program_type_compatible_with", false, false], [13, 13, 5, 5, "related-to", "program_type_compatible_with", false, false], [13, 13, 7, 7, "related-to", "program_type_compatible_with", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "Perl", ",", "Java", ",", "ActiveX", "oder", ".NET", "geschriebene", "Bibliotheken", "k\u00f6nnen", "direkt", "von", "MATLAB", "aus", "aufgerufen", "werden", ","], "sentence-detokenized": "In Perl, Java, ActiveX oder .NET geschriebene Bibliotheken k\u00f6nnen direkt von MATLAB aus aufgerufen werden,", "token2charspan": [[0, 2], [3, 7], [7, 8], [9, 13], [13, 14], [15, 22], [23, 27], [28, 32], [33, 45], [46, 58], [59, 65], [66, 72], [73, 76], [77, 83], [84, 87], [88, 98], [99, 105], [105, 106]]}
{"doc_key": "ai-train-69", "ner": [[3, 8, "task"], [11, 13, "task"], [30, 31, "task"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[3, 8, 11, 13, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Die", "Aufgabe", ",", "benannte", "Entit\u00e4ten", "im", "Text", "zu", "erkennen", ",", "hei\u00dft", "Named", "Entity", "Recognition", ",", "w\u00e4hrend", "die", "Aufgabe", ",", "die", "Identit\u00e4t", "der", "im", "Text", "erw\u00e4hnten", "benannten", "Entit\u00e4ten", "zu", "bestimmen", ",", "Entity", "Linking", "genannt", "wird", "."], "sentence-detokenized": "Die Aufgabe, benannte Entit\u00e4ten im Text zu erkennen, hei\u00dft Named Entity Recognition, w\u00e4hrend die Aufgabe, die Identit\u00e4t der im Text erw\u00e4hnten benannten Entit\u00e4ten zu bestimmen, Entity Linking genannt wird.", "token2charspan": [[0, 3], [4, 11], [11, 12], [13, 21], [22, 31], [32, 34], [35, 39], [40, 42], [43, 51], [51, 52], [53, 58], [59, 64], [65, 71], [72, 83], [83, 84], [85, 92], [93, 96], [97, 104], [104, 105], [106, 109], [110, 119], [120, 123], [124, 126], [127, 131], [132, 141], [142, 151], [152, 161], [162, 164], [165, 174], [174, 175], [176, 182], [183, 190], [191, 198], [199, 203], [203, 204]]}
{"doc_key": "ai-train-70", "ner": [[22, 22, "algorithm"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "im", "Paket", "verwendeten", "Sigmoid-Funktionen", "und", "-Ableitungen", "waren", "urspr\u00fcnglich", "im", "Paket", "enthalten", ".", "Ab", "Version", "0.8.0", "wurden", "sie", "in", "einem", "separaten", "R-Paket", "sigmoid", "ver\u00f6ffentlicht", ",", "um", "eine", "allgemeinere", "Verwendung", "zu", "erm\u00f6glichen", "."], "sentence-detokenized": "Die im Paket verwendeten Sigmoid-Funktionen und -Ableitungen waren urspr\u00fcnglich im Paket enthalten. Ab Version 0.8.0 wurden sie in einem separaten R-Paket sigmoid ver\u00f6ffentlicht, um eine allgemeinere Verwendung zu erm\u00f6glichen.", "token2charspan": [[0, 3], [4, 6], [7, 12], [13, 24], [25, 43], [44, 47], [48, 60], [61, 66], [67, 79], [80, 82], [83, 88], [89, 98], [98, 99], [100, 102], [103, 110], [111, 116], [117, 123], [124, 127], [128, 130], [131, 136], [137, 146], [147, 154], [155, 162], [163, 177], [177, 178], [179, 181], [182, 186], [187, 199], [200, 210], [211, 213], [214, 225], [225, 226]]}
{"doc_key": "ai-train-71", "ner": [[0, 0, "programlang"], [13, 17, "organisation"], [19, 19, "organisation"], [25, 25, "location"], [27, 27, "location"], [4, 5, "researcher"], [7, 8, "researcher"], [10, 11, "researcher"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "relations": [[0, 0, 4, 5, "artifact", "", true, false], [0, 0, 7, 8, "artifact", "", true, false], [0, 0, 10, 11, "artifact", "", true, false], [19, 19, 13, 17, "named", "", false, false], [19, 19, 25, 25, "physical", "", false, false], [25, 25, 27, 27, "physical", "", false, false], [4, 5, 13, 17, "role", "", false, false], [7, 8, 13, 17, "role", "", false, false], [10, 11, 13, 17, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Logo", "wurde", "1967", "von", "Wally", "Feurzeig", ",", "Cynthia", "Solomon", "und", "Seymour", "Papert", "bei", "Bolt", ",", "Beranek", "und", "Newman", "(", "BBN", ")", ",", "einem", "Forschungsunternehmen", "in", "Cambridge", ",", "Massachusetts", ",", "entwickelt", "."], "sentence-detokenized": "Logo wurde 1967 von Wally Feurzeig, Cynthia Solomon und Seymour Papert bei Bolt, Beranek und Newman (BBN), einem Forschungsunternehmen in Cambridge, Massachusetts, entwickelt.", "token2charspan": [[0, 4], [5, 10], [11, 15], [16, 19], [20, 25], [26, 34], [34, 35], [36, 43], [44, 51], [52, 55], [56, 63], [64, 70], [71, 74], [75, 79], [79, 80], [81, 88], [89, 92], [93, 99], [100, 101], [101, 104], [104, 105], [105, 106], [107, 112], [113, 134], [135, 137], [138, 147], [147, 148], [149, 162], [162, 163], [164, 174], [174, 175]]}
{"doc_key": "ai-train-72", "ner": [[0, 1, "misc"], [8, 9, "field"], [20, 20, "algorithm"], [23, 24, "algorithm"]], "ner_mapping_to_source": [0, 1, 3, 4], "relations": [[0, 1, 8, 9, "part-of", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Neuroevolution", "wird", "h\u00e4ufig", "als", "Teil", "des", "Paradigmas", "des", "verst\u00e4rkenden", "Lernens", "verwendet", "und", "kann", "mit", "konventionellen", "Deep-Learning-Techniken", "verglichen", "werden", ",", "die", "Gradientenabstieg", "auf", "ein", "neuronales", "Netzwerk", "mit", "einer", "festen", "Topologie", "anwenden", "."], "sentence-detokenized": "Neuroevolution wird h\u00e4ufig als Teil des Paradigmas des verst\u00e4rkenden Lernens verwendet und kann mit konventionellen Deep-Learning-Techniken verglichen werden, die Gradientenabstieg auf ein neuronales Netzwerk mit einer festen Topologie anwenden.", "token2charspan": [[0, 14], [15, 19], [20, 26], [27, 30], [31, 35], [36, 39], [40, 50], [51, 54], [55, 68], [69, 76], [77, 86], [87, 90], [91, 95], [96, 99], [100, 115], [116, 139], [140, 150], [151, 157], [157, 158], [159, 162], [163, 180], [181, 184], [185, 188], [189, 199], [200, 208], [209, 212], [213, 218], [219, 225], [226, 235], [236, 244], [244, 245]]}
{"doc_key": "ai-train-73", "ner": [[3, 4, "algorithm"], [54, 56, "metrics"], [58, 58, "metrics"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[58, 58, 54, 56, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["Wenn", "wir", "die", "kleinsten", "Quadrate", "verwenden", ",", "um", "eine", "Funktion", "in", "Form", "einer", "Hyperebene", "\u0177", "=", "a", "+", "\u03b2", "supT", "/", "sup", "x", "an", "die", "Daten", "(", "x", "sub", "i", "/", "sub", ",", "y", "sub", "i", "/", "sub", ")", "sub", "1", "\u2264", "i", "\u2264n", "/", "sub", "anzupassen", ",", "k\u00f6nnen", "wir", "die", "Anpassung", "anhand", "des", "mittleren", "quadratischen", "Fehlers", "(", "MSE", ")", "bewerten", "."], "sentence-detokenized": "Wenn wir die kleinsten Quadrate verwenden, um eine Funktion in Form einer Hyperebene \u0177 = a + \u03b2 supT / sup x an die Daten (x sub i / sub, y sub i / sub) sub 1 \u2264 i \u2264n / sub anzupassen, k\u00f6nnen wir die Anpassung anhand des mittleren quadratischen Fehlers (MSE) bewerten.", "token2charspan": [[0, 4], [5, 8], [9, 12], [13, 22], [23, 31], [32, 41], [41, 42], [43, 45], [46, 50], [51, 59], [60, 62], [63, 67], [68, 73], [74, 84], [85, 86], [87, 88], [89, 90], [91, 92], [93, 94], [95, 99], [100, 101], [102, 105], [106, 107], [108, 110], [111, 114], [115, 120], [121, 122], [122, 123], [124, 127], [128, 129], [130, 131], [132, 135], [135, 136], [137, 138], [139, 142], [143, 144], [145, 146], [147, 150], [150, 151], [152, 155], [156, 157], [158, 159], [160, 161], [162, 164], [165, 166], [167, 170], [171, 181], [181, 182], [183, 189], [190, 193], [194, 197], [198, 207], [208, 214], [215, 218], [219, 228], [229, 242], [243, 250], [251, 252], [252, 255], [255, 256], [257, 265], [265, 266]]}
{"doc_key": "ai-train-74", "ner": [[7, 7, "country"], [9, 9, "country"], [11, 11, "country"], [13, 13, "country"], [15, 15, "country"], [17, 17, "country"], [19, 19, "country"], [21, 21, "country"], [23, 23, "country"], [25, 25, "country"], [27, 27, "country"], [29, 29, "country"], [32, 32, "country"], [34, 34, "country"], [36, 36, "country"], [38, 38, "country"], [40, 40, "country"], [42, 42, "country"], [44, 44, "country"], [47, 47, "country"], [50, 51, "country"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "relations": [], "relations_mapping_to_source": [], "sentence": ["Das", "Unternehmen", "verf\u00fcgt", "\u00fcber", "internationale", "Standorte", "in", "Australien", ",", "Brasilien", ",", "Kanada", ",", "China", ",", "Deutschland", ",", "Indien", ",", "Italien", ",", "Japan", ",", "Korea", ",", "Litauen", ",", "Polen", ",", "Malaysia", ",", "den", "Philippinen", ",", "Russland", ",", "Singapur", ",", "S\u00fcdafrika", ",", "Spanien", ",", "Taiwan", ",", "Thailand", ",", "der", "T\u00fcrkei", "und", "dem", "Vereinigten", "K\u00f6nigreich", "."], "sentence-detokenized": "Das Unternehmen verf\u00fcgt \u00fcber internationale Standorte in Australien, Brasilien, Kanada, China, Deutschland, Indien, Italien, Japan, Korea, Litauen, Polen, Malaysia, den Philippinen, Russland, Singapur, S\u00fcdafrika, Spanien, Taiwan, Thailand, der T\u00fcrkei und dem Vereinigten K\u00f6nigreich.", "token2charspan": [[0, 3], [4, 15], [16, 23], [24, 28], [29, 43], [44, 53], [54, 56], [57, 67], [67, 68], [69, 78], [78, 79], [80, 86], [86, 87], [88, 93], [93, 94], [95, 106], [106, 107], [108, 114], [114, 115], [116, 123], [123, 124], [125, 130], [130, 131], [132, 137], [137, 138], [139, 146], [146, 147], [148, 153], [153, 154], [155, 163], [163, 164], [165, 168], [169, 180], [180, 181], [182, 190], [190, 191], [192, 200], [200, 201], [202, 211], [211, 212], [213, 220], [220, 221], [222, 228], [228, 229], [230, 238], [238, 239], [240, 243], [244, 250], [251, 254], [255, 258], [259, 270], [271, 281], [281, 282]]}
{"doc_key": "ai-train-75", "ner": [[3, 3, "misc"], [5, 7, "field"], [12, 12, "organisation"], [15, 21, "university"], [26, 28, "organisation"], [31, 34, "university"], [39, 40, "university"], [43, 44, "university"], [47, 49, "university"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[3, 3, 5, 7, "topic", "", false, false], [3, 3, 12, 12, "origin", "", false, false], [3, 3, 15, 21, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Er", "hat", "einen", "Doktortitel", "in", "Elektro-", "und", "Computertechnik", "(", "2000", ")", "von", "Inria", "und", "der", "Universit\u00e4t", "von", "Nizza", "Sophia", "Antipolis", ".", "Er", "hatte", "feste", "Stellen", "bei", "Siemens", "Corporate", "Technology", ",", "der", "\u00c9cole", "des", "ponts", "ParisTech", "sowie", "Gaststellen", "an", "der", "Rutgers", "University", ",", "der", "Yale", "University", "und", "der", "University", "of", "Houston", "."], "sentence-detokenized": "Er hat einen Doktortitel in Elektro- und Computertechnik (2000) von Inria und der Universit\u00e4t von Nizza Sophia Antipolis. Er hatte feste Stellen bei Siemens Corporate Technology, der \u00c9cole des ponts ParisTech sowie Gaststellen an der Rutgers University, der Yale University und der University of Houston.", "token2charspan": [[0, 2], [3, 6], [7, 12], [13, 24], [25, 27], [28, 36], [37, 40], [41, 56], [57, 58], [58, 62], [62, 63], [64, 67], [68, 73], [74, 77], [78, 81], [82, 93], [94, 97], [98, 103], [104, 110], [111, 120], [120, 121], [122, 124], [125, 130], [131, 136], [137, 144], [145, 148], [149, 156], [157, 166], [167, 177], [177, 178], [179, 182], [183, 188], [189, 192], [193, 198], [199, 208], [209, 214], [215, 226], [227, 229], [230, 233], [234, 241], [242, 252], [252, 253], [254, 257], [258, 262], [263, 273], [274, 277], [278, 281], [282, 292], [293, 295], [296, 303], [303, 304]]}
{"doc_key": "ai-train-76", "ner": [[7, 8, "researcher"], [0, 0, "researcher"], [17, 17, "product"], [20, 21, "country"], [24, 24, "product"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 7, 8, "role", "licensing_patent_to", false, false], [0, 0, 20, 21, "physical", "", false, false], [24, 24, 0, 0, "artifact", "", false, false], [24, 24, 17, 17, "type-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Engelberger", "lizenzierte", "das", "urspr\u00fcngliche", "Patent", "des", "Erfinders", "George", "Devol", "und", "entwickelte", "in", "den", "1950er", "Jahren", "den", "ersten", "Industrieroboter", "in", "den", "Vereinigten", "Staaten", ",", "den", "Unimate", "."], "sentence-detokenized": "Engelberger lizenzierte das urspr\u00fcngliche Patent des Erfinders George Devol und entwickelte in den 1950er Jahren den ersten Industrieroboter in den Vereinigten Staaten, den Unimate.", "token2charspan": [[0, 11], [12, 23], [24, 27], [28, 41], [42, 48], [49, 52], [53, 62], [63, 69], [70, 75], [76, 79], [80, 91], [92, 94], [95, 98], [99, 105], [106, 112], [113, 116], [117, 123], [124, 140], [141, 143], [144, 147], [148, 159], [160, 167], [167, 168], [169, 172], [173, 180], [180, 181]]}
{"doc_key": "ai-train-77", "ner": [[4, 4, "task"], [9, 10, "task"]], "ner_mapping_to_source": [0, 1], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "Eingabe", "wird", "als", "Spracherkennung", "und", "die", "Ausgabe", "als", "Sprachsynthese", "bezeichnet", "."], "sentence-detokenized": "Die Eingabe wird als Spracherkennung und die Ausgabe als Sprachsynthese bezeichnet.", "token2charspan": [[0, 3], [4, 11], [12, 16], [17, 20], [21, 36], [37, 40], [41, 44], [45, 52], [53, 56], [57, 71], [72, 82], [82, 83]]}
{"doc_key": "ai-train-78", "ner": [[6, 6, "programlang"], [12, 12, "programlang"], [15, 15, "programlang"], [27, 27, "programlang"]], "ner_mapping_to_source": [1, 2, 3, 4], "relations": [[6, 6, 15, 15, "general-affiliation", "", false, false], [6, 6, 27, 27, "named", "", false, false]], "relations_mapping_to_source": [2, 3], "sentence": ["Zu", "den", "Abk\u00f6mmlingen", "der", "CLIPS-Sprache", "geh\u00f6ren", "Jess", "(", "der", "regelbasierte", "Teil", "von", "CLIPS", "wurde", "in", "Java", "umgeschrieben", "und", "entwickelte", "sich", "sp\u00e4ter", "in", "eine", "andere", "Richtung", ")", ",", "JESS", "war", "urspr\u00fcnglich", "inspiriert", "von"], "sentence-detokenized": "Zu den Abk\u00f6mmlingen der CLIPS-Sprache geh\u00f6ren Jess (der regelbasierte Teil von CLIPS wurde in Java umgeschrieben und entwickelte sich sp\u00e4ter in eine andere Richtung), JESS war urspr\u00fcnglich inspiriert von", "token2charspan": [[0, 2], [3, 6], [7, 19], [20, 23], [24, 37], [38, 45], [46, 50], [51, 52], [52, 55], [56, 69], [70, 74], [75, 78], [79, 84], [85, 90], [91, 93], [94, 98], [99, 112], [113, 116], [117, 128], [129, 133], [134, 140], [141, 143], [144, 148], [149, 155], [156, 164], [164, 165], [165, 166], [167, 171], [172, 175], [176, 188], [189, 199], [200, 203]]}
{"doc_key": "ai-train-79", "ner": [[10, 10, "product"], [15, 16, "organisation"], [21, 22, "product"], [37, 37, "product"], [39, 43, "product"], [53, 54, "misc"]], "ner_mapping_to_source": [1, 2, 3, 4, 5, 6], "relations": [[15, 16, 10, 10, "usage", "", false, false], [21, 22, 15, 16, "artifact", "", false, false], [37, 37, 15, 16, "origin", "", true, false], [37, 37, 53, 54, "related-to", "", true, false], [39, 43, 15, 16, "origin", "", true, false], [39, 43, 53, 54, "related-to", "", true, false]], "relations_mapping_to_source": [1, 2, 3, 4, 5, 6], "sentence": ["Das", "Unternehmen", "hat", "auch", "flexible", "intelligente", "FTS-Anwendungen", "entwickelt", "und", "das", "Motivity-Steuerungssystem", "entworfen", ",", "das", "von", "RMT", "Robotics", "f\u00fcr", "die", "Entwicklung", "des", "ADAM", "iAGV", "(", "Self-Guided", "Vehicle", ")", "verwendet", "wird", ",", "das", "f\u00fcr", "komplexe", "Pick-and-Place-Vorg\u00e4nge", "in", "Verbindung", "mit", "Portalsystemen", "und", "Industrieroboterarmen", "eingesetzt", "wird", ",", "die", "in", "erstklassigen", "Automobilzulieferbetrieben", "verwendet", "werden", ",", "um", "Produkte", "in", "nichtlinearen", "Layouts", "von", "Prozess", "zu", "Prozess", "zu", "bewegen", "."], "sentence-detokenized": "Das Unternehmen hat auch flexible intelligente FTS-Anwendungen entwickelt und das Motivity-Steuerungssystem entworfen, das von RMT Robotics f\u00fcr die Entwicklung des ADAM iAGV (Self-Guided Vehicle) verwendet wird, das f\u00fcr komplexe Pick-and-Place-Vorg\u00e4nge in Verbindung mit Portalsystemen und Industrieroboterarmen eingesetzt wird, die in erstklassigen Automobilzulieferbetrieben verwendet werden, um Produkte in nichtlinearen Layouts von Prozess zu Prozess zu bewegen.", "token2charspan": [[0, 3], [4, 15], [16, 19], [20, 24], [25, 33], [34, 46], [47, 62], [63, 73], [74, 77], [78, 81], [82, 107], [108, 117], [117, 118], [119, 122], [123, 126], [127, 130], [131, 139], [140, 143], [144, 147], [148, 159], [160, 163], [164, 168], [169, 173], [174, 175], [175, 186], [187, 194], [194, 195], [196, 205], [206, 210], [210, 211], [212, 215], [216, 219], [220, 228], [229, 252], [253, 255], [256, 266], [267, 270], [271, 285], [286, 289], [290, 311], [312, 322], [323, 327], [327, 328], [329, 332], [333, 335], [336, 349], [350, 376], [377, 386], [387, 393], [393, 394], [395, 397], [398, 406], [407, 409], [410, 423], [424, 431], [432, 435], [436, 443], [444, 446], [447, 454], [455, 457], [458, 465], [465, 466]]}
{"doc_key": "ai-train-80", "ner": [[9, 9, "metrics"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "Parameter", "\u03b2", "werden", "in", "der", "Regel", "nach", "dem", "Maximum-Likelihood-Verfahren", "gesch\u00e4tzt", "."], "sentence-detokenized": "Die Parameter \u03b2 werden in der Regel nach dem Maximum-Likelihood-Verfahren gesch\u00e4tzt.", "token2charspan": [[0, 3], [4, 13], [14, 15], [16, 22], [23, 25], [26, 29], [30, 35], [36, 40], [41, 44], [45, 73], [74, 83], [83, 84]]}
{"doc_key": "ai-train-81", "ner": [[3, 4, "task"], [6, 6, "metrics"], [8, 8, "metrics"], [10, 10, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[6, 6, 3, 4, "part-of", "", false, false], [8, 8, 3, 4, "part-of", "", false, false], [10, 10, 3, 4, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2], "sentence": ["Die", "Metriken", "des", "Information", "Retrieval", "wie", "Precision", "und", "Recall", "oder", "DCG", "sind", "n\u00fctzlich", ",", "um", "die", "Qualit\u00e4t", "einer", "Empfehlungsmethode", "zu", "bewerten", "."], "sentence-detokenized": "Die Metriken des Information Retrieval wie Precision und Recall oder DCG sind n\u00fctzlich, um die Qualit\u00e4t einer Empfehlungsmethode zu bewerten.", "token2charspan": [[0, 3], [4, 12], [13, 16], [17, 28], [29, 38], [39, 42], [43, 52], [53, 56], [57, 63], [64, 68], [69, 72], [73, 77], [78, 86], [86, 87], [88, 90], [91, 94], [95, 103], [104, 109], [110, 128], [129, 131], [132, 140], [140, 141]]}
{"doc_key": "ai-train-82", "ner": [[7, 7, "product"]], "ner_mapping_to_source": [0], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "einer", "typischen", "Fabrik", "arbeiten", "Hunderte", "von", "Industrierobotern", "an", "vollautomatischen", "Produktionslinien", ",", "wobei", "ein", "Roboter", "auf", "zehn", "menschliche", "Arbeiter", "kommt", "."], "sentence-detokenized": "In einer typischen Fabrik arbeiten Hunderte von Industrierobotern an vollautomatischen Produktionslinien, wobei ein Roboter auf zehn menschliche Arbeiter kommt.", "token2charspan": [[0, 2], [3, 8], [9, 18], [19, 25], [26, 34], [35, 43], [44, 47], [48, 65], [66, 68], [69, 86], [87, 104], [104, 105], [106, 111], [112, 115], [116, 123], [124, 127], [128, 132], [133, 144], [145, 153], [154, 159], [159, 160]]}
{"doc_key": "ai-train-83", "ner": [[6, 6, "product"], [16, 16, "task"], [18, 18, "task"], [20, 20, "task"], [22, 22, "task"], [24, 24, "task"], [26, 26, "task"]], "ner_mapping_to_source": [0, 2, 3, 4, 5, 6, 7], "relations": [], "relations_mapping_to_source": [], "sentence": ["In", "den", "letzten", "zehn", "Jahren", "wurden", "PCNNs", "in", "einer", "Vielzahl", "von", "Bildverarbeitungsanwendungen", "eingesetzt", ",", "darunter", ":", "Bildsegmentierung", ",", "Merkmalserzeugung", ",", "Gesichtsextraktion", ",", "Bewegungserkennung", ",", "Regionenwachstum", "und", "Rauschunterdr\u00fcckung", "."], "sentence-detokenized": "In den letzten zehn Jahren wurden PCNNs in einer Vielzahl von Bildverarbeitungsanwendungen eingesetzt, darunter: Bildsegmentierung, Merkmalserzeugung, Gesichtsextraktion, Bewegungserkennung, Regionenwachstum und Rauschunterdr\u00fcckung.", "token2charspan": [[0, 2], [3, 6], [7, 14], [15, 19], [20, 26], [27, 33], [34, 39], [40, 42], [43, 48], [49, 57], [58, 61], [62, 90], [91, 101], [101, 102], [103, 111], [111, 112], [113, 130], [130, 131], [132, 149], [149, 150], [151, 169], [169, 170], [171, 189], [189, 190], [191, 207], [208, 211], [212, 231], [231, 232]]}
{"doc_key": "ai-train-84", "ner": [[0, 0, "researcher"], [14, 15, "field"], [19, 21, "misc"], [24, 29, "conference"], [31, 31, "conference"], [36, 38, "misc"], [41, 47, "conference"], [48, 49, "conference"], [51, 55, "conference"], [57, 57, "conference"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 0, 14, 15, "related-to", "contributes_to", false, false], [0, 0, 19, 21, "win-defeat", "", false, false], [0, 0, 36, 38, "win-defeat", "", false, false], [19, 21, 24, 29, "temporal", "", false, false], [31, 31, 24, 29, "named", "", false, false], [36, 38, 41, 47, "temporal", "", false, false], [36, 38, 51, 55, "temporal", "", false, false], [48, 49, 41, 47, "named", "", false, false], [57, 57, 51, 55, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "sentence": ["Xu", "hat", "mehr", "als", "50", "Beitr\u00e4ge", "auf", "internationalen", "Konferenzen", "und", "in", "Fachzeitschriften", "im", "Bereich", "Computer", "Vision", "ver\u00f6ffentlicht", "und", "den", "Best", "Paper", "Award", "auf", "der", "internationalen", "Konferenz", "Non-Photorealistic", "Rendering", "and", "Animation", "(", "NPAR", ")", "2012", "sowie", "den", "Best", "Reviewer", "Award", "auf", "den", "internationalen", "Konferenzen", "Asian", "Conference", "on", "Computer", "Vision", "ACCV", "2012", "und", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "2015", "gewonnen", "."], "sentence-detokenized": "Xu hat mehr als 50 Beitr\u00e4ge auf internationalen Konferenzen und in Fachzeitschriften im Bereich Computer Vision ver\u00f6ffentlicht und den Best Paper Award auf der internationalen Konferenz Non-Photorealistic Rendering and Animation (NPAR) 2012 sowie den Best Reviewer Award auf den internationalen Konferenzen Asian Conference on Computer Vision ACCV 2012 und International Conference on Computer Vision (ICCV) 2015 gewonnen.", "token2charspan": [[0, 2], [3, 6], [7, 11], [12, 15], [16, 18], [19, 27], [28, 31], [32, 47], [48, 59], [60, 63], [64, 66], [67, 84], [85, 87], [88, 95], [96, 104], [105, 111], [112, 126], [127, 130], [131, 134], [135, 139], [140, 145], [146, 151], [152, 155], [156, 159], [160, 175], [176, 185], [186, 204], [205, 214], [215, 218], [219, 228], [229, 230], [230, 234], [234, 235], [236, 240], [241, 246], [247, 250], [251, 255], [256, 264], [265, 270], [271, 274], [275, 278], [279, 294], [295, 306], [307, 312], [313, 323], [324, 326], [327, 335], [336, 342], [343, 347], [348, 352], [353, 356], [357, 370], [371, 381], [382, 384], [385, 393], [394, 400], [401, 402], [402, 406], [406, 407], [408, 412], [413, 421], [421, 422]]}
{"doc_key": "ai-train-85", "ner": [[0, 1, "programlang"], [4, 4, "field"], [7, 8, "field"], [10, 10, "misc"], [14, 14, "researcher"], [16, 18, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[0, 1, 4, 4, "part-of", "", false, false], [0, 1, 7, 8, "part-of", "", false, false], [0, 1, 10, 10, "type-of", "", false, false], [16, 18, 0, 1, "usage", "", false, false], [16, 18, 14, 14, "origin", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4], "sentence": ["CycL", "ist", "in", "der", "Informatik", "und", "der", "k\u00fcnstlichen", "Intelligenz", "eine", "Ontologiesprache", ",", "die", "von", "Doug", "Lenats", "Projekt", "Cyc", "artificial", "verwendet", "wird", "."], "sentence-detokenized": "CycL ist in der Informatik und der k\u00fcnstlichen Intelligenz eine Ontologiesprache, die von Doug Lenats Projekt Cyc artificial verwendet wird.", "token2charspan": [[0, 4], [5, 8], [9, 11], [12, 15], [16, 26], [27, 30], [31, 34], [35, 46], [47, 58], [59, 63], [64, 80], [80, 81], [82, 85], [86, 89], [90, 94], [95, 101], [102, 109], [110, 113], [114, 124], [125, 134], [135, 139], [139, 140]]}
{"doc_key": "ai-train-86", "ner": [[3, 3, "task"], [7, 9, "metrics"], [14, 16, "metrics"], [18, 25, "metrics"], [31, 32, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[7, 9, 3, 3, "part-of", "", false, false], [14, 16, 7, 9, "named", "", false, false], [18, 25, 7, 9, "named", "", false, false], [31, 32, 7, 9, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Auch", "in", "der", "Regressionsanalyse", "kann", "sich", "der", "mittlere", "quadratische", "Fehler", ",", "der", "oft", "als", "mittlerer", "quadratischer", "Vorhersagefehler", "oder", "mittlerer", "quadratischer", "Fehler", "au\u00dferhalb", "der", "Stichprobe", "bezeichnet", "wird", ",", "auf", "den", "Mittelwert", "der", "quadratischen", "Abweichungen", "der", "Vorhersagen", "von", "den", "WAHREN", "Werten", "\u00fcber", "einen", "Testraum", "au\u00dferhalb", "der", "Stichprobe", "beziehen", ",", "der", "durch", "ein", "Modell", "erzeugt", "wird", ",", "das", "\u00fcber", "einen", "bestimmten", "Stichprobenraum", "gesch\u00e4tzt", "wird", "."], "sentence-detokenized": "Auch in der Regressionsanalyse kann sich der mittlere quadratische Fehler, der oft als mittlerer quadratischer Vorhersagefehler oder mittlerer quadratischer Fehler au\u00dferhalb der Stichprobe bezeichnet wird, auf den Mittelwert der quadratischen Abweichungen der Vorhersagen von den WAHREN Werten \u00fcber einen Testraum au\u00dferhalb der Stichprobe beziehen, der durch ein Modell erzeugt wird, das \u00fcber einen bestimmten Stichprobenraum gesch\u00e4tzt wird.", "token2charspan": [[0, 4], [5, 7], [8, 11], [12, 30], [31, 35], [36, 40], [41, 44], [45, 53], [54, 66], [67, 73], [73, 74], [75, 78], [79, 82], [83, 86], [87, 96], [97, 110], [111, 127], [128, 132], [133, 142], [143, 156], [157, 163], [164, 173], [174, 177], [178, 188], [189, 199], [200, 204], [204, 205], [206, 209], [210, 213], [214, 224], [225, 228], [229, 242], [243, 255], [256, 259], [260, 271], [272, 275], [276, 279], [280, 286], [287, 293], [294, 298], [299, 304], [305, 313], [314, 323], [324, 327], [328, 338], [339, 347], [347, 348], [349, 352], [353, 358], [359, 362], [363, 369], [370, 377], [378, 382], [382, 383], [384, 387], [388, 392], [393, 398], [399, 409], [410, 425], [426, 435], [436, 440], [440, 441]]}
{"doc_key": "ai-train-87", "ner": [[15, 15, "algorithm"]], "ner_mapping_to_source": [2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "Ergebnisse", "zeigen", ",", "dass", "die", "C-HOG-", "und", "R-HOG-Blockdeskriptoren", "vergleichbare", "Leistungen", "erbringen", ",", "wobei", "die", "C-HOG-Deskriptoren", "bei", "festen", "FALSCH-positiv-Raten", "in", "beiden", "Datens\u00e4tzen", "einen", "leichten", "Vorteil", "bei", "der", "Fehlerkennungsrate", "haben", "."], "sentence-detokenized": "Die Ergebnisse zeigen, dass die C-HOG- und R-HOG-Blockdeskriptoren vergleichbare Leistungen erbringen, wobei die C-HOG-Deskriptoren bei festen FALSCH-positiv-Raten in beiden Datens\u00e4tzen einen leichten Vorteil bei der Fehlerkennungsrate haben.", "token2charspan": [[0, 3], [4, 14], [15, 21], [21, 22], [23, 27], [28, 31], [32, 38], [39, 42], [43, 66], [67, 80], [81, 91], [92, 101], [101, 102], [103, 108], [109, 112], [113, 131], [132, 135], [136, 142], [143, 163], [164, 166], [167, 173], [174, 185], [186, 191], [192, 200], [201, 208], [209, 212], [213, 216], [217, 235], [236, 241], [241, 242]]}
{"doc_key": "ai-train-88", "ner": [[4, 4, "algorithm"], [8, 8, "misc"], [11, 12, "algorithm"], [15, 16, "algorithm"], [20, 20, "algorithm"], [23, 24, "algorithm"], [27, 28, "algorithm"], [32, 32, "misc"], [37, 38, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8], "relations": [[4, 4, 8, 8, "usage", "", false, false], [11, 12, 32, 32, "usage", "", false, false], [15, 16, 32, 32, "usage", "", false, false], [20, 20, 32, 32, "usage", "", false, false], [23, 24, 32, 32, "usage", "", false, false], [27, 28, 32, 32, "usage", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5], "sentence": ["Beliebte", "Erkennungsalgorithmen", "sind", "die", "Hauptkomponentenanalyse", "unter", "Verwendung", "von", "Eigengesichtern", ",", "die", "lineare", "Diskriminanzanalyse", ",", "das", "elastische", "Matching", "unter", "Verwendung", "des", "Fisherface-Algorithmus", ",", "das", "versteckte", "Markov-Modell", ",", "das", "multilineare", "Unterraumlernen", "unter", "Verwendung", "der", "Tensordarstellung", "und", "das", "neuronal", "motivierte", "dynamische", "Link-Matching", "."], "sentence-detokenized": "Beliebte Erkennungsalgorithmen sind die Hauptkomponentenanalyse unter Verwendung von Eigengesichtern, die lineare Diskriminanzanalyse, das elastische Matching unter Verwendung des Fisherface-Algorithmus, das versteckte Markov-Modell, das multilineare Unterraumlernen unter Verwendung der Tensordarstellung und das neuronal motivierte dynamische Link-Matching.", "token2charspan": [[0, 8], [9, 30], [31, 35], [36, 39], [40, 63], [64, 69], [70, 80], [81, 84], [85, 100], [100, 101], [102, 105], [106, 113], [114, 133], [133, 134], [135, 138], [139, 149], [150, 158], [159, 164], [165, 175], [176, 179], [180, 202], [202, 203], [204, 207], [208, 218], [219, 232], [232, 233], [234, 237], [238, 250], [251, 266], [267, 272], [273, 283], [284, 287], [288, 305], [306, 309], [310, 313], [314, 322], [323, 333], [334, 344], [345, 358], [358, 359]]}
{"doc_key": "ai-train-89", "ner": [[2, 6, "misc"], [12, 14, "location"], [34, 36, "location"], [49, 49, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[12, 14, 2, 6, "temporal", "", false, false], [34, 36, 2, 6, "temporal", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["Ab", "dem", "Toronto", "International", "Film", "Festival", "2019", "d\u00fcrfen", "Filme", "nicht", "mehr", "im", "Scotiabank", "Theatre", "Toronto", "-", "einem", "der", "Hauptveranstaltungsorte", "des", "Festivals", "-", "gezeigt", "werden", ",", "sondern", "nur", "noch", "in", "anderen", "Kinos", "(", "z.", "B.", "TIFF", "Bell", "Lightbox", "und", "anderen", "lokalen", "Kinos", ")", ",", "wenn", "sie", "\u00fcber", "einen", "Dienst", "wie", "Netflix", "vertrieben", "werden", "."], "sentence-detokenized": "Ab dem Toronto International Film Festival 2019 d\u00fcrfen Filme nicht mehr im Scotiabank Theatre Toronto - einem der Hauptveranstaltungsorte des Festivals - gezeigt werden, sondern nur noch in anderen Kinos (z. B. TIFF Bell Lightbox und anderen lokalen Kinos), wenn sie \u00fcber einen Dienst wie Netflix vertrieben werden.", "token2charspan": [[0, 2], [3, 6], [7, 14], [15, 28], [29, 33], [34, 42], [43, 47], [48, 54], [55, 60], [61, 66], [67, 71], [72, 74], [75, 85], [86, 93], [94, 101], [102, 103], [104, 109], [110, 113], [114, 137], [138, 141], [142, 151], [152, 153], [154, 161], [162, 168], [168, 169], [170, 177], [178, 181], [182, 186], [187, 189], [190, 197], [198, 203], [204, 205], [205, 207], [208, 210], [211, 215], [216, 220], [221, 229], [230, 233], [234, 241], [242, 249], [250, 255], [255, 256], [256, 257], [258, 262], [263, 266], [267, 271], [272, 277], [278, 284], [285, 288], [289, 296], [297, 307], [308, 314], [314, 315]]}
{"doc_key": "ai-train-90", "ner": [[0, 0, "organisation"], [7, 8, "researcher"], [4, 5, "organisation"], [19, 23, "product"], [33, 33, "researcher"], [35, 36, "programlang"]], "ner_mapping_to_source": [0, 1, 2, 4, 5, 6], "relations": [[0, 0, 4, 5, "related-to", "purchases", false, false], [7, 8, 33, 33, "named", "same", false, false], [4, 5, 7, 8, "origin", "founded_by", false, false], [19, 23, 0, 0, "artifact", "", false, false], [35, 36, 33, 33, "artifact", "", true, false]], "relations_mapping_to_source": [0, 2, 3, 4, 5], "sentence": ["Unimation", "kaufte", "1977", "die", "Vicarm", "Inc.", "von", "Victor", "Scheinman", ".", "Mit", "Scheinmans", "Hilfe", "entwickelte", "und", "produzierte", "das", "Unternehmen", "die", "Programmable", "Universal", "Machine", "for", "Assembly", ",", "ein", "neues", "Modell", "eines", "Roboterarms", ",", "und", "verwendete", "Scheinmans", "innovative", "Programmiersprache", "VAL", "."], "sentence-detokenized": "Unimation kaufte 1977 die Vicarm Inc. von Victor Scheinman. Mit Scheinmans Hilfe entwickelte und produzierte das Unternehmen die Programmable Universal Machine for Assembly, ein neues Modell eines Roboterarms, und verwendete Scheinmans innovative Programmiersprache VAL.", "token2charspan": [[0, 9], [10, 16], [17, 21], [22, 25], [26, 32], [33, 37], [38, 41], [42, 48], [49, 58], [58, 59], [60, 63], [64, 74], [75, 80], [81, 92], [93, 96], [97, 108], [109, 112], [113, 124], [125, 128], [129, 141], [142, 151], [152, 159], [160, 163], [164, 172], [172, 173], [174, 177], [178, 183], [184, 190], [191, 196], [197, 208], [208, 209], [210, 213], [214, 224], [225, 235], [236, 246], [247, 265], [266, 269], [269, 270]]}
{"doc_key": "ai-train-91", "ner": [[0, 0, "product"], [5, 5, "algorithm"], [7, 8, "product"]], "ner_mapping_to_source": [0, 2, 3], "relations": [[0, 0, 5, 5, "origin", "implementation_of", false, false], [0, 0, 7, 8, "part-of", "", false, false]], "relations_mapping_to_source": [1, 2], "sentence": ["J48", "ist", "eine", "Open-Source-Java-Implementierung", "des", "C4.5-Algorithmus", "im", "Data-Mining-Tool", "Weka", "."], "sentence-detokenized": "J48 ist eine Open-Source-Java-Implementierung des C4.5-Algorithmus im Data-Mining-Tool Weka.", "token2charspan": [[0, 3], [4, 7], [8, 12], [13, 45], [46, 49], [50, 66], [67, 69], [70, 86], [87, 91], [91, 92]]}
{"doc_key": "ai-train-92", "ner": [[8, 9, "product"], [21, 28, "misc"]], "ner_mapping_to_source": [1, 2], "relations": [], "relations_mapping_to_source": [], "sentence": ["Die", "SSIM-Ver\u00f6ffentlichung", "aus", "dem", "Jahr", "2004", "wurde", "laut", "Google", "Scholar", "mehr", "als", "20.000", "Mal", "zitiert", ".", "Au\u00dferdem", "erhielt", "sie", "2016", "den", "Sustained", "Impact", "Award", "der", "IEEE", "Signal", "Processing", "Society", ",", "der", "eine", "Arbeit", "auszeichnet", ",", "die", "mindestens", "10", "Jahre", "nach", "ihrer", "Ver\u00f6ffentlichung", "einen", "ungew\u00f6hnlich", "hohen", "Einfluss", "hat", "."], "sentence-detokenized": "Die SSIM-Ver\u00f6ffentlichung aus dem Jahr 2004 wurde laut Google Scholar mehr als 20.000 Mal zitiert. Au\u00dferdem erhielt sie 2016 den Sustained Impact Award der IEEE Signal Processing Society, der eine Arbeit auszeichnet, die mindestens 10 Jahre nach ihrer Ver\u00f6ffentlichung einen ungew\u00f6hnlich hohen Einfluss hat.", "token2charspan": [[0, 3], [4, 25], [26, 29], [30, 33], [34, 38], [39, 43], [44, 49], [50, 54], [55, 61], [62, 69], [70, 74], [75, 78], [79, 85], [86, 89], [90, 97], [97, 98], [99, 107], [108, 115], [116, 119], [120, 124], [125, 128], [129, 138], [139, 145], [146, 151], [152, 155], [156, 160], [161, 167], [168, 178], [179, 186], [186, 187], [188, 191], [192, 196], [197, 203], [204, 215], [215, 216], [217, 220], [221, 231], [232, 234], [235, 240], [241, 245], [246, 251], [252, 268], [269, 274], [275, 287], [288, 293], [294, 302], [303, 306], [306, 307]]}
{"doc_key": "ai-train-93", "ner": [[1, 2, "task"], [10, 11, "product"], [19, 21, "product"], [26, 26, "organisation"], [27, 27, "product"], [32, 32, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5], "relations": [[1, 2, 26, 26, "artifact", "", false, false], [10, 11, 1, 2, "related-to", "performs", false, false], [10, 11, 19, 21, "part-of", "", false, false], [26, 26, 32, 32, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Die", "Sprachsynthese", "ist", "mit", "der", "2016", "vorgestellten", "Sprachbearbeitungs-", "und", "-erzeugungssoftware", "Adobe", "Voco", ",", "einem", "Prototyp", ",", "der", "Teil", "der", "Adobe", "Creative", "Suite", "sein", "soll", ",", "und", "DeepMind", "WaveNet", ",", "einem", "Prototyp", "von", "Google", ",", "fast", "nicht", "mehr", "von", "einer", "echten", "menschlichen", "Stimme", "zu", "unterscheiden", "."], "sentence-detokenized": "Die Sprachsynthese ist mit der 2016 vorgestellten Sprachbearbeitungs- und -erzeugungssoftware Adobe Voco, einem Prototyp, der Teil der Adobe Creative Suite sein soll, und DeepMind WaveNet, einem Prototyp von Google, fast nicht mehr von einer echten menschlichen Stimme zu unterscheiden.", "token2charspan": [[0, 3], [4, 18], [19, 22], [23, 26], [27, 30], [31, 35], [36, 49], [50, 69], [70, 73], [74, 93], [94, 99], [100, 104], [104, 105], [106, 111], [112, 120], [120, 121], [122, 125], [126, 130], [131, 134], [135, 140], [141, 149], [150, 155], [156, 160], [161, 165], [165, 166], [167, 170], [171, 179], [180, 187], [187, 188], [189, 194], [195, 203], [204, 207], [208, 214], [214, 215], [216, 220], [221, 226], [227, 231], [232, 235], [236, 241], [242, 248], [249, 261], [262, 268], [269, 271], [272, 285], [285, 286]]}
{"doc_key": "ai-train-94", "ner": [[0, 0, "researcher"], [4, 6, "organisation"], [10, 15, "organisation"], [19, 19, "conference"], [23, 27, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[0, 0, 4, 6, "role", "", false, false], [0, 0, 10, 15, "role", "", false, false], [0, 0, 19, 19, "role", "", false, false], [0, 0, 23, 27, "role", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Poggio", "ist", "Ehrenmitglied", "des", "Neuroscience", "Research", "Program", ",", "Mitglied", "der", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "Gr\u00fcndungsmitglied", "des", "AAAI", "und", "Gr\u00fcndungsmitglied", "des", "McGovern", "Institute", "for", "Brain", "Research", "."], "sentence-detokenized": "Poggio ist Ehrenmitglied des Neuroscience Research Program, Mitglied der American Academy of Arts and Sciences, Gr\u00fcndungsmitglied des AAAI und Gr\u00fcndungsmitglied des McGovern Institute for Brain Research.", "token2charspan": [[0, 6], [7, 10], [11, 24], [25, 28], [29, 41], [42, 50], [51, 58], [58, 59], [60, 68], [69, 72], [73, 81], [82, 89], [90, 92], [93, 97], [98, 101], [102, 110], [110, 111], [112, 129], [130, 133], [134, 138], [139, 142], [143, 160], [161, 164], [165, 173], [174, 183], [184, 187], [188, 193], [194, 202], [202, 203]]}
{"doc_key": "ai-train-95", "ner": [[12, 12, "task"], [14, 14, "task"], [19, 19, "task"], [24, 24, "misc"], [25, 25, "misc"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[12, 12, 19, 19, "cause-effect", "", false, false], [14, 14, 19, 19, "cause-effect", "", false, false], [25, 25, 19, 19, "topic", "", false, false], [25, 25, 24, 24, "general-affiliation", "nationality", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["In", "den", "1990er", "Jahren", "begannen", ",", "ermutigt", "durch", "die", "Erfolge", "bei", "der", "Spracherkennung", "und", "Sprachsynthese", ",", "die", "Forschungen", "zur", "Sprach\u00fcbersetzung", "mit", "der", "Entwicklung", "des", "deutschen", "Verbmobil-Projekts", "."], "sentence-detokenized": "In den 1990er Jahren begannen, ermutigt durch die Erfolge bei der Spracherkennung und Sprachsynthese, die Forschungen zur Sprach\u00fcbersetzung mit der Entwicklung des deutschen Verbmobil-Projekts.", "token2charspan": [[0, 2], [3, 6], [7, 13], [14, 20], [21, 29], [29, 30], [31, 39], [40, 45], [46, 49], [50, 57], [58, 61], [62, 65], [66, 81], [82, 85], [86, 100], [100, 101], [102, 105], [106, 117], [118, 121], [122, 139], [140, 143], [144, 147], [148, 159], [160, 163], [164, 173], [174, 192], [192, 193]]}
{"doc_key": "ai-train-96", "ner": [[4, 5, "researcher"], [9, 10, "researcher"], [12, 13, "researcher"], [15, 15, "algorithm"], [18, 19, "algorithm"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[4, 5, 9, 10, "role", "", false, false], [15, 15, 4, 5, "origin", "", false, false], [15, 15, 9, 10, "origin", "", false, false], [15, 15, 12, 13, "origin", "", false, false], [18, 19, 15, 15, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 5], "sentence": ["Im", "Jahr", "1999", "f\u00fchrten", "Felix", "Gers", "und", "seine", "Berater", "J\u00fcrgen", "Schmidhuber", "und", "Fred", "Cummins", "das", "Vergessensgatter", "(", "auch", "Keep", "Gate", "genannt", ")", "in", "die", "LSTM-Architektur", "ein", ","], "sentence-detokenized": "Im Jahr 1999 f\u00fchrten Felix Gers und seine Berater J\u00fcrgen Schmidhuber und Fred Cummins das Vergessensgatter (auch Keep Gate genannt) in die LSTM-Architektur ein,", "token2charspan": [[0, 2], [3, 7], [8, 12], [13, 20], [21, 26], [27, 31], [32, 35], [36, 41], [42, 49], [50, 56], [57, 68], [69, 72], [73, 77], [78, 85], [86, 89], [90, 106], [107, 108], [108, 112], [113, 117], [118, 122], [123, 130], [130, 131], [132, 134], [135, 138], [139, 155], [156, 159], [159, 160]]}
{"doc_key": "ai-train-97", "ner": [[2, 3, "field"], [6, 7, "field"], [9, 10, "algorithm"]], "ner_mapping_to_source": [0, 1, 2], "relations": [[9, 10, 2, 3, "part-of", "", false, false], [9, 10, 6, 7, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1], "sentence": ["In", "der", "digitalen", "Signalverarbeitung", "und", "der", "Informationstheorie", "wird", "die", "normalisierte", "Sinusfunktion", "\u00fcblicherweise", "wie", "folgt", "definiert"], "sentence-detokenized": "In der digitalen Signalverarbeitung und der Informationstheorie wird die normalisierte Sinusfunktion \u00fcblicherweise wie folgt definiert", "token2charspan": [[0, 2], [3, 6], [7, 16], [17, 35], [36, 39], [40, 43], [44, 63], [64, 68], [69, 72], [73, 86], [87, 100], [101, 114], [115, 118], [119, 124], [125, 134]]}
{"doc_key": "ai-train-98", "ner": [[2, 2, "field"], [7, 8, "researcher"], [15, 18, "conference"], [22, 26, "organisation"], [28, 28, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4], "relations": [[2, 2, 7, 8, "origin", "coined_term", false, false], [7, 8, 15, 18, "role", "", false, false], [7, 8, 22, 26, "role", "", false, false], [28, 28, 22, 26, "named", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3], "sentence": ["Der", "Begriff", "Computerlinguistik", "selbst", "wurde", "erstmals", "von", "David", "Hays", "gepr\u00e4gt", ",", "einem", "Gr\u00fcndungsmitglied", "sowohl", "der", "Association", "for", "Computational", "Linguistics", "als", "auch", "des", "International", "Committee", "on", "Computational", "Linguistics", "(", "ICCL", ")", "."], "sentence-detokenized": "Der Begriff Computerlinguistik selbst wurde erstmals von David Hays gepr\u00e4gt, einem Gr\u00fcndungsmitglied sowohl der Association for Computational Linguistics als auch des International Committee on Computational Linguistics (ICCL).", "token2charspan": [[0, 3], [4, 11], [12, 30], [31, 37], [38, 43], [44, 52], [53, 56], [57, 62], [63, 67], [68, 75], [75, 76], [77, 82], [83, 100], [101, 107], [108, 111], [112, 123], [124, 127], [128, 141], [142, 153], [154, 157], [158, 162], [163, 166], [167, 180], [181, 190], [191, 193], [194, 207], [208, 219], [220, 221], [221, 225], [225, 226], [226, 227]]}
{"doc_key": "ai-train-99", "ner": [[9, 11, "misc"], [16, 16, "misc"], [53, 55, "metrics"], [57, 59, "metrics"]], "ner_mapping_to_source": [0, 1, 2, 3], "relations": [[57, 59, 53, 55, "named", "", false, false]], "relations_mapping_to_source": [0], "sentence": ["59", ",", "pp.", "2547-2553", ",", "Oct", ".", "2011", "In", "eindimensionalem", "polynomial-basiertem", "Speicher", "(", "oder", "speicherlosem", ")", "DPD", "muss", "die", "verzerrte", "Ausgabe", "des", "nichtlinearen", "Systems", "mit", "einer", "Rate", "\u00fcberabgetastet", "werden", ",", "die", "die", "Erfassung", "der", "nichtlinearen", "Produkte", "der", "Ordnung", "des", "digitalen", "Vorverzerrers", "erm\u00f6glicht", ",", "um", "die", "Koeffizienten", "der", "digitalen", "Vorverzerrer-Polynome", "zu", "l\u00f6sen", "und", "den", "mittleren", "quadratischen", "Fehler", "(", "MSE", ")", "zu", "minimieren", "."], "sentence-detokenized": "59, pp. 2547-2553, Oct. 2011 In eindimensionalem polynomial-basiertem Speicher (oder speicherlosem) DPD muss die verzerrte Ausgabe des nichtlinearen Systems mit einer Rate \u00fcberabgetastet werden, die die Erfassung der nichtlinearen Produkte der Ordnung des digitalen Vorverzerrers erm\u00f6glicht, um die Koeffizienten der digitalen Vorverzerrer-Polynome zu l\u00f6sen und den mittleren quadratischen Fehler (MSE) zu minimieren.", "token2charspan": [[0, 2], [2, 3], [4, 7], [8, 17], [17, 18], [19, 22], [22, 23], [24, 28], [29, 31], [32, 48], [49, 69], [70, 78], [79, 80], [80, 84], [85, 98], [98, 99], [100, 103], [104, 108], [109, 112], [113, 122], [123, 130], [131, 134], [135, 148], [149, 156], [157, 160], [161, 166], [167, 171], [172, 186], [187, 193], [193, 194], [195, 198], [199, 202], [203, 212], [213, 216], [217, 230], [231, 239], [240, 243], [244, 251], [252, 255], [256, 265], [266, 279], [280, 290], [290, 291], [292, 294], [295, 298], [299, 312], [313, 316], [317, 326], [327, 348], [349, 351], [352, 357], [358, 361], [362, 365], [366, 375], [376, 389], [390, 396], [397, 398], [398, 401], [401, 402], [403, 405], [406, 416], [416, 417]]}
{"doc_key": "ai-train-100", "ner": [[0, 1, "researcher"], [10, 10, "location"], [12, 13, "location"], [15, 15, "country"], [19, 19, "location"], [21, 21, "country"], [33, 39, "organisation"], [41, 44, "organisation"], [46, 46, "location"], [50, 51, "organisation"]], "ner_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "relations": [[0, 1, 10, 10, "physical", "", false, false], [0, 1, 41, 44, "physical", "", false, false], [0, 1, 50, 51, "role", "", false, false], [10, 10, 12, 13, "physical", "", false, false], [12, 13, 15, 15, "physical", "", false, false], [33, 39, 41, 44, "part-of", "", false, false], [41, 44, 46, 46, "physical", "", false, false], [50, 51, 33, 39, "part-of", "", false, false]], "relations_mapping_to_source": [0, 1, 2, 3, 4, 5, 6, 7], "sentence": ["Boris", "Katz", ",", "(", "geboren", "am", "5.", "Oktober", "1947", "in", "Chi\u0219in\u0103u", ",", "Moldawische", "SSR", ",", "Sowjetunion", ",", "(", "heute", "Chi\u0219in\u0103u", ",", "Moldawien", ")", ")", "ist", "ein", "leitender", "amerikanischer", "Forscher", "(", "Informatiker", ")", "am", "MIT", "Computer", "Science", "and", "Artificial", "Intelligence", "Laboratory", "am", "Massachusetts", "Institute", "of", "Technology", "in", "Cambridge", "und", "Leiter", "der", "InfoLab", "Group", "des", "Labors", "."], "sentence-detokenized": "Boris Katz, (geboren am 5. Oktober 1947 in Chi\u0219in\u0103u, Moldawische SSR, Sowjetunion, (heute Chi\u0219in\u0103u, Moldawien)) ist ein leitender amerikanischer Forscher (Informatiker) am MIT Computer Science and Artificial Intelligence Laboratory am Massachusetts Institute of Technology in Cambridge und Leiter der InfoLab Group des Labors.", "token2charspan": [[0, 5], [6, 10], [10, 11], [12, 13], [13, 20], [21, 23], [24, 26], [27, 34], [35, 39], [40, 42], [43, 51], [51, 52], [53, 64], [65, 68], [68, 69], [70, 81], [81, 82], [83, 84], [84, 89], [90, 98], [98, 99], [100, 109], [109, 110], [110, 111], [112, 115], [116, 119], [120, 129], [130, 144], [145, 153], [154, 155], [155, 167], [167, 168], [169, 171], [172, 175], [176, 184], [185, 192], [193, 196], [197, 207], [208, 220], [221, 231], [232, 234], [235, 248], [249, 258], [259, 261], [262, 272], [273, 275], [276, 285], [286, 289], [290, 296], [297, 300], [301, 308], [309, 314], [315, 318], [319, 325], [325, 326]]}
